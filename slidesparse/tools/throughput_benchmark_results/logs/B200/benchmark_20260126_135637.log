======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-26 13:56:37
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model llama3.2-1b-fp8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768,65536

命令行参数:
  --model: llama3.2-1b-fp8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: prefill
  --M: 512,1024,2048,4096,8192,16384,32768,65536
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: B200
  Compute Capability: cc100
  VRAM: 178.4 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  Llama3.2-1B-FP8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:56:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=539900) WARNING 01-26 13:56:56 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=539900) WARNING 01-26 13:57:03 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 48.11 requests/s, 24678.83 total tokens/s, 48.11 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 13:56:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:56:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:56:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:56:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:56:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:56:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:56:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:56:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:56:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:56:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:56:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:56:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:56:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:56:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:56:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:56:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:56:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:56:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:56:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:56:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:56:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:56:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:56:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:56:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:56:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:56:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:56:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:56:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=539900) [2026-01-26 13:56:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=539900) [2026-01-26 13:56:51] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=539900) [2026-01-26 13:56:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=539900) [2026-01-26 13:56:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=539900) [2026-01-26 13:56:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=539900) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=539900) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.58it/s]
(EngineCore_DP0 pid=539900) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.57it/s]
(EngineCore_DP0 pid=539900) 
(EngineCore_DP0 pid=539900) 2026-01-26 13:57:03,192 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=539900) 2026-01-26 13:57:03,213 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=539900) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 18.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 18.41it/s]
(EngineCore_DP0 pid=539900) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 18.94it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 626.32it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 678.28it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:01, 110.20it/s, est. speed input: 56424.89 toks/s, output: 110.20 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 63.98it/s, est. speed input: 35050.18 toks/s, output: 68.46 toks/s]  
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 57.70it/s, est. speed input: 31940.90 toks/s, output: 62.38 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 54.57it/s, est. speed input: 30427.80 toks/s, output: 59.43 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 52.87it/s, est. speed input: 29586.28 toks/s, output: 57.79 toks/s]
Processed prompts:  41%|████      | 52/128 [00:00<00:01, 51.67it/s, est. speed input: 28973.57 toks/s, output: 56.59 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 50.90it/s, est. speed input: 28520.18 toks/s, output: 55.70 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 50.30it/s, est. speed input: 28153.25 toks/s, output: 54.99 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 49.91it/s, est. speed input: 27860.68 toks/s, output: 54.42 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 49.65it/s, est. speed input: 27621.51 toks/s, output: 53.95 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 49.51it/s, est. speed input: 27454.74 toks/s, output: 53.62 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:00, 49.37it/s, est. speed input: 27305.27 toks/s, output: 53.33 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 49.29it/s, est. speed input: 27175.38 toks/s, output: 53.08 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 48.90it/s, est. speed input: 27025.55 toks/s, output: 52.78 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:01<00:00, 48.90it/s, est. speed input: 26919.34 toks/s, output: 52.58 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 49.06it/s, est. speed input: 26839.00 toks/s, output: 52.42 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 49.07it/s, est. speed input: 26757.63 toks/s, output: 52.26 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 49.03it/s, est. speed input: 26679.33 toks/s, output: 52.11 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 49.00it/s, est. speed input: 26608.38 toks/s, output: 51.97 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 49.02it/s, est. speed input: 26545.56 toks/s, output: 51.85 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 49.02it/s, est. speed input: 26523.99 toks/s, output: 51.80 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 51.80it/s, est. speed input: 26523.99 toks/s, output: 51.80 toks/s]
[rank0]:[W126 13:57:07.057540707 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 32.8s

测试结果:
  Requests/s:   48.11
  Tokens/s:     24678.83
  Total Reqs:   128
  Elapsed:      2.66s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     24630.72

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:57:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=540913) WARNING 01-26 13:57:29 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=540913) WARNING 01-26 13:57:36 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 47.09 requests/s, 48266.32 total tokens/s, 47.09 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 13:57:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:57:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:57:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:57:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:57:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:57:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:57:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:57:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:57:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:57:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:57:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:57:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:57:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:57:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:57:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:57:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:57:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:57:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=540913) [2026-01-26 13:57:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=540913) [2026-01-26 13:57:25] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=540913) [2026-01-26 13:57:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=540913) [2026-01-26 13:57:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=540913) [2026-01-26 13:57:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=540913) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=540913) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.49it/s]
(EngineCore_DP0 pid=540913) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.48it/s]
(EngineCore_DP0 pid=540913) 
(EngineCore_DP0 pid=540913) 2026-01-26 13:57:36,299 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=540913) 2026-01-26 13:57:36,320 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=540913) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.06it/s]
(EngineCore_DP0 pid=540913) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.15it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|███       | 39/128 [00:00<00:00, 385.96it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 426.59it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 428.56it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:00, 157.25it/s, est. speed input: 161034.04 toks/s, output: 157.25 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 68.23it/s, est. speed input: 76572.28 toks/s, output: 74.78 toks/s]   
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 59.64it/s, est. speed input: 67720.17 toks/s, output: 66.13 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:01, 55.90it/s, est. speed input: 63960.91 toks/s, output: 62.46 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:00<00:01, 53.77it/s, est. speed input: 61809.19 toks/s, output: 60.36 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 52.33it/s, est. speed input: 60389.71 toks/s, output: 58.97 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 51.21it/s, est. speed input: 59252.49 toks/s, output: 57.86 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 50.44it/s, est. speed input: 58357.47 toks/s, output: 56.99 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:00, 49.92it/s, est. speed input: 57631.96 toks/s, output: 56.28 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 49.18it/s, est. speed input: 56905.32 toks/s, output: 55.57 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:01<00:00, 49.02it/s, est. speed input: 56464.65 toks/s, output: 55.14 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:01<00:00, 48.85it/s, est. speed input: 56064.58 toks/s, output: 54.75 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:01<00:00, 48.72it/s, est. speed input: 55708.20 toks/s, output: 54.40 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 48.04it/s, est. speed input: 55267.78 toks/s, output: 53.97 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 48.01it/s, est. speed input: 54961.75 toks/s, output: 53.67 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 47.97it/s, est. speed input: 54679.70 toks/s, output: 53.40 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 48.00it/s, est. speed input: 54434.91 toks/s, output: 53.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 47.94it/s, est. speed input: 54196.85 toks/s, output: 52.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 47.94it/s, est. speed input: 54196.85 toks/s, output: 52.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 52.92it/s, est. speed input: 54196.85 toks/s, output: 52.93 toks/s]
[rank0]:[W126 13:57:41.233794330 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 32.9s

测试结果:
  Requests/s:   47.09
  Tokens/s:     48266.32
  Total Reqs:   128
  Elapsed:      2.72s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     48219.23

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:57:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=541940) WARNING 01-26 13:58:03 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=541940) WARNING 01-26 13:58:10 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 84.47 requests/s, 86583.79 total tokens/s, 84.47 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 13:57:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:57:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:57:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:57:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:57:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:57:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:57:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:57:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:57:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:57:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:57:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:57:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:57:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:57:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:57:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:57:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:57:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:57:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:57:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=541940) [2026-01-26 13:57:58] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=541940) [2026-01-26 13:57:58] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=541940) [2026-01-26 13:57:58] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=541940) [2026-01-26 13:57:58] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=541940) [2026-01-26 13:57:58] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=541940) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=541940) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.79it/s]
(EngineCore_DP0 pid=541940) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.79it/s]
(EngineCore_DP0 pid=541940) 
(EngineCore_DP0 pid=541940) 2026-01-26 13:58:10,179 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=541940) 2026-01-26 13:58:10,200 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=541940) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 17.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 17.80it/s]
(EngineCore_DP0 pid=541940) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 21.32it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<01:00,  4.18it/s]
Adding requests:  15%|█▍        | 38/256 [00:00<00:01, 139.86it/s]
Adding requests:  31%|███▏      | 80/256 [00:00<00:00, 236.84it/s]
Adding requests:  48%|████▊     | 124/256 [00:00<00:00, 303.45it/s]
Adding requests:  65%|██████▌   | 167/256 [00:00<00:00, 344.00it/s]
Adding requests:  82%|████████▏ | 211/256 [00:00<00:00, 371.68it/s]
Adding requests:  99%|█████████▉| 254/256 [00:00<00:00, 386.68it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 300.42it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:00<00:00, 544.90it/s, est. speed input: 558014.11 toks/s, output: 544.91 toks/s]
Processed prompts:  44%|████▍     | 113/256 [00:00<00:00, 149.65it/s, est. speed input: 172515.32 toks/s, output: 168.47 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:00<00:00, 126.17it/s, est. speed input: 147801.80 toks/s, output: 144.34 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:01<00:00, 117.50it/s, est. speed input: 139016.09 toks/s, output: 135.76 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:01<00:00, 112.17it/s, est. speed input: 133966.73 toks/s, output: 130.83 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:01<00:00, 108.34it/s, est. speed input: 130479.17 toks/s, output: 127.42 toks/s]
Processed prompts:  80%|████████  | 205/256 [00:01<00:00, 107.40it/s, est. speed input: 128650.27 toks/s, output: 125.63 toks/s]
Processed prompts:  85%|████████▍ | 217/256 [00:01<00:00, 104.36it/s, est. speed input: 126385.69 toks/s, output: 123.42 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:01<00:00, 100.07it/s, est. speed input: 123940.08 toks/s, output: 121.03 toks/s]
Processed prompts:  93%|█████████▎| 239/256 [00:01<00:00, 101.15it/s, est. speed input: 123054.59 toks/s, output: 120.17 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:02<00:00, 97.27it/s, est. speed input: 121072.22 toks/s, output: 118.23 toks/s] 
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 97.27it/s, est. speed input: 120411.01 toks/s, output: 117.59 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 117.58it/s, est. speed input: 120411.01 toks/s, output: 117.59 toks/s]
[rank0]:[W126 13:58:15.572372808 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.3s

测试结果:
  Requests/s:   84.47
  Tokens/s:     86583.79
  Total Reqs:   256
  Elapsed:      3.03s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     86499.32

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:58:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=542982) WARNING 01-26 13:58:38 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=542982) WARNING 01-26 13:58:45 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 178.15 requests/s, 182606.48 total tokens/s, 178.15 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 13:58:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:58:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:58:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:58:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:58:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:58:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:58:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:58:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:58:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:58:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:58:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:58:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:58:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:58:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:58:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:58:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:58:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:58:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:58:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:58:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:58:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:58:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:58:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:58:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:58:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:58:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:58:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:58:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=542982) [2026-01-26 13:58:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=542982) [2026-01-26 13:58:34] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=542982) [2026-01-26 13:58:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=542982) [2026-01-26 13:58:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=542982) [2026-01-26 13:58:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=542982) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=542982) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.51it/s]
(EngineCore_DP0 pid=542982) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.51it/s]
(EngineCore_DP0 pid=542982) 
(EngineCore_DP0 pid=542982) 2026-01-26 13:58:45,247 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=542982) 2026-01-26 13:58:45,268 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=542982) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 21.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 18.85it/s]
(EngineCore_DP0 pid=542982) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 21.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 21.28it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 39/512 [00:00<00:01, 382.70it/s]
Adding requests:  16%|█▋        | 84/512 [00:00<00:01, 417.24it/s]
Adding requests:  25%|██▌       | 130/512 [00:00<00:00, 434.80it/s]
Adding requests:  34%|███▍      | 175/512 [00:00<00:00, 436.50it/s]
Adding requests:  43%|████▎     | 222/512 [00:00<00:00, 446.91it/s]
Adding requests:  52%|█████▏    | 268/512 [00:00<00:00, 448.73it/s]
Adding requests:  61%|██████▏   | 314/512 [00:00<00:00, 449.82it/s]
Adding requests:  70%|███████   | 360/512 [00:00<00:00, 452.46it/s]
Adding requests:  80%|███████▉  | 408/512 [00:00<00:00, 459.05it/s]
Adding requests:  89%|████████▉ | 455/512 [00:01<00:00, 460.85it/s]
Adding requests:  98%|█████████▊| 502/512 [00:01<00:00, 461.10it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 449.99it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:00<00:00, 1819.37it/s, est. speed input: 1863170.31 toks/s, output: 1819.40 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [00:01<00:00, 314.05it/s, est. speed input: 369891.09 toks/s, output: 361.22 toks/s]   
Processed prompts:  92%|█████████▏| 471/512 [00:01<00:00, 265.00it/s, est. speed input: 316307.43 toks/s, output: 308.89 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 265.00it/s, est. speed input: 302199.08 toks/s, output: 295.12 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 295.10it/s, est. speed input: 302199.08 toks/s, output: 295.12 toks/s]
[rank0]:[W126 13:58:50.866086940 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.3s

测试结果:
  Requests/s:   178.15
  Tokens/s:     182606.48
  Total Reqs:   512
  Elapsed:      2.87s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     182428.33

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:59:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=544070) WARNING 01-26 13:59:16 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=544070) WARNING 01-26 13:59:23 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 333.59 requests/s, 341925.82 total tokens/s, 333.59 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 13:59:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:59:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:59:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:59:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:59:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:59:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:59:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:59:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:59:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:59:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:59:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:59:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:59:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:59:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:59:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:59:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:59:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:59:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=544070) [2026-01-26 13:59:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=544070) [2026-01-26 13:59:11] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=544070) [2026-01-26 13:59:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=544070) [2026-01-26 13:59:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=544070) [2026-01-26 13:59:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=544070) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=544070) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.52it/s]
(EngineCore_DP0 pid=544070) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.51it/s]
(EngineCore_DP0 pid=544070) 
(EngineCore_DP0 pid=544070) 2026-01-26 13:59:23,113 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=544070) 2026-01-26 13:59:23,135 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=544070) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 21.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 19.16it/s]
(EngineCore_DP0 pid=544070) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 21.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 21.20it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 32/1024 [00:00<00:03, 318.20it/s]
Adding requests:   7%|▋         | 75/1024 [00:00<00:02, 382.16it/s]
Adding requests:  12%|█▏        | 120/1024 [00:00<00:02, 411.14it/s]
Adding requests:  16%|█▌        | 165/1024 [00:00<00:02, 424.75it/s]
Adding requests:  20%|██        | 209/1024 [00:00<00:01, 430.09it/s]
Adding requests:  25%|██▌       | 257/1024 [00:00<00:01, 446.28it/s]
Adding requests:  29%|██▉       | 302/1024 [00:00<00:01, 444.76it/s]
Adding requests:  34%|███▍      | 348/1024 [00:00<00:01, 447.70it/s]
Adding requests:  39%|███▊      | 395/1024 [00:00<00:01, 452.32it/s]
Adding requests:  43%|████▎     | 441/1024 [00:01<00:01, 453.70it/s]
Adding requests:  48%|████▊     | 488/1024 [00:01<00:01, 458.20it/s]
Adding requests:  52%|█████▏    | 534/1024 [00:01<00:01, 448.68it/s]
Adding requests:  57%|█████▋    | 582/1024 [00:01<00:00, 457.24it/s]
Adding requests:  62%|██████▏   | 630/1024 [00:01<00:00, 462.30it/s]
Adding requests:  66%|██████▌   | 677/1024 [00:01<00:00, 464.45it/s]
Adding requests:  71%|███████   | 726/1024 [00:01<00:00, 469.94it/s]
Adding requests:  76%|███████▌  | 774/1024 [00:01<00:00, 465.82it/s]
Adding requests:  80%|████████  | 821/1024 [00:01<00:00, 457.72it/s]
Adding requests:  85%|████████▍ | 868/1024 [00:01<00:00, 461.21it/s]
Adding requests:  89%|████████▉ | 916/1024 [00:02<00:00, 466.09it/s]
Adding requests:  94%|█████████▍| 964/1024 [00:02<00:00, 468.43it/s]
Adding requests:  99%|█████████▉| 1012/1024 [00:02<00:00, 469.59it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 452.86it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:00<00:00, 6434.32it/s, est. speed input: 6589110.54 toks/s, output: 6434.42 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:00<00:00, 6434.32it/s, est. speed input: 1299099.03 toks/s, output: 1268.64 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:00<00:00, 1268.46it/s, est. speed input: 1299099.03 toks/s, output: 1268.64 toks/s]
[rank0]:[W126 13:59:28.969814403 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.1s

测试结果:
  Requests/s:   333.59
  Tokens/s:     341925.82
  Total Reqs:   1024
  Elapsed:      3.07s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     341592.23

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:59:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=545214) WARNING 01-26 13:59:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=545214) WARNING 01-26 14:00:05 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 456.59 requests/s, 468004.33 total tokens/s, 456.59 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 13:59:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:59:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:59:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:59:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:59:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:59:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:59:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:59:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:59:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:59:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:59:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:59:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:59:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:59:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:59:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:59:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:59:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:59:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:59:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=545214) [2026-01-26 13:59:54] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=545214) [2026-01-26 13:59:54] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=545214) [2026-01-26 13:59:54] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=545214) [2026-01-26 13:59:54] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=545214) [2026-01-26 13:59:54] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=545214) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=545214) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.65it/s]
(EngineCore_DP0 pid=545214) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.65it/s]
(EngineCore_DP0 pid=545214) 
(EngineCore_DP0 pid=545214) 2026-01-26 14:00:05,831 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=545214) 2026-01-26 14:00:05,854 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=545214) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 16.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 19.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 18.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 18.59it/s]
(EngineCore_DP0 pid=545214) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 20.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 21.14it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 381.20it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 419.23it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 430.60it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:04, 434.96it/s]
Adding requests:  11%|█         | 220/2048 [00:00<00:04, 443.61it/s]
Adding requests:  13%|█▎        | 268/2048 [00:00<00:03, 453.81it/s]
Adding requests:  15%|█▌        | 314/2048 [00:00<00:03, 452.28it/s]
Adding requests:  18%|█▊        | 360/2048 [00:00<00:03, 453.80it/s]
Adding requests:  20%|█▉        | 408/2048 [00:00<00:03, 459.17it/s]
Adding requests:  22%|██▏       | 455/2048 [00:01<00:03, 461.64it/s]
Adding requests:  25%|██▍       | 502/2048 [00:01<00:03, 460.30it/s]
Adding requests:  27%|██▋       | 549/2048 [00:01<00:03, 455.32it/s]
Adding requests:  29%|██▉       | 596/2048 [00:01<00:03, 457.10it/s]
Adding requests:  31%|███▏      | 643/2048 [00:01<00:03, 460.41it/s]
Adding requests:  34%|███▍      | 692/2048 [00:01<00:02, 467.98it/s]
Adding requests:  36%|███▌      | 740/2048 [00:01<00:02, 469.04it/s]
Adding requests:  38%|███▊      | 787/2048 [00:01<00:02, 467.70it/s]
Adding requests:  41%|████      | 834/2048 [00:01<00:02, 456.01it/s]
Adding requests:  43%|████▎     | 881/2048 [00:01<00:02, 459.88it/s]
Adding requests:  45%|████▌     | 929/2048 [00:02<00:02, 465.49it/s]
Adding requests:  48%|████▊     | 977/2048 [00:02<00:02, 468.74it/s]
Adding requests:  50%|█████     | 1025/2048 [00:02<00:02, 470.62it/s]
Adding requests:  52%|█████▏    | 1073/2048 [00:02<00:02, 465.88it/s]
Adding requests:  55%|█████▍    | 1120/2048 [00:02<00:02, 463.48it/s]
Adding requests:  57%|█████▋    | 1169/2048 [00:02<00:01, 470.66it/s]
Adding requests:  60%|█████▉    | 1220/2048 [00:02<00:01, 479.69it/s]
Adding requests:  62%|██████▏   | 1268/2048 [00:02<00:01, 472.21it/s]
Adding requests:  64%|██████▍   | 1316/2048 [00:02<00:01, 466.03it/s]
Adding requests:  67%|██████▋   | 1364/2048 [00:02<00:01, 468.10it/s]
Adding requests:  69%|██████▉   | 1412/2048 [00:03<00:01, 471.45it/s]
Adding requests:  71%|███████▏  | 1460/2048 [00:03<00:01, 467.95it/s]
Adding requests:  74%|███████▎  | 1508/2048 [00:03<00:01, 470.27it/s]
Adding requests:  76%|███████▌  | 1556/2048 [00:03<00:01, 467.77it/s]
Adding requests:  78%|███████▊  | 1603/2048 [00:03<00:01, 439.84it/s]
Adding requests:  81%|████████  | 1650/2048 [00:03<00:00, 445.75it/s]
Adding requests:  83%|████████▎ | 1696/2048 [00:03<00:00, 448.35it/s]
Adding requests:  85%|████████▌ | 1744/2048 [00:03<00:00, 454.05it/s]
Adding requests:  87%|████████▋ | 1790/2048 [00:03<00:00, 455.19it/s]
Adding requests:  90%|████████▉ | 1838/2048 [00:04<00:00, 460.04it/s]
Adding requests:  92%|█████████▏| 1885/2048 [00:04<00:00, 462.74it/s]
Adding requests:  94%|█████████▍| 1932/2048 [00:04<00:00, 463.80it/s]
Adding requests:  97%|█████████▋| 1979/2048 [00:04<00:00, 465.36it/s]
Adding requests:  99%|█████████▉| 2028/2048 [00:04<00:00, 470.82it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 460.43it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 57169.04it/s, est. speed input: 58565266.17 toks/s, output: 57185.03 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 57007.04it/s, est. speed input: 58565266.17 toks/s, output: 57185.03 toks/s]
[rank0]:[W126 14:00:12.023381888 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.1s

测试结果:
  Requests/s:   456.59
  Tokens/s:     468004.33
  Total Reqs:   2048
  Elapsed:      4.49s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     467547.74

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:00:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=546504) WARNING 01-26 14:00:52 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=546504) WARNING 01-26 14:00:59 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 460.64 requests/s, 472156.75 total tokens/s, 460.64 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 14:00:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:00:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:00:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:00:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:00:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:00:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:00:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:00:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:00:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:00:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:00:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:00:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:00:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:00:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:00:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:00:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:00:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:00:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:00:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:00:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:00:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:00:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:00:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:00:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:00:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:00:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:00:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:00:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=546504) [2026-01-26 14:00:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=546504) [2026-01-26 14:00:48] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=546504) [2026-01-26 14:00:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=546504) [2026-01-26 14:00:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=546504) [2026-01-26 14:00:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=546504) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=546504) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=546504) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.63it/s]
(EngineCore_DP0 pid=546504) 
(EngineCore_DP0 pid=546504) [rank0]:W0126 14:00:55.975000 546504 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=546504) [rank0]:W0126 14:00:56.044000 546504 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=546504) [rank0]:W0126 14:00:56.954000 546504 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=546504) [rank0]:W0126 14:00:57.051000 546504 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=546504) 2026-01-26 14:00:59,802 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=546504) 2026-01-26 14:00:59,851 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=546504) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 19.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 21.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 21.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 20.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 20.53it/s]
(EngineCore_DP0 pid=546504) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 19.54it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00, 18.63it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 19.15it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 38/4096 [00:00<00:10, 379.41it/s]
Adding requests:   2%|▏         | 82/4096 [00:00<00:09, 413.83it/s]
Adding requests:   3%|▎         | 127/4096 [00:00<00:09, 430.22it/s]
Adding requests:   4%|▍         | 172/4096 [00:00<00:09, 434.70it/s]
Adding requests:   5%|▌         | 217/4096 [00:00<00:08, 439.17it/s]
Adding requests:   6%|▋         | 265/4096 [00:00<00:08, 451.71it/s]
Adding requests:   8%|▊         | 311/4096 [00:00<00:08, 447.03it/s]
Adding requests:   9%|▊         | 358/4096 [00:00<00:08, 450.17it/s]
Adding requests:  10%|▉         | 405/4096 [00:00<00:08, 456.02it/s]
Adding requests:  11%|█         | 452/4096 [00:01<00:07, 457.74it/s]
Adding requests:  12%|█▏        | 499/4096 [00:01<00:07, 459.51it/s]
Adding requests:  13%|█▎        | 545/4096 [00:01<00:07, 451.28it/s]
Adding requests:  15%|█▍        | 594/4096 [00:01<00:07, 461.26it/s]
Adding requests:  16%|█▌        | 641/4096 [00:01<00:07, 462.65it/s]
Adding requests:  17%|█▋        | 690/4096 [00:01<00:07, 468.62it/s]
Adding requests:  18%|█▊        | 739/4096 [00:01<00:07, 471.95it/s]
Adding requests:  19%|█▉        | 787/4096 [00:01<00:07, 460.50it/s]
Adding requests:  20%|██        | 834/4096 [00:01<00:07, 451.17it/s]
Adding requests:  22%|██▏       | 882/4096 [00:01<00:07, 457.55it/s]
Adding requests:  23%|██▎       | 930/4096 [00:02<00:06, 463.80it/s]
Adding requests:  24%|██▍       | 978/4096 [00:02<00:06, 464.83it/s]
Adding requests:  25%|██▌       | 1026/4096 [00:02<00:06, 468.97it/s]
Adding requests:  26%|██▌       | 1073/4096 [00:02<00:06, 463.84it/s]
Adding requests:  27%|██▋       | 1120/4096 [00:02<00:06, 461.99it/s]
Adding requests:  29%|██▊       | 1169/4096 [00:02<00:06, 468.50it/s]
Adding requests:  30%|██▉       | 1217/4096 [00:02<00:06, 469.49it/s]
Adding requests:  31%|███       | 1264/4096 [00:02<00:06, 464.56it/s]
Adding requests:  32%|███▏      | 1312/4096 [00:02<00:05, 465.97it/s]
Adding requests:  33%|███▎      | 1360/4096 [00:02<00:05, 469.33it/s]
Adding requests:  34%|███▍      | 1409/4096 [00:03<00:05, 475.29it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:03<00:05, 474.01it/s]
Adding requests:  37%|███▋      | 1506/4096 [00:03<00:05, 476.31it/s]
Adding requests:  38%|███▊      | 1554/4096 [00:03<00:05, 475.43it/s]
Adding requests:  39%|███▉      | 1604/4096 [00:03<00:05, 482.66it/s]
Adding requests:  40%|████      | 1653/4096 [00:03<00:05, 478.79it/s]
Adding requests:  42%|████▏     | 1701/4096 [00:03<00:05, 474.68it/s]
Adding requests:  43%|████▎     | 1749/4096 [00:03<00:04, 474.67it/s]
Adding requests:  44%|████▍     | 1797/4096 [00:03<00:04, 475.62it/s]
Adding requests:  45%|████▌     | 1845/4096 [00:03<00:04, 467.83it/s]
Adding requests:  46%|████▌     | 1892/4096 [00:04<00:04, 467.25it/s]
Adding requests:  47%|████▋     | 1940/4096 [00:04<00:04, 467.96it/s]
Adding requests:  49%|████▊     | 1987/4096 [00:04<00:04, 468.38it/s]
Adding requests:  50%|████▉     | 2036/4096 [00:04<00:04, 472.74it/s]
Adding requests:  51%|█████     | 2085/4096 [00:04<00:04, 474.81it/s]
Adding requests:  52%|█████▏    | 2133/4096 [00:04<00:04, 471.65it/s]
Adding requests:  53%|█████▎    | 2181/4096 [00:04<00:04, 463.96it/s]
Adding requests:  54%|█████▍    | 2230/4096 [00:04<00:03, 468.77it/s]
Adding requests:  56%|█████▌    | 2278/4096 [00:04<00:03, 470.27it/s]
Adding requests:  57%|█████▋    | 2326/4096 [00:05<00:03, 471.03it/s]
Adding requests:  58%|█████▊    | 2374/4096 [00:05<00:03, 450.69it/s]
Adding requests:  59%|█████▉    | 2421/4096 [00:05<00:03, 456.03it/s]
Adding requests:  60%|██████    | 2468/4096 [00:05<00:03, 458.22it/s]
Adding requests:  61%|██████▏   | 2515/4096 [00:05<00:03, 460.39it/s]
Adding requests:  63%|██████▎   | 2564/4096 [00:05<00:03, 468.15it/s]
Adding requests:  64%|██████▍   | 2612/4096 [00:05<00:03, 469.19it/s]
Adding requests:  65%|██████▍   | 2661/4096 [00:05<00:03, 474.39it/s]
Adding requests:  66%|██████▌   | 2709/4096 [00:05<00:02, 468.70it/s]
Adding requests:  67%|██████▋   | 2757/4096 [00:05<00:02, 469.66it/s]
Adding requests:  68%|██████▊   | 2804/4096 [00:06<00:02, 466.76it/s]
Adding requests:  70%|██████▉   | 2852/4096 [00:06<00:02, 468.94it/s]
Adding requests:  71%|███████   | 2900/4096 [00:06<00:02, 471.02it/s]
Adding requests:  72%|███████▏  | 2948/4096 [00:06<00:02, 466.96it/s]
Adding requests:  73%|███████▎  | 2995/4096 [00:06<00:02, 458.21it/s]
Adding requests:  74%|███████▍  | 3042/4096 [00:06<00:02, 458.85it/s]
Adding requests:  75%|███████▌  | 3089/4096 [00:06<00:02, 460.46it/s]
Adding requests:  77%|███████▋  | 3136/4096 [00:06<00:02, 461.10it/s]
Adding requests:  78%|███████▊  | 3183/4096 [00:06<00:01, 462.68it/s]
Adding requests:  79%|███████▉  | 3231/4096 [00:06<00:01, 465.34it/s]
Adding requests:  80%|████████  | 3279/4096 [00:07<00:01, 467.65it/s]
Adding requests:  81%|████████  | 3326/4096 [00:07<00:01, 467.29it/s]
Adding requests:  82%|████████▏ | 3374/4096 [00:07<00:01, 469.99it/s]
Adding requests:  84%|████████▎ | 3423/4096 [00:07<00:01, 474.15it/s]
Adding requests:  85%|████████▍ | 3471/4096 [00:07<00:01, 462.23it/s]
Adding requests:  86%|████████▌ | 3518/4096 [00:07<00:01, 460.22it/s]
Adding requests:  87%|████████▋ | 3565/4096 [00:07<00:01, 459.34it/s]
Adding requests:  88%|████████▊ | 3613/4096 [00:07<00:01, 462.99it/s]
Adding requests:  89%|████████▉ | 3660/4096 [00:07<00:00, 460.72it/s]
Adding requests:  91%|█████████ | 3709/4096 [00:07<00:00, 466.59it/s]
Adding requests:  92%|█████████▏| 3756/4096 [00:08<00:00, 466.26it/s]
Adding requests:  93%|█████████▎| 3805/4096 [00:08<00:00, 471.95it/s]
Adding requests:  94%|█████████▍| 3854/4096 [00:08<00:00, 474.42it/s]
Adding requests:  95%|█████████▌| 3902/4096 [00:08<00:00, 474.95it/s]
Adding requests:  96%|█████████▋| 3950/4096 [00:08<00:00, 472.61it/s]
Adding requests:  98%|█████████▊| 3998/4096 [00:08<00:00, 467.75it/s]
Adding requests:  99%|█████████▉| 4045/4096 [00:08<00:00, 462.06it/s]
Adding requests: 100%|█████████▉| 4092/4096 [00:08<00:00, 464.27it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 464.52it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 56705.60it/s, est. speed input: 58078037.56 toks/s, output: 56713.65 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 56619.07it/s, est. speed input: 58078037.56 toks/s, output: 56713.65 toks/s]
[rank0]:[W126 14:01:11.908491941 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 59.1s

测试结果:
  Requests/s:   460.64
  Tokens/s:     472156.75
  Total Reqs:   4096
  Elapsed:      8.89s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     471696.11

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:01:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=548112) WARNING 01-26 14:02:10 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=548112) WARNING 01-26 14:02:17 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 461.61 requests/s, 473146.30 total tokens/s, 461.61 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192


─── STDERR ───
[2026-01-26 14:01:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:01:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:01:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:01:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:01:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:01:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:01:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:01:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:01:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:01:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:01:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:01:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:01:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:01:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:02:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:02:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:02:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:02:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:02:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:02:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:02:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:02:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:02:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:05] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=548112) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=548112) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.66it/s]
(EngineCore_DP0 pid=548112) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.66it/s]
(EngineCore_DP0 pid=548112) 
(EngineCore_DP0 pid=548112) [rank0]:W0126 14:02:13.482000 548112 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=548112) [rank0]:W0126 14:02:13.551000 548112 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:13] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [rank0]:W0126 14:02:14.330000 548112 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=548112) [rank0]:W0126 14:02:14.427000 548112 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:15] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:16] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) 2026-01-26 14:02:17,085 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=548112) [2026-01-26 14:02:17] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=548112) 2026-01-26 14:02:17,116 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=548112) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 18.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 20.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 18.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:00<00:00, 17.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:00<00:00, 19.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:00<00:00, 20.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 19.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 19.54it/s]
(EngineCore_DP0 pid=548112) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 21.89it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 21.98it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 21.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 21.95it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 38/8192 [00:00<00:21, 375.76it/s]
Adding requests:   1%|          | 82/8192 [00:00<00:19, 409.51it/s]
Adding requests:   2%|▏         | 126/8192 [00:00<00:19, 422.58it/s]
Adding requests:   2%|▏         | 170/8192 [00:00<00:18, 428.47it/s]
Adding requests:   3%|▎         | 215/8192 [00:00<00:18, 434.22it/s]
Adding requests:   3%|▎         | 263/8192 [00:00<00:17, 447.24it/s]
Adding requests:   4%|▍         | 308/8192 [00:00<00:17, 447.49it/s]
Adding requests:   4%|▍         | 355/8192 [00:00<00:17, 454.05it/s]
Adding requests:   5%|▍         | 402/8192 [00:00<00:17, 456.62it/s]
Adding requests:   5%|▌         | 448/8192 [00:01<00:16, 455.62it/s]
Adding requests:   6%|▌         | 495/8192 [00:01<00:16, 458.24it/s]
Adding requests:   7%|▋         | 541/8192 [00:01<00:17, 450.04it/s]
Adding requests:   7%|▋         | 590/8192 [00:01<00:16, 460.99it/s]
Adding requests:   8%|▊         | 637/8192 [00:01<00:16, 461.92it/s]
Adding requests:   8%|▊         | 686/8192 [00:01<00:16, 468.72it/s]
Adding requests:   9%|▉         | 734/8192 [00:01<00:15, 469.16it/s]
Adding requests:  10%|▉         | 781/8192 [00:01<00:15, 464.87it/s]
Adding requests:  10%|█         | 828/8192 [00:01<00:16, 456.86it/s]
Adding requests:  11%|█         | 876/8192 [00:01<00:15, 460.86it/s]
Adding requests:  11%|█▏        | 925/8192 [00:02<00:15, 467.40it/s]
Adding requests:  12%|█▏        | 973/8192 [00:02<00:15, 468.96it/s]
Adding requests:  12%|█▏        | 1021/8192 [00:02<00:15, 471.80it/s]
Adding requests:  13%|█▎        | 1069/8192 [00:02<00:15, 470.11it/s]
Adding requests:  14%|█▎        | 1117/8192 [00:02<00:15, 464.75it/s]
Adding requests:  14%|█▍        | 1166/8192 [00:02<00:14, 471.90it/s]
Adding requests:  15%|█▍        | 1216/8192 [00:02<00:14, 477.50it/s]
Adding requests:  15%|█▌        | 1264/8192 [00:02<00:14, 471.82it/s]
Adding requests:  16%|█▌        | 1312/8192 [00:02<00:14, 471.25it/s]
Adding requests:  17%|█▋        | 1361/8192 [00:02<00:14, 475.37it/s]
Adding requests:  17%|█▋        | 1411/8192 [00:03<00:14, 479.86it/s]
Adding requests:  18%|█▊        | 1459/8192 [00:03<00:14, 477.88it/s]
Adding requests:  18%|█▊        | 1507/8192 [00:03<00:14, 469.05it/s]
Adding requests:  19%|█▉        | 1554/8192 [00:03<00:14, 469.32it/s]
Adding requests:  20%|█▉        | 1605/8192 [00:03<00:13, 478.50it/s]
Adding requests:  20%|██        | 1653/8192 [00:03<00:13, 475.88it/s]
Adding requests:  21%|██        | 1701/8192 [00:03<00:13, 472.70it/s]
Adding requests:  21%|██▏       | 1749/8192 [00:03<00:13, 473.96it/s]
Adding requests:  22%|██▏       | 1797/8192 [00:03<00:13, 475.08it/s]
Adding requests:  23%|██▎       | 1845/8192 [00:03<00:13, 475.59it/s]
Adding requests:  23%|██▎       | 1893/8192 [00:04<00:13, 472.98it/s]
Adding requests:  24%|██▎       | 1941/8192 [00:04<00:13, 472.40it/s]
Adding requests:  24%|██▍       | 1989/8192 [00:04<00:13, 472.22it/s]
Adding requests:  25%|██▍       | 2038/8192 [00:04<00:12, 474.70it/s]
Adding requests:  25%|██▌       | 2087/8192 [00:04<00:12, 477.74it/s]
Adding requests:  26%|██▌       | 2135/8192 [00:04<00:12, 471.92it/s]
Adding requests:  27%|██▋       | 2183/8192 [00:04<00:12, 466.25it/s]
Adding requests:  27%|██▋       | 2232/8192 [00:04<00:12, 471.15it/s]
Adding requests:  28%|██▊       | 2280/8192 [00:04<00:12, 471.45it/s]
Adding requests:  28%|██▊       | 2328/8192 [00:05<00:12, 472.90it/s]
Adding requests:  29%|██▉       | 2376/8192 [00:05<00:12, 472.96it/s]
Adding requests:  30%|██▉       | 2425/8192 [00:05<00:12, 475.49it/s]
Adding requests:  30%|███       | 2473/8192 [00:05<00:12, 473.80it/s]
Adding requests:  31%|███       | 2521/8192 [00:05<00:11, 473.19it/s]
Adding requests:  31%|███▏      | 2569/8192 [00:05<00:12, 468.11it/s]
Adding requests:  32%|███▏      | 2617/8192 [00:05<00:11, 469.72it/s]
Adding requests:  33%|███▎      | 2666/8192 [00:05<00:11, 474.29it/s]
Adding requests:  33%|███▎      | 2714/8192 [00:05<00:11, 468.92it/s]
Adding requests:  34%|███▎      | 2761/8192 [00:05<00:11, 469.07it/s]
Adding requests:  34%|███▍      | 2808/8192 [00:06<00:11, 467.77it/s]
Adding requests:  35%|███▍      | 2856/8192 [00:06<00:11, 467.48it/s]
Adding requests:  35%|███▌      | 2905/8192 [00:06<00:11, 471.77it/s]
Adding requests:  36%|███▌      | 2953/8192 [00:06<00:11, 469.01it/s]
Adding requests:  37%|███▋      | 3000/8192 [00:06<00:11, 469.20it/s]
Adding requests:  37%|███▋      | 3048/8192 [00:06<00:10, 470.43it/s]
Adding requests:  38%|███▊      | 3096/8192 [00:06<00:10, 469.20it/s]
Adding requests:  38%|███▊      | 3143/8192 [00:06<00:10, 468.32it/s]
Adding requests:  39%|███▉      | 3191/8192 [00:06<00:10, 470.82it/s]
Adding requests:  40%|███▉      | 3240/8192 [00:06<00:10, 473.46it/s]
Adding requests:  40%|████      | 3288/8192 [00:07<00:10, 473.66it/s]
Adding requests:  41%|████      | 3336/8192 [00:07<00:10, 467.36it/s]
Adding requests:  41%|████▏     | 3385/8192 [00:07<00:10, 471.87it/s]
Adding requests:  42%|████▏     | 3434/8192 [00:07<00:10, 475.52it/s]
Adding requests:  43%|████▎     | 3482/8192 [00:07<00:10, 464.46it/s]
Adding requests:  43%|████▎     | 3529/8192 [00:07<00:10, 465.85it/s]
Adding requests:  44%|████▎     | 3577/8192 [00:07<00:09, 467.79it/s]
Adding requests:  44%|████▍     | 3624/8192 [00:07<00:09, 465.32it/s]
Adding requests:  45%|████▍     | 3671/8192 [00:07<00:09, 466.68it/s]
Adding requests:  45%|████▌     | 3718/8192 [00:07<00:09, 465.45it/s]
Adding requests:  46%|████▌     | 3765/8192 [00:08<00:09, 456.98it/s]
Adding requests:  47%|████▋     | 3814/8192 [00:08<00:09, 464.46it/s]
Adding requests:  47%|████▋     | 3863/8192 [00:08<00:09, 469.68it/s]
Adding requests:  48%|████▊     | 3910/8192 [00:08<00:09, 469.20it/s]
Adding requests:  48%|████▊     | 3958/8192 [00:08<00:09, 469.70it/s]
Adding requests:  49%|████▉     | 4005/8192 [00:08<00:08, 468.21it/s]
Adding requests:  49%|████▉     | 4052/8192 [00:08<00:08, 464.60it/s]
Adding requests:  50%|█████     | 4100/8192 [00:08<00:08, 467.72it/s]
Adding requests:  51%|█████     | 4147/8192 [00:08<00:08, 467.00it/s]
Adding requests:  51%|█████     | 4196/8192 [00:08<00:08, 471.77it/s]
Adding requests:  52%|█████▏    | 4244/8192 [00:09<00:08, 471.40it/s]
Adding requests:  52%|█████▏    | 4292/8192 [00:09<00:08, 469.13it/s]
Adding requests:  53%|█████▎    | 4341/8192 [00:09<00:08, 474.08it/s]
Adding requests:  54%|█████▎    | 4390/8192 [00:09<00:07, 478.76it/s]
Adding requests:  54%|█████▍    | 4438/8192 [00:09<00:07, 476.65it/s]
Adding requests:  55%|█████▍    | 4486/8192 [00:09<00:07, 472.59it/s]
Adding requests:  55%|█████▌    | 4534/8192 [00:09<00:07, 469.93it/s]
Adding requests:  56%|█████▌    | 4582/8192 [00:09<00:07, 471.64it/s]
Adding requests:  57%|█████▋    | 4630/8192 [00:09<00:07, 473.32it/s]
Adding requests:  57%|█████▋    | 4678/8192 [00:10<00:07, 471.99it/s]
Adding requests:  58%|█████▊    | 4726/8192 [00:10<00:07, 472.74it/s]
Adding requests:  58%|█████▊    | 4774/8192 [00:10<00:07, 473.96it/s]
Adding requests:  59%|█████▉    | 4822/8192 [00:10<00:07, 471.19it/s]
Adding requests:  59%|█████▉    | 4870/8192 [00:10<00:07, 470.90it/s]
Adding requests:  60%|██████    | 4918/8192 [00:10<00:07, 458.23it/s]
Adding requests:  61%|██████    | 4966/8192 [00:10<00:06, 464.28it/s]
Adding requests:  61%|██████    | 5014/8192 [00:10<00:06, 466.87it/s]
Adding requests:  62%|██████▏   | 5062/8192 [00:10<00:06, 469.44it/s]
Adding requests:  62%|██████▏   | 5111/8192 [00:10<00:06, 474.43it/s]
Adding requests:  63%|██████▎   | 5159/8192 [00:11<00:06, 475.69it/s]
Adding requests:  64%|██████▎   | 5207/8192 [00:11<00:06, 474.21it/s]
Adding requests:  64%|██████▍   | 5255/8192 [00:11<00:06, 468.63it/s]
Adding requests:  65%|██████▍   | 5303/8192 [00:11<00:06, 471.92it/s]
Adding requests:  65%|██████▌   | 5351/8192 [00:11<00:06, 473.50it/s]
Adding requests:  66%|██████▌   | 5399/8192 [00:11<00:05, 473.03it/s]
Adding requests:  66%|██████▋   | 5447/8192 [00:11<00:05, 471.90it/s]
Adding requests:  67%|██████▋   | 5495/8192 [00:11<00:05, 466.89it/s]
Adding requests:  68%|██████▊   | 5542/8192 [00:11<00:05, 466.96it/s]
Adding requests:  68%|██████▊   | 5589/8192 [00:11<00:05, 467.81it/s]
Adding requests:  69%|██████▉   | 5636/8192 [00:12<00:05, 465.70it/s]
Adding requests:  69%|██████▉   | 5683/8192 [00:12<00:05, 463.55it/s]
Adding requests:  70%|██████▉   | 5731/8192 [00:12<00:05, 467.27it/s]
Adding requests:  71%|███████   | 5780/8192 [00:12<00:05, 468.54it/s]
Adding requests:  71%|███████   | 5827/8192 [00:12<00:05, 464.67it/s]
Adding requests:  72%|███████▏  | 5875/8192 [00:12<00:04, 468.40it/s]
Adding requests:  72%|███████▏  | 5923/8192 [00:12<00:04, 471.00it/s]
Adding requests:  73%|███████▎  | 5971/8192 [00:12<00:04, 469.91it/s]
Adding requests:  73%|███████▎  | 6019/8192 [00:12<00:04, 472.37it/s]
Adding requests:  74%|███████▍  | 6068/8192 [00:12<00:04, 476.64it/s]
Adding requests:  75%|███████▍  | 6116/8192 [00:13<00:04, 461.07it/s]
Adding requests:  75%|███████▌  | 6163/8192 [00:13<00:04, 460.20it/s]
Adding requests:  76%|███████▌  | 6210/8192 [00:13<00:04, 428.55it/s]
Adding requests:  76%|███████▋  | 6255/8192 [00:13<00:04, 432.70it/s]
Adding requests:  77%|███████▋  | 6302/8192 [00:13<00:04, 442.52it/s]
Adding requests:  78%|███████▊  | 6349/8192 [00:13<00:04, 449.86it/s]
Adding requests:  78%|███████▊  | 6397/8192 [00:13<00:03, 456.75it/s]
Adding requests:  79%|███████▊  | 6446/8192 [00:13<00:03, 465.35it/s]
Adding requests:  79%|███████▉  | 6493/8192 [00:13<00:03, 466.55it/s]
Adding requests:  80%|███████▉  | 6542/8192 [00:14<00:03, 471.06it/s]
Adding requests:  80%|████████  | 6590/8192 [00:14<00:03, 470.12it/s]
Adding requests:  81%|████████  | 6638/8192 [00:14<00:03, 468.06it/s]
Adding requests:  82%|████████▏ | 6685/8192 [00:14<00:03, 466.85it/s]
Adding requests:  82%|████████▏ | 6733/8192 [00:14<00:03, 470.27it/s]
Adding requests:  83%|████████▎ | 6781/8192 [00:14<00:03, 468.65it/s]
Adding requests:  83%|████████▎ | 6829/8192 [00:14<00:02, 471.69it/s]
Adding requests:  84%|████████▍ | 6877/8192 [00:14<00:02, 473.83it/s]
Adding requests:  85%|████████▍ | 6925/8192 [00:14<00:02, 475.34it/s]
Adding requests:  85%|████████▌ | 6973/8192 [00:14<00:02, 475.98it/s]
Adding requests:  86%|████████▌ | 7021/8192 [00:15<00:02, 469.46it/s]
Adding requests:  86%|████████▋ | 7068/8192 [00:15<00:02, 467.48it/s]
Adding requests:  87%|████████▋ | 7117/8192 [00:15<00:02, 472.67it/s]
Adding requests:  87%|████████▋ | 7165/8192 [00:15<00:02, 467.57it/s]
Adding requests:  88%|████████▊ | 7212/8192 [00:15<00:02, 453.48it/s]
Adding requests:  89%|████████▊ | 7258/8192 [00:15<00:02, 449.80it/s]
Adding requests:  89%|████████▉ | 7306/8192 [00:15<00:01, 457.12it/s]
Adding requests:  90%|████████▉ | 7353/8192 [00:15<00:01, 457.72it/s]
Adding requests:  90%|█████████ | 7402/8192 [00:15<00:01, 465.07it/s]
Adding requests:  91%|█████████ | 7451/8192 [00:15<00:01, 471.88it/s]
Adding requests:  92%|█████████▏| 7499/8192 [00:16<00:01, 469.36it/s]
Adding requests:  92%|█████████▏| 7546/8192 [00:16<00:01, 438.17it/s]
Adding requests:  93%|█████████▎| 7592/8192 [00:16<00:01, 444.06it/s]
Adding requests:  93%|█████████▎| 7637/8192 [00:16<00:01, 439.22it/s]
Adding requests:  94%|█████████▍| 7686/8192 [00:16<00:01, 453.36it/s]
Adding requests:  94%|█████████▍| 7733/8192 [00:16<00:01, 456.29it/s]
Adding requests:  95%|█████████▍| 7781/8192 [00:16<00:00, 462.26it/s]
Adding requests:  96%|█████████▌| 7828/8192 [00:16<00:00, 461.36it/s]
Adding requests:  96%|█████████▌| 7875/8192 [00:16<00:00, 462.63it/s]
Adding requests:  97%|█████████▋| 7922/8192 [00:17<00:00, 452.36it/s]
Adding requests:  97%|█████████▋| 7969/8192 [00:17<00:00, 455.27it/s]
Adding requests:  98%|█████████▊| 8015/8192 [00:17<00:00, 449.30it/s]
Adding requests:  98%|█████████▊| 8062/8192 [00:17<00:00, 452.67it/s]
Adding requests:  99%|█████████▉| 8111/8192 [00:17<00:00, 462.67it/s]
Adding requests: 100%|█████████▉| 8158/8192 [00:17<00:00, 464.47it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 465.90it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  64%|██████▍   | 5283/8192 [00:00<00:00, 52826.52it/s, est. speed input: 54098100.57 toks/s, output: 52827.28 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 52826.52it/s, est. speed input: 52425256.49 toks/s, output: 51195.09 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 51157.97it/s, est. speed input: 52425256.49 toks/s, output: 51195.09 toks/s]
[rank0]:[W126 14:02:38.731154338 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 86.8s

测试结果:
  Requests/s:   461.61
  Tokens/s:     473146.30
  Total Reqs:   8192
  Elapsed:      17.75s

  [Prefill 分析]
  Total Prefill Tokens: 8388608
  Prefill Tokens/s:     472684.70


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cublaslt/Llama3.2-1B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,48.1069,24678.8299,2.6607
1024,1024,1,128,128,47.0891,48266.3168,2.7183
2048,1024,2,256,128,84.4720,86583.7912,3.0306
4096,1024,4,512,128,178.1527,182606.4844,2.8739
8192,1024,8,1024,128,333.5862,341925.8152,3.0697
16384,1024,16,2048,128,456.5896,468004.3306,4.4854
32768,1024,32,4096,128,460.6407,472156.7522,8.8920
65536,1024,64,8192,128,461.6062,473146.3038,17.7467

------------------------------------------------------------

[INFO] 完成: 8 成功, 0 失败

============================================================
  Llama3.2-1B-FP8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:02:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=549365) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=549365) WARNING 01-26 14:03:00 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=549365) WARNING 01-26 14:03:07 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 43.39 requests/s, 22257.29 total tokens/s, 43.39 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 14:02:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:02:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:02:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:02:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:02:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:02:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:02:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:02:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:02:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:02:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:02:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:02:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:02:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:02:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:02:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:02:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:02:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:02:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=549365) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=549365) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.21it/s]
(EngineCore_DP0 pid=549365) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.21it/s]
(EngineCore_DP0 pid=549365) 
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=549365) 2026-01-26 14:03:06,998 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=549365) 2026-01-26 14:03:07,021 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=549365) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.99it/s]
(EngineCore_DP0 pid=549365) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 21.07it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  55%|█████▍    | 70/128 [00:00<00:00, 692.54it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 333.76it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:00, 172.37it/s, est. speed input: 88257.23 toks/s, output: 172.37 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:01, 65.42it/s, est. speed input: 37133.04 toks/s, output: 72.52 toks/s]  
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 56.85it/s, est. speed input: 32704.20 toks/s, output: 63.87 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:00<00:01, 52.88it/s, est. speed input: 30741.45 toks/s, output: 60.04 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 50.39it/s, est. speed input: 29546.76 toks/s, output: 57.71 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 48.83it/s, est. speed input: 28787.71 toks/s, output: 56.23 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 47.58it/s, est. speed input: 28171.97 toks/s, output: 55.02 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 46.78it/s, est. speed input: 27750.41 toks/s, output: 54.20 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 46.10it/s, est. speed input: 27383.35 toks/s, output: 53.48 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 45.18it/s, est. speed input: 27007.07 toks/s, output: 52.75 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 44.90it/s, est. speed input: 26733.10 toks/s, output: 52.21 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 44.65it/s, est. speed input: 26487.02 toks/s, output: 51.73 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 44.58it/s, est. speed input: 26279.75 toks/s, output: 51.33 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 44.35it/s, est. speed input: 26076.03 toks/s, output: 50.93 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 44.30it/s, est. speed input: 25904.20 toks/s, output: 50.59 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 44.35it/s, est. speed input: 25756.20 toks/s, output: 50.30 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 44.33it/s, est. speed input: 25616.58 toks/s, output: 50.03 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.33it/s, est. speed input: 25544.14 toks/s, output: 49.89 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 49.89it/s, est. speed input: 25544.14 toks/s, output: 49.89 toks/s]
[rank0]:[W126 14:03:11.145822209 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.2s

测试结果:
  Requests/s:   43.39
  Tokens/s:     22257.29
  Total Reqs:   128
  Elapsed:      2.95s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     22213.90

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:03:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=550386) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=550386) WARNING 01-26 14:03:33 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=550386) WARNING 01-26 14:03:40 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.14 requests/s, 42170.06 total tokens/s, 41.14 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 14:03:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:03:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:03:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:03:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:03:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:03:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:03:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:03:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:03:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:03:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:03:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:03:28] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:03:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:03:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:03:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:03:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:03:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:03:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:28] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=550386) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=550386) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=550386) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=550386) 
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=550386) 2026-01-26 14:03:40,606 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=550386) 2026-01-26 14:03:40,628 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=550386) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.48it/s]
(EngineCore_DP0 pid=550386) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 19.99it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|███       | 39/128 [00:00<00:00, 381.25it/s]
Adding requests:  66%|██████▌   | 84/128 [00:00<00:00, 421.06it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 424.25it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:01, 104.74it/s, est. speed input: 107261.31 toks/s, output: 104.74 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:01, 57.06it/s, est. speed input: 62715.94 toks/s, output: 61.25 toks/s]   
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 51.36it/s, est. speed input: 57002.86 toks/s, output: 55.67 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:01, 48.70it/s, est. speed input: 54372.89 toks/s, output: 53.10 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 47.04it/s, est. speed input: 52686.37 toks/s, output: 51.45 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 45.95it/s, est. speed input: 51607.82 toks/s, output: 50.40 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 45.20it/s, est. speed input: 50794.72 toks/s, output: 49.60 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 44.65it/s, est. speed input: 50141.75 toks/s, output: 48.97 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 44.30it/s, est. speed input: 49624.09 toks/s, output: 48.46 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 43.99it/s, est. speed input: 49175.28 toks/s, output: 48.02 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 43.79it/s, est. speed input: 48800.71 toks/s, output: 47.66 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 43.60it/s, est. speed input: 48467.48 toks/s, output: 47.33 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 43.21it/s, est. speed input: 48116.68 toks/s, output: 46.99 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:00, 43.27it/s, est. speed input: 47887.89 toks/s, output: 46.77 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 43.32it/s, est. speed input: 47687.02 toks/s, output: 46.57 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 42.90it/s, est. speed input: 47414.02 toks/s, output: 46.30 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 43.09it/s, est. speed input: 47265.29 toks/s, output: 46.16 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 43.44it/s, est. speed input: 47172.04 toks/s, output: 46.07 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 42.88it/s, est. speed input: 46946.41 toks/s, output: 45.85 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 43.12it/s, est. speed input: 46846.43 toks/s, output: 45.75 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 43.32it/s, est. speed input: 46759.70 toks/s, output: 45.66 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 43.53it/s, est. speed input: 46691.25 toks/s, output: 45.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.53it/s, est. speed input: 46672.14 toks/s, output: 45.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.58it/s, est. speed input: 46672.14 toks/s, output: 45.58 toks/s]
[rank0]:[W126 14:03:45.994239662 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.0s

测试结果:
  Requests/s:   41.14
  Tokens/s:     42170.06
  Total Reqs:   128
  Elapsed:      3.11s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     42128.92

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:03:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=551441) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=551441) WARNING 01-26 14:04:08 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=551441) WARNING 01-26 14:04:15 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 84.91 requests/s, 87028.03 total tokens/s, 84.91 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 14:03:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:03:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:03:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:03:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:03:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:03:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:03:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:03:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:03:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:04:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:04:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:04:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:04:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:04:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:04:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:04:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:04:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:04:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:03] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:03] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:03] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:03] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:03] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=551441) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=551441) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.10it/s]
(EngineCore_DP0 pid=551441) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.10it/s]
(EngineCore_DP0 pid=551441) 
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=551441) 2026-01-26 14:04:15,063 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=551441) 2026-01-26 14:04:15,084 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=551441) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 19.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 19.34it/s]
(EngineCore_DP0 pid=551441) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 23.65it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█         | 28/256 [00:00<00:00, 273.48it/s]
Adding requests:  27%|██▋       | 70/256 [00:00<00:00, 356.53it/s]
Adding requests:  41%|████▏     | 106/256 [00:00<00:00, 316.31it/s]
Adding requests:  58%|█████▊    | 148/256 [00:00<00:00, 352.38it/s]
Adding requests:  75%|███████▍  | 191/256 [00:00<00:00, 378.09it/s]
Adding requests:  93%|█████████▎| 238/256 [00:00<00:00, 405.54it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 378.54it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:00<00:00, 558.15it/s, est. speed input: 571639.92 toks/s, output: 558.18 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:00<00:01, 136.61it/s, est. speed input: 158112.73 toks/s, output: 154.41 toks/s]
Processed prompts:  56%|█████▌    | 143/256 [00:01<00:00, 118.96it/s, est. speed input: 138727.24 toks/s, output: 135.48 toks/s]
Processed prompts:  64%|██████▎   | 163/256 [00:01<00:00, 110.35it/s, est. speed input: 130287.79 toks/s, output: 127.23 toks/s]
Processed prompts:  70%|██████▉   | 179/256 [00:01<00:00, 104.91it/s, est. speed input: 125336.29 toks/s, output: 122.40 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:01<00:00, 99.56it/s, est. speed input: 121306.90 toks/s, output: 118.46 toks/s] 
Processed prompts:  80%|███████▉  | 204/256 [00:01<00:00, 97.35it/s, est. speed input: 119045.22 toks/s, output: 116.25 toks/s]
Processed prompts:  84%|████████▍ | 215/256 [00:01<00:00, 97.45it/s, est. speed input: 117910.15 toks/s, output: 115.15 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:02<00:00, 93.40it/s, est. speed input: 115640.66 toks/s, output: 112.93 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 92.33it/s, est. speed input: 114332.88 toks/s, output: 111.65 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:02<00:00, 91.55it/s, est. speed input: 113177.95 toks/s, output: 110.53 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 90.97it/s, est. speed input: 112142.65 toks/s, output: 109.51 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 90.97it/s, est. speed input: 112142.65 toks/s, output: 109.51 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 109.51it/s, est. speed input: 112142.65 toks/s, output: 109.51 toks/s]
[rank0]:[W126 14:04:20.769801221 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.6s

测试结果:
  Requests/s:   84.91
  Tokens/s:     87028.03
  Total Reqs:   256
  Elapsed:      3.02s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     86943.12

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:04:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=552488) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=552488) WARNING 01-26 14:04:43 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=552488) WARNING 01-26 14:04:50 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 159.46 requests/s, 163449.75 total tokens/s, 159.46 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 14:04:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:04:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:04:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:04:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:04:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:04:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:04:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:04:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:04:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:04:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:04:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:04:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:04:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:04:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:04:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:04:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:04:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:04:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=552488) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=552488) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.06it/s]
(EngineCore_DP0 pid=552488) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.06it/s]
(EngineCore_DP0 pid=552488) 
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=552488) 2026-01-26 14:04:50,575 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=552488) 2026-01-26 14:04:50,597 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=552488) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 22.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 17.81it/s]
(EngineCore_DP0 pid=552488) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 19.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 20.06it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 38/512 [00:00<00:01, 375.35it/s]
Adding requests:  16%|█▌        | 83/512 [00:00<00:01, 414.61it/s]
Adding requests:  25%|██▌       | 128/512 [00:00<00:00, 427.77it/s]
Adding requests:  34%|███▎      | 172/512 [00:00<00:00, 429.19it/s]
Adding requests:  42%|████▏     | 217/512 [00:00<00:00, 435.33it/s]
Adding requests:  51%|█████▏    | 263/512 [00:00<00:00, 441.56it/s]
Adding requests:  60%|██████    | 308/512 [00:00<00:00, 442.76it/s]
Adding requests:  69%|██████▉   | 354/512 [00:00<00:00, 448.04it/s]
Adding requests:  78%|███████▊  | 400/512 [00:00<00:00, 449.26it/s]
Adding requests:  87%|████████▋ | 445/512 [00:01<00:00, 446.80it/s]
Adding requests:  96%|█████████▌| 491/512 [00:01<00:00, 450.26it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 441.09it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:00<00:00, 1517.69it/s, est. speed input: 1554209.96 toks/s, output: 1517.72 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:01<00:00, 289.68it/s, est. speed input: 342366.00 toks/s, output: 334.34 toks/s]   
Processed prompts:  80%|████████  | 411/512 [00:01<00:00, 241.96it/s, est. speed input: 290827.25 toks/s, output: 284.01 toks/s]
Processed prompts:  90%|████████▉ | 459/512 [00:01<00:00, 221.28it/s, est. speed input: 270563.42 toks/s, output: 264.22 toks/s]
Processed prompts:  97%|█████████▋| 495/512 [00:01<00:00, 210.47it/s, est. speed input: 260367.83 toks/s, output: 254.26 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 210.47it/s, est. speed input: 255891.14 toks/s, output: 249.89 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 249.88it/s, est. speed input: 255891.14 toks/s, output: 249.89 toks/s]
[rank0]:[W126 14:04:56.412511425 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.6s

测试结果:
  Requests/s:   159.46
  Tokens/s:     163449.75
  Total Reqs:   512
  Elapsed:      3.21s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     163290.29

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:05:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=553581) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=553581) WARNING 01-26 14:05:21 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=553581) WARNING 01-26 14:05:28 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 300.32 requests/s, 307824.88 total tokens/s, 300.32 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 14:05:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:05:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:05:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:05:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:05:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:05:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:05:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:05:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:05:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:05:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:05:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:05:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:05:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:05:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:05:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:05:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:05:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:05:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=553581) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=553581) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.09it/s]
(EngineCore_DP0 pid=553581) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.09it/s]
(EngineCore_DP0 pid=553581) 
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=553581) 2026-01-26 14:05:28,876 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=553581) 2026-01-26 14:05:28,898 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=553581) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 22.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 20.71it/s]
(EngineCore_DP0 pid=553581) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.10it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 38/1024 [00:00<00:02, 376.58it/s]
Adding requests:   8%|▊         | 83/1024 [00:00<00:02, 418.00it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 432.38it/s]
Adding requests:  17%|█▋        | 173/1024 [00:00<00:01, 436.14it/s]
Adding requests:  21%|██▏       | 219/1024 [00:00<00:01, 442.99it/s]
Adding requests:  26%|██▌       | 267/1024 [00:00<00:01, 454.66it/s]
Adding requests:  31%|███       | 313/1024 [00:00<00:01, 450.55it/s]
Adding requests:  35%|███▌      | 359/1024 [00:00<00:01, 453.12it/s]
Adding requests:  40%|███▉      | 407/1024 [00:00<00:01, 458.38it/s]
Adding requests:  44%|████▍     | 454/1024 [00:01<00:01, 461.01it/s]
Adding requests:  49%|████▉     | 501/1024 [00:01<00:01, 460.48it/s]
Adding requests:  54%|█████▎    | 548/1024 [00:01<00:01, 456.17it/s]
Adding requests:  58%|█████▊    | 596/1024 [00:01<00:00, 462.25it/s]
Adding requests:  63%|██████▎   | 644/1024 [00:01<00:00, 467.04it/s]
Adding requests:  68%|██████▊   | 693/1024 [00:01<00:00, 473.13it/s]
Adding requests:  72%|███████▏  | 741/1024 [00:01<00:00, 471.11it/s]
Adding requests:  77%|███████▋  | 789/1024 [00:01<00:00, 469.52it/s]
Adding requests:  82%|████████▏ | 836/1024 [00:01<00:00, 460.37it/s]
Adding requests:  86%|████████▋ | 884/1024 [00:01<00:00, 465.86it/s]
Adding requests:  91%|█████████ | 932/1024 [00:02<00:00, 469.67it/s]
Adding requests:  96%|█████████▌| 980/1024 [00:02<00:00, 469.88it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 459.65it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:00<00:00, 5848.72it/s, est. speed input: 5989526.71 toks/s, output: 5848.82 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 5848.72it/s, est. speed input: 888116.75 toks/s, output: 867.30 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 867.21it/s, est. speed input: 888116.75 toks/s, output: 867.30 toks/s] 
[rank0]:[W126 14:05:35.254307884 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.9s

测试结果:
  Requests/s:   300.32
  Tokens/s:     307824.88
  Total Reqs:   1024
  Elapsed:      3.41s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     307524.57

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:05:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=554745) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=554745) WARNING 01-26 14:06:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=554745) WARNING 01-26 14:06:12 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 449.92 requests/s, 461165.37 total tokens/s, 449.92 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 14:05:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:05:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:05:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:05:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:05:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:05:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:05:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:05:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:05:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:05:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:06:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:06:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:06:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:06:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:06:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:06:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:06:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:06:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:00] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=554745) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=554745) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.37it/s]
(EngineCore_DP0 pid=554745) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.37it/s]
(EngineCore_DP0 pid=554745) 
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=554745) 2026-01-26 14:06:12,713 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=554745) 2026-01-26 14:06:12,736 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=554745) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 21.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 23.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 21.11it/s]
(EngineCore_DP0 pid=554745) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 22.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 23.04it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 38/2048 [00:00<00:05, 379.23it/s]
Adding requests:   4%|▍         | 83/2048 [00:00<00:04, 417.60it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:04, 429.12it/s]
Adding requests:   8%|▊         | 172/2048 [00:00<00:04, 432.57it/s]
Adding requests:  11%|█         | 217/2048 [00:00<00:04, 438.36it/s]
Adding requests:  13%|█▎        | 265/2048 [00:00<00:03, 450.33it/s]
Adding requests:  15%|█▌        | 311/2048 [00:00<00:03, 449.74it/s]
Adding requests:  17%|█▋        | 358/2048 [00:00<00:03, 453.07it/s]
Adding requests:  20%|█▉        | 405/2048 [00:00<00:03, 457.43it/s]
Adding requests:  22%|██▏       | 452/2048 [00:01<00:03, 459.70it/s]
Adding requests:  24%|██▍       | 498/2048 [00:01<00:03, 456.47it/s]
Adding requests:  27%|██▋       | 544/2048 [00:01<00:03, 450.29it/s]
Adding requests:  29%|██▉       | 593/2048 [00:01<00:03, 459.28it/s]
Adding requests:  31%|███▏      | 641/2048 [00:01<00:03, 463.05it/s]
Adding requests:  34%|███▎      | 689/2048 [00:01<00:02, 467.36it/s]
Adding requests:  36%|███▌      | 736/2048 [00:01<00:02, 463.28it/s]
Adding requests:  38%|███▊      | 783/2048 [00:01<00:02, 451.47it/s]
Adding requests:  40%|████      | 829/2048 [00:01<00:02, 439.94it/s]
Adding requests:  43%|████▎     | 874/2048 [00:01<00:02, 440.66it/s]
Adding requests:  45%|████▍     | 919/2048 [00:02<00:02, 442.84it/s]
Adding requests:  47%|████▋     | 964/2048 [00:02<00:02, 443.53it/s]
Adding requests:  49%|████▉     | 1009/2048 [00:02<00:02, 443.25it/s]
Adding requests:  51%|█████▏    | 1054/2048 [00:02<00:02, 443.50it/s]
Adding requests:  54%|█████▎    | 1099/2048 [00:02<00:02, 440.72it/s]
Adding requests:  56%|█████▌    | 1144/2048 [00:02<00:02, 435.83it/s]
Adding requests:  58%|█████▊    | 1190/2048 [00:02<00:01, 441.30it/s]
Adding requests:  60%|██████    | 1238/2048 [00:02<00:01, 452.20it/s]
Adding requests:  63%|██████▎   | 1285/2048 [00:02<00:01, 455.34it/s]
Adding requests:  65%|██████▌   | 1334/2048 [00:02<00:01, 462.34it/s]
Adding requests:  67%|██████▋   | 1381/2048 [00:03<00:01, 462.09it/s]
Adding requests:  70%|██████▉   | 1428/2048 [00:03<00:01, 464.11it/s]
Adding requests:  72%|███████▏  | 1477/2048 [00:03<00:01, 469.05it/s]
Adding requests:  74%|███████▍  | 1524/2048 [00:03<00:01, 464.89it/s]
Adding requests:  77%|███████▋  | 1572/2048 [00:03<00:01, 468.81it/s]
Adding requests:  79%|███████▉  | 1619/2048 [00:03<00:00, 452.37it/s]
Adding requests:  81%|████████▏ | 1666/2048 [00:03<00:00, 456.14it/s]
Adding requests:  84%|████████▎ | 1714/2048 [00:03<00:00, 460.57it/s]
Adding requests:  86%|████████▌ | 1761/2048 [00:03<00:00, 461.45it/s]
Adding requests:  88%|████████▊ | 1808/2048 [00:03<00:00, 462.64it/s]
Adding requests:  91%|█████████ | 1856/2048 [00:04<00:00, 465.41it/s]
Adding requests:  93%|█████████▎| 1903/2048 [00:04<00:00, 465.76it/s]
Adding requests:  95%|█████████▌| 1950/2048 [00:04<00:00, 466.28it/s]
Adding requests:  98%|█████████▊| 1998/2048 [00:04<00:00, 468.01it/s]
Adding requests: 100%|█████████▉| 2046/2048 [00:04<00:00, 470.22it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 454.58it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 45370.17it/s, est. speed input: 46473292.52 toks/s, output: 45379.52 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 45265.22it/s, est. speed input: 46473292.52 toks/s, output: 45379.52 toks/s]
[rank0]:[W126 14:06:19.991271887 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.8s

测试结果:
  Requests/s:   449.92
  Tokens/s:     461165.37
  Total Reqs:   2048
  Elapsed:      4.55s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     460715.45

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:06:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=556042) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=556042) WARNING 01-26 14:07:00 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=556042) WARNING 01-26 14:07:07 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 463.59 requests/s, 475180.22 total tokens/s, 463.59 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 14:06:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:06:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:06:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:06:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:06:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:06:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:06:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:06:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:06:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:06:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:06:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:06:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:06:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:06:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:06:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:06:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:06:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:06:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=556042) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=556042) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=556042) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=556042) 
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=556042) [rank0]:W0126 14:07:03.335000 556042 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=556042) [rank0]:W0126 14:07:03.403000 556042 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=556042) [rank0]:W0126 14:07:04.239000 556042 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=556042) [rank0]:W0126 14:07:04.336000 556042 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=556042) 2026-01-26 14:07:06,960 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=556042) 2026-01-26 14:07:06,983 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=556042) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 18.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 21.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 20.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 19.89it/s]
(EngineCore_DP0 pid=556042) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 23.32it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 23.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 23.46it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 39/4096 [00:00<00:10, 381.18it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 416.25it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 430.91it/s]
Adding requests:   4%|▍         | 174/4096 [00:00<00:08, 435.90it/s]
Adding requests:   5%|▌         | 220/4096 [00:00<00:08, 442.43it/s]
Adding requests:   7%|▋         | 268/4096 [00:00<00:08, 454.21it/s]
Adding requests:   8%|▊         | 314/4096 [00:00<00:08, 452.07it/s]
Adding requests:   9%|▉         | 360/4096 [00:00<00:08, 454.36it/s]
Adding requests:  10%|▉         | 406/4096 [00:00<00:08, 455.46it/s]
Adding requests:  11%|█         | 453/4096 [00:01<00:07, 456.66it/s]
Adding requests:  12%|█▏        | 500/4096 [00:01<00:07, 457.81it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:07, 453.57it/s]
Adding requests:  15%|█▍        | 595/4096 [00:01<00:07, 463.74it/s]
Adding requests:  16%|█▌        | 642/4096 [00:01<00:07, 464.10it/s]
Adding requests:  17%|█▋        | 690/4096 [00:01<00:07, 467.59it/s]
Adding requests:  18%|█▊        | 738/4096 [00:01<00:07, 470.21it/s]
Adding requests:  19%|█▉        | 786/4096 [00:01<00:07, 464.60it/s]
Adding requests:  20%|██        | 833/4096 [00:01<00:07, 454.18it/s]
Adding requests:  21%|██▏       | 879/4096 [00:01<00:07, 448.71it/s]
Adding requests:  23%|██▎       | 927/4096 [00:02<00:06, 455.51it/s]
Adding requests:  24%|██▍       | 974/4096 [00:02<00:06, 458.86it/s]
Adding requests:  25%|██▍       | 1022/4096 [00:02<00:06, 463.53it/s]
Adding requests:  26%|██▌       | 1069/4096 [00:02<00:06, 462.88it/s]
Adding requests:  27%|██▋       | 1116/4096 [00:02<00:06, 458.57it/s]
Adding requests:  28%|██▊       | 1165/4096 [00:02<00:06, 465.60it/s]
Adding requests:  30%|██▉       | 1215/4096 [00:02<00:06, 473.17it/s]
Adding requests:  31%|███       | 1263/4096 [00:02<00:06, 467.34it/s]
Adding requests:  32%|███▏      | 1310/4096 [00:02<00:06, 463.87it/s]
Adding requests:  33%|███▎      | 1359/4096 [00:02<00:05, 467.69it/s]
Adding requests:  34%|███▍      | 1409/4096 [00:03<00:05, 475.49it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:03<00:05, 474.12it/s]
Adding requests:  37%|███▋      | 1506/4096 [00:03<00:05, 476.66it/s]
Adding requests:  38%|███▊      | 1554/4096 [00:03<00:05, 476.46it/s]
Adding requests:  39%|███▉      | 1604/4096 [00:03<00:05, 482.90it/s]
Adding requests:  40%|████      | 1653/4096 [00:03<00:05, 480.13it/s]
Adding requests:  42%|████▏     | 1702/4096 [00:03<00:05, 477.09it/s]
Adding requests:  43%|████▎     | 1750/4096 [00:03<00:04, 476.93it/s]
Adding requests:  44%|████▍     | 1798/4096 [00:03<00:04, 476.88it/s]
Adding requests:  45%|████▌     | 1846/4096 [00:03<00:04, 476.93it/s]
Adding requests:  46%|████▌     | 1894/4096 [00:04<00:04, 473.97it/s]
Adding requests:  47%|████▋     | 1942/4096 [00:04<00:04, 464.98it/s]
Adding requests:  49%|████▊     | 1990/4096 [00:04<00:04, 466.81it/s]
Adding requests:  50%|████▉     | 2039/4096 [00:04<00:04, 471.58it/s]
Adding requests:  51%|█████     | 2088/4096 [00:04<00:04, 475.43it/s]
Adding requests:  52%|█████▏    | 2136/4096 [00:04<00:04, 470.96it/s]
Adding requests:  53%|█████▎    | 2184/4096 [00:04<00:04, 465.44it/s]
Adding requests:  55%|█████▍    | 2233/4096 [00:04<00:03, 471.40it/s]
Adding requests:  56%|█████▌    | 2281/4096 [00:04<00:03, 470.66it/s]
Adding requests:  57%|█████▋    | 2329/4096 [00:05<00:03, 473.16it/s]
Adding requests:  58%|█████▊    | 2377/4096 [00:05<00:03, 471.70it/s]
Adding requests:  59%|█████▉    | 2426/4096 [00:05<00:03, 475.75it/s]
Adding requests:  60%|██████    | 2474/4096 [00:05<00:03, 454.97it/s]
Adding requests:  62%|██████▏   | 2520/4096 [00:05<00:03, 456.05it/s]
Adding requests:  63%|██████▎   | 2569/4096 [00:05<00:03, 465.14it/s]
Adding requests:  64%|██████▍   | 2617/4096 [00:05<00:03, 466.72it/s]
Adding requests:  65%|██████▌   | 2666/4096 [00:05<00:03, 473.50it/s]
Adding requests:  66%|██████▋   | 2714/4096 [00:05<00:02, 468.09it/s]
Adding requests:  67%|██████▋   | 2762/4096 [00:05<00:02, 470.11it/s]
Adding requests:  69%|██████▊   | 2810/4096 [00:06<00:02, 466.97it/s]
Adding requests:  70%|██████▉   | 2858/4096 [00:06<00:02, 468.21it/s]
Adding requests:  71%|███████   | 2907/4096 [00:06<00:02, 472.11it/s]
Adding requests:  72%|███████▏  | 2955/4096 [00:06<00:02, 470.14it/s]
Adding requests:  73%|███████▎  | 3003/4096 [00:06<00:02, 471.36it/s]
Adding requests:  74%|███████▍  | 3051/4096 [00:06<00:02, 473.37it/s]
Adding requests:  76%|███████▌  | 3099/4096 [00:06<00:02, 460.29it/s]
Adding requests:  77%|███████▋  | 3147/4096 [00:06<00:02, 463.31it/s]
Adding requests:  78%|███████▊  | 3195/4096 [00:06<00:01, 467.67it/s]
Adding requests:  79%|███████▉  | 3244/4096 [00:06<00:01, 472.47it/s]
Adding requests:  80%|████████  | 3292/4096 [00:07<00:01, 474.17it/s]
Adding requests:  82%|████████▏ | 3341/4096 [00:07<00:01, 476.29it/s]
Adding requests:  83%|████████▎ | 3389/4096 [00:07<00:01, 476.29it/s]
Adding requests:  84%|████████▍ | 3438/4096 [00:07<00:01, 473.00it/s]
Adding requests:  85%|████████▌ | 3486/4096 [00:07<00:01, 465.16it/s]
Adding requests:  86%|████████▋ | 3534/4096 [00:07<00:01, 468.70it/s]
Adding requests:  87%|████████▋ | 3582/4096 [00:07<00:01, 469.70it/s]
Adding requests:  89%|████████▊ | 3629/4096 [00:07<00:00, 469.71it/s]
Adding requests:  90%|████████▉ | 3677/4096 [00:07<00:00, 470.81it/s]
Adding requests:  91%|█████████ | 3725/4096 [00:07<00:00, 473.36it/s]
Adding requests:  92%|█████████▏| 3774/4096 [00:08<00:00, 477.68it/s]
Adding requests:  93%|█████████▎| 3823/4096 [00:08<00:00, 479.08it/s]
Adding requests:  95%|█████████▍| 3873/4096 [00:08<00:00, 484.39it/s]
Adding requests:  96%|█████████▌| 3922/4096 [00:08<00:00, 482.56it/s]
Adding requests:  97%|█████████▋| 3971/4096 [00:08<00:00, 480.10it/s]
Adding requests:  98%|█████████▊| 4020/4096 [00:08<00:00, 480.48it/s]
Adding requests:  99%|█████████▉| 4069/4096 [00:08<00:00, 473.37it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 467.48it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 57427.79it/s, est. speed input: 58818054.61 toks/s, output: 57435.86 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 57343.45it/s, est. speed input: 58818054.61 toks/s, output: 57435.86 toks/s]
[rank0]:[W126 14:07:18.955784151 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 59.0s

测试结果:
  Requests/s:   463.59
  Tokens/s:     475180.22
  Total Reqs:   4096
  Elapsed:      8.84s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     474716.63

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:08:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=557647) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=557647) WARNING 01-26 14:08:17 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=557647) WARNING 01-26 14:08:24 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 468.28 requests/s, 479982.25 total tokens/s, 468.28 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192


─── STDERR ───
[2026-01-26 14:08:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:08:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:08:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:08:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:08:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:08:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:08:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:08:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:08:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:08:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:08:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:08:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:08:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:08:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:08:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:08:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:08:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:08:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=557647) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=557647) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.09it/s]
(EngineCore_DP0 pid=557647) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.09it/s]
(EngineCore_DP0 pid=557647) 
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=557647) [rank0]:W0126 14:08:20.351000 557647 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=557647) [rank0]:W0126 14:08:20.420000 557647 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=557647) [rank0]:W0126 14:08:21.225000 557647 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=557647) [rank0]:W0126 14:08:21.322000 557647 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=557647) 2026-01-26 14:08:24,081 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=557647) 2026-01-26 14:08:24,105 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=557647) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 17.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 19.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 20.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 20.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 19.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 19.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 18.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 19.34it/s]
(EngineCore_DP0 pid=557647) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 22.93it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 22.95it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 23.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.04it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 38/8192 [00:00<00:21, 376.42it/s]
Adding requests:   1%|          | 82/8192 [00:00<00:19, 411.63it/s]
Adding requests:   2%|▏         | 127/8192 [00:00<00:18, 425.26it/s]
Adding requests:   2%|▏         | 172/8192 [00:00<00:18, 432.19it/s]
Adding requests:   3%|▎         | 218/8192 [00:00<00:18, 439.67it/s]
Adding requests:   3%|▎         | 265/8192 [00:00<00:17, 449.34it/s]
Adding requests:   4%|▍         | 310/8192 [00:00<00:17, 449.12it/s]
Adding requests:   4%|▍         | 357/8192 [00:00<00:17, 453.31it/s]
Adding requests:   5%|▍         | 404/8192 [00:00<00:17, 457.68it/s]
Adding requests:   5%|▌         | 450/8192 [00:01<00:16, 456.65it/s]
Adding requests:   6%|▌         | 497/8192 [00:01<00:16, 459.78it/s]
Adding requests:   7%|▋         | 543/8192 [00:01<00:16, 452.99it/s]
Adding requests:   7%|▋         | 593/8192 [00:01<00:16, 463.90it/s]
Adding requests:   8%|▊         | 641/8192 [00:01<00:16, 466.86it/s]
Adding requests:   8%|▊         | 690/8192 [00:01<00:15, 473.33it/s]
Adding requests:   9%|▉         | 739/8192 [00:01<00:15, 476.61it/s]
Adding requests:  10%|▉         | 787/8192 [00:01<00:15, 472.87it/s]
Adding requests:  10%|█         | 835/8192 [00:01<00:15, 462.35it/s]
Adding requests:  11%|█         | 884/8192 [00:01<00:15, 468.11it/s]
Adding requests:  11%|█▏        | 933/8192 [00:02<00:15, 474.42it/s]
Adding requests:  12%|█▏        | 981/8192 [00:02<00:15, 473.72it/s]
Adding requests:  13%|█▎        | 1030/8192 [00:02<00:14, 478.22it/s]
Adding requests:  13%|█▎        | 1078/8192 [00:02<00:15, 472.58it/s]
Adding requests:  14%|█▎        | 1126/8192 [00:02<00:15, 470.85it/s]
Adding requests:  14%|█▍        | 1176/8192 [00:02<00:14, 477.18it/s]
Adding requests:  15%|█▍        | 1226/8192 [00:02<00:14, 482.78it/s]
Adding requests:  16%|█▌        | 1275/8192 [00:02<00:14, 477.95it/s]
Adding requests:  16%|█▌        | 1324/8192 [00:02<00:14, 481.18it/s]
Adding requests:  17%|█▋        | 1373/8192 [00:02<00:14, 482.64it/s]
Adding requests:  17%|█▋        | 1422/8192 [00:03<00:14, 482.85it/s]
Adding requests:  18%|█▊        | 1471/8192 [00:03<00:14, 475.52it/s]
Adding requests:  19%|█▊        | 1520/8192 [00:03<00:13, 479.14it/s]
Adding requests:  19%|█▉        | 1569/8192 [00:03<00:13, 479.66it/s]
Adding requests:  20%|█▉        | 1619/8192 [00:03<00:13, 483.97it/s]
Adding requests:  20%|██        | 1668/8192 [00:03<00:13, 480.18it/s]
Adding requests:  21%|██        | 1718/8192 [00:03<00:13, 483.63it/s]
Adding requests:  22%|██▏       | 1767/8192 [00:03<00:13, 479.47it/s]
Adding requests:  22%|██▏       | 1816/8192 [00:03<00:13, 481.07it/s]
Adding requests:  23%|██▎       | 1865/8192 [00:03<00:13, 478.48it/s]
Adding requests:  23%|██▎       | 1913/8192 [00:04<00:13, 478.06it/s]
Adding requests:  24%|██▍       | 1962/8192 [00:04<00:13, 479.14it/s]
Adding requests:  25%|██▍       | 2011/8192 [00:04<00:12, 481.85it/s]
Adding requests:  25%|██▌       | 2060/8192 [00:04<00:12, 482.64it/s]
Adding requests:  26%|██▌       | 2110/8192 [00:04<00:12, 486.47it/s]
Adding requests:  26%|██▋       | 2159/8192 [00:04<00:12, 475.31it/s]
Adding requests:  27%|██▋       | 2207/8192 [00:04<00:12, 472.36it/s]
Adding requests:  28%|██▊       | 2257/8192 [00:04<00:12, 479.15it/s]
Adding requests:  28%|██▊       | 2305/8192 [00:04<00:12, 472.30it/s]
Adding requests:  29%|██▊       | 2353/8192 [00:05<00:12, 473.16it/s]
Adding requests:  29%|██▉       | 2401/8192 [00:05<00:12, 471.69it/s]
Adding requests:  30%|██▉       | 2450/8192 [00:05<00:12, 474.76it/s]
Adding requests:  31%|███       | 2499/8192 [00:05<00:11, 478.22it/s]
Adding requests:  31%|███       | 2547/8192 [00:05<00:11, 478.11it/s]
Adding requests:  32%|███▏      | 2595/8192 [00:05<00:11, 467.66it/s]
Adding requests:  32%|███▏      | 2644/8192 [00:05<00:11, 472.44it/s]
Adding requests:  33%|███▎      | 2692/8192 [00:05<00:11, 471.01it/s]
Adding requests:  33%|███▎      | 2740/8192 [00:05<00:11, 470.89it/s]
Adding requests:  34%|███▍      | 2788/8192 [00:05<00:11, 469.38it/s]
Adding requests:  35%|███▍      | 2835/8192 [00:06<00:11, 467.87it/s]
Adding requests:  35%|███▌      | 2883/8192 [00:06<00:11, 471.01it/s]
Adding requests:  36%|███▌      | 2931/8192 [00:06<00:11, 469.63it/s]
Adding requests:  36%|███▋      | 2979/8192 [00:06<00:11, 471.96it/s]
Adding requests:  37%|███▋      | 3027/8192 [00:06<00:10, 471.14it/s]
Adding requests:  38%|███▊      | 3075/8192 [00:06<00:10, 468.67it/s]
Adding requests:  38%|███▊      | 3124/8192 [00:06<00:10, 473.20it/s]
Adding requests:  39%|███▊      | 3172/8192 [00:06<00:10, 469.88it/s]
Adding requests:  39%|███▉      | 3220/8192 [00:06<00:10, 471.25it/s]
Adding requests:  40%|███▉      | 3269/8192 [00:06<00:10, 473.56it/s]
Adding requests:  41%|████      | 3318/8192 [00:07<00:10, 475.65it/s]
Adding requests:  41%|████      | 3367/8192 [00:07<00:10, 479.52it/s]
Adding requests:  42%|████▏     | 3416/8192 [00:07<00:09, 479.62it/s]
Adding requests:  42%|████▏     | 3464/8192 [00:07<00:10, 471.49it/s]
Adding requests:  43%|████▎     | 3512/8192 [00:07<00:09, 472.90it/s]
Adding requests:  43%|████▎     | 3560/8192 [00:07<00:10, 461.89it/s]
Adding requests:  44%|████▍     | 3607/8192 [00:07<00:09, 459.53it/s]
Adding requests:  45%|████▍     | 3653/8192 [00:07<00:09, 458.50it/s]
Adding requests:  45%|████▌     | 3701/8192 [00:07<00:09, 464.11it/s]
Adding requests:  46%|████▌     | 3748/8192 [00:07<00:09, 451.25it/s]
Adding requests:  46%|████▋     | 3798/8192 [00:08<00:09, 463.70it/s]
Adding requests:  47%|████▋     | 3846/8192 [00:08<00:09, 467.40it/s]
Adding requests:  48%|████▊     | 3894/8192 [00:08<00:09, 469.00it/s]
Adding requests:  48%|████▊     | 3941/8192 [00:08<00:09, 468.76it/s]
Adding requests:  49%|████▊     | 3988/8192 [00:08<00:09, 466.50it/s]
Adding requests:  49%|████▉     | 4036/8192 [00:08<00:08, 467.61it/s]
Adding requests:  50%|████▉     | 4083/8192 [00:08<00:08, 467.23it/s]
Adding requests:  50%|█████     | 4131/8192 [00:08<00:08, 470.99it/s]
Adding requests:  51%|█████     | 4179/8192 [00:08<00:08, 473.66it/s]
Adding requests:  52%|█████▏    | 4227/8192 [00:08<00:08, 472.71it/s]
Adding requests:  52%|█████▏    | 4275/8192 [00:09<00:08, 473.61it/s]
Adding requests:  53%|█████▎    | 4323/8192 [00:09<00:08, 475.36it/s]
Adding requests:  53%|█████▎    | 4372/8192 [00:09<00:07, 479.19it/s]
Adding requests:  54%|█████▍    | 4420/8192 [00:09<00:07, 477.80it/s]
Adding requests:  55%|█████▍    | 4469/8192 [00:09<00:07, 480.63it/s]
Adding requests:  55%|█████▌    | 4518/8192 [00:09<00:07, 469.01it/s]
Adding requests:  56%|█████▌    | 4567/8192 [00:09<00:07, 473.57it/s]
Adding requests:  56%|█████▋    | 4615/8192 [00:09<00:07, 475.13it/s]
Adding requests:  57%|█████▋    | 4663/8192 [00:09<00:07, 476.11it/s]
Adding requests:  58%|█████▊    | 4711/8192 [00:10<00:07, 474.54it/s]
Adding requests:  58%|█████▊    | 4759/8192 [00:10<00:07, 474.76it/s]
Adding requests:  59%|█████▊    | 4807/8192 [00:10<00:07, 473.07it/s]
Adding requests:  59%|█████▉    | 4855/8192 [00:10<00:07, 474.75it/s]
Adding requests:  60%|█████▉    | 4903/8192 [00:10<00:07, 460.96it/s]
Adding requests:  60%|██████    | 4952/8192 [00:10<00:06, 467.61it/s]
Adding requests:  61%|██████    | 4999/8192 [00:10<00:06, 467.54it/s]
Adding requests:  62%|██████▏   | 5048/8192 [00:10<00:06, 472.74it/s]
Adding requests:  62%|██████▏   | 5097/8192 [00:10<00:06, 477.09it/s]
Adding requests:  63%|██████▎   | 5145/8192 [00:10<00:06, 476.25it/s]
Adding requests:  63%|██████▎   | 5193/8192 [00:11<00:06, 477.26it/s]
Adding requests:  64%|██████▍   | 5241/8192 [00:11<00:06, 473.48it/s]
Adding requests:  65%|██████▍   | 5289/8192 [00:11<00:06, 471.94it/s]
Adding requests:  65%|██████▌   | 5338/8192 [00:11<00:06, 474.34it/s]
Adding requests:  66%|██████▌   | 5386/8192 [00:11<00:05, 474.89it/s]
Adding requests:  66%|██████▋   | 5434/8192 [00:11<00:05, 475.31it/s]
Adding requests:  67%|██████▋   | 5482/8192 [00:11<00:05, 470.07it/s]
Adding requests:  68%|██████▊   | 5530/8192 [00:11<00:05, 469.69it/s]
Adding requests:  68%|██████▊   | 5578/8192 [00:11<00:05, 470.57it/s]
Adding requests:  69%|██████▊   | 5626/8192 [00:11<00:05, 466.73it/s]
Adding requests:  69%|██████▉   | 5673/8192 [00:12<00:05, 464.96it/s]
Adding requests:  70%|██████▉   | 5721/8192 [00:12<00:05, 468.80it/s]
Adding requests:  70%|███████   | 5770/8192 [00:12<00:05, 472.80it/s]
Adding requests:  71%|███████   | 5818/8192 [00:12<00:05, 464.88it/s]
Adding requests:  72%|███████▏  | 5866/8192 [00:12<00:04, 468.67it/s]
Adding requests:  72%|███████▏  | 5914/8192 [00:12<00:04, 471.14it/s]
Adding requests:  73%|███████▎  | 5962/8192 [00:12<00:04, 470.51it/s]
Adding requests:  73%|███████▎  | 6011/8192 [00:12<00:04, 474.61it/s]
Adding requests:  74%|███████▍  | 6059/8192 [00:12<00:04, 462.05it/s]
Adding requests:  75%|███████▍  | 6106/8192 [00:12<00:04, 453.64it/s]
Adding requests:  75%|███████▌  | 6153/8192 [00:13<00:04, 458.11it/s]
Adding requests:  76%|███████▌  | 6201/8192 [00:13<00:04, 463.84it/s]
Adding requests:  76%|███████▋  | 6250/8192 [00:13<00:04, 471.52it/s]
Adding requests:  77%|███████▋  | 6299/8192 [00:13<00:03, 475.08it/s]
Adding requests:  77%|███████▋  | 6348/8192 [00:13<00:03, 477.60it/s]
Adding requests:  78%|███████▊  | 6397/8192 [00:13<00:03, 479.82it/s]
Adding requests:  79%|███████▊  | 6447/8192 [00:13<00:03, 484.41it/s]
Adding requests:  79%|███████▉  | 6496/8192 [00:13<00:03, 484.20it/s]
Adding requests:  80%|███████▉  | 6545/8192 [00:13<00:03, 484.90it/s]
Adding requests:  80%|████████  | 6594/8192 [00:13<00:03, 480.75it/s]
Adding requests:  81%|████████  | 6643/8192 [00:14<00:03, 480.45it/s]
Adding requests:  82%|████████▏ | 6692/8192 [00:14<00:03, 478.11it/s]
Adding requests:  82%|████████▏ | 6740/8192 [00:14<00:03, 478.50it/s]
Adding requests:  83%|████████▎ | 6789/8192 [00:14<00:02, 480.17it/s]
Adding requests:  83%|████████▎ | 6838/8192 [00:14<00:02, 481.53it/s]
Adding requests:  84%|████████▍ | 6887/8192 [00:14<00:02, 482.86it/s]
Adding requests:  85%|████████▍ | 6937/8192 [00:14<00:02, 486.81it/s]
Adding requests:  85%|████████▌ | 6986/8192 [00:14<00:02, 480.89it/s]
Adding requests:  86%|████████▌ | 7035/8192 [00:14<00:02, 477.84it/s]
Adding requests:  86%|████████▋ | 7083/8192 [00:15<00:02, 476.97it/s]
Adding requests:  87%|████████▋ | 7132/8192 [00:15<00:02, 480.18it/s]
Adding requests:  88%|████████▊ | 7181/8192 [00:15<00:02, 472.97it/s]
Adding requests:  88%|████████▊ | 7229/8192 [00:15<00:02, 474.06it/s]
Adding requests:  89%|████████▉ | 7277/8192 [00:15<00:01, 467.06it/s]
Adding requests:  89%|████████▉ | 7326/8192 [00:15<00:01, 471.77it/s]
Adding requests:  90%|█████████ | 7374/8192 [00:15<00:01, 470.31it/s]
Adding requests:  91%|█████████ | 7424/8192 [00:15<00:01, 477.88it/s]
Adding requests:  91%|█████████ | 7473/8192 [00:15<00:01, 479.82it/s]
Adding requests:  92%|█████████▏| 7522/8192 [00:15<00:01, 478.53it/s]
Adding requests:  92%|█████████▏| 7570/8192 [00:16<00:01, 477.84it/s]
Adding requests:  93%|█████████▎| 7618/8192 [00:16<00:01, 474.56it/s]
Adding requests:  94%|█████████▎| 7668/8192 [00:16<00:01, 479.11it/s]
Adding requests:  94%|█████████▍| 7717/8192 [00:16<00:00, 481.02it/s]
Adding requests:  95%|█████████▍| 7766/8192 [00:16<00:00, 477.27it/s]
Adding requests:  95%|█████████▌| 7814/8192 [00:16<00:00, 476.13it/s]
Adding requests:  96%|█████████▌| 7862/8192 [00:16<00:00, 477.18it/s]
Adding requests:  97%|█████████▋| 7910/8192 [00:16<00:00, 473.03it/s]
Adding requests:  97%|█████████▋| 7958/8192 [00:16<00:00, 470.26it/s]
Adding requests:  98%|█████████▊| 8006/8192 [00:16<00:00, 470.86it/s]
Adding requests:  98%|█████████▊| 8054/8192 [00:17<00:00, 470.42it/s]
Adding requests:  99%|█████████▉| 8103/8192 [00:17<00:00, 475.42it/s]
Adding requests:  99%|█████████▉| 8151/8192 [00:17<00:00, 475.13it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 472.15it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  70%|███████   | 5751/8192 [00:00<00:00, 57506.08it/s, est. speed input: 58890296.16 toks/s, output: 57506.90 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 57506.08it/s, est. speed input: 59809497.09 toks/s, output: 58405.63 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 58358.41it/s, est. speed input: 59809497.09 toks/s, output: 58405.63 toks/s]
[rank0]:[W126 14:08:45.476232405 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 86.6s

测试结果:
  Requests/s:   468.28
  Tokens/s:     479982.25
  Total Reqs:   8192
  Elapsed:      17.49s

  [Prefill 分析]
  Total Prefill Tokens: 8388608
  Prefill Tokens/s:     479513.97


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/Llama3.2-1B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,43.3865,22257.2886,2.9502
1024,1024,1,128,128,41.1415,42170.0616,3.1112
2048,1024,2,256,128,84.9054,87028.0303,3.0151
4096,1024,4,512,128,159.4632,163449.7518,3.2108
8192,1024,8,1024,128,300.3170,307824.8838,3.4097
16384,1024,16,2048,128,449.9174,461165.3671,4.5519
32768,1024,32,4096,128,463.5905,475180.2162,8.8354
65536,1024,64,8192,128,468.2754,479982.2451,17.4940

------------------------------------------------------------

[INFO] 完成: 8 成功, 0 失败

============================================================
  Llama3.2-1B-FP8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:08:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=558891) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=558891) WARNING 01-26 14:09:06 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=558891) WARNING 01-26 14:09:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 43.00 requests/s, 22058.07 total tokens/s, 43.00 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 14:08:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:08:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:08:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:08:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:08:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:08:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:08:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:08:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:08:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:09:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:09:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:09:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:09:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:09:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:09:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:09:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:09:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:09:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:01] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:01] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=558891) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=558891) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.81it/s]
(EngineCore_DP0 pid=558891) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.81it/s]
(EngineCore_DP0 pid=558891) 
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=558891) [2026-01-26 14:09:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=558891) 2026-01-26 14:09:13,655 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=558891) 2026-01-26 14:09:13,678 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=558891) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 18.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 18.87it/s]
(EngineCore_DP0 pid=558891) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 18.28it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  54%|█████▍    | 69/128 [00:00<00:00, 684.99it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 292.84it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:00, 192.65it/s, est. speed input: 98643.56 toks/s, output: 192.65 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 65.74it/s, est. speed input: 37548.45 toks/s, output: 73.34 toks/s]  
Processed prompts:  41%|████▏     | 53/128 [00:00<00:01, 56.87it/s, est. speed input: 32892.41 toks/s, output: 64.24 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 52.90it/s, est. speed input: 30946.40 toks/s, output: 60.44 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 50.43it/s, est. speed input: 29770.11 toks/s, output: 58.14 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 48.73it/s, est. speed input: 28983.36 toks/s, output: 56.61 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 47.41it/s, est. speed input: 28353.01 toks/s, output: 55.38 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 46.54it/s, est. speed input: 27916.15 toks/s, output: 54.52 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 45.87it/s, est. speed input: 27544.69 toks/s, output: 53.80 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 45.35it/s, est. speed input: 27222.50 toks/s, output: 53.17 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 44.90it/s, est. speed input: 26931.33 toks/s, output: 52.60 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 44.61it/s, est. speed input: 26678.40 toks/s, output: 52.11 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 44.41it/s, est. speed input: 26453.87 toks/s, output: 51.67 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 44.25it/s, est. speed input: 26250.24 toks/s, output: 51.27 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 44.16it/s, est. speed input: 26069.01 toks/s, output: 50.92 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 44.14it/s, est. speed input: 25908.79 toks/s, output: 50.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.14it/s, est. speed input: 25820.21 toks/s, output: 50.43 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 50.43it/s, est. speed input: 25820.21 toks/s, output: 50.43 toks/s]
[rank0]:[W126 14:09:18.879843764 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.2s

测试结果:
  Requests/s:   43.00
  Tokens/s:     22058.07
  Total Reqs:   128
  Elapsed:      2.98s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     22015.07

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:09:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=559907) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=559907) WARNING 01-26 14:09:40 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=559907) WARNING 01-26 14:09:47 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.60 requests/s, 42636.83 total tokens/s, 41.60 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 14:09:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:09:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:09:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:09:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:09:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:09:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:09:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:09:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:09:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:09:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:09:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:09:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:09:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:09:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:09:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:09:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:09:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:09:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:09:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=559907) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=559907) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=559907) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.70it/s]
(EngineCore_DP0 pid=559907) 
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=559907) [2026-01-26 14:09:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=559907) 2026-01-26 14:09:47,367 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=559907) 2026-01-26 14:09:47,389 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=559907) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 17.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 17.15it/s]
(EngineCore_DP0 pid=559907) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.26it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|███       | 39/128 [00:00<00:00, 384.89it/s]
Adding requests:  66%|██████▌   | 84/128 [00:00<00:00, 422.73it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 425.85it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:01, 100.38it/s, est. speed input: 102799.15 toks/s, output: 100.38 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:01, 57.16it/s, est. speed input: 62580.32 toks/s, output: 61.11 toks/s]   
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 51.71it/s, est. speed input: 57160.16 toks/s, output: 55.82 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:01, 49.03it/s, est. speed input: 54571.94 toks/s, output: 53.29 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 47.42it/s, est. speed input: 52941.47 toks/s, output: 51.70 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 46.52it/s, est. speed input: 51978.24 toks/s, output: 50.76 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 45.80it/s, est. speed input: 51206.14 toks/s, output: 50.01 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 45.31it/s, est. speed input: 50601.97 toks/s, output: 49.42 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 44.93it/s, est. speed input: 50095.27 toks/s, output: 48.92 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 44.66it/s, est. speed input: 49675.35 toks/s, output: 48.51 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 44.50it/s, est. speed input: 49329.54 toks/s, output: 48.17 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 44.33it/s, est. speed input: 49018.07 toks/s, output: 47.87 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 44.22it/s, est. speed input: 48751.28 toks/s, output: 47.61 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:00, 44.14it/s, est. speed input: 48517.17 toks/s, output: 47.38 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 43.79it/s, est. speed input: 48245.60 toks/s, output: 47.11 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 43.77it/s, est. speed input: 48050.49 toks/s, output: 46.92 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 43.69it/s, est. speed input: 47864.39 toks/s, output: 46.74 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 43.68it/s, est. speed input: 47705.50 toks/s, output: 46.59 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 43.71it/s, est. speed input: 47567.72 toks/s, output: 46.45 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 43.83it/s, est. speed input: 47459.34 toks/s, output: 46.35 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 43.87it/s, est. speed input: 47353.89 toks/s, output: 46.24 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 43.88it/s, est. speed input: 47253.30 toks/s, output: 46.15 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.88it/s, est. speed input: 47225.58 toks/s, output: 46.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 46.12it/s, est. speed input: 47225.58 toks/s, output: 46.12 toks/s]
[rank0]:[W126 14:09:52.671524193 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.1s

测试结果:
  Requests/s:   41.60
  Tokens/s:     42636.83
  Total Reqs:   128
  Elapsed:      3.08s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     42595.23

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:10:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=560965) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=560965) WARNING 01-26 14:10:15 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=560965) WARNING 01-26 14:10:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 78.11 requests/s, 80059.59 total tokens/s, 78.11 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 14:10:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:10:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:10:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:10:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:10:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:10:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:10:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:10:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:10:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:10:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:10:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:10:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:10:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:10:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=560965) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=560965) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.68it/s]
(EngineCore_DP0 pid=560965) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.68it/s]
(EngineCore_DP0 pid=560965) 
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=560965) [2026-01-26 14:10:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=560965) 2026-01-26 14:10:21,899 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=560965) 2026-01-26 14:10:21,921 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=560965) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.50it/s]
(EngineCore_DP0 pid=560965) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 22.48it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:47,  5.36it/s]
Adding requests:  16%|█▌        | 40/256 [00:00<00:01, 171.16it/s]
Adding requests:  33%|███▎      | 85/256 [00:00<00:00, 277.26it/s]
Adding requests:  50%|█████     | 129/256 [00:00<00:00, 331.59it/s]
Adding requests:  67%|██████▋   | 172/256 [00:00<00:00, 362.92it/s]
Adding requests:  84%|████████▍ | 215/256 [00:00<00:00, 383.86it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 327.49it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:00<00:00, 496.90it/s, est. speed input: 508871.53 toks/s, output: 496.91 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:00<00:01, 130.73it/s, est. speed input: 150510.86 toks/s, output: 146.98 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:00<00:01, 112.92it/s, est. speed input: 131333.98 toks/s, output: 128.26 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:01<00:01, 105.24it/s, est. speed input: 123689.39 toks/s, output: 120.79 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:01<00:00, 101.69it/s, est. speed input: 119918.53 toks/s, output: 117.11 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:00, 96.23it/s, est. speed input: 115878.96 toks/s, output: 113.16 toks/s] 
Processed prompts:  71%|███████▏  | 183/256 [00:01<00:00, 95.93it/s, est. speed input: 114542.88 toks/s, output: 111.86 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:01<00:00, 91.25it/s, est. speed input: 111805.77 toks/s, output: 109.18 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:01<00:00, 90.11it/s, est. speed input: 110377.35 toks/s, output: 107.79 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:02<00:00, 88.99it/s, est. speed input: 109065.32 toks/s, output: 106.51 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:02<00:00, 88.12it/s, est. speed input: 107896.28 toks/s, output: 105.37 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:02<00:00, 87.77it/s, est. speed input: 106921.53 toks/s, output: 104.42 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 87.40it/s, est. speed input: 106019.23 toks/s, output: 103.53 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:02<00:00, 87.24it/s, est. speed input: 105221.42 toks/s, output: 102.76 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 87.24it/s, est. speed input: 105085.38 toks/s, output: 102.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.62it/s, est. speed input: 105085.38 toks/s, output: 102.62 toks/s]
[rank0]:[W126 14:10:27.519999788 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.6s

测试结果:
  Requests/s:   78.11
  Tokens/s:     80059.59
  Total Reqs:   256
  Elapsed:      3.28s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     79981.49

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:10:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=562014) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=562014) WARNING 01-26 14:10:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=562014) WARNING 01-26 14:10:57 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 161.47 requests/s, 165506.87 total tokens/s, 161.47 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 14:10:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:10:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:10:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:10:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:10:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:10:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:10:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:10:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:10:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:10:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:10:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:10:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:10:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:10:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:10:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:10:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=562014) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=562014) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.02it/s]
(EngineCore_DP0 pid=562014) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.02it/s]
(EngineCore_DP0 pid=562014) 
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=562014) [2026-01-26 14:10:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=562014) 2026-01-26 14:10:57,483 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=562014) 2026-01-26 14:10:57,503 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=562014) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.13it/s]
(EngineCore_DP0 pid=562014) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.10it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 26/512 [00:00<00:01, 256.53it/s]
Adding requests:  13%|█▎        | 68/512 [00:00<00:01, 351.91it/s]
Adding requests:  22%|██▏       | 114/512 [00:00<00:01, 397.96it/s]
Adding requests:  31%|███       | 157/512 [00:00<00:00, 410.18it/s]
Adding requests:  39%|███▉      | 201/512 [00:00<00:00, 420.62it/s]
Adding requests:  48%|████▊     | 248/512 [00:00<00:00, 436.72it/s]
Adding requests:  57%|█████▋    | 293/512 [00:00<00:00, 441.03it/s]
Adding requests:  66%|██████▌   | 338/512 [00:00<00:00, 442.49it/s]
Adding requests:  75%|███████▌  | 384/512 [00:00<00:00, 446.87it/s]
Adding requests:  84%|████████▍ | 432/512 [00:01<00:00, 454.36it/s]
Adding requests:  93%|█████████▎| 478/512 [00:01<00:00, 453.84it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 431.82it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:00<00:00, 1794.35it/s, est. speed input: 1837535.01 toks/s, output: 1794.38 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:01<00:00, 278.68it/s, est. speed input: 328063.15 toks/s, output: 320.37 toks/s]   
Processed prompts:  89%|████████▊ | 454/512 [00:01<00:00, 236.42it/s, est. speed input: 281628.37 toks/s, output: 275.03 toks/s]
Processed prompts:  99%|█████████▉| 508/512 [00:01<00:00, 222.50it/s, est. speed input: 266821.14 toks/s, output: 260.57 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 222.50it/s, est. speed input: 264260.99 toks/s, output: 258.07 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 258.06it/s, est. speed input: 264260.99 toks/s, output: 258.07 toks/s]
[rank0]:[W126 14:11:03.567098554 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.0s

测试结果:
  Requests/s:   161.47
  Tokens/s:     165506.87
  Total Reqs:   512
  Elapsed:      3.17s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     165345.40

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:11:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=563103) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=563103) WARNING 01-26 14:11:29 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=563103) WARNING 01-26 14:11:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 308.26 requests/s, 315971.14 total tokens/s, 308.26 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 14:11:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:11:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:11:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:11:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:11:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:11:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:11:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:11:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:11:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:11:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:11:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:11:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:11:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:11:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:11:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:11:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:11:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:11:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:24] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:24] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:24] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:24] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=563103) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=563103) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.38it/s]
(EngineCore_DP0 pid=563103) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.38it/s]
(EngineCore_DP0 pid=563103) 
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=563103) [2026-01-26 14:11:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=563103) 2026-01-26 14:11:35,885 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=563103) 2026-01-26 14:11:35,912 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=563103) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 22.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 18.87it/s]
(EngineCore_DP0 pid=563103) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.13it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 38/1024 [00:00<00:02, 378.69it/s]
Adding requests:   8%|▊         | 83/1024 [00:00<00:02, 418.18it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 431.81it/s]
Adding requests:  17%|█▋        | 173/1024 [00:00<00:01, 435.65it/s]
Adding requests:  21%|██▏       | 219/1024 [00:00<00:01, 441.74it/s]
Adding requests:  26%|██▌       | 267/1024 [00:00<00:01, 452.89it/s]
Adding requests:  31%|███       | 313/1024 [00:00<00:01, 449.31it/s]
Adding requests:  35%|███▌      | 360/1024 [00:00<00:01, 452.60it/s]
Adding requests:  40%|███▉      | 408/1024 [00:00<00:01, 458.25it/s]
Adding requests:  44%|████▍     | 455/1024 [00:01<00:01, 461.20it/s]
Adding requests:  49%|████▉     | 502/1024 [00:01<00:01, 460.21it/s]
Adding requests:  54%|█████▎    | 549/1024 [00:01<00:01, 455.94it/s]
Adding requests:  58%|█████▊    | 597/1024 [00:01<00:00, 461.98it/s]
Adding requests:  63%|██████▎   | 645/1024 [00:01<00:00, 466.66it/s]
Adding requests:  68%|██████▊   | 694/1024 [00:01<00:00, 472.97it/s]
Adding requests:  72%|███████▏  | 742/1024 [00:01<00:00, 471.82it/s]
Adding requests:  77%|███████▋  | 790/1024 [00:01<00:00, 469.75it/s]
Adding requests:  82%|████████▏ | 837/1024 [00:01<00:00, 460.00it/s]
Adding requests:  87%|████████▋ | 886/1024 [00:01<00:00, 467.78it/s]
Adding requests:  91%|█████████▏| 935/1024 [00:02<00:00, 470.53it/s]
Adding requests:  96%|█████████▌| 983/1024 [00:02<00:00, 472.95it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 459.97it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:00<00:00, 6687.88it/s, est. speed input: 6848906.43 toks/s, output: 6688.03 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 6687.88it/s, est. speed input: 958173.60 toks/s, output: 935.71 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 935.61it/s, est. speed input: 958173.60 toks/s, output: 935.71 toks/s] 
[rank0]:[W126 14:11:41.210065475 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.6s

测试结果:
  Requests/s:   308.26
  Tokens/s:     315971.14
  Total Reqs:   1024
  Elapsed:      3.32s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     315662.87

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:11:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=564266) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=564266) WARNING 01-26 14:12:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=564266) WARNING 01-26 14:12:19 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 449.22 requests/s, 460450.70 total tokens/s, 449.22 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 14:11:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:11:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:11:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:11:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:11:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:11:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:11:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:11:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:11:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:11:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:12:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:12:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:12:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:12:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:12:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:12:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:12:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:12:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:12:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:07] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:07] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:07] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:07] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=564266) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=564266) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.40it/s]
(EngineCore_DP0 pid=564266) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.40it/s]
(EngineCore_DP0 pid=564266) 
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=564266) [2026-01-26 14:12:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=564266) 2026-01-26 14:12:19,129 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=564266) 2026-01-26 14:12:19,152 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=564266) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 15.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 17.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 19.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 18.45it/s]
(EngineCore_DP0 pid=564266) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 23.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 23.65it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 381.63it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 419.81it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 431.58it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:04, 436.43it/s]
Adding requests:  11%|█         | 221/2048 [00:00<00:04, 445.28it/s]
Adding requests:  13%|█▎        | 269/2048 [00:00<00:03, 455.92it/s]
Adding requests:  15%|█▌        | 315/2048 [00:00<00:03, 452.69it/s]
Adding requests:  18%|█▊        | 362/2048 [00:00<00:03, 457.49it/s]
Adding requests:  20%|█▉        | 409/2048 [00:00<00:03, 461.10it/s]
Adding requests:  22%|██▏       | 456/2048 [00:01<00:03, 463.52it/s]
Adding requests:  25%|██▍       | 503/2048 [00:01<00:03, 462.21it/s]
Adding requests:  27%|██▋       | 550/2048 [00:01<00:03, 456.58it/s]
Adding requests:  29%|██▉       | 596/2048 [00:01<00:03, 450.78it/s]
Adding requests:  31%|███▏      | 642/2048 [00:01<00:03, 448.61it/s]
Adding requests:  34%|███▎      | 688/2048 [00:01<00:03, 449.56it/s]
Adding requests:  36%|███▌      | 734/2048 [00:01<00:02, 450.98it/s]
Adding requests:  38%|███▊      | 780/2048 [00:01<00:02, 441.97it/s]
Adding requests:  40%|████      | 825/2048 [00:01<00:02, 434.00it/s]
Adding requests:  42%|████▏     | 869/2048 [00:01<00:02, 434.61it/s]
Adding requests:  45%|████▍     | 914/2048 [00:02<00:02, 438.36it/s]
Adding requests:  47%|████▋     | 959/2048 [00:02<00:02, 439.68it/s]
Adding requests:  49%|████▉     | 1004/2048 [00:02<00:02, 440.24it/s]
Adding requests:  51%|█████     | 1049/2048 [00:02<00:02, 442.29it/s]
Adding requests:  53%|█████▎    | 1094/2048 [00:02<00:02, 437.05it/s]
Adding requests:  56%|█████▌    | 1138/2048 [00:02<00:02, 428.48it/s]
Adding requests:  58%|█████▊    | 1186/2048 [00:02<00:01, 439.17it/s]
Adding requests:  60%|██████    | 1233/2048 [00:02<00:01, 445.04it/s]
Adding requests:  62%|██████▏   | 1278/2048 [00:02<00:01, 437.97it/s]
Adding requests:  65%|██████▍   | 1325/2048 [00:02<00:01, 445.23it/s]
Adding requests:  67%|██████▋   | 1371/2048 [00:03<00:01, 448.60it/s]
Adding requests:  69%|██████▉   | 1420/2048 [00:03<00:01, 458.87it/s]
Adding requests:  72%|███████▏  | 1469/2048 [00:03<00:01, 466.25it/s]
Adding requests:  74%|███████▍  | 1519/2048 [00:03<00:01, 473.29it/s]
Adding requests:  77%|███████▋  | 1567/2048 [00:03<00:01, 467.85it/s]
Adding requests:  79%|███████▉  | 1614/2048 [00:03<00:00, 467.56it/s]
Adding requests:  81%|████████  | 1662/2048 [00:03<00:00, 470.43it/s]
Adding requests:  83%|████████▎ | 1710/2048 [00:03<00:00, 471.66it/s]
Adding requests:  86%|████████▌ | 1758/2048 [00:03<00:00, 473.80it/s]
Adding requests:  88%|████████▊ | 1806/2048 [00:03<00:00, 472.92it/s]
Adding requests:  91%|█████████ | 1855/2048 [00:04<00:00, 476.33it/s]
Adding requests:  93%|█████████▎| 1903/2048 [00:04<00:00, 475.52it/s]
Adding requests:  95%|█████████▌| 1951/2048 [00:04<00:00, 475.94it/s]
Adding requests:  98%|█████████▊| 2000/2048 [00:04<00:00, 478.46it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 478.45it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 454.88it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 37081.36it/s, est. speed input: 37982136.24 toks/s, output: 37088.57 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 37010.74it/s, est. speed input: 37982136.24 toks/s, output: 37088.57 toks/s]
[rank0]:[W126 14:12:26.522390291 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.4s

测试结果:
  Requests/s:   449.22
  Tokens/s:     460450.70
  Total Reqs:   2048
  Elapsed:      4.56s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     460001.48

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:12:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=565544) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=565544) WARNING 01-26 14:13:06 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=565544) WARNING 01-26 14:13:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 458.78 requests/s, 470245.84 total tokens/s, 458.78 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 14:12:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:12:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:12:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:12:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:12:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:12:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:12:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:12:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:12:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:12:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:13:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:13:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:13:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:13:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:13:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:13:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:13:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:13:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:13:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:13:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:13:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:13:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:13:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:13:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:01] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:01] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=565544) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=565544) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.38it/s]
(EngineCore_DP0 pid=565544) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.38it/s]
(EngineCore_DP0 pid=565544) 
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=565544) [2026-01-26 14:13:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=565544) [rank0]:W0126 14:13:09.683000 565544 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=565544) [rank0]:W0126 14:13:09.752000 565544 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=565544) [rank0]:W0126 14:13:10.557000 565544 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=565544) [rank0]:W0126 14:13:10.655000 565544 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=565544) 2026-01-26 14:13:13,103 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=565544) 2026-01-26 14:13:13,139 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=565544) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 19.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 20.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 22.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 21.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 21.30it/s]
(EngineCore_DP0 pid=565544) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 23.40it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 21.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 21.44it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 39/4096 [00:00<00:10, 380.56it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 415.36it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 429.51it/s]
Adding requests:   4%|▍         | 174/4096 [00:00<00:09, 434.34it/s]
Adding requests:   5%|▌         | 220/4096 [00:00<00:08, 441.26it/s]
Adding requests:   7%|▋         | 268/4096 [00:00<00:08, 452.75it/s]
Adding requests:   8%|▊         | 314/4096 [00:00<00:08, 450.39it/s]
Adding requests:   9%|▉         | 360/4096 [00:00<00:08, 452.61it/s]
Adding requests:  10%|▉         | 407/4096 [00:00<00:08, 457.05it/s]
Adding requests:  11%|█         | 453/4096 [00:01<00:07, 457.56it/s]
Adding requests:  12%|█▏        | 499/4096 [00:01<00:07, 453.85it/s]
Adding requests:  13%|█▎        | 545/4096 [00:01<00:07, 447.01it/s]
Adding requests:  15%|█▍        | 594/4096 [00:01<00:07, 457.65it/s]
Adding requests:  16%|█▌        | 641/4096 [00:01<00:07, 459.68it/s]
Adding requests:  17%|█▋        | 689/4096 [00:01<00:07, 465.15it/s]
Adding requests:  18%|█▊        | 738/4096 [00:01<00:07, 470.11it/s]
Adding requests:  19%|█▉        | 786/4096 [00:01<00:07, 466.13it/s]
Adding requests:  20%|██        | 833/4096 [00:01<00:07, 450.32it/s]
Adding requests:  22%|██▏       | 881/4096 [00:01<00:07, 456.27it/s]
Adding requests:  23%|██▎       | 929/4096 [00:02<00:06, 463.16it/s]
Adding requests:  24%|██▍       | 977/4096 [00:02<00:06, 465.72it/s]
Adding requests:  25%|██▌       | 1025/4096 [00:02<00:06, 468.38it/s]
Adding requests:  26%|██▌       | 1072/4096 [00:02<00:06, 462.57it/s]
Adding requests:  27%|██▋       | 1119/4096 [00:02<00:06, 461.87it/s]
Adding requests:  29%|██▊       | 1168/4096 [00:02<00:06, 468.52it/s]
Adding requests:  30%|██▉       | 1217/4096 [00:02<00:06, 474.80it/s]
Adding requests:  31%|███       | 1265/4096 [00:02<00:06, 467.77it/s]
Adding requests:  32%|███▏      | 1312/4096 [00:02<00:05, 468.15it/s]
Adding requests:  33%|███▎      | 1361/4096 [00:02<00:05, 472.75it/s]
Adding requests:  34%|███▍      | 1411/4096 [00:03<00:05, 477.98it/s]
Adding requests:  36%|███▌      | 1459/4096 [00:03<00:05, 475.56it/s]
Adding requests:  37%|███▋      | 1507/4096 [00:03<00:05, 476.79it/s]
Adding requests:  38%|███▊      | 1555/4096 [00:03<00:05, 474.88it/s]
Adding requests:  39%|███▉      | 1605/4096 [00:03<00:05, 480.62it/s]
Adding requests:  40%|████      | 1654/4096 [00:03<00:05, 476.94it/s]
Adding requests:  42%|████▏     | 1702/4096 [00:03<00:05, 473.66it/s]
Adding requests:  43%|████▎     | 1750/4096 [00:03<00:04, 473.16it/s]
Adding requests:  44%|████▍     | 1798/4096 [00:03<00:05, 452.72it/s]
Adding requests:  45%|████▌     | 1844/4096 [00:04<00:05, 446.38it/s]
Adding requests:  46%|████▌     | 1891/4096 [00:04<00:04, 450.24it/s]
Adding requests:  47%|████▋     | 1938/4096 [00:04<00:04, 455.05it/s]
Adding requests:  48%|████▊     | 1985/4096 [00:04<00:04, 458.37it/s]
Adding requests:  50%|████▉     | 2033/4096 [00:04<00:04, 463.46it/s]
Adding requests:  51%|█████     | 2081/4096 [00:04<00:04, 468.03it/s]
Adding requests:  52%|█████▏    | 2128/4096 [00:04<00:04, 463.52it/s]
Adding requests:  53%|█████▎    | 2175/4096 [00:04<00:04, 460.96it/s]
Adding requests:  54%|█████▍    | 2222/4096 [00:04<00:04, 461.39it/s]
Adding requests:  55%|█████▌    | 2269/4096 [00:04<00:03, 463.25it/s]
Adding requests:  57%|█████▋    | 2318/4096 [00:05<00:03, 468.60it/s]
Adding requests:  58%|█████▊    | 2365/4096 [00:05<00:03, 465.53it/s]
Adding requests:  59%|█████▉    | 2412/4096 [00:05<00:03, 465.51it/s]
Adding requests:  60%|██████    | 2460/4096 [00:05<00:03, 468.95it/s]
Adding requests:  61%|██████    | 2507/4096 [00:05<00:03, 465.94it/s]
Adding requests:  62%|██████▏   | 2556/4096 [00:05<00:03, 470.77it/s]
Adding requests:  64%|██████▎   | 2604/4096 [00:05<00:03, 468.03it/s]
Adding requests:  65%|██████▍   | 2652/4096 [00:05<00:03, 471.45it/s]
Adding requests:  66%|██████▌   | 2700/4096 [00:05<00:02, 467.56it/s]
Adding requests:  67%|██████▋   | 2747/4096 [00:05<00:02, 465.92it/s]
Adding requests:  68%|██████▊   | 2794/4096 [00:06<00:02, 463.17it/s]
Adding requests:  69%|██████▉   | 2841/4096 [00:06<00:02, 464.00it/s]
Adding requests:  71%|███████   | 2889/4096 [00:06<00:02, 467.62it/s]
Adding requests:  72%|███████▏  | 2936/4096 [00:06<00:02, 462.29it/s]
Adding requests:  73%|███████▎  | 2983/4096 [00:06<00:02, 454.66it/s]
Adding requests:  74%|███████▍  | 3030/4096 [00:06<00:02, 456.99it/s]
Adding requests:  75%|███████▌  | 3076/4096 [00:06<00:02, 446.99it/s]
Adding requests:  76%|███████▋  | 3124/4096 [00:06<00:02, 454.92it/s]
Adding requests:  77%|███████▋  | 3170/4096 [00:06<00:02, 453.02it/s]
Adding requests:  79%|███████▊  | 3218/4096 [00:06<00:01, 456.77it/s]
Adding requests:  80%|███████▉  | 3266/4096 [00:07<00:01, 460.87it/s]
Adding requests:  81%|████████  | 3313/4096 [00:07<00:01, 462.71it/s]
Adding requests:  82%|████████▏ | 3361/4096 [00:07<00:01, 465.20it/s]
Adding requests:  83%|████████▎ | 3408/4096 [00:07<00:01, 464.84it/s]
Adding requests:  84%|████████▍ | 3455/4096 [00:07<00:01, 461.87it/s]
Adding requests:  85%|████████▌ | 3502/4096 [00:07<00:01, 460.11it/s]
Adding requests:  87%|████████▋ | 3549/4096 [00:07<00:01, 462.17it/s]
Adding requests:  88%|████████▊ | 3596/4096 [00:07<00:01, 462.32it/s]
Adding requests:  89%|████████▉ | 3643/4096 [00:07<00:00, 459.32it/s]
Adding requests:  90%|█████████ | 3690/4096 [00:07<00:00, 461.88it/s]
Adding requests:  91%|█████████ | 3737/4096 [00:08<00:00, 461.32it/s]
Adding requests:  92%|█████████▏| 3786/4096 [00:08<00:00, 468.59it/s]
Adding requests:  94%|█████████▎| 3834/4096 [00:08<00:00, 469.24it/s]
Adding requests:  95%|█████████▍| 3882/4096 [00:08<00:00, 470.77it/s]
Adding requests:  96%|█████████▌| 3930/4096 [00:08<00:00, 471.23it/s]
Adding requests:  97%|█████████▋| 3978/4096 [00:08<00:00, 469.88it/s]
Adding requests:  98%|█████████▊| 4025/4096 [00:08<00:00, 467.94it/s]
Adding requests:  99%|█████████▉| 4072/4096 [00:08<00:00, 463.56it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 462.29it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 62210.29it/s, est. speed input: 63717872.63 toks/s, output: 62220.43 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 62111.55it/s, est. speed input: 63717872.63 toks/s, output: 62220.43 toks/s]
[rank0]:[W126 14:13:24.205375091 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 58.8s

测试结果:
  Requests/s:   458.78
  Tokens/s:     470245.84
  Total Reqs:   4096
  Elapsed:      8.93s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     469787.06

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:14:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=567149) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=567149) WARNING 01-26 14:14:23 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=567149) WARNING 01-26 14:14:29 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 464.33 requests/s, 475937.09 total tokens/s, 464.33 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192


─── STDERR ───
[2026-01-26 14:14:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:14:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:14:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:14:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:14:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:14:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:14:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:14:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:14:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:14:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:14:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:14:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:14:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:14:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:14:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:14:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:14:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:14:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:14:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:18] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:18] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=567149) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=567149) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.40it/s]
(EngineCore_DP0 pid=567149) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.40it/s]
(EngineCore_DP0 pid=567149) 
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=567149) [2026-01-26 14:14:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=567149) [rank0]:W0126 14:14:26.516000 567149 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=567149) [rank0]:W0126 14:14:26.585000 567149 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=567149) [rank0]:W0126 14:14:27.384000 567149 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=567149) [rank0]:W0126 14:14:27.481000 567149 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=567149) 2026-01-26 14:14:29,794 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=567149) 2026-01-26 14:14:29,818 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=567149) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 18.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 20.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 21.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 21.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 21.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 22.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 21.09it/s]
(EngineCore_DP0 pid=567149) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 18.65it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 19.20it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 19.52it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 21.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 21.13it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 38/8192 [00:00<00:21, 375.18it/s]
Adding requests:   1%|          | 82/8192 [00:00<00:19, 410.22it/s]
Adding requests:   2%|▏         | 126/8192 [00:00<00:19, 422.20it/s]
Adding requests:   2%|▏         | 170/8192 [00:00<00:18, 427.56it/s]
Adding requests:   3%|▎         | 215/8192 [00:00<00:18, 433.10it/s]
Adding requests:   3%|▎         | 262/8192 [00:00<00:17, 444.71it/s]
Adding requests:   4%|▎         | 307/8192 [00:00<00:17, 444.59it/s]
Adding requests:   4%|▍         | 354/8192 [00:00<00:17, 450.32it/s]
Adding requests:   5%|▍         | 400/8192 [00:00<00:17, 452.99it/s]
Adding requests:   5%|▌         | 446/8192 [00:01<00:17, 455.01it/s]
Adding requests:   6%|▌         | 492/8192 [00:01<00:16, 453.08it/s]
Adding requests:   7%|▋         | 538/8192 [00:01<00:17, 441.60it/s]
Adding requests:   7%|▋         | 587/8192 [00:01<00:16, 454.75it/s]
Adding requests:   8%|▊         | 634/8192 [00:01<00:16, 457.83it/s]
Adding requests:   8%|▊         | 683/8192 [00:01<00:16, 465.39it/s]
Adding requests:   9%|▉         | 731/8192 [00:01<00:15, 468.67it/s]
Adding requests:   9%|▉         | 778/8192 [00:01<00:15, 463.80it/s]
Adding requests:  10%|█         | 825/8192 [00:01<00:16, 456.26it/s]
Adding requests:  11%|█         | 873/8192 [00:01<00:15, 460.38it/s]
Adding requests:  11%|█         | 921/8192 [00:02<00:15, 465.86it/s]
Adding requests:  12%|█▏        | 969/8192 [00:02<00:15, 466.49it/s]
Adding requests:  12%|█▏        | 1018/8192 [00:02<00:15, 470.14it/s]
Adding requests:  13%|█▎        | 1066/8192 [00:02<00:15, 469.38it/s]
Adding requests:  14%|█▎        | 1113/8192 [00:02<00:15, 465.63it/s]
Adding requests:  14%|█▍        | 1160/8192 [00:02<00:15, 466.22it/s]
Adding requests:  15%|█▍        | 1210/8192 [00:02<00:14, 475.09it/s]
Adding requests:  15%|█▌        | 1258/8192 [00:02<00:14, 468.17it/s]
Adding requests:  16%|█▌        | 1306/8192 [00:02<00:14, 469.00it/s]
Adding requests:  17%|█▋        | 1355/8192 [00:02<00:14, 473.09it/s]
Adding requests:  17%|█▋        | 1404/8192 [00:03<00:14, 476.89it/s]
Adding requests:  18%|█▊        | 1452/8192 [00:03<00:14, 473.93it/s]
Adding requests:  18%|█▊        | 1500/8192 [00:03<00:14, 468.62it/s]
Adding requests:  19%|█▉        | 1548/8192 [00:03<00:14, 470.24it/s]
Adding requests:  19%|█▉        | 1597/8192 [00:03<00:13, 476.00it/s]
Adding requests:  20%|██        | 1646/8192 [00:03<00:13, 478.08it/s]
Adding requests:  21%|██        | 1694/8192 [00:03<00:13, 472.40it/s]
Adding requests:  21%|██▏       | 1743/8192 [00:03<00:13, 475.33it/s]
Adding requests:  22%|██▏       | 1791/8192 [00:03<00:13, 472.98it/s]
Adding requests:  22%|██▏       | 1839/8192 [00:03<00:13, 472.84it/s]
Adding requests:  23%|██▎       | 1887/8192 [00:04<00:13, 474.07it/s]
Adding requests:  24%|██▎       | 1935/8192 [00:04<00:13, 473.31it/s]
Adding requests:  24%|██▍       | 1983/8192 [00:04<00:13, 473.84it/s]
Adding requests:  25%|██▍       | 2032/8192 [00:04<00:12, 476.14it/s]
Adding requests:  25%|██▌       | 2081/8192 [00:04<00:12, 479.12it/s]
Adding requests:  26%|██▌       | 2129/8192 [00:04<00:12, 472.93it/s]
Adding requests:  27%|██▋       | 2177/8192 [00:04<00:12, 467.68it/s]
Adding requests:  27%|██▋       | 2225/8192 [00:04<00:12, 469.78it/s]
Adding requests:  28%|██▊       | 2273/8192 [00:04<00:12, 470.11it/s]
Adding requests:  28%|██▊       | 2321/8192 [00:05<00:12, 472.74it/s]
Adding requests:  29%|██▉       | 2369/8192 [00:05<00:12, 471.23it/s]
Adding requests:  30%|██▉       | 2417/8192 [00:05<00:12, 471.29it/s]
Adding requests:  30%|███       | 2465/8192 [00:05<00:12, 473.35it/s]
Adding requests:  31%|███       | 2513/8192 [00:05<00:12, 466.21it/s]
Adding requests:  31%|███▏      | 2562/8192 [00:05<00:11, 470.87it/s]
Adding requests:  32%|███▏      | 2610/8192 [00:05<00:12, 460.54it/s]
Adding requests:  32%|███▏      | 2659/8192 [00:05<00:11, 466.98it/s]
Adding requests:  33%|███▎      | 2706/8192 [00:05<00:11, 462.92it/s]
Adding requests:  34%|███▎      | 2754/8192 [00:05<00:11, 464.64it/s]
Adding requests:  34%|███▍      | 2801/8192 [00:06<00:11, 463.27it/s]
Adding requests:  35%|███▍      | 2849/8192 [00:06<00:11, 466.83it/s]
Adding requests:  35%|███▌      | 2896/8192 [00:06<00:11, 467.76it/s]
Adding requests:  36%|███▌      | 2943/8192 [00:06<00:11, 464.99it/s]
Adding requests:  37%|███▋      | 2991/8192 [00:06<00:11, 468.61it/s]
Adding requests:  37%|███▋      | 3038/8192 [00:06<00:11, 467.45it/s]
Adding requests:  38%|███▊      | 3085/8192 [00:06<00:10, 465.37it/s]
Adding requests:  38%|███▊      | 3133/8192 [00:06<00:10, 469.48it/s]
Adding requests:  39%|███▉      | 3181/8192 [00:06<00:10, 470.65it/s]
Adding requests:  39%|███▉      | 3229/8192 [00:06<00:10, 469.23it/s]
Adding requests:  40%|████      | 3277/8192 [00:07<00:10, 470.80it/s]
Adding requests:  41%|████      | 3325/8192 [00:07<00:10, 470.72it/s]
Adding requests:  41%|████      | 3374/8192 [00:07<00:10, 475.93it/s]
Adding requests:  42%|████▏     | 3423/8192 [00:07<00:09, 479.16it/s]
Adding requests:  42%|████▏     | 3471/8192 [00:07<00:10, 466.51it/s]
Adding requests:  43%|████▎     | 3519/8192 [00:07<00:09, 469.67it/s]
Adding requests:  44%|████▎     | 3567/8192 [00:07<00:09, 466.59it/s]
Adding requests:  44%|████▍     | 3615/8192 [00:07<00:09, 466.82it/s]
Adding requests:  45%|████▍     | 3662/8192 [00:07<00:09, 465.42it/s]
Adding requests:  45%|████▌     | 3709/8192 [00:07<00:09, 457.04it/s]
Adding requests:  46%|████▌     | 3755/8192 [00:08<00:09, 444.25it/s]
Adding requests:  46%|████▋     | 3804/8192 [00:08<00:09, 455.65it/s]
Adding requests:  47%|████▋     | 3852/8192 [00:08<00:09, 461.61it/s]
Adding requests:  48%|████▊     | 3899/8192 [00:08<00:09, 463.20it/s]
Adding requests:  48%|████▊     | 3946/8192 [00:08<00:09, 464.51it/s]
Adding requests:  49%|████▊     | 3993/8192 [00:08<00:09, 462.26it/s]
Adding requests:  49%|████▉     | 4040/8192 [00:08<00:08, 462.89it/s]
Adding requests:  50%|████▉     | 4087/8192 [00:08<00:08, 464.83it/s]
Adding requests:  50%|█████     | 4135/8192 [00:08<00:08, 468.98it/s]
Adding requests:  51%|█████     | 4183/8192 [00:08<00:08, 469.25it/s]
Adding requests:  52%|█████▏    | 4230/8192 [00:09<00:08, 468.94it/s]
Adding requests:  52%|█████▏    | 4277/8192 [00:09<00:08, 468.39it/s]
Adding requests:  53%|█████▎    | 4326/8192 [00:09<00:08, 472.46it/s]
Adding requests:  53%|█████▎    | 4375/8192 [00:09<00:08, 476.42it/s]
Adding requests:  54%|█████▍    | 4423/8192 [00:09<00:07, 473.82it/s]
Adding requests:  55%|█████▍    | 4472/8192 [00:09<00:07, 477.34it/s]
Adding requests:  55%|█████▌    | 4520/8192 [00:09<00:07, 465.90it/s]
Adding requests:  56%|█████▌    | 4569/8192 [00:09<00:07, 470.56it/s]
Adding requests:  56%|█████▋    | 4617/8192 [00:09<00:07, 472.49it/s]
Adding requests:  57%|█████▋    | 4665/8192 [00:10<00:07, 472.57it/s]
Adding requests:  58%|█████▊    | 4713/8192 [00:10<00:07, 471.03it/s]
Adding requests:  58%|█████▊    | 4761/8192 [00:10<00:07, 471.36it/s]
Adding requests:  59%|█████▊    | 4809/8192 [00:10<00:07, 470.73it/s]
Adding requests:  59%|█████▉    | 4857/8192 [00:10<00:07, 471.73it/s]
Adding requests:  60%|█████▉    | 4905/8192 [00:10<00:07, 467.20it/s]
Adding requests:  60%|██████    | 4952/8192 [00:10<00:07, 460.10it/s]
Adding requests:  61%|██████    | 4999/8192 [00:10<00:06, 462.17it/s]
Adding requests:  62%|██████▏   | 5047/8192 [00:10<00:06, 467.38it/s]
Adding requests:  62%|██████▏   | 5096/8192 [00:10<00:06, 473.60it/s]
Adding requests:  63%|██████▎   | 5144/8192 [00:11<00:06, 472.09it/s]
Adding requests:  63%|██████▎   | 5192/8192 [00:11<00:06, 471.81it/s]
Adding requests:  64%|██████▍   | 5240/8192 [00:11<00:06, 469.45it/s]
Adding requests:  65%|██████▍   | 5287/8192 [00:11<00:06, 467.58it/s]
Adding requests:  65%|██████▌   | 5335/8192 [00:11<00:06, 471.13it/s]
Adding requests:  66%|██████▌   | 5383/8192 [00:11<00:05, 470.98it/s]
Adding requests:  66%|██████▋   | 5431/8192 [00:11<00:05, 472.62it/s]
Adding requests:  67%|██████▋   | 5479/8192 [00:11<00:05, 467.52it/s]
Adding requests:  67%|██████▋   | 5526/8192 [00:11<00:05, 464.83it/s]
Adding requests:  68%|██████▊   | 5573/8192 [00:11<00:05, 465.40it/s]
Adding requests:  69%|██████▊   | 5620/8192 [00:12<00:05, 466.60it/s]
Adding requests:  69%|██████▉   | 5667/8192 [00:12<00:05, 459.56it/s]
Adding requests:  70%|██████▉   | 5715/8192 [00:12<00:05, 464.61it/s]
Adding requests:  70%|███████   | 5763/8192 [00:12<00:05, 468.39it/s]
Adding requests:  71%|███████   | 5810/8192 [00:12<00:05, 463.84it/s]
Adding requests:  71%|███████▏  | 5857/8192 [00:12<00:05, 464.02it/s]
Adding requests:  72%|███████▏  | 5906/8192 [00:12<00:04, 468.68it/s]
Adding requests:  73%|███████▎  | 5954/8192 [00:12<00:04, 467.06it/s]
Adding requests:  73%|███████▎  | 6003/8192 [00:12<00:04, 470.93it/s]
Adding requests:  74%|███████▍  | 6052/8192 [00:12<00:04, 476.35it/s]
Adding requests:  74%|███████▍  | 6100/8192 [00:13<00:04, 445.44it/s]
Adding requests:  75%|███████▌  | 6148/8192 [00:13<00:04, 453.80it/s]
Adding requests:  76%|███████▌  | 6196/8192 [00:13<00:04, 459.49it/s]
Adding requests:  76%|███████▌  | 6245/8192 [00:13<00:04, 467.38it/s]
Adding requests:  77%|███████▋  | 6294/8192 [00:13<00:04, 472.91it/s]
Adding requests:  77%|███████▋  | 6343/8192 [00:13<00:03, 475.20it/s]
Adding requests:  78%|███████▊  | 6391/8192 [00:13<00:03, 476.20it/s]
Adding requests:  79%|███████▊  | 6441/8192 [00:13<00:03, 480.81it/s]
Adding requests:  79%|███████▉  | 6490/8192 [00:13<00:03, 480.10it/s]
Adding requests:  80%|███████▉  | 6540/8192 [00:14<00:03, 483.84it/s]
Adding requests:  80%|████████  | 6589/8192 [00:14<00:03, 480.51it/s]
Adding requests:  81%|████████  | 6638/8192 [00:14<00:03, 477.51it/s]
Adding requests:  82%|████████▏ | 6686/8192 [00:14<00:03, 475.81it/s]
Adding requests:  82%|████████▏ | 6735/8192 [00:14<00:03, 477.46it/s]
Adding requests:  83%|████████▎ | 6783/8192 [00:14<00:02, 476.08it/s]
Adding requests:  83%|████████▎ | 6832/8192 [00:14<00:02, 478.44it/s]
Adding requests:  84%|████████▍ | 6881/8192 [00:14<00:02, 478.94it/s]
Adding requests:  85%|████████▍ | 6931/8192 [00:14<00:02, 483.14it/s]
Adding requests:  85%|████████▌ | 6980/8192 [00:14<00:02, 478.90it/s]
Adding requests:  86%|████████▌ | 7028/8192 [00:15<00:02, 476.13it/s]
Adding requests:  86%|████████▋ | 7076/8192 [00:15<00:02, 473.91it/s]
Adding requests:  87%|████████▋ | 7125/8192 [00:15<00:02, 477.77it/s]
Adding requests:  88%|████████▊ | 7173/8192 [00:15<00:02, 471.02it/s]
Adding requests:  88%|████████▊ | 7222/8192 [00:15<00:02, 474.40it/s]
Adding requests:  89%|████████▊ | 7270/8192 [00:15<00:01, 467.96it/s]
Adding requests:  89%|████████▉ | 7318/8192 [00:15<00:01, 471.05it/s]
Adding requests:  90%|████████▉ | 7366/8192 [00:15<00:01, 468.29it/s]
Adding requests:  91%|█████████ | 7416/8192 [00:15<00:01, 476.53it/s]
Adding requests:  91%|█████████ | 7464/8192 [00:15<00:01, 477.10it/s]
Adding requests:  92%|█████████▏| 7513/8192 [00:16<00:01, 478.30it/s]
Adding requests:  92%|█████████▏| 7561/8192 [00:16<00:01, 472.78it/s]
Adding requests:  93%|█████████▎| 7609/8192 [00:16<00:01, 469.57it/s]
Adding requests:  93%|█████████▎| 7659/8192 [00:16<00:01, 475.64it/s]
Adding requests:  94%|█████████▍| 7708/8192 [00:16<00:01, 477.13it/s]
Adding requests:  95%|█████████▍| 7756/8192 [00:16<00:00, 473.81it/s]
Adding requests:  95%|█████████▌| 7804/8192 [00:16<00:00, 469.82it/s]
Adding requests:  96%|█████████▌| 7853/8192 [00:16<00:00, 472.91it/s]
Adding requests:  96%|█████████▋| 7901/8192 [00:16<00:00, 471.42it/s]
Adding requests:  97%|█████████▋| 7949/8192 [00:16<00:00, 468.25it/s]
Adding requests:  98%|█████████▊| 7997/8192 [00:17<00:00, 468.80it/s]
Adding requests:  98%|█████████▊| 8044/8192 [00:17<00:00, 465.52it/s]
Adding requests:  99%|█████████▉| 8093/8192 [00:17<00:00, 471.66it/s]
Adding requests:  99%|█████████▉| 8141/8192 [00:17<00:00, 470.75it/s]
Adding requests: 100%|█████████▉| 8190/8192 [00:17<00:00, 475.53it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 468.09it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  71%|███████   | 5807/8192 [00:00<00:00, 58068.26it/s, est. speed input: 59465296.25 toks/s, output: 58069.09 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 58068.26it/s, est. speed input: 60494993.34 toks/s, output: 59075.01 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 59024.47it/s, est. speed input: 60494993.34 toks/s, output: 59075.01 toks/s]
[rank0]:[W126 14:14:51.261244311 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 86.1s

测试结果:
  Requests/s:   464.33
  Tokens/s:     475937.09
  Total Reqs:   8192
  Elapsed:      17.64s

  [Prefill 分析]
  Total Prefill Tokens: 8388608
  Prefill Tokens/s:     475472.76


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/Llama3.2-1B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,42.9982,22058.0698,2.9769
1024,1024,1,128,128,41.5969,42636.8267,3.0772
2048,1024,2,256,128,78.1069,80059.5935,3.2776
4096,1024,4,512,128,161.4701,165506.8666,3.1709
8192,1024,8,1024,128,308.2645,315971.1371,3.3218
16384,1024,16,2048,128,449.2202,460450.6963,4.5590
32768,1024,32,4096,128,458.7764,470245.8355,8.9281
65536,1024,64,8192,128,464.3289,475937.0918,17.6427

------------------------------------------------------------

[INFO] 完成: 8 成功, 0 失败

============================================================
  Llama3.2-1B-FP8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:15:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=568383) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=568383) WARNING 01-26 14:15:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=568383) WARNING 01-26 14:15:19 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 40.67 requests/s, 20861.62 total tokens/s, 40.67 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 14:14:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:15:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:15:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:15:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:15:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:15:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:15:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:15:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:15:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:15:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:15:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:15:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:15:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:15:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=568383) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=568383) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.63it/s]
(EngineCore_DP0 pid=568383) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=568383) 
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=568383) 2026-01-26 14:15:19,907 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=568383) 2026-01-26 14:15:19,929 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=568383) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.05it/s]
(EngineCore_DP0 pid=568383) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.30it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  48%|████▊     | 62/128 [00:00<00:00, 612.16it/s]
Adding requests:  97%|█████████▋| 124/128 [00:00<00:00, 616.64it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 617.07it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:04, 25.16it/s, est. speed input: 12881.63 toks/s, output: 25.16 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 36.07it/s, est. speed input: 17608.35 toks/s, output: 34.39 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:02, 39.73it/s, est. speed input: 19251.62 toks/s, output: 37.60 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 41.59it/s, est. speed input: 20117.84 toks/s, output: 39.29 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 42.50it/s, est. speed input: 20607.86 toks/s, output: 40.25 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 43.15it/s, est. speed input: 20958.84 toks/s, output: 40.93 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 43.68it/s, est. speed input: 21234.70 toks/s, output: 41.47 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:02, 44.01it/s, est. speed input: 21440.55 toks/s, output: 41.88 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:01, 44.22it/s, est. speed input: 21598.20 toks/s, output: 42.18 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:01, 44.22it/s, est. speed input: 21702.91 toks/s, output: 42.39 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 44.27it/s, est. speed input: 21794.33 toks/s, output: 42.57 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 44.27it/s, est. speed input: 21867.48 toks/s, output: 42.71 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 44.33it/s, est. speed input: 21936.74 toks/s, output: 42.85 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 44.38it/s, est. speed input: 21996.22 toks/s, output: 42.96 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 44.40it/s, est. speed input: 22046.59 toks/s, output: 43.06 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 44.36it/s, est. speed input: 22085.12 toks/s, output: 43.13 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:01, 44.34it/s, est. speed input: 22120.31 toks/s, output: 43.20 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:00, 44.35it/s, est. speed input: 22153.45 toks/s, output: 43.27 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 44.19it/s, est. speed input: 22168.71 toks/s, output: 43.30 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 44.21it/s, est. speed input: 22192.91 toks/s, output: 43.35 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 44.18it/s, est. speed input: 22211.80 toks/s, output: 43.38 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 44.21it/s, est. speed input: 22232.37 toks/s, output: 43.42 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 44.19it/s, est. speed input: 22248.43 toks/s, output: 43.45 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 44.20it/s, est. speed input: 22265.11 toks/s, output: 43.49 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 44.27it/s, est. speed input: 22284.06 toks/s, output: 43.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.30it/s, est. speed input: 22300.80 toks/s, output: 43.56 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.30it/s, est. speed input: 22300.80 toks/s, output: 43.56 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.55it/s, est. speed input: 22300.80 toks/s, output: 43.56 toks/s]
[rank0]:[W126 14:15:25.979421060 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.7s

测试结果:
  Requests/s:   40.67
  Tokens/s:     20861.62
  Total Reqs:   128
  Elapsed:      3.15s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     20820.96

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:15:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=569501) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=569501) WARNING 01-26 14:15:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=569501) WARNING 01-26 14:15:54 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.09 requests/s, 42115.22 total tokens/s, 41.09 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 14:15:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:15:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:15:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:15:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:15:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:15:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:15:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:15:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:15:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:15:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:15:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:15:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:15:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:15:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:42] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:42] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:42] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:42] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:42] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=569501) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=569501) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=569501) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=569501) 
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=569501) 2026-01-26 14:15:54,673 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=569501) 2026-01-26 14:15:54,695 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=569501) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 16.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 16.18it/s]
(EngineCore_DP0 pid=569501) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 18.71it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|██▉       | 38/128 [00:00<00:00, 378.55it/s]
Adding requests:  65%|██████▍   | 83/128 [00:00<00:00, 420.00it/s]
Adding requests:  98%|█████████▊| 126/128 [00:00<00:00, 219.39it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 250.73it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:00, 187.19it/s, est. speed input: 191697.68 toks/s, output: 187.20 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 64.27it/s, est. speed input: 73211.43 toks/s, output: 71.49 toks/s]   
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 55.60it/s, est. speed input: 64051.43 toks/s, output: 62.55 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:00<00:01, 51.90it/s, est. speed input: 60346.72 toks/s, output: 58.93 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 49.65it/s, est. speed input: 58149.51 toks/s, output: 56.79 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 48.11it/s, est. speed input: 56688.37 toks/s, output: 55.36 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 46.91it/s, est. speed input: 55515.99 toks/s, output: 54.21 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 45.47it/s, est. speed input: 54456.99 toks/s, output: 53.18 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 44.97it/s, est. speed input: 53769.78 toks/s, output: 52.51 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 44.51it/s, est. speed input: 53150.65 toks/s, output: 51.90 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 44.23it/s, est. speed input: 52624.42 toks/s, output: 51.39 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 43.70it/s, est. speed input: 52080.95 toks/s, output: 50.86 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 43.76it/s, est. speed input: 51699.81 toks/s, output: 50.49 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 43.71it/s, est. speed input: 51336.49 toks/s, output: 50.13 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 43.67it/s, est. speed input: 51008.19 toks/s, output: 49.81 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 43.40it/s, est. speed input: 50666.24 toks/s, output: 49.48 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 43.43it/s, est. speed input: 50393.89 toks/s, output: 49.21 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.43it/s, est. speed input: 50341.72 toks/s, output: 49.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 49.16it/s, est. speed input: 50341.72 toks/s, output: 49.16 toks/s]
[rank0]:[W126 14:15:59.094599117 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.9s

测试结果:
  Requests/s:   41.09
  Tokens/s:     42115.22
  Total Reqs:   128
  Elapsed:      3.12s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     42074.13

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:16:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=570545) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=570545) WARNING 01-26 14:16:22 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=570545) WARNING 01-26 14:16:29 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 82.56 requests/s, 84620.47 total tokens/s, 82.56 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 14:16:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:16:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:16:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:16:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:16:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:16:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:16:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:16:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:16:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:16:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:16:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:16:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:16:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:16:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=570545) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=570545) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.83it/s]
(EngineCore_DP0 pid=570545) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.83it/s]
(EngineCore_DP0 pid=570545) 
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=570545) 2026-01-26 14:16:29,013 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=570545) 2026-01-26 14:16:29,035 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=570545) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.14it/s]
(EngineCore_DP0 pid=570545) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 22.51it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  15%|█▌        | 39/256 [00:00<00:00, 382.74it/s]
Adding requests:  33%|███▎      | 84/256 [00:00<00:00, 419.81it/s]
Adding requests:  50%|█████     | 128/256 [00:00<00:00, 426.87it/s]
Adding requests:  67%|██████▋   | 171/256 [00:00<00:00, 425.51it/s]
Adding requests:  84%|████████▎ | 214/256 [00:00<00:00, 427.02it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 428.31it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:00<00:00, 433.95it/s, est. speed input: 444408.41 toks/s, output: 433.96 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:00<00:01, 133.93it/s, est. speed input: 154157.85 toks/s, output: 150.54 toks/s]
Processed prompts:  46%|████▌     | 117/256 [00:00<00:01, 117.22it/s, est. speed input: 136224.64 toks/s, output: 133.03 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:01<00:01, 106.23it/s, est. speed input: 126130.13 toks/s, output: 123.17 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:01<00:01, 101.14it/s, est. speed input: 121227.98 toks/s, output: 118.39 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:01<00:00, 97.61it/s, est. speed input: 117940.37 toks/s, output: 115.18 toks/s] 
Processed prompts:  67%|██████▋   | 171/256 [00:01<00:00, 97.07it/s, est. speed input: 116367.82 toks/s, output: 113.64 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:01<00:00, 92.11it/s, est. speed input: 113368.10 toks/s, output: 110.71 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:01<00:00, 90.67it/s, est. speed input: 111719.49 toks/s, output: 109.10 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:01<00:00, 89.59it/s, est. speed input: 110292.69 toks/s, output: 107.71 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:01<00:00, 88.70it/s, est. speed input: 109019.46 toks/s, output: 106.46 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:02<00:00, 87.96it/s, est. speed input: 107866.76 toks/s, output: 105.34 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:02<00:00, 87.33it/s, est. speed input: 106815.82 toks/s, output: 104.31 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:02<00:00, 87.06it/s, est. speed input: 105907.55 toks/s, output: 103.43 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:02<00:00, 86.86it/s, est. speed input: 105085.32 toks/s, output: 102.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 86.86it/s, est. speed input: 104779.68 toks/s, output: 102.32 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.32it/s, est. speed input: 104779.68 toks/s, output: 102.32 toks/s]
[rank0]:[W126 14:16:34.719046908 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.5s

测试结果:
  Requests/s:   82.56
  Tokens/s:     84620.47
  Total Reqs:   256
  Elapsed:      3.10s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     84537.91

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:16:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=571601) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=571601) WARNING 01-26 14:16:58 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=571601) WARNING 01-26 14:17:04 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 157.69 requests/s, 161637.08 total tokens/s, 157.69 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 14:16:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:16:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:16:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:16:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:16:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:16:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:16:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:16:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:16:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:16:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:16:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:16:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:16:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:16:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=571601) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=571601) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.48it/s]
(EngineCore_DP0 pid=571601) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.48it/s]
(EngineCore_DP0 pid=571601) 
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=571601) 2026-01-26 14:17:04,816 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=571601) 2026-01-26 14:17:04,838 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=571601) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 22.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 19.68it/s]
(EngineCore_DP0 pid=571601) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.25it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.23it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 39/512 [00:00<00:01, 380.84it/s]
Adding requests:  16%|█▋        | 84/512 [00:00<00:01, 418.74it/s]
Adding requests:  25%|██▌       | 129/512 [00:00<00:00, 430.77it/s]
Adding requests:  34%|███▍      | 174/512 [00:00<00:00, 435.41it/s]
Adding requests:  43%|████▎     | 220/512 [00:00<00:00, 443.90it/s]
Adding requests:  52%|█████▏    | 268/512 [00:00<00:00, 454.14it/s]
Adding requests:  61%|██████▏   | 314/512 [00:00<00:00, 452.20it/s]
Adding requests:  70%|███████   | 360/512 [00:00<00:00, 453.35it/s]
Adding requests:  80%|███████▉  | 408/512 [00:00<00:00, 459.02it/s]
Adding requests:  89%|████████▊ | 454/512 [00:01<00:00, 457.87it/s]
Adding requests:  98%|█████████▊| 500/512 [00:01<00:00, 457.39it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 449.18it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:00<00:00, 1775.49it/s, est. speed input: 1818229.30 toks/s, output: 1775.53 toks/s]
Processed prompts:  70%|███████   | 360/512 [00:01<00:00, 262.09it/s, est. speed input: 308234.28 toks/s, output: 301.01 toks/s]   
Processed prompts:  87%|████████▋ | 443/512 [00:01<00:00, 226.63it/s, est. speed input: 268445.95 toks/s, output: 262.15 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [00:02<00:00, 211.96it/s, est. speed input: 253525.20 toks/s, output: 247.58 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 211.96it/s, est. speed input: 248968.92 toks/s, output: 243.13 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 243.12it/s, est. speed input: 248968.92 toks/s, output: 243.13 toks/s]
[rank0]:[W126 14:17:10.700311667 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.1s

测试结果:
  Requests/s:   157.69
  Tokens/s:     161637.08
  Total Reqs:   512
  Elapsed:      3.25s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     161479.38

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:17:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=572684) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=572684) WARNING 01-26 14:17:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=572684) WARNING 01-26 14:17:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 306.29 requests/s, 313949.15 total tokens/s, 306.29 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 14:17:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:17:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:17:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:17:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:17:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:17:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:17:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:17:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:17:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:17:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:17:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:17:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:17:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:17:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:17:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:17:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:17:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:17:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:31] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:31] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:31] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:31] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:31] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=572684) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=572684) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=572684) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.61it/s]
(EngineCore_DP0 pid=572684) 
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=572684) 2026-01-26 14:17:43,045 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=572684) 2026-01-26 14:17:43,067 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=572684) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 19.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 18.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 19.08it/s]
(EngineCore_DP0 pid=572684) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.82it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 37/1024 [00:00<00:02, 363.92it/s]
Adding requests:   8%|▊         | 81/1024 [00:00<00:02, 407.84it/s]
Adding requests:  12%|█▏        | 126/1024 [00:00<00:02, 425.17it/s]
Adding requests:  17%|█▋        | 169/1024 [00:00<00:02, 408.73it/s]
Adding requests:  21%|██        | 213/1024 [00:00<00:01, 419.26it/s]
Adding requests:  25%|██▌       | 261/1024 [00:00<00:01, 437.63it/s]
Adding requests:  30%|██▉       | 305/1024 [00:00<00:01, 437.15it/s]
Adding requests:  34%|███▍      | 351/1024 [00:00<00:01, 442.80it/s]
Adding requests:  39%|███▉      | 397/1024 [00:00<00:01, 447.21it/s]
Adding requests:  43%|████▎     | 443/1024 [00:01<00:01, 450.80it/s]
Adding requests:  48%|████▊     | 489/1024 [00:01<00:01, 451.84it/s]
Adding requests:  52%|█████▏    | 535/1024 [00:01<00:01, 445.24it/s]
Adding requests:  57%|█████▋    | 583/1024 [00:01<00:00, 454.26it/s]
Adding requests:  62%|██████▏   | 631/1024 [00:01<00:00, 460.24it/s]
Adding requests:  66%|██████▋   | 679/1024 [00:01<00:00, 463.37it/s]
Adding requests:  71%|███████   | 728/1024 [00:01<00:00, 468.78it/s]
Adding requests:  76%|███████▌  | 775/1024 [00:01<00:00, 464.82it/s]
Adding requests:  80%|████████  | 822/1024 [00:01<00:00, 456.81it/s]
Adding requests:  85%|████████▍ | 869/1024 [00:01<00:00, 460.62it/s]
Adding requests:  90%|████████▉ | 918/1024 [00:02<00:00, 465.72it/s]
Adding requests:  94%|█████████▍| 966/1024 [00:02<00:00, 468.53it/s]
Adding requests:  99%|█████████▉| 1013/1024 [00:02<00:00, 466.95it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 450.90it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:00<00:00, 6899.63it/s, est. speed input: 7065707.81 toks/s, output: 6899.74 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 6899.63it/s, est. speed input: 979120.66 toks/s, output: 956.17 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 956.07it/s, est. speed input: 979120.66 toks/s, output: 956.17 toks/s] 
[rank0]:[W126 14:17:49.274160156 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.5s

测试结果:
  Requests/s:   306.29
  Tokens/s:     313949.15
  Total Reqs:   1024
  Elapsed:      3.34s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     313642.86

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:18:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=573845) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=573845) WARNING 01-26 14:18:20 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=573845) WARNING 01-26 14:18:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 455.29 requests/s, 466671.42 total tokens/s, 455.29 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 14:18:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:18:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:18:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:18:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:18:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:18:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:18:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:18:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:18:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:18:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:18:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:18:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:18:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:18:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:18:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:18:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:18:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:18:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=573845) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=573845) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=573845) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.64it/s]
(EngineCore_DP0 pid=573845) 
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=573845) 2026-01-26 14:18:26,797 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=573845) 2026-01-26 14:18:26,821 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=573845) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 17.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 20.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 19.41it/s]
(EngineCore_DP0 pid=573845) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 23.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 23.06it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 381.37it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 419.27it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 432.15it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:04, 436.31it/s]
Adding requests:  11%|█         | 218/2048 [00:00<00:04, 433.70it/s]
Adding requests:  13%|█▎        | 266/2048 [00:00<00:03, 447.90it/s]
Adding requests:  15%|█▌        | 312/2048 [00:00<00:03, 449.12it/s]
Adding requests:  17%|█▋        | 358/2048 [00:00<00:03, 451.63it/s]
Adding requests:  20%|█▉        | 405/2048 [00:00<00:03, 457.13it/s]
Adding requests:  22%|██▏       | 452/2048 [00:01<00:03, 460.66it/s]
Adding requests:  24%|██▍       | 499/2048 [00:01<00:03, 461.17it/s]
Adding requests:  27%|██▋       | 546/2048 [00:01<00:03, 454.52it/s]
Adding requests:  29%|██▉       | 593/2048 [00:01<00:03, 457.92it/s]
Adding requests:  31%|███▏      | 641/2048 [00:01<00:03, 462.13it/s]
Adding requests:  34%|███▎      | 689/2048 [00:01<00:02, 467.17it/s]
Adding requests:  36%|███▌      | 738/2048 [00:01<00:02, 472.75it/s]
Adding requests:  38%|███▊      | 786/2048 [00:01<00:02, 467.99it/s]
Adding requests:  41%|████      | 833/2048 [00:01<00:02, 459.54it/s]
Adding requests:  43%|████▎     | 880/2048 [00:01<00:02, 462.00it/s]
Adding requests:  45%|████▌     | 928/2048 [00:02<00:02, 467.22it/s]
Adding requests:  48%|████▊     | 976/2048 [00:02<00:02, 470.82it/s]
Adding requests:  50%|█████     | 1024/2048 [00:02<00:02, 471.74it/s]
Adding requests:  52%|█████▏    | 1072/2048 [00:02<00:02, 465.40it/s]
Adding requests:  55%|█████▍    | 1119/2048 [00:02<00:02, 463.19it/s]
Adding requests:  57%|█████▋    | 1166/2048 [00:02<00:01, 456.34it/s]
Adding requests:  59%|█████▉    | 1212/2048 [00:02<00:01, 455.69it/s]
Adding requests:  61%|██████▏   | 1258/2048 [00:02<00:01, 452.68it/s]
Adding requests:  64%|██████▎   | 1305/2048 [00:02<00:01, 456.61it/s]
Adding requests:  66%|██████▌   | 1353/2048 [00:02<00:01, 460.92it/s]
Adding requests:  68%|██████▊   | 1401/2048 [00:03<00:01, 466.54it/s]
Adding requests:  71%|███████   | 1448/2048 [00:03<00:01, 465.53it/s]
Adding requests:  73%|███████▎  | 1497/2048 [00:03<00:01, 472.26it/s]
Adding requests:  75%|███████▌  | 1545/2048 [00:03<00:01, 472.22it/s]
Adding requests:  78%|███████▊  | 1594/2048 [00:03<00:00, 476.36it/s]
Adding requests:  80%|████████  | 1642/2048 [00:03<00:00, 465.33it/s]
Adding requests:  82%|████████▏ | 1689/2048 [00:03<00:00, 462.17it/s]
Adding requests:  85%|████████▍ | 1738/2048 [00:03<00:00, 468.13it/s]
Adding requests:  87%|████████▋ | 1785/2048 [00:03<00:00, 462.02it/s]
Adding requests:  90%|████████▉ | 1833/2048 [00:03<00:00, 466.24it/s]
Adding requests:  92%|█████████▏| 1881/2048 [00:04<00:00, 467.86it/s]
Adding requests:  94%|█████████▍| 1928/2048 [00:04<00:00, 467.68it/s]
Adding requests:  96%|█████████▋| 1975/2048 [00:04<00:00, 468.23it/s]
Adding requests:  99%|█████████▉| 2024/2048 [00:04<00:00, 473.69it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 461.38it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 35348.80it/s, est. speed input: 36207151.71 toks/s, output: 35355.20 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 35287.81it/s, est. speed input: 36207151.71 toks/s, output: 35355.20 toks/s]
[rank0]:[W126 14:18:33.087215972 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.8s

测试结果:
  Requests/s:   455.29
  Tokens/s:     466671.42
  Total Reqs:   2048
  Elapsed:      4.50s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     466216.13

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:19:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=575134) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=575134) WARNING 01-26 14:19:13 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=575134) WARNING 01-26 14:19:20 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 461.01 requests/s, 472539.37 total tokens/s, 461.01 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 14:19:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:19:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:19:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:19:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:19:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:19:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:19:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:19:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:19:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:19:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:19:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:19:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:19:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:19:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:19:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:19:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:19:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:19:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=575134) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=575134) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=575134) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=575134) 
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=575134) [rank0]:W0126 14:19:17.238000 575134 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=575134) [rank0]:W0126 14:19:17.306000 575134 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=575134) [rank0]:W0126 14:19:18.116000 575134 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=575134) [rank0]:W0126 14:19:18.214000 575134 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=575134) 2026-01-26 14:19:20,635 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=575134) 2026-01-26 14:19:20,660 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=575134) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 19.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 20.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 22.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 21.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 21.30it/s]
(EngineCore_DP0 pid=575134) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 23.41it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 20.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 20.07it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 38/4096 [00:00<00:10, 378.85it/s]
Adding requests:   2%|▏         | 82/4096 [00:00<00:09, 413.46it/s]
Adding requests:   3%|▎         | 127/4096 [00:00<00:09, 429.19it/s]
Adding requests:   4%|▍         | 172/4096 [00:00<00:09, 434.88it/s]
Adding requests:   5%|▌         | 217/4096 [00:00<00:08, 439.75it/s]
Adding requests:   6%|▋         | 265/4096 [00:00<00:08, 452.53it/s]
Adding requests:   8%|▊         | 311/4096 [00:00<00:08, 451.60it/s]
Adding requests:   9%|▊         | 358/4096 [00:00<00:08, 454.85it/s]
Adding requests:  10%|▉         | 405/4096 [00:00<00:08, 457.97it/s]
Adding requests:  11%|█         | 451/4096 [00:01<00:08, 452.82it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:07, 456.19it/s]
Adding requests:  13%|█▎        | 544/4096 [00:01<00:07, 449.46it/s]
Adding requests:  14%|█▍        | 593/4096 [00:01<00:07, 458.88it/s]
Adding requests:  16%|█▌        | 640/4096 [00:01<00:07, 461.32it/s]
Adding requests:  17%|█▋        | 689/4096 [00:01<00:07, 466.91it/s]
Adding requests:  18%|█▊        | 738/4096 [00:01<00:07, 471.56it/s]
Adding requests:  19%|█▉        | 786/4096 [00:01<00:07, 467.58it/s]
Adding requests:  20%|██        | 833/4096 [00:01<00:07, 458.03it/s]
Adding requests:  21%|██▏       | 879/4096 [00:01<00:07, 454.09it/s]
Adding requests:  23%|██▎       | 927/4096 [00:02<00:06, 460.94it/s]
Adding requests:  24%|██▍       | 974/4096 [00:02<00:06, 463.33it/s]
Adding requests:  25%|██▍       | 1022/4096 [00:02<00:06, 466.73it/s]
Adding requests:  26%|██▌       | 1069/4096 [00:02<00:06, 465.04it/s]
Adding requests:  27%|██▋       | 1116/4096 [00:02<00:06, 460.27it/s]
Adding requests:  28%|██▊       | 1165/4096 [00:02<00:06, 467.36it/s]
Adding requests:  30%|██▉       | 1215/4096 [00:02<00:06, 474.46it/s]
Adding requests:  31%|███       | 1263/4096 [00:02<00:06, 468.05it/s]
Adding requests:  32%|███▏      | 1311/4096 [00:02<00:05, 468.55it/s]
Adding requests:  33%|███▎      | 1359/4096 [00:02<00:05, 471.24it/s]
Adding requests:  34%|███▍      | 1409/4096 [00:03<00:05, 477.47it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:03<00:05, 475.16it/s]
Adding requests:  37%|███▋      | 1506/4096 [00:03<00:05, 476.64it/s]
Adding requests:  38%|███▊      | 1554/4096 [00:03<00:05, 475.99it/s]
Adding requests:  39%|███▉      | 1604/4096 [00:03<00:05, 482.37it/s]
Adding requests:  40%|████      | 1653/4096 [00:03<00:05, 478.26it/s]
Adding requests:  42%|████▏     | 1701/4096 [00:03<00:05, 473.92it/s]
Adding requests:  43%|████▎     | 1749/4096 [00:03<00:05, 454.62it/s]
Adding requests:  44%|████▍     | 1797/4096 [00:03<00:04, 460.01it/s]
Adding requests:  45%|████▌     | 1845/4096 [00:03<00:04, 464.20it/s]
Adding requests:  46%|████▌     | 1892/4096 [00:04<00:04, 464.52it/s]
Adding requests:  47%|████▋     | 1939/4096 [00:04<00:04, 455.67it/s]
Adding requests:  49%|████▊     | 1987/4096 [00:04<00:04, 460.27it/s]
Adding requests:  50%|████▉     | 2036/4096 [00:04<00:04, 466.28it/s]
Adding requests:  51%|█████     | 2085/4096 [00:04<00:04, 471.69it/s]
Adding requests:  52%|█████▏    | 2133/4096 [00:04<00:04, 468.33it/s]
Adding requests:  53%|█████▎    | 2180/4096 [00:04<00:04, 453.31it/s]
Adding requests:  54%|█████▍    | 2228/4096 [00:04<00:04, 458.81it/s]
Adding requests:  56%|█████▌    | 2275/4096 [00:04<00:03, 461.86it/s]
Adding requests:  57%|█████▋    | 2323/4096 [00:05<00:03, 465.85it/s]
Adding requests:  58%|█████▊    | 2370/4096 [00:05<00:03, 466.13it/s]
Adding requests:  59%|█████▉    | 2417/4096 [00:05<00:03, 466.76it/s]
Adding requests:  60%|██████    | 2465/4096 [00:05<00:03, 469.65it/s]
Adding requests:  61%|██████▏   | 2512/4096 [00:05<00:03, 468.77it/s]
Adding requests:  63%|██████▎   | 2561/4096 [00:05<00:03, 472.91it/s]
Adding requests:  64%|██████▎   | 2609/4096 [00:05<00:03, 471.26it/s]
Adding requests:  65%|██████▍   | 2658/4096 [00:05<00:03, 476.07it/s]
Adding requests:  66%|██████▌   | 2706/4096 [00:05<00:02, 468.21it/s]
Adding requests:  67%|██████▋   | 2754/4096 [00:05<00:02, 468.04it/s]
Adding requests:  68%|██████▊   | 2801/4096 [00:06<00:02, 465.46it/s]
Adding requests:  70%|██████▉   | 2849/4096 [00:06<00:02, 468.28it/s]
Adding requests:  71%|███████   | 2896/4096 [00:06<00:02, 468.79it/s]
Adding requests:  72%|███████▏  | 2943/4096 [00:06<00:02, 464.81it/s]
Adding requests:  73%|███████▎  | 2991/4096 [00:06<00:02, 467.87it/s]
Adding requests:  74%|███████▍  | 3038/4096 [00:06<00:02, 466.32it/s]
Adding requests:  75%|███████▌  | 3085/4096 [00:06<00:02, 452.87it/s]
Adding requests:  76%|███████▋  | 3133/4096 [00:06<00:02, 459.63it/s]
Adding requests:  78%|███████▊  | 3180/4096 [00:06<00:01, 462.27it/s]
Adding requests:  79%|███████▉  | 3227/4096 [00:06<00:01, 462.48it/s]
Adding requests:  80%|███████▉  | 3275/4096 [00:07<00:01, 465.74it/s]
Adding requests:  81%|████████  | 3323/4096 [00:07<00:01, 467.92it/s]
Adding requests:  82%|████████▏ | 3371/4096 [00:07<00:01, 469.05it/s]
Adding requests:  83%|████████▎ | 3420/4096 [00:07<00:01, 474.01it/s]
Adding requests:  85%|████████▍ | 3468/4096 [00:07<00:01, 465.27it/s]
Adding requests:  86%|████████▌ | 3515/4096 [00:07<00:01, 466.13it/s]
Adding requests:  87%|████████▋ | 3562/4096 [00:07<00:01, 463.38it/s]
Adding requests:  88%|████████▊ | 3610/4096 [00:07<00:01, 465.55it/s]
Adding requests:  89%|████████▉ | 3657/4096 [00:07<00:00, 462.88it/s]
Adding requests:  90%|█████████ | 3705/4096 [00:07<00:00, 467.57it/s]
Adding requests:  92%|█████████▏| 3752/4096 [00:08<00:00, 466.30it/s]
Adding requests:  93%|█████████▎| 3801/4096 [00:08<00:00, 472.69it/s]
Adding requests:  94%|█████████▍| 3849/4096 [00:08<00:00, 474.41it/s]
Adding requests:  95%|█████████▌| 3897/4096 [00:08<00:00, 474.95it/s]
Adding requests:  96%|█████████▋| 3945/4096 [00:08<00:00, 474.36it/s]
Adding requests:  97%|█████████▋| 3993/4096 [00:08<00:00, 469.33it/s]
Adding requests:  99%|█████████▊| 4041/4096 [00:08<00:00, 469.61it/s]
Adding requests: 100%|█████████▉| 4088/4096 [00:08<00:00, 469.72it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 464.56it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 62167.52it/s, est. speed input: 63673823.66 toks/s, output: 62177.19 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 62063.98it/s, est. speed input: 63673823.66 toks/s, output: 62177.19 toks/s]
[rank0]:[W126 14:19:32.663273124 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 58.7s

测试结果:
  Requests/s:   461.01
  Tokens/s:     472539.37
  Total Reqs:   4096
  Elapsed:      8.88s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     472078.35

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:20:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=576744) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=576744) WARNING 01-26 14:20:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=576744) WARNING 01-26 14:20:38 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 468.88 requests/s, 480603.23 total tokens/s, 468.88 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192


─── STDERR ───
[2026-01-26 14:20:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:20:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:20:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:20:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:20:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:20:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:20:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:20:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:20:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:20:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:20:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:20:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:20:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:20:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:20:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:20:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:20:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:20:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=576744) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=576744) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.15it/s]
(EngineCore_DP0 pid=576744) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.15it/s]
(EngineCore_DP0 pid=576744) 
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=576744) [rank0]:W0126 14:20:34.737000 576744 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=576744) [rank0]:W0126 14:20:34.805000 576744 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=576744) [rank0]:W0126 14:20:35.612000 576744 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=576744) [rank0]:W0126 14:20:35.710000 576744 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=576744) 2026-01-26 14:20:38,097 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=576744) 2026-01-26 14:20:38,121 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=576744) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:03,  5.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:02,  5.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  9.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01, 12.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:00, 15.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:00<00:00, 17.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:00<00:00, 19.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:01<00:00, 20.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 16.14it/s]
(EngineCore_DP0 pid=576744) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 23.13it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 23.28it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 23.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.20it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 38/8192 [00:00<00:21, 374.20it/s]
Adding requests:   1%|          | 82/8192 [00:00<00:19, 408.26it/s]
Adding requests:   2%|▏         | 126/8192 [00:00<00:19, 421.93it/s]
Adding requests:   2%|▏         | 170/8192 [00:00<00:18, 428.41it/s]
Adding requests:   3%|▎         | 215/8192 [00:00<00:18, 435.03it/s]
Adding requests:   3%|▎         | 263/8192 [00:00<00:17, 447.87it/s]
Adding requests:   4%|▍         | 308/8192 [00:00<00:17, 448.01it/s]
Adding requests:   4%|▍         | 355/8192 [00:00<00:17, 454.83it/s]
Adding requests:   5%|▍         | 402/8192 [00:00<00:17, 457.58it/s]
Adding requests:   5%|▌         | 448/8192 [00:01<00:16, 456.92it/s]
Adding requests:   6%|▌         | 495/8192 [00:01<00:16, 460.05it/s]
Adding requests:   7%|▋         | 542/8192 [00:01<00:16, 452.01it/s]
Adding requests:   7%|▋         | 590/8192 [00:01<00:16, 459.49it/s]
Adding requests:   8%|▊         | 637/8192 [00:01<00:16, 461.41it/s]
Adding requests:   8%|▊         | 686/8192 [00:01<00:16, 468.97it/s]
Adding requests:   9%|▉         | 735/8192 [00:01<00:15, 473.59it/s]
Adding requests:  10%|▉         | 783/8192 [00:01<00:15, 468.87it/s]
Adding requests:  10%|█         | 830/8192 [00:01<00:16, 459.93it/s]
Adding requests:  11%|█         | 878/8192 [00:01<00:15, 463.64it/s]
Adding requests:  11%|█▏        | 927/8192 [00:02<00:15, 469.54it/s]
Adding requests:  12%|█▏        | 975/8192 [00:02<00:15, 471.19it/s]
Adding requests:  12%|█▎        | 1024/8192 [00:02<00:15, 474.59it/s]
Adding requests:  13%|█▎        | 1072/8192 [00:02<00:15, 468.59it/s]
Adding requests:  14%|█▎        | 1119/8192 [00:02<00:15, 467.81it/s]
Adding requests:  14%|█▍        | 1169/8192 [00:02<00:14, 474.07it/s]
Adding requests:  15%|█▍        | 1219/8192 [00:02<00:14, 481.33it/s]
Adding requests:  15%|█▌        | 1268/8192 [00:02<00:14, 474.87it/s]
Adding requests:  16%|█▌        | 1316/8192 [00:02<00:14, 475.25it/s]
Adding requests:  17%|█▋        | 1365/8192 [00:02<00:14, 479.26it/s]
Adding requests:  17%|█▋        | 1414/8192 [00:03<00:14, 482.01it/s]
Adding requests:  18%|█▊        | 1463/8192 [00:03<00:13, 481.33it/s]
Adding requests:  18%|█▊        | 1512/8192 [00:03<00:14, 470.92it/s]
Adding requests:  19%|█▉        | 1560/8192 [00:03<00:14, 472.61it/s]
Adding requests:  20%|█▉        | 1610/8192 [00:03<00:13, 478.45it/s]
Adding requests:  20%|██        | 1658/8192 [00:03<00:13, 475.69it/s]
Adding requests:  21%|██        | 1706/8192 [00:03<00:13, 475.22it/s]
Adding requests:  21%|██▏       | 1754/8192 [00:03<00:13, 475.40it/s]
Adding requests:  22%|██▏       | 1802/8192 [00:03<00:13, 475.89it/s]
Adding requests:  23%|██▎       | 1850/8192 [00:03<00:13, 476.19it/s]
Adding requests:  23%|██▎       | 1898/8192 [00:04<00:13, 476.65it/s]
Adding requests:  24%|██▍       | 1946/8192 [00:04<00:13, 476.20it/s]
Adding requests:  24%|██▍       | 1994/8192 [00:04<00:13, 475.39it/s]
Adding requests:  25%|██▍       | 2043/8192 [00:04<00:12, 479.09it/s]
Adding requests:  26%|██▌       | 2093/8192 [00:04<00:12, 483.24it/s]
Adding requests:  26%|██▌       | 2142/8192 [00:04<00:12, 474.92it/s]
Adding requests:  27%|██▋       | 2190/8192 [00:04<00:12, 472.34it/s]
Adding requests:  27%|██▋       | 2239/8192 [00:04<00:12, 475.45it/s]
Adding requests:  28%|██▊       | 2287/8192 [00:04<00:12, 474.33it/s]
Adding requests:  29%|██▊       | 2335/8192 [00:04<00:12, 474.33it/s]
Adding requests:  29%|██▉       | 2384/8192 [00:05<00:12, 476.91it/s]
Adding requests:  30%|██▉       | 2432/8192 [00:05<00:12, 477.54it/s]
Adding requests:  30%|███       | 2480/8192 [00:05<00:11, 476.51it/s]
Adding requests:  31%|███       | 2528/8192 [00:05<00:11, 475.15it/s]
Adding requests:  31%|███▏      | 2578/8192 [00:05<00:11, 481.27it/s]
Adding requests:  32%|███▏      | 2627/8192 [00:05<00:11, 467.79it/s]
Adding requests:  33%|███▎      | 2675/8192 [00:05<00:11, 471.15it/s]
Adding requests:  33%|███▎      | 2723/8192 [00:05<00:11, 471.05it/s]
Adding requests:  34%|███▍      | 2771/8192 [00:05<00:11, 472.47it/s]
Adding requests:  34%|███▍      | 2819/8192 [00:06<00:11, 468.26it/s]
Adding requests:  35%|███▍      | 2867/8192 [00:06<00:11, 469.84it/s]
Adding requests:  36%|███▌      | 2915/8192 [00:06<00:11, 471.91it/s]
Adding requests:  36%|███▌      | 2963/8192 [00:06<00:11, 469.05it/s]
Adding requests:  37%|███▋      | 3012/8192 [00:06<00:10, 472.77it/s]
Adding requests:  37%|███▋      | 3060/8192 [00:06<00:10, 470.89it/s]
Adding requests:  38%|███▊      | 3108/8192 [00:06<00:10, 471.34it/s]
Adding requests:  39%|███▊      | 3156/8192 [00:06<00:10, 467.19it/s]
Adding requests:  39%|███▉      | 3203/8192 [00:06<00:10, 467.16it/s]
Adding requests:  40%|███▉      | 3252/8192 [00:06<00:10, 473.53it/s]
Adding requests:  40%|████      | 3300/8192 [00:07<00:10, 474.56it/s]
Adding requests:  41%|████      | 3349/8192 [00:07<00:10, 478.37it/s]
Adding requests:  41%|████▏     | 3397/8192 [00:07<00:10, 476.12it/s]
Adding requests:  42%|████▏     | 3445/8192 [00:07<00:09, 475.75it/s]
Adding requests:  43%|████▎     | 3493/8192 [00:07<00:09, 470.83it/s]
Adding requests:  43%|████▎     | 3542/8192 [00:07<00:09, 473.70it/s]
Adding requests:  44%|████▍     | 3590/8192 [00:07<00:09, 472.25it/s]
Adding requests:  44%|████▍     | 3638/8192 [00:07<00:09, 469.53it/s]
Adding requests:  45%|████▍     | 3686/8192 [00:07<00:09, 472.17it/s]
Adding requests:  46%|████▌     | 3734/8192 [00:07<00:09, 470.19it/s]
Adding requests:  46%|████▌     | 3782/8192 [00:08<00:09, 466.94it/s]
Adding requests:  47%|████▋     | 3830/8192 [00:08<00:09, 469.54it/s]
Adding requests:  47%|████▋     | 3879/8192 [00:08<00:09, 474.21it/s]
Adding requests:  48%|████▊     | 3927/8192 [00:08<00:08, 474.23it/s]
Adding requests:  49%|████▊     | 3975/8192 [00:08<00:08, 474.01it/s]
Adding requests:  49%|████▉     | 4023/8192 [00:08<00:08, 475.12it/s]
Adding requests:  50%|████▉     | 4071/8192 [00:08<00:08, 469.35it/s]
Adding requests:  50%|█████     | 4120/8192 [00:08<00:08, 472.59it/s]
Adding requests:  51%|█████     | 4168/8192 [00:08<00:08, 474.70it/s]
Adding requests:  51%|█████▏    | 4217/8192 [00:08<00:08, 477.36it/s]
Adding requests:  52%|█████▏    | 4265/8192 [00:09<00:08, 476.38it/s]
Adding requests:  53%|█████▎    | 4313/8192 [00:09<00:08, 476.76it/s]
Adding requests:  53%|█████▎    | 4363/8192 [00:09<00:07, 482.16it/s]
Adding requests:  54%|█████▍    | 4413/8192 [00:09<00:07, 485.44it/s]
Adding requests:  54%|█████▍    | 4462/8192 [00:09<00:07, 482.66it/s]
Adding requests:  55%|█████▌    | 4511/8192 [00:09<00:07, 478.60it/s]
Adding requests:  56%|█████▌    | 4559/8192 [00:09<00:07, 474.35it/s]
Adding requests:  56%|█████▋    | 4609/8192 [00:09<00:07, 478.65it/s]
Adding requests:  57%|█████▋    | 4657/8192 [00:09<00:07, 478.92it/s]
Adding requests:  57%|█████▋    | 4705/8192 [00:09<00:07, 476.80it/s]
Adding requests:  58%|█████▊    | 4754/8192 [00:10<00:07, 478.51it/s]
Adding requests:  59%|█████▊    | 4802/8192 [00:10<00:07, 477.91it/s]
Adding requests:  59%|█████▉    | 4850/8192 [00:10<00:07, 476.00it/s]
Adding requests:  60%|█████▉    | 4898/8192 [00:10<00:06, 471.79it/s]
Adding requests:  60%|██████    | 4946/8192 [00:10<00:06, 465.26it/s]
Adding requests:  61%|██████    | 4994/8192 [00:10<00:06, 467.44it/s]
Adding requests:  62%|██████▏   | 5043/8192 [00:10<00:06, 471.89it/s]
Adding requests:  62%|██████▏   | 5093/8192 [00:10<00:06, 480.04it/s]
Adding requests:  63%|██████▎   | 5142/8192 [00:10<00:06, 476.93it/s]
Adding requests:  63%|██████▎   | 5191/8192 [00:11<00:06, 477.78it/s]
Adding requests:  64%|██████▍   | 5239/8192 [00:11<00:06, 474.74it/s]
Adding requests:  65%|██████▍   | 5287/8192 [00:11<00:06, 473.07it/s]
Adding requests:  65%|██████▌   | 5336/8192 [00:11<00:05, 477.03it/s]
Adding requests:  66%|██████▌   | 5384/8192 [00:11<00:05, 476.57it/s]
Adding requests:  66%|██████▋   | 5433/8192 [00:11<00:05, 478.64it/s]
Adding requests:  67%|██████▋   | 5481/8192 [00:11<00:05, 473.07it/s]
Adding requests:  67%|██████▋   | 5529/8192 [00:11<00:05, 471.05it/s]
Adding requests:  68%|██████▊   | 5577/8192 [00:11<00:05, 471.59it/s]
Adding requests:  69%|██████▊   | 5625/8192 [00:11<00:05, 468.45it/s]
Adding requests:  69%|██████▉   | 5672/8192 [00:12<00:05, 465.94it/s]
Adding requests:  70%|██████▉   | 5721/8192 [00:12<00:05, 471.24it/s]
Adding requests:  70%|███████   | 5770/8192 [00:12<00:05, 475.10it/s]
Adding requests:  71%|███████   | 5818/8192 [00:12<00:05, 466.94it/s]
Adding requests:  72%|███████▏  | 5866/8192 [00:12<00:04, 470.64it/s]
Adding requests:  72%|███████▏  | 5914/8192 [00:12<00:04, 473.38it/s]
Adding requests:  73%|███████▎  | 5962/8192 [00:12<00:04, 472.66it/s]
Adding requests:  73%|███████▎  | 6011/8192 [00:12<00:04, 476.64it/s]
Adding requests:  74%|███████▍  | 6060/8192 [00:12<00:04, 479.98it/s]
Adding requests:  75%|███████▍  | 6109/8192 [00:12<00:04, 466.54it/s]
Adding requests:  75%|███████▌  | 6156/8192 [00:13<00:04, 466.99it/s]
Adding requests:  76%|███████▌  | 6205/8192 [00:13<00:04, 472.85it/s]
Adding requests:  76%|███████▋  | 6255/8192 [00:13<00:04, 479.91it/s]
Adding requests:  77%|███████▋  | 6304/8192 [00:13<00:04, 464.75it/s]
Adding requests:  78%|███████▊  | 6353/8192 [00:13<00:03, 469.73it/s]
Adding requests:  78%|███████▊  | 6401/8192 [00:13<00:03, 471.93it/s]
Adding requests:  79%|███████▊  | 6451/8192 [00:13<00:03, 478.29it/s]
Adding requests:  79%|███████▉  | 6501/8192 [00:13<00:03, 481.28it/s]
Adding requests:  80%|███████▉  | 6550/8192 [00:13<00:03, 482.79it/s]
Adding requests:  81%|████████  | 6599/8192 [00:13<00:03, 480.85it/s]
Adding requests:  81%|████████  | 6648/8192 [00:14<00:03, 481.16it/s]
Adding requests:  82%|████████▏ | 6697/8192 [00:14<00:03, 479.51it/s]
Adding requests:  82%|████████▏ | 6745/8192 [00:14<00:03, 477.31it/s]
Adding requests:  83%|████████▎ | 6795/8192 [00:14<00:02, 483.16it/s]
Adding requests:  84%|████████▎ | 6844/8192 [00:14<00:02, 482.26it/s]
Adding requests:  84%|████████▍ | 6893/8192 [00:14<00:02, 484.31it/s]
Adding requests:  85%|████████▍ | 6943/8192 [00:14<00:02, 487.48it/s]
Adding requests:  85%|████████▌ | 6992/8192 [00:14<00:02, 484.42it/s]
Adding requests:  86%|████████▌ | 7041/8192 [00:14<00:02, 479.21it/s]
Adding requests:  87%|████████▋ | 7090/8192 [00:15<00:02, 479.28it/s]
Adding requests:  87%|████████▋ | 7139/8192 [00:15<00:02, 480.85it/s]
Adding requests:  88%|████████▊ | 7188/8192 [00:15<00:02, 476.63it/s]
Adding requests:  88%|████████▊ | 7236/8192 [00:15<00:02, 477.17it/s]
Adding requests:  89%|████████▉ | 7284/8192 [00:15<00:01, 470.45it/s]
Adding requests:  90%|████████▉ | 7333/8192 [00:15<00:01, 475.04it/s]
Adding requests:  90%|█████████ | 7381/8192 [00:15<00:01, 473.50it/s]
Adding requests:  91%|█████████ | 7432/8192 [00:15<00:01, 482.20it/s]
Adding requests:  91%|█████████▏| 7481/8192 [00:15<00:01, 482.13it/s]
Adding requests:  92%|█████████▏| 7530/8192 [00:15<00:01, 481.20it/s]
Adding requests:  93%|█████████▎| 7579/8192 [00:16<00:01, 479.13it/s]
Adding requests:  93%|█████████▎| 7627/8192 [00:16<00:01, 474.98it/s]
Adding requests:  94%|█████████▎| 7677/8192 [00:16<00:01, 481.08it/s]
Adding requests:  94%|█████████▍| 7726/8192 [00:16<00:00, 481.46it/s]
Adding requests:  95%|█████████▍| 7775/8192 [00:16<00:00, 476.41it/s]
Adding requests:  95%|█████████▌| 7823/8192 [00:16<00:00, 474.22it/s]
Adding requests:  96%|█████████▌| 7871/8192 [00:16<00:00, 460.15it/s]
Adding requests:  97%|█████████▋| 7918/8192 [00:16<00:00, 462.19it/s]
Adding requests:  97%|█████████▋| 7966/8192 [00:16<00:00, 465.97it/s]
Adding requests:  98%|█████████▊| 8013/8192 [00:16<00:00, 466.20it/s]
Adding requests:  98%|█████████▊| 8060/8192 [00:17<00:00, 466.19it/s]
Adding requests:  99%|█████████▉| 8109/8192 [00:17<00:00, 472.70it/s]
Adding requests: 100%|█████████▉| 8157/8192 [00:17<00:00, 472.95it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 472.60it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  74%|███████▍  | 6102/8192 [00:00<00:00, 61013.80it/s, est. speed input: 62482156.08 toks/s, output: 61014.97 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 61013.80it/s, est. speed input: 62500216.87 toks/s, output: 61032.98 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 60976.34it/s, est. speed input: 62500216.87 toks/s, output: 61032.98 toks/s]
[rank0]:[W126 14:20:59.644779059 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 87.1s

测试结果:
  Requests/s:   468.88
  Tokens/s:     480603.23
  Total Reqs:   8192
  Elapsed:      17.47s

  [Prefill 分析]
  Total Prefill Tokens: 8388608
  Prefill Tokens/s:     480134.35


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/Llama3.2-1B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,40.6659,20861.6213,3.1476
1024,1024,1,128,128,41.0880,42115.2202,3.1153
2048,1024,2,256,128,82.5566,84620.4675,3.1009
4096,1024,4,512,128,157.6947,161637.0763,3.2468
8192,1024,8,1024,128,306.2919,313949.1528,3.3432
16384,1024,16,2048,128,455.2892,466671.4235,4.4982
32768,1024,32,4096,128,461.0140,472539.3672,8.8848
65536,1024,64,8192,128,468.8812,480603.2313,17.4714

------------------------------------------------------------

[INFO] 完成: 8 成功, 0 失败

============================================================
  Llama3.2-1B-FP8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:21:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=578000) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=578000) WARNING 01-26 14:21:21 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=578000) WARNING 01-26 14:21:27 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 42.49 requests/s, 21796.23 total tokens/s, 42.49 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 14:21:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:21:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:21:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:21:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:21:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:21:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:21:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:21:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:21:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:21:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:21:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:21:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:21:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:21:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=578000) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=578000) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=578000) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=578000) 
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=578000) 2026-01-26 14:21:27,899 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=578000) 2026-01-26 14:21:27,922 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=578000) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.41it/s]
(EngineCore_DP0 pid=578000) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.06it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  54%|█████▍    | 69/128 [00:00<00:00, 689.57it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 321.13it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:00, 182.58it/s, est. speed input: 93487.14 toks/s, output: 182.58 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 63.96it/s, est. speed input: 36387.31 toks/s, output: 71.07 toks/s]  
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 55.33it/s, est. speed input: 31842.49 toks/s, output: 62.19 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:00<00:01, 51.61it/s, est. speed input: 29996.47 toks/s, output: 58.59 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 49.34it/s, est. speed input: 28892.91 toks/s, output: 56.43 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 47.83it/s, est. speed input: 28171.44 toks/s, output: 55.02 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 46.62it/s, est. speed input: 27584.83 toks/s, output: 53.88 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 45.57it/s, est. speed input: 27132.31 toks/s, output: 52.99 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 45.04it/s, est. speed input: 26794.70 toks/s, output: 52.33 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 44.59it/s, est. speed input: 26495.61 toks/s, output: 51.75 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 43.87it/s, est. speed input: 26183.91 toks/s, output: 51.14 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 43.70it/s, est. speed input: 25952.28 toks/s, output: 50.69 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 43.68it/s, est. speed input: 25757.77 toks/s, output: 50.31 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 43.47it/s, est. speed input: 25562.66 toks/s, output: 49.93 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 43.39it/s, est. speed input: 25393.68 toks/s, output: 49.60 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 43.35it/s, est. speed input: 25241.56 toks/s, output: 49.30 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 43.36it/s, est. speed input: 25106.63 toks/s, output: 49.04 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.36it/s, est. speed input: 25081.65 toks/s, output: 48.99 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.99it/s, est. speed input: 25081.65 toks/s, output: 48.99 toks/s]
[rank0]:[W126 14:21:32.157709725 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.3s

测试结果:
  Requests/s:   42.49
  Tokens/s:     21796.23
  Total Reqs:   128
  Elapsed:      3.01s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     21753.75

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:21:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=579010) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=579010) WARNING 01-26 14:21:54 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=579010) WARNING 01-26 14:22:00 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.57 requests/s, 42609.20 total tokens/s, 41.57 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 14:21:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:21:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:21:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:21:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:21:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:21:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:21:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:21:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:21:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:21:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:21:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:21:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:21:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:21:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:49] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:49] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:49] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:49] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:49] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=579010) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=579010) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=579010) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=579010) 
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=579010) 2026-01-26 14:22:00,930 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=579010) 2026-01-26 14:22:00,950 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=579010) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 21.19it/s]
(EngineCore_DP0 pid=579010) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.96it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  29%|██▉       | 37/128 [00:00<00:00, 366.46it/s]
Adding requests:  63%|██████▎   | 81/128 [00:00<00:00, 405.46it/s]
Adding requests:  95%|█████████▌| 122/128 [00:00<00:00, 185.43it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 219.72it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:00, 192.73it/s, est. speed input: 197384.53 toks/s, output: 192.74 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 67.53it/s, est. speed input: 77204.21 toks/s, output: 75.39 toks/s]   
Processed prompts:  42%|████▏     | 54/128 [00:00<00:01, 58.25it/s, est. speed input: 67534.41 toks/s, output: 65.95 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:00<00:01, 54.20it/s, est. speed input: 63551.30 toks/s, output: 62.06 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 51.55it/s, est. speed input: 61070.48 toks/s, output: 59.64 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 49.48it/s, est. speed input: 59285.51 toks/s, output: 57.90 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 48.20it/s, est. speed input: 57993.68 toks/s, output: 56.63 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:00, 47.32it/s, est. speed input: 57086.13 toks/s, output: 55.75 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 46.33it/s, est. speed input: 56213.44 toks/s, output: 54.90 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 45.75it/s, est. speed input: 55510.61 toks/s, output: 54.21 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:01<00:00, 45.20it/s, est. speed input: 54860.11 toks/s, output: 53.57 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 44.93it/s, est. speed input: 54320.48 toks/s, output: 53.05 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 44.65it/s, est. speed input: 53819.12 toks/s, output: 52.56 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 44.53it/s, est. speed input: 53386.53 toks/s, output: 52.14 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 44.49it/s, est. speed input: 53003.18 toks/s, output: 51.76 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 44.41it/s, est. speed input: 52646.92 toks/s, output: 51.41 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.41it/s, est. speed input: 52524.28 toks/s, output: 51.29 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 51.29it/s, est. speed input: 52524.28 toks/s, output: 51.29 toks/s]
[rank0]:[W126 14:22:05.204275027 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.0s

测试结果:
  Requests/s:   41.57
  Tokens/s:     42609.20
  Total Reqs:   128
  Elapsed:      3.08s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     42567.63

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:22:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=580056) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=580056) WARNING 01-26 14:22:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=580056) WARNING 01-26 14:22:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 82.77 requests/s, 84839.16 total tokens/s, 82.77 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 14:22:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:22:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:22:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:22:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:22:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:22:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:22:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:22:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:22:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:22:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:22:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:22:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:22:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:22:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=580056) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=580056) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=580056) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=580056) 
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=580056) 2026-01-26 14:22:35,013 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=580056) 2026-01-26 14:22:35,034 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=580056) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.55it/s]
(EngineCore_DP0 pid=580056) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 22.73it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 37/256 [00:00<00:00, 362.67it/s]
Adding requests:  31%|███▏      | 80/256 [00:00<00:00, 399.61it/s]
Adding requests:  48%|████▊     | 124/256 [00:00<00:00, 416.52it/s]
Adding requests:  66%|██████▌   | 168/256 [00:00<00:00, 419.87it/s]
Adding requests:  82%|████████▏ | 211/256 [00:00<00:00, 423.19it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 423.17it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|██        | 52/256 [00:00<00:00, 423.15it/s, est. speed input: 433325.83 toks/s, output: 423.15 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:00<00:01, 139.18it/s, est. speed input: 160172.59 toks/s, output: 156.42 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:00<00:01, 116.20it/s, est. speed input: 136651.03 toks/s, output: 133.45 toks/s]
Processed prompts:  53%|█████▎    | 135/256 [00:01<00:01, 109.46it/s, est. speed input: 129351.25 toks/s, output: 126.32 toks/s]
Processed prompts:  58%|█████▊    | 149/256 [00:01<00:01, 103.33it/s, est. speed input: 123890.42 toks/s, output: 120.99 toks/s]
Processed prompts:  63%|██████▎   | 161/256 [00:01<00:00, 98.56it/s, est. speed input: 119956.54 toks/s, output: 117.14 toks/s] 
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:00, 93.64it/s, est. speed input: 116398.08 toks/s, output: 113.67 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:01<00:00, 91.90it/s, est. speed input: 114394.84 toks/s, output: 111.71 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:01<00:00, 90.16it/s, est. speed input: 112551.91 toks/s, output: 109.91 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:01<00:00, 89.25it/s, est. speed input: 111079.12 toks/s, output: 108.48 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:01<00:00, 88.65it/s, est. speed input: 109803.43 toks/s, output: 107.23 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:02<00:00, 87.73it/s, est. speed input: 108553.60 toks/s, output: 106.01 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:02<00:00, 87.43it/s, est. speed input: 107519.67 toks/s, output: 105.00 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:02<00:00, 87.05it/s, est. speed input: 106553.83 toks/s, output: 104.06 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:02<00:00, 86.84it/s, est. speed input: 105692.50 toks/s, output: 103.22 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 86.84it/s, est. speed input: 105411.71 toks/s, output: 102.94 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.94it/s, est. speed input: 105411.71 toks/s, output: 102.94 toks/s]
[rank0]:[W126 14:22:40.604037886 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.6s

测试结果:
  Requests/s:   82.77
  Tokens/s:     84839.16
  Total Reqs:   256
  Elapsed:      3.09s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     84756.39

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:22:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=581109) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=581109) WARNING 01-26 14:23:04 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=581109) WARNING 01-26 14:23:10 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 164.64 requests/s, 168758.05 total tokens/s, 164.64 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 14:22:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:22:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:22:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:22:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:22:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:22:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:22:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:22:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:22:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:22:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:22:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:22:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:22:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:22:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=581109) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=581109) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=581109) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=581109) 
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=581109) 2026-01-26 14:23:10,842 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=581109) 2026-01-26 14:23:10,864 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=581109) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 17.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 16.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 16.83it/s]
(EngineCore_DP0 pid=581109) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 22.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 22.93it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 34/512 [00:00<00:01, 335.57it/s]
Adding requests:  15%|█▌        | 78/512 [00:00<00:01, 395.11it/s]
Adding requests:  24%|██▍       | 123/512 [00:00<00:00, 417.18it/s]
Adding requests:  33%|███▎      | 168/512 [00:00<00:00, 424.69it/s]
Adding requests:  42%|████▏     | 214/512 [00:00<00:00, 435.92it/s]
Adding requests:  51%|█████     | 262/512 [00:00<00:00, 447.80it/s]
Adding requests:  60%|██████    | 308/512 [00:00<00:00, 449.23it/s]
Adding requests:  69%|██████▉   | 355/512 [00:00<00:00, 454.70it/s]
Adding requests:  79%|███████▊  | 402/512 [00:00<00:00, 457.01it/s]
Adding requests:  88%|████████▊ | 449/512 [00:01<00:00, 459.18it/s]
Adding requests:  97%|█████████▋| 496/512 [00:01<00:00, 460.86it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 444.62it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:00<00:00, 1733.03it/s, est. speed input: 1774741.28 toks/s, output: 1733.06 toks/s]
Processed prompts:  71%|███████   | 364/512 [00:01<00:00, 289.36it/s, est. speed input: 340761.42 toks/s, output: 332.77 toks/s]   
Processed prompts:  87%|████████▋ | 446/512 [00:01<00:00, 243.31it/s, est. speed input: 290626.61 toks/s, output: 283.81 toks/s]
Processed prompts:  97%|█████████▋| 499/512 [00:01<00:00, 224.12it/s, est. speed input: 271645.15 toks/s, output: 265.28 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 224.12it/s, est. speed input: 267900.34 toks/s, output: 261.62 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 261.61it/s, est. speed input: 267900.34 toks/s, output: 261.62 toks/s]
[rank0]:[W126 14:23:16.720984053 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.0s

测试结果:
  Requests/s:   164.64
  Tokens/s:     168758.05
  Total Reqs:   512
  Elapsed:      3.11s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     168593.40

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:23:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=582204) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=582204) WARNING 01-26 14:23:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=582204) WARNING 01-26 14:23:48 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 290.04 requests/s, 297288.96 total tokens/s, 290.04 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 14:23:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:23:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:23:29] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:23:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:23:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:23:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:23:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:23:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:23:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:23:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:23:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:23:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:23:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:23:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:23:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:23:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:23:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:23:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=582204) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=582204) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=582204) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=582204) 
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=582204) 2026-01-26 14:23:48,858 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=582204) 2026-01-26 14:23:48,923 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=582204) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 22.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 20.53it/s]
(EngineCore_DP0 pid=582204) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 22.99it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 38/1024 [00:00<00:02, 377.78it/s]
Adding requests:   8%|▊         | 80/1024 [00:00<00:02, 397.59it/s]
Adding requests:  12%|█▏        | 125/1024 [00:00<00:02, 419.55it/s]
Adding requests:  17%|█▋        | 169/1024 [00:00<00:02, 427.27it/s]
Adding requests:  21%|██        | 214/1024 [00:00<00:01, 434.14it/s]
Adding requests:  26%|██▌       | 262/1024 [00:00<00:01, 447.01it/s]
Adding requests:  30%|██▉       | 307/1024 [00:00<00:01, 432.96it/s]
Adding requests:  34%|███▍      | 352/1024 [00:00<00:01, 437.23it/s]
Adding requests:  39%|███▉      | 398/1024 [00:00<00:01, 442.45it/s]
Adding requests:  43%|████▎     | 444/1024 [00:01<00:01, 447.19it/s]
Adding requests:  48%|████▊     | 491/1024 [00:01<00:01, 452.37it/s]
Adding requests:  52%|█████▏    | 537/1024 [00:01<00:01, 445.86it/s]
Adding requests:  57%|█████▋    | 585/1024 [00:01<00:00, 453.73it/s]
Adding requests:  62%|██████▏   | 633/1024 [00:01<00:00, 458.70it/s]
Adding requests:  67%|██████▋   | 682/1024 [00:01<00:00, 465.90it/s]
Adding requests:  71%|███████▏  | 730/1024 [00:01<00:00, 469.47it/s]
Adding requests:  76%|███████▌  | 777/1024 [00:01<00:00, 462.33it/s]
Adding requests:  80%|████████  | 824/1024 [00:01<00:00, 455.91it/s]
Adding requests:  85%|████████▌ | 871/1024 [00:01<00:00, 459.28it/s]
Adding requests:  90%|████████▉ | 920/1024 [00:02<00:00, 466.08it/s]
Adding requests:  95%|█████████▍| 968/1024 [00:02<00:00, 468.58it/s]
Adding requests:  99%|█████████▉| 1015/1024 [00:02<00:00, 468.99it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 451.67it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:00<00:00, 5403.79it/s, est. speed input: 5533951.89 toks/s, output: 5404.01 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 5403.79it/s, est. speed input: 830847.37 toks/s, output: 811.37 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 811.29it/s, est. speed input: 830847.37 toks/s, output: 811.37 toks/s] 
[rank0]:[W126 14:23:54.211511718 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.5s

测试结果:
  Requests/s:   290.04
  Tokens/s:     297288.96
  Total Reqs:   1024
  Elapsed:      3.53s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     296998.93

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:24:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=583349) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=583349) WARNING 01-26 14:24:25 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=583349) WARNING 01-26 14:24:32 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 447.43 requests/s, 458616.64 total tokens/s, 447.43 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 14:24:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:24:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:24:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:24:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:24:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:24:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:24:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:24:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:24:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:24:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:24:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:24:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:24:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:24:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:24:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:24:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:24:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:24:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=583349) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=583349) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=583349) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=583349) 
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=583349) 2026-01-26 14:24:32,365 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=583349) 2026-01-26 14:24:32,389 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=583349) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 21.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 22.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 21.02it/s]
(EngineCore_DP0 pid=583349) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 22.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 22.58it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 381.35it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 419.92it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 431.67it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:04, 435.91it/s]
Adding requests:  11%|█         | 220/2048 [00:00<00:04, 444.51it/s]
Adding requests:  13%|█▎        | 268/2048 [00:00<00:03, 454.43it/s]
Adding requests:  15%|█▌        | 314/2048 [00:00<00:03, 453.36it/s]
Adding requests:  18%|█▊        | 360/2048 [00:00<00:03, 454.58it/s]
Adding requests:  20%|█▉        | 407/2048 [00:00<00:03, 459.34it/s]
Adding requests:  22%|██▏       | 454/2048 [00:01<00:03, 461.27it/s]
Adding requests:  24%|██▍       | 501/2048 [00:01<00:03, 460.81it/s]
Adding requests:  27%|██▋       | 548/2048 [00:01<00:03, 456.18it/s]
Adding requests:  29%|██▉       | 595/2048 [00:01<00:03, 459.87it/s]
Adding requests:  31%|███▏      | 643/2048 [00:01<00:03, 462.93it/s]
Adding requests:  34%|███▍      | 692/2048 [00:01<00:02, 469.62it/s]
Adding requests:  36%|███▌      | 740/2048 [00:01<00:02, 470.47it/s]
Adding requests:  38%|███▊      | 788/2048 [00:01<00:02, 467.83it/s]
Adding requests:  41%|████      | 835/2048 [00:01<00:02, 456.81it/s]
Adding requests:  43%|████▎     | 883/2048 [00:01<00:02, 463.43it/s]
Adding requests:  45%|████▌     | 931/2048 [00:02<00:02, 467.95it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:02, 469.18it/s]
Adding requests:  50%|█████     | 1026/2048 [00:02<00:02, 466.57it/s]
Adding requests:  52%|█████▏    | 1073/2048 [00:02<00:02, 463.85it/s]
Adding requests:  55%|█████▍    | 1120/2048 [00:02<00:02, 462.47it/s]
Adding requests:  57%|█████▋    | 1170/2048 [00:02<00:01, 470.61it/s]
Adding requests:  60%|█████▉    | 1220/2048 [00:02<00:01, 478.79it/s]
Adding requests:  62%|██████▏   | 1268/2048 [00:02<00:01, 471.57it/s]
Adding requests:  64%|██████▍   | 1316/2048 [00:02<00:01, 472.89it/s]
Adding requests:  67%|██████▋   | 1364/2048 [00:02<00:01, 474.73it/s]
Adding requests:  69%|██████▉   | 1414/2048 [00:03<00:01, 479.43it/s]
Adding requests:  71%|███████▏  | 1462/2048 [00:03<00:01, 478.29it/s]
Adding requests:  74%|███████▎  | 1510/2048 [00:03<00:01, 472.78it/s]
Adding requests:  76%|███████▌  | 1558/2048 [00:03<00:01, 473.18it/s]
Adding requests:  78%|███████▊  | 1606/2048 [00:03<00:00, 470.53it/s]
Adding requests:  81%|████████  | 1654/2048 [00:03<00:00, 471.33it/s]
Adding requests:  83%|████████▎ | 1702/2048 [00:03<00:00, 469.21it/s]
Adding requests:  85%|████████▌ | 1750/2048 [00:03<00:00, 471.27it/s]
Adding requests:  88%|████████▊ | 1798/2048 [00:03<00:00, 449.93it/s]
Adding requests:  90%|█████████ | 1846/2048 [00:03<00:00, 456.60it/s]
Adding requests:  92%|█████████▏| 1893/2048 [00:04<00:00, 457.58it/s]
Adding requests:  95%|█████████▍| 1941/2048 [00:04<00:00, 461.85it/s]
Adding requests:  97%|█████████▋| 1989/2048 [00:04<00:00, 464.49it/s]
Adding requests: 100%|█████████▉| 2038/2048 [00:04<00:00, 469.70it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 463.40it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  98%|█████████▊| 2017/2048 [00:00<00:00, 16368.21it/s, est. speed input: 16761991.17 toks/s, output: 16368.44 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 16368.21it/s, est. speed input: 13429969.80 toks/s, output: 13114.78 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 13107.02it/s, est. speed input: 13429969.80 toks/s, output: 13114.78 toks/s]
[rank0]:[W126 14:24:39.727009117 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.6s

测试结果:
  Requests/s:   447.43
  Tokens/s:     458616.64
  Total Reqs:   2048
  Elapsed:      4.58s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     458169.21

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:25:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=584650) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=584650) WARNING 01-26 14:25:19 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=584650) WARNING 01-26 14:25:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 465.40 requests/s, 477038.71 total tokens/s, 465.40 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 14:25:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:25:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:25:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:25:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:25:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:25:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:25:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:25:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:25:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:25:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:25:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:25:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:25:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:25:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:25:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:25:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:25:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:25:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=584650) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=584650) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.03it/s]
(EngineCore_DP0 pid=584650) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.03it/s]
(EngineCore_DP0 pid=584650) 
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=584650) [rank0]:W0126 14:25:22.835000 584650 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=584650) [rank0]:W0126 14:25:22.904000 584650 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=584650) [rank0]:W0126 14:25:23.707000 584650 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=584650) [rank0]:W0126 14:25:23.806000 584650 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=584650) 2026-01-26 14:25:26,201 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=584650) 2026-01-26 14:25:26,225 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=584650) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 19.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 20.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 22.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 21.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 21.51it/s]
(EngineCore_DP0 pid=584650) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 23.02it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 23.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 23.35it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 39/4096 [00:00<00:10, 383.34it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 420.09it/s]
Adding requests:   3%|▎         | 130/4096 [00:00<00:09, 436.43it/s]
Adding requests:   4%|▍         | 175/4096 [00:00<00:08, 439.25it/s]
Adding requests:   5%|▌         | 222/4096 [00:00<00:08, 448.24it/s]
Adding requests:   7%|▋         | 270/4096 [00:00<00:08, 458.43it/s]
Adding requests:   8%|▊         | 316/4096 [00:00<00:08, 433.70it/s]
Adding requests:   9%|▉         | 363/4096 [00:00<00:08, 443.07it/s]
Adding requests:  10%|█         | 410/4096 [00:00<00:08, 450.07it/s]
Adding requests:  11%|█         | 457/4096 [00:01<00:08, 454.86it/s]
Adding requests:  12%|█▏        | 504/4096 [00:01<00:07, 457.14it/s]
Adding requests:  13%|█▎        | 550/4096 [00:01<00:07, 452.27it/s]
Adding requests:  15%|█▍        | 599/4096 [00:01<00:07, 460.47it/s]
Adding requests:  16%|█▌        | 647/4096 [00:01<00:07, 464.72it/s]
Adding requests:  17%|█▋        | 696/4096 [00:01<00:07, 471.41it/s]
Adding requests:  18%|█▊        | 744/4096 [00:01<00:07, 471.05it/s]
Adding requests:  19%|█▉        | 792/4096 [00:01<00:07, 460.01it/s]
Adding requests:  20%|██        | 839/4096 [00:01<00:07, 454.09it/s]
Adding requests:  22%|██▏       | 885/4096 [00:01<00:07, 454.16it/s]
Adding requests:  23%|██▎       | 934/4096 [00:02<00:06, 463.35it/s]
Adding requests:  24%|██▍       | 981/4096 [00:02<00:06, 464.98it/s]
Adding requests:  25%|██▌       | 1030/4096 [00:02<00:06, 470.73it/s]
Adding requests:  26%|██▋       | 1078/4096 [00:02<00:06, 466.38it/s]
Adding requests:  27%|██▋       | 1125/4096 [00:02<00:06, 465.84it/s]
Adding requests:  29%|██▊       | 1174/4096 [00:02<00:06, 471.11it/s]
Adding requests:  30%|██▉       | 1224/4096 [00:02<00:06, 478.58it/s]
Adding requests:  31%|███       | 1272/4096 [00:02<00:05, 470.67it/s]
Adding requests:  32%|███▏      | 1320/4096 [00:02<00:05, 473.18it/s]
Adding requests:  33%|███▎      | 1369/4096 [00:02<00:05, 476.50it/s]
Adding requests:  35%|███▍      | 1417/4096 [00:03<00:05, 476.65it/s]
Adding requests:  36%|███▌      | 1466/4096 [00:03<00:05, 479.80it/s]
Adding requests:  37%|███▋      | 1515/4096 [00:03<00:05, 481.29it/s]
Adding requests:  38%|███▊      | 1564/4096 [00:03<00:05, 481.79it/s]
Adding requests:  39%|███▉      | 1613/4096 [00:03<00:05, 483.84it/s]
Adding requests:  41%|████      | 1662/4096 [00:03<00:05, 481.38it/s]
Adding requests:  42%|████▏     | 1711/4096 [00:03<00:04, 480.46it/s]
Adding requests:  43%|████▎     | 1760/4096 [00:03<00:04, 479.62it/s]
Adding requests:  44%|████▍     | 1809/4096 [00:03<00:04, 480.57it/s]
Adding requests:  45%|████▌     | 1858/4096 [00:03<00:04, 480.88it/s]
Adding requests:  47%|████▋     | 1907/4096 [00:04<00:04, 478.08it/s]
Adding requests:  48%|████▊     | 1955/4096 [00:04<00:04, 468.47it/s]
Adding requests:  49%|████▉     | 2004/4096 [00:04<00:04, 473.62it/s]
Adding requests:  50%|█████     | 2053/4096 [00:04<00:04, 476.21it/s]
Adding requests:  51%|█████▏    | 2102/4096 [00:04<00:04, 474.01it/s]
Adding requests:  52%|█████▏    | 2150/4096 [00:04<00:04, 469.62it/s]
Adding requests:  54%|█████▎    | 2197/4096 [00:04<00:04, 468.63it/s]
Adding requests:  55%|█████▍    | 2246/4096 [00:04<00:03, 473.80it/s]
Adding requests:  56%|█████▌    | 2294/4096 [00:04<00:03, 472.72it/s]
Adding requests:  57%|█████▋    | 2342/4096 [00:05<00:03, 472.76it/s]
Adding requests:  58%|█████▊    | 2391/4096 [00:05<00:03, 475.27it/s]
Adding requests:  60%|█████▉    | 2439/4096 [00:05<00:03, 475.68it/s]
Adding requests:  61%|██████    | 2488/4096 [00:05<00:03, 477.79it/s]
Adding requests:  62%|██████▏   | 2536/4096 [00:05<00:03, 476.69it/s]
Adding requests:  63%|██████▎   | 2585/4096 [00:05<00:03, 479.74it/s]
Adding requests:  64%|██████▍   | 2633/4096 [00:05<00:03, 478.80it/s]
Adding requests:  65%|██████▌   | 2682/4096 [00:05<00:02, 480.07it/s]
Adding requests:  67%|██████▋   | 2731/4096 [00:05<00:02, 460.60it/s]
Adding requests:  68%|██████▊   | 2778/4096 [00:05<00:02, 462.81it/s]
Adding requests:  69%|██████▉   | 2825/4096 [00:06<00:02, 460.81it/s]
Adding requests:  70%|███████   | 2874/4096 [00:06<00:02, 467.56it/s]
Adding requests:  71%|███████▏  | 2922/4096 [00:06<00:02, 471.06it/s]
Adding requests:  73%|███████▎  | 2970/4096 [00:06<00:02, 470.81it/s]
Adding requests:  74%|███████▎  | 3018/4096 [00:06<00:02, 472.94it/s]
Adding requests:  75%|███████▍  | 3066/4096 [00:06<00:02, 471.80it/s]
Adding requests:  76%|███████▌  | 3114/4096 [00:06<00:02, 459.97it/s]
Adding requests:  77%|███████▋  | 3161/4096 [00:06<00:02, 462.50it/s]
Adding requests:  78%|███████▊  | 3208/4096 [00:06<00:01, 464.07it/s]
Adding requests:  80%|███████▉  | 3257/4096 [00:06<00:01, 470.78it/s]
Adding requests:  81%|████████  | 3305/4096 [00:07<00:01, 472.02it/s]
Adding requests:  82%|████████▏ | 3354/4096 [00:07<00:01, 474.90it/s]
Adding requests:  83%|████████▎ | 3402/4096 [00:07<00:01, 472.72it/s]
Adding requests:  84%|████████▍ | 3450/4096 [00:07<00:01, 473.85it/s]
Adding requests:  85%|████████▌ | 3498/4096 [00:07<00:01, 469.31it/s]
Adding requests:  87%|████████▋ | 3547/4096 [00:07<00:01, 472.81it/s]
Adding requests:  88%|████████▊ | 3595/4096 [00:07<00:01, 472.48it/s]
Adding requests:  89%|████████▉ | 3643/4096 [00:07<00:00, 469.07it/s]
Adding requests:  90%|█████████ | 3692/4096 [00:07<00:00, 473.04it/s]
Adding requests:  91%|█████████▏| 3740/4096 [00:07<00:00, 470.06it/s]
Adding requests:  93%|█████████▎| 3790/4096 [00:08<00:00, 478.52it/s]
Adding requests:  94%|█████████▎| 3838/4096 [00:08<00:00, 475.52it/s]
Adding requests:  95%|█████████▍| 3887/4096 [00:08<00:00, 477.77it/s]
Adding requests:  96%|█████████▌| 3935/4096 [00:08<00:00, 476.84it/s]
Adding requests:  97%|█████████▋| 3983/4096 [00:08<00:00, 477.39it/s]
Adding requests:  98%|█████████▊| 4031/4096 [00:08<00:00, 475.56it/s]
Adding requests: 100%|█████████▉| 4079/4096 [00:08<00:00, 471.67it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 469.03it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 62240.72it/s, est. speed input: 63748119.48 toks/s, output: 62250.19 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 62142.78it/s, est. speed input: 63748119.48 toks/s, output: 62250.19 toks/s]
[rank0]:[W126 14:25:37.170256074 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 58.5s

测试结果:
  Requests/s:   465.40
  Tokens/s:     477038.71
  Total Reqs:   4096
  Elapsed:      8.80s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     476573.30

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:26:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=586259) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=586259) WARNING 01-26 14:26:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=586259) WARNING 01-26 14:26:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 460.64 requests/s, 472157.66 total tokens/s, 460.64 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192


─── STDERR ───
[2026-01-26 14:26:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:26:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:26:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:26:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:26:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:26:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:26:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:26:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:26:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:26:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:26:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:26:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:26:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:26:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:26:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:26:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:26:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:26:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:31] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=586259) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=586259) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.50it/s]
(EngineCore_DP0 pid=586259) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.50it/s]
(EngineCore_DP0 pid=586259) 
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=586259) [rank0]:W0126 14:26:40.131000 586259 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=586259) [rank0]:W0126 14:26:40.200000 586259 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=586259) [rank0]:W0126 14:26:41.009000 586259 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=586259) [rank0]:W0126 14:26:41.107000 586259 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=586259) 2026-01-26 14:26:43,435 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=586259) 2026-01-26 14:26:43,461 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=586259) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 18.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 20.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 21.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 21.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 18.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 20.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 19.98it/s]
(EngineCore_DP0 pid=586259) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 23.62it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 23.74it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 23.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.79it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 33/8192 [00:00<00:24, 327.33it/s]
Adding requests:   1%|          | 76/8192 [00:00<00:21, 383.04it/s]
Adding requests:   1%|▏         | 120/8192 [00:00<00:19, 408.67it/s]
Adding requests:   2%|▏         | 164/8192 [00:00<00:19, 418.79it/s]
Adding requests:   3%|▎         | 208/8192 [00:00<00:18, 425.22it/s]
Adding requests:   3%|▎         | 256/8192 [00:00<00:17, 440.99it/s]
Adding requests:   4%|▎         | 301/8192 [00:00<00:17, 439.27it/s]
Adding requests:   4%|▍         | 348/8192 [00:00<00:17, 446.16it/s]
Adding requests:   5%|▍         | 396/8192 [00:00<00:17, 453.47it/s]
Adding requests:   5%|▌         | 442/8192 [00:01<00:17, 454.22it/s]
Adding requests:   6%|▌         | 488/8192 [00:01<00:16, 455.77it/s]
Adding requests:   7%|▋         | 534/8192 [00:01<00:17, 446.57it/s]
Adding requests:   7%|▋         | 582/8192 [00:01<00:16, 454.58it/s]
Adding requests:   8%|▊         | 630/8192 [00:01<00:16, 458.50it/s]
Adding requests:   8%|▊         | 678/8192 [00:01<00:16, 463.22it/s]
Adding requests:   9%|▉         | 726/8192 [00:01<00:15, 467.18it/s]
Adding requests:   9%|▉         | 773/8192 [00:01<00:16, 463.00it/s]
Adding requests:  10%|█         | 820/8192 [00:01<00:16, 456.45it/s]
Adding requests:  11%|█         | 867/8192 [00:01<00:16, 457.78it/s]
Adding requests:  11%|█         | 915/8192 [00:02<00:15, 462.51it/s]
Adding requests:  12%|█▏        | 962/8192 [00:02<00:15, 464.51it/s]
Adding requests:  12%|█▏        | 1009/8192 [00:02<00:15, 465.70it/s]
Adding requests:  13%|█▎        | 1056/8192 [00:02<00:15, 465.80it/s]
Adding requests:  13%|█▎        | 1103/8192 [00:02<00:15, 464.62it/s]
Adding requests:  14%|█▍        | 1150/8192 [00:02<00:15, 460.90it/s]
Adding requests:  15%|█▍        | 1200/8192 [00:02<00:14, 471.48it/s]
Adding requests:  15%|█▌        | 1248/8192 [00:02<00:14, 470.04it/s]
Adding requests:  16%|█▌        | 1296/8192 [00:02<00:15, 455.89it/s]
Adding requests:  16%|█▋        | 1343/8192 [00:02<00:14, 457.60it/s]
Adding requests:  17%|█▋        | 1391/8192 [00:03<00:14, 463.99it/s]
Adding requests:  18%|█▊        | 1438/8192 [00:03<00:14, 464.22it/s]
Adding requests:  18%|█▊        | 1487/8192 [00:03<00:14, 469.12it/s]
Adding requests:  19%|█▊        | 1534/8192 [00:03<00:14, 460.81it/s]
Adding requests:  19%|█▉        | 1582/8192 [00:03<00:14, 463.47it/s]
Adding requests:  20%|█▉        | 1632/8192 [00:03<00:13, 471.45it/s]
Adding requests:  21%|██        | 1680/8192 [00:03<00:13, 466.72it/s]
Adding requests:  21%|██        | 1729/8192 [00:03<00:13, 470.75it/s]
Adding requests:  22%|██▏       | 1777/8192 [00:03<00:13, 466.43it/s]
Adding requests:  22%|██▏       | 1825/8192 [00:03<00:13, 469.15it/s]
Adding requests:  23%|██▎       | 1872/8192 [00:04<00:13, 467.98it/s]
Adding requests:  23%|██▎       | 1919/8192 [00:04<00:13, 468.48it/s]
Adding requests:  24%|██▍       | 1966/8192 [00:04<00:13, 467.59it/s]
Adding requests:  25%|██▍       | 2014/8192 [00:04<00:13, 469.74it/s]
Adding requests:  25%|██▌       | 2063/8192 [00:04<00:12, 473.16it/s]
Adding requests:  26%|██▌       | 2111/8192 [00:04<00:12, 472.53it/s]
Adding requests:  26%|██▋       | 2159/8192 [00:04<00:13, 463.80it/s]
Adding requests:  27%|██▋       | 2206/8192 [00:04<00:13, 460.31it/s]
Adding requests:  28%|██▊       | 2255/8192 [00:04<00:12, 466.72it/s]
Adding requests:  28%|██▊       | 2302/8192 [00:05<00:12, 466.40it/s]
Adding requests:  29%|██▊       | 2349/8192 [00:05<00:12, 465.84it/s]
Adding requests:  29%|██▉       | 2396/8192 [00:05<00:12, 465.20it/s]
Adding requests:  30%|██▉       | 2443/8192 [00:05<00:12, 466.16it/s]
Adding requests:  30%|███       | 2491/8192 [00:05<00:12, 468.56it/s]
Adding requests:  31%|███       | 2538/8192 [00:05<00:12, 467.17it/s]
Adding requests:  32%|███▏      | 2586/8192 [00:05<00:11, 470.11it/s]
Adding requests:  32%|███▏      | 2634/8192 [00:05<00:12, 455.33it/s]
Adding requests:  33%|███▎      | 2681/8192 [00:05<00:11, 459.44it/s]
Adding requests:  33%|███▎      | 2728/8192 [00:05<00:11, 459.23it/s]
Adding requests:  34%|███▍      | 2775/8192 [00:06<00:11, 461.28it/s]
Adding requests:  34%|███▍      | 2822/8192 [00:06<00:11, 455.84it/s]
Adding requests:  35%|███▌      | 2870/8192 [00:06<00:11, 460.57it/s]
Adding requests:  36%|███▌      | 2917/8192 [00:06<00:11, 461.24it/s]
Adding requests:  36%|███▌      | 2964/8192 [00:06<00:11, 459.61it/s]
Adding requests:  37%|███▋      | 3012/8192 [00:06<00:11, 463.30it/s]
Adding requests:  37%|███▋      | 3059/8192 [00:06<00:11, 463.41it/s]
Adding requests:  38%|███▊      | 3106/8192 [00:06<00:11, 461.29it/s]
Adding requests:  38%|███▊      | 3153/8192 [00:06<00:10, 462.39it/s]
Adding requests:  39%|███▉      | 3200/8192 [00:06<00:10, 463.13it/s]
Adding requests:  40%|███▉      | 3248/8192 [00:07<00:10, 467.76it/s]
Adding requests:  40%|████      | 3295/8192 [00:07<00:10, 466.97it/s]
Adding requests:  41%|████      | 3343/8192 [00:07<00:10, 468.68it/s]
Adding requests:  41%|████▏     | 3390/8192 [00:07<00:10, 467.26it/s]
Adding requests:  42%|████▏     | 3439/8192 [00:07<00:10, 471.55it/s]
Adding requests:  43%|████▎     | 3487/8192 [00:07<00:10, 461.80it/s]
Adding requests:  43%|████▎     | 3534/8192 [00:07<00:10, 463.97it/s]
Adding requests:  44%|████▎     | 3581/8192 [00:07<00:09, 463.69it/s]
Adding requests:  44%|████▍     | 3628/8192 [00:07<00:09, 463.40it/s]
Adding requests:  45%|████▍     | 3676/8192 [00:07<00:09, 466.09it/s]
Adding requests:  45%|████▌     | 3723/8192 [00:08<00:09, 463.16it/s]
Adding requests:  46%|████▌     | 3772/8192 [00:08<00:09, 469.05it/s]
Adding requests:  47%|████▋     | 3819/8192 [00:08<00:09, 460.78it/s]
Adding requests:  47%|████▋     | 3867/8192 [00:08<00:09, 466.04it/s]
Adding requests:  48%|████▊     | 3914/8192 [00:08<00:09, 464.44it/s]
Adding requests:  48%|████▊     | 3962/8192 [00:08<00:09, 466.29it/s]
Adding requests:  49%|████▉     | 4009/8192 [00:08<00:09, 464.77it/s]
Adding requests:  50%|████▉     | 4056/8192 [00:08<00:08, 461.02it/s]
Adding requests:  50%|█████     | 4103/8192 [00:08<00:08, 463.10it/s]
Adding requests:  51%|█████     | 4150/8192 [00:09<00:08, 461.51it/s]
Adding requests:  51%|█████     | 4198/8192 [00:09<00:08, 466.62it/s]
Adding requests:  52%|█████▏    | 4245/8192 [00:09<00:08, 467.47it/s]
Adding requests:  52%|█████▏    | 4292/8192 [00:09<00:08, 464.90it/s]
Adding requests:  53%|█████▎    | 4341/8192 [00:09<00:08, 471.39it/s]
Adding requests:  54%|█████▎    | 4390/8192 [00:09<00:08, 474.85it/s]
Adding requests:  54%|█████▍    | 4438/8192 [00:09<00:07, 474.08it/s]
Adding requests:  55%|█████▍    | 4486/8192 [00:09<00:07, 469.04it/s]
Adding requests:  55%|█████▌    | 4533/8192 [00:09<00:07, 467.01it/s]
Adding requests:  56%|█████▌    | 4581/8192 [00:09<00:07, 468.43it/s]
Adding requests:  57%|█████▋    | 4629/8192 [00:10<00:07, 470.82it/s]
Adding requests:  57%|█████▋    | 4677/8192 [00:10<00:07, 468.61it/s]
Adding requests:  58%|█████▊    | 4725/8192 [00:10<00:07, 470.61it/s]
Adding requests:  58%|█████▊    | 4773/8192 [00:10<00:07, 469.61it/s]
Adding requests:  59%|█████▉    | 4820/8192 [00:10<00:07, 468.85it/s]
Adding requests:  59%|█████▉    | 4867/8192 [00:10<00:07, 467.89it/s]
Adding requests:  60%|█████▉    | 4914/8192 [00:10<00:07, 465.21it/s]
Adding requests:  61%|██████    | 4961/8192 [00:10<00:07, 455.32it/s]
Adding requests:  61%|██████    | 5009/8192 [00:10<00:06, 459.64it/s]
Adding requests:  62%|██████▏   | 5057/8192 [00:10<00:06, 464.58it/s]
Adding requests:  62%|██████▏   | 5106/8192 [00:11<00:06, 470.97it/s]
Adding requests:  63%|██████▎   | 5154/8192 [00:11<00:06, 465.90it/s]
Adding requests:  63%|██████▎   | 5201/8192 [00:11<00:06, 464.18it/s]
Adding requests:  64%|██████▍   | 5248/8192 [00:11<00:06, 461.34it/s]
Adding requests:  65%|██████▍   | 5295/8192 [00:11<00:06, 462.08it/s]
Adding requests:  65%|██████▌   | 5343/8192 [00:11<00:06, 465.20it/s]
Adding requests:  66%|██████▌   | 5390/8192 [00:11<00:06, 465.02it/s]
Adding requests:  66%|██████▋   | 5437/8192 [00:11<00:05, 465.55it/s]
Adding requests:  67%|██████▋   | 5484/8192 [00:11<00:05, 460.85it/s]
Adding requests:  68%|██████▊   | 5531/8192 [00:11<00:05, 460.70it/s]
Adding requests:  68%|██████▊   | 5578/8192 [00:12<00:05, 459.70it/s]
Adding requests:  69%|██████▊   | 5624/8192 [00:12<00:05, 457.67it/s]
Adding requests:  69%|██████▉   | 5670/8192 [00:12<00:05, 453.51it/s]
Adding requests:  70%|██████▉   | 5718/8192 [00:12<00:05, 459.60it/s]
Adding requests:  70%|███████   | 5765/8192 [00:12<00:05, 461.08it/s]
Adding requests:  71%|███████   | 5812/8192 [00:12<00:05, 456.79it/s]
Adding requests:  72%|███████▏  | 5859/8192 [00:12<00:05, 457.15it/s]
Adding requests:  72%|███████▏  | 5907/8192 [00:12<00:04, 462.87it/s]
Adding requests:  73%|███████▎  | 5954/8192 [00:12<00:04, 460.97it/s]
Adding requests:  73%|███████▎  | 6002/8192 [00:12<00:04, 465.35it/s]
Adding requests:  74%|███████▍  | 6051/8192 [00:13<00:04, 469.89it/s]
Adding requests:  74%|███████▍  | 6098/8192 [00:13<00:04, 466.37it/s]
Adding requests:  75%|███████▌  | 6145/8192 [00:13<00:04, 467.22it/s]
Adding requests:  76%|███████▌  | 6192/8192 [00:13<00:04, 455.91it/s]
Adding requests:  76%|███████▌  | 6241/8192 [00:13<00:04, 463.19it/s]
Adding requests:  77%|███████▋  | 6290/8192 [00:13<00:04, 468.72it/s]
Adding requests:  77%|███████▋  | 6338/8192 [00:13<00:03, 471.29it/s]
Adding requests:  78%|███████▊  | 6386/8192 [00:13<00:03, 471.42it/s]
Adding requests:  79%|███████▊  | 6436/8192 [00:13<00:03, 477.22it/s]
Adding requests:  79%|███████▉  | 6484/8192 [00:14<00:03, 477.24it/s]
Adding requests:  80%|███████▉  | 6533/8192 [00:14<00:03, 480.81it/s]
Adding requests:  80%|████████  | 6582/8192 [00:14<00:03, 474.81it/s]
Adding requests:  81%|████████  | 6630/8192 [00:14<00:03, 474.16it/s]
Adding requests:  82%|████████▏ | 6678/8192 [00:14<00:03, 471.61it/s]
Adding requests:  82%|████████▏ | 6726/8192 [00:14<00:03, 462.77it/s]
Adding requests:  83%|████████▎ | 6773/8192 [00:14<00:03, 462.73it/s]
Adding requests:  83%|████████▎ | 6822/8192 [00:14<00:02, 467.95it/s]
Adding requests:  84%|████████▍ | 6871/8192 [00:14<00:02, 473.39it/s]
Adding requests:  84%|████████▍ | 6919/8192 [00:14<00:02, 474.17it/s]
Adding requests:  85%|████████▌ | 6968/8192 [00:15<00:02, 477.52it/s]
Adding requests:  86%|████████▌ | 7016/8192 [00:15<00:02, 470.97it/s]
Adding requests:  86%|████████▌ | 7064/8192 [00:15<00:02, 471.56it/s]
Adding requests:  87%|████████▋ | 7112/8192 [00:15<00:02, 473.32it/s]
Adding requests:  87%|████████▋ | 7160/8192 [00:15<00:02, 470.17it/s]
Adding requests:  88%|████████▊ | 7208/8192 [00:15<00:02, 470.08it/s]
Adding requests:  89%|████████▊ | 7256/8192 [00:15<00:01, 472.69it/s]
Adding requests:  89%|████████▉ | 7304/8192 [00:15<00:01, 474.05it/s]
Adding requests:  90%|████████▉ | 7352/8192 [00:15<00:01, 457.79it/s]
Adding requests:  90%|█████████ | 7401/8192 [00:15<00:01, 464.78it/s]
Adding requests:  91%|█████████ | 7451/8192 [00:16<00:01, 473.85it/s]
Adding requests:  92%|█████████▏| 7499/8192 [00:16<00:01, 471.82it/s]
Adding requests:  92%|█████████▏| 7547/8192 [00:16<00:01, 472.92it/s]
Adding requests:  93%|█████████▎| 7595/8192 [00:16<00:01, 471.42it/s]
Adding requests:  93%|█████████▎| 7643/8192 [00:16<00:01, 470.22it/s]
Adding requests:  94%|█████████▍| 7692/8192 [00:16<00:01, 475.20it/s]
Adding requests:  94%|█████████▍| 7740/8192 [00:16<00:00, 472.19it/s]
Adding requests:  95%|█████████▌| 7788/8192 [00:16<00:00, 466.88it/s]
Adding requests:  96%|█████████▌| 7836/8192 [00:16<00:00, 469.96it/s]
Adding requests:  96%|█████████▌| 7884/8192 [00:16<00:00, 469.06it/s]
Adding requests:  97%|█████████▋| 7931/8192 [00:17<00:00, 466.02it/s]
Adding requests:  97%|█████████▋| 7978/8192 [00:17<00:00, 465.23it/s]
Adding requests:  98%|█████████▊| 8025/8192 [00:17<00:00, 461.64it/s]
Adding requests:  99%|█████████▊| 8074/8192 [00:17<00:00, 468.04it/s]
Adding requests:  99%|█████████▉| 8122/8192 [00:17<00:00, 471.40it/s]
Adding requests: 100%|█████████▉| 8170/8192 [00:17<00:00, 470.36it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 464.34it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  73%|███████▎  | 5976/8192 [00:00<00:00, 59585.32it/s, est. speed input: 61020154.58 toks/s, output: 59586.59 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 59585.32it/s, est. speed input: 60588333.10 toks/s, output: 59166.46 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 59119.22it/s, est. speed input: 60588333.10 toks/s, output: 59166.46 toks/s]
[rank0]:[W126 14:27:04.075224663 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 87.0s

测试结果:
  Requests/s:   460.64
  Tokens/s:     472157.66
  Total Reqs:   8192
  Elapsed:      17.78s

  [Prefill 分析]
  Total Prefill Tokens: 8388608
  Prefill Tokens/s:     471697.02


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/Llama3.2-1B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,42.4878,21796.2336,3.0126
1024,1024,1,128,128,41.5700,42609.2003,3.0791
2048,1024,2,256,128,82.7699,84839.1592,3.0929
4096,1024,4,512,128,164.6420,168758.0463,3.1098
8192,1024,8,1024,128,290.0380,297288.9647,3.5306
16384,1024,16,2048,128,447.4309,458616.6443,4.5772
32768,1024,32,4096,128,465.4036,477038.7070,8.8010
65536,1024,64,8192,128,460.6416,472157.6577,17.7839

------------------------------------------------------------

[INFO] 完成: 8 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 40 成功, 0 失败
============================================================
