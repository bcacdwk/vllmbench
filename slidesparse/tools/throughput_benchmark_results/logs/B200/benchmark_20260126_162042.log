======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-26 16:20:42
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768,65536

命令行参数:
  --model: qwen2.5-7b-fp8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: prefill
  --M: 512,1024,2048,4096,8192,16384,32768,65536
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: B200
  Compute Capability: cc100
  VRAM: 178.4 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  Qwen2.5-7B-FP8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:20:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=750685) WARNING 01-26 16:21:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=750685) WARNING 01-26 16:21:15 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 36.04 requests/s, 18489.05 total tokens/s, 36.04 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:20:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:20:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:20:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:20:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:20:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:20:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:20:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:20:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:20:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:20:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:20:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:20:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:20:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:20:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:20:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:20:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:20:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:20:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:20:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:20:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:20:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:20:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:20:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:20:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:20:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:20:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:20:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:20:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=750685) [2026-01-26 16:20:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=750685) [2026-01-26 16:20:57] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=750685) [2026-01-26 16:20:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=750685) [2026-01-26 16:20:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=750685) [2026-01-26 16:20:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=750685) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=750685) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.41it/s]
(EngineCore_DP0 pid=750685) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
(EngineCore_DP0 pid=750685) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]
(EngineCore_DP0 pid=750685) 
(EngineCore_DP0 pid=750685) 2026-01-26 16:21:15,303 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=750685) 2026-01-26 16:21:15,328 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=750685) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.96it/s]
(EngineCore_DP0 pid=750685) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.74it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 519.73it/s]
Adding requests:  88%|████████▊ | 112/128 [00:00<00:00, 565.37it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 564.26it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 48.27it/s, est. speed input: 24718.10 toks/s, output: 48.27 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:02, 41.77it/s, est. speed input: 21827.02 toks/s, output: 42.63 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 39.36it/s, est. speed input: 20739.30 toks/s, output: 40.51 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 38.92it/s, est. speed input: 20469.29 toks/s, output: 39.98 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 38.73it/s, est. speed input: 20316.86 toks/s, output: 39.68 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 38.51it/s, est. speed input: 20188.08 toks/s, output: 39.43 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 38.45it/s, est. speed input: 20113.18 toks/s, output: 39.28 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:02, 38.39it/s, est. speed input: 20052.67 toks/s, output: 39.17 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:02, 38.35it/s, est. speed input: 20003.88 toks/s, output: 39.07 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 38.32it/s, est. speed input: 19964.06 toks/s, output: 38.99 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 38.31it/s, est. speed input: 19932.26 toks/s, output: 38.93 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 38.19it/s, est. speed input: 19890.88 toks/s, output: 38.85 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 38.20it/s, est. speed input: 19867.14 toks/s, output: 38.80 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 38.26it/s, est. speed input: 19852.55 toks/s, output: 38.77 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 38.17it/s, est. speed input: 19826.13 toks/s, output: 38.72 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 38.20it/s, est. speed input: 19811.88 toks/s, output: 38.69 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 38.21it/s, est. speed input: 19798.05 toks/s, output: 38.67 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 38.23it/s, est. speed input: 19787.25 toks/s, output: 38.65 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 38.23it/s, est. speed input: 19776.90 toks/s, output: 38.63 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 38.29it/s, est. speed input: 19772.16 toks/s, output: 38.62 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 38.28it/s, est. speed input: 19763.56 toks/s, output: 38.60 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 38.38it/s, est. speed input: 19763.86 toks/s, output: 38.60 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 38.36it/s, est. speed input: 19757.17 toks/s, output: 38.59 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 38.25it/s, est. speed input: 19744.63 toks/s, output: 38.56 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 38.27it/s, est. speed input: 19740.25 toks/s, output: 38.56 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 38.34it/s, est. speed input: 19739.19 toks/s, output: 38.55 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 38.29it/s, est. speed input: 19731.86 toks/s, output: 38.54 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 38.31it/s, est. speed input: 19728.91 toks/s, output: 38.53 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 38.32it/s, est. speed input: 19725.25 toks/s, output: 38.53 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 38.28it/s, est. speed input: 19719.99 toks/s, output: 38.52 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 38.29it/s, est. speed input: 19716.56 toks/s, output: 38.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.29it/s, est. speed input: 19718.33 toks/s, output: 38.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.51it/s, est. speed input: 19718.33 toks/s, output: 38.51 toks/s]
[rank0]:[W126 16:21:21.343749929 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.6s

测试结果:
  Requests/s:   36.04
  Tokens/s:     18489.05
  Total Reqs:   128
  Elapsed:      3.55s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     18453.01

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:21:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=751915) WARNING 01-26 16:21:46 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=751915) WARNING 01-26 16:21:55 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.68 requests/s, 38624.78 total tokens/s, 37.68 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:21:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:21:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:21:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:21:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:21:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:21:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:21:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:21:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:21:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:21:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:21:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:21:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:21:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:21:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:21:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:21:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:21:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:21:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:21:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:21:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:21:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:21:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:21:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:21:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:21:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:21:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:21:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:21:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=751915) [2026-01-26 16:21:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=751915) [2026-01-26 16:21:37] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=751915) [2026-01-26 16:21:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=751915) [2026-01-26 16:21:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=751915) [2026-01-26 16:21:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=751915) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=751915) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=751915) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=751915) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.23it/s]
(EngineCore_DP0 pid=751915) 
(EngineCore_DP0 pid=751915) 2026-01-26 16:21:55,777 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=751915) 2026-01-26 16:21:55,801 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=751915) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.18it/s]
(EngineCore_DP0 pid=751915) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.76it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 272.38it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 315.56it/s]
Adding requests:  74%|███████▍  | 95/128 [00:00<00:00, 293.53it/s]
Adding requests:  98%|█████████▊| 125/128 [00:00<00:00, 295.77it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 295.73it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:00, 160.07it/s, est. speed input: 163923.46 toks/s, output: 160.07 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 56.95it/s, est. speed input: 64936.89 toks/s, output: 63.41 toks/s]   
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 49.68it/s, est. speed input: 57361.46 toks/s, output: 56.02 toks/s]
Processed prompts:  41%|████      | 52/128 [00:00<00:01, 46.24it/s, est. speed input: 53983.30 toks/s, output: 52.72 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 44.09it/s, est. speed input: 51951.92 toks/s, output: 50.73 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 42.75it/s, est. speed input: 50677.65 toks/s, output: 49.49 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 41.66it/s, est. speed input: 49636.19 toks/s, output: 48.47 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 40.77it/s, est. speed input: 48756.34 toks/s, output: 47.61 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 40.19it/s, est. speed input: 48043.53 toks/s, output: 46.92 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:01, 39.71it/s, est. speed input: 47418.49 toks/s, output: 46.31 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:01, 39.39it/s, est. speed input: 46973.17 toks/s, output: 45.87 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 39.12it/s, est. speed input: 46572.19 toks/s, output: 45.48 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 38.99it/s, est. speed input: 46228.16 toks/s, output: 45.14 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 38.31it/s, est. speed input: 45797.31 toks/s, output: 44.72 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 38.88it/s, est. speed input: 45545.08 toks/s, output: 44.48 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 38.77it/s, est. speed input: 45283.75 toks/s, output: 44.22 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 38.65it/s, est. speed input: 45036.69 toks/s, output: 43.98 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 38.63it/s, est. speed input: 44819.89 toks/s, output: 43.77 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 38.56it/s, est. speed input: 44611.80 toks/s, output: 43.57 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:02<00:00, 38.47it/s, est. speed input: 44412.80 toks/s, output: 43.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 38.50it/s, est. speed input: 44240.44 toks/s, output: 43.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 38.50it/s, est. speed input: 44240.44 toks/s, output: 43.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.20it/s, est. speed input: 44240.44 toks/s, output: 43.20 toks/s]
[rank0]:[W126 16:22:00.027754006 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.8s

测试结果:
  Requests/s:   37.68
  Tokens/s:     38624.78
  Total Reqs:   128
  Elapsed:      3.40s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     38587.09

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:22:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=753038) WARNING 01-26 16:22:26 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=753038) WARNING 01-26 16:22:36 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 69.48 requests/s, 71213.01 total tokens/s, 69.48 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 16:22:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:22:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:22:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:22:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:22:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:22:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:22:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:22:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:22:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:22:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:22:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:22:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:22:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:22:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:22:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:22:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=753038) [2026-01-26 16:22:18] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=753038) [2026-01-26 16:22:18] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=753038) [2026-01-26 16:22:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=753038) [2026-01-26 16:22:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=753038) [2026-01-26 16:22:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=753038) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=753038) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.62it/s]
(EngineCore_DP0 pid=753038) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=753038) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.24it/s]
(EngineCore_DP0 pid=753038) 
(EngineCore_DP0 pid=753038) 2026-01-26 16:22:36,188 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=753038) 2026-01-26 16:22:36,212 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=753038) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 13.58it/s]
(EngineCore_DP0 pid=753038) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.34it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:51,  4.93it/s]
Adding requests:  11%|█▏        | 29/256 [00:00<00:01, 118.01it/s]
Adding requests:  25%|██▌       | 64/256 [00:00<00:00, 203.34it/s]
Adding requests:  37%|███▋      | 95/256 [00:00<00:00, 239.68it/s]
Adding requests:  50%|█████     | 129/256 [00:00<00:00, 269.77it/s]
Adding requests:  64%|██████▎   | 163/256 [00:00<00:00, 291.52it/s]
Adding requests:  77%|███████▋  | 197/256 [00:00<00:00, 305.68it/s]
Adding requests:  90%|█████████ | 231/256 [00:00<00:00, 314.76it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 258.21it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:00<00:00, 518.50it/s, est. speed input: 531015.36 toks/s, output: 518.52 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:00<00:01, 122.50it/s, est. speed input: 142992.20 toks/s, output: 139.64 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:01<00:01, 104.46it/s, est. speed input: 123556.22 toks/s, output: 120.66 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:01<00:01, 96.62it/s, est. speed input: 115764.95 toks/s, output: 113.05 toks/s] 
Processed prompts:  66%|██████▋   | 170/256 [00:01<00:00, 91.86it/s, est. speed input: 111362.38 toks/s, output: 108.75 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:01<00:00, 88.42it/s, est. speed input: 108347.12 toks/s, output: 105.81 toks/s]
Processed prompts:  75%|███████▌  | 193/256 [00:01<00:00, 87.48it/s, est. speed input: 106762.03 toks/s, output: 104.26 toks/s]
Processed prompts:  79%|███████▉  | 203/256 [00:01<00:00, 84.89it/s, est. speed input: 104867.89 toks/s, output: 102.41 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 80.80it/s, est. speed input: 102705.75 toks/s, output: 100.30 toks/s]
Processed prompts:  86%|████████▋ | 221/256 [00:02<00:00, 81.72it/s, est. speed input: 101954.61 toks/s, output: 99.56 toks/s] 
Processed prompts:  90%|████████▉ | 230/256 [00:02<00:00, 77.99it/s, est. speed input: 100178.73 toks/s, output: 97.83 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:02<00:00, 77.68it/s, est. speed input: 99259.30 toks/s, output: 96.93 toks/s] 
Processed prompts:  96%|█████████▌| 246/256 [00:02<00:00, 77.12it/s, est. speed input: 98352.82 toks/s, output: 96.05 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:02<00:00, 76.88it/s, est. speed input: 97554.55 toks/s, output: 95.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 76.88it/s, est. speed input: 97370.29 toks/s, output: 95.09 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 95.09it/s, est. speed input: 97370.29 toks/s, output: 95.09 toks/s]
[rank0]:[W126 16:22:41.866344640 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.8s

测试结果:
  Requests/s:   69.48
  Tokens/s:     71213.01
  Total Reqs:   256
  Elapsed:      3.68s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     71143.53

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:22:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=754190) WARNING 01-26 16:23:09 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=754190) WARNING 01-26 16:23:18 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 113.95 requests/s, 116802.32 total tokens/s, 113.95 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 16:22:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:22:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:22:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:22:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:22:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:22:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:22:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:22:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:22:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:22:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:22:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:22:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:22:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:22:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:22:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:22:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:22:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=754190) [2026-01-26 16:23:00] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=754190) [2026-01-26 16:23:01] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=754190) [2026-01-26 16:23:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=754190) [2026-01-26 16:23:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=754190) [2026-01-26 16:23:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=754190) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=754190) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=754190) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
(EngineCore_DP0 pid=754190) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=754190) 
(EngineCore_DP0 pid=754190) 2026-01-26 16:23:18,856 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=754190) 2026-01-26 16:23:18,879 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=754190) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.07it/s]
(EngineCore_DP0 pid=754190) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 14.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.34it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 275.17it/s]
Adding requests:  12%|█▏        | 63/512 [00:00<00:01, 316.61it/s]
Adding requests:  19%|█▊        | 95/512 [00:00<00:01, 314.10it/s]
Adding requests:  25%|██▌       | 129/512 [00:00<00:01, 320.46it/s]
Adding requests:  32%|███▏      | 164/512 [00:00<00:01, 328.84it/s]
Adding requests:  39%|███▉      | 201/512 [00:00<00:00, 339.78it/s]
Adding requests:  46%|████▋     | 237/512 [00:00<00:00, 344.83it/s]
Adding requests:  53%|█████▎    | 272/512 [00:00<00:00, 342.64it/s]
Adding requests:  60%|██████    | 309/512 [00:00<00:00, 349.54it/s]
Adding requests:  68%|██████▊   | 346/512 [00:01<00:00, 355.70it/s]
Adding requests:  75%|███████▍  | 382/512 [00:01<00:00, 351.89it/s]
Adding requests:  82%|████████▏ | 421/512 [00:01<00:00, 361.09it/s]
Adding requests:  89%|████████▉ | 458/512 [00:01<00:00, 358.28it/s]
Adding requests:  97%|█████████▋| 498/512 [00:01<00:00, 367.39it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 347.65it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:00<00:00, 1575.74it/s, est. speed input: 1613652.24 toks/s, output: 1575.76 toks/s]
Processed prompts:  65%|██████▍   | 332/512 [00:01<00:00, 195.99it/s, est. speed input: 232736.93 toks/s, output: 227.28 toks/s]   
Processed prompts:  79%|███████▉  | 404/512 [00:02<00:00, 164.75it/s, est. speed input: 198199.88 toks/s, output: 193.55 toks/s]
Processed prompts:  88%|████████▊ | 449/512 [00:02<00:00, 153.10it/s, est. speed input: 186314.29 toks/s, output: 181.95 toks/s]
Processed prompts:  94%|█████████▍| 481/512 [00:02<00:00, 145.08it/s, est. speed input: 179295.45 toks/s, output: 175.09 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:02<00:00, 136.26it/s, est. speed input: 173212.14 toks/s, output: 169.15 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 136.26it/s, est. speed input: 173654.89 toks/s, output: 169.58 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 169.58it/s, est. speed input: 173654.89 toks/s, output: 169.58 toks/s]
[rank0]:[W126 16:23:25.953880740 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.9s

测试结果:
  Requests/s:   113.95
  Tokens/s:     116802.32
  Total Reqs:   512
  Elapsed:      4.49s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     116688.36

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:23:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=755398) WARNING 01-26 16:23:56 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=755398) WARNING 01-26 16:24:05 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 119.80 requests/s, 122799.08 total tokens/s, 119.80 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 16:23:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:23:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:23:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:23:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:23:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:23:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:23:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:23:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:23:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:23:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:23:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:23:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:23:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:23:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:23:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:23:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:23:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:23:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:23:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:23:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:23:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:23:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:23:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:23:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:23:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:23:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:23:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:23:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=755398) [2026-01-26 16:23:47] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=755398) [2026-01-26 16:23:47] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=755398) [2026-01-26 16:23:47] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=755398) [2026-01-26 16:23:47] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=755398) [2026-01-26 16:23:47] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=755398) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=755398) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=755398) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]
(EngineCore_DP0 pid=755398) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=755398) 
(EngineCore_DP0 pid=755398) 2026-01-26 16:24:05,918 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=755398) 2026-01-26 16:24:05,946 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=755398) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 15.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 16.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 14.81it/s]
(EngineCore_DP0 pid=755398) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 17.23it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.45it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 25/1024 [00:00<00:04, 242.87it/s]
Adding requests:   6%|▌         | 58/1024 [00:00<00:03, 293.09it/s]
Adding requests:   9%|▉         | 91/1024 [00:00<00:03, 307.62it/s]
Adding requests:  12%|█▏        | 124/1024 [00:00<00:02, 314.80it/s]
Adding requests:  16%|█▌        | 159/1024 [00:00<00:02, 325.68it/s]
Adding requests:  19%|█▉        | 195/1024 [00:00<00:02, 336.68it/s]
Adding requests:  23%|██▎       | 231/1024 [00:00<00:02, 343.58it/s]
Adding requests:  26%|██▌       | 266/1024 [00:00<00:02, 344.18it/s]
Adding requests:  29%|██▉       | 302/1024 [00:00<00:02, 347.12it/s]
Adding requests:  33%|███▎      | 339/1024 [00:01<00:01, 352.97it/s]
Adding requests:  37%|███▋      | 376/1024 [00:01<00:01, 358.10it/s]
Adding requests:  40%|████      | 412/1024 [00:01<00:01, 356.05it/s]
Adding requests:  44%|████▍     | 448/1024 [00:01<00:01, 354.49it/s]
Adding requests:  48%|████▊     | 488/1024 [00:01<00:01, 367.55it/s]
Adding requests:  52%|█████▏    | 528/1024 [00:01<00:01, 373.99it/s]
Adding requests:  55%|█████▌    | 566/1024 [00:01<00:01, 371.62it/s]
Adding requests:  59%|█████▉    | 604/1024 [00:01<00:01, 358.43it/s]
Adding requests:  62%|██████▎   | 640/1024 [00:01<00:01, 357.55it/s]
Adding requests:  66%|██████▌   | 676/1024 [00:01<00:00, 353.33it/s]
Adding requests:  70%|██████▉   | 714/1024 [00:02<00:00, 359.01it/s]
Adding requests:  73%|███████▎  | 750/1024 [00:02<00:00, 352.48it/s]
Adding requests:  77%|███████▋  | 787/1024 [00:02<00:00, 354.61it/s]
Adding requests:  80%|████████  | 823/1024 [00:02<00:00, 354.59it/s]
Adding requests:  84%|████████▍ | 861/1024 [00:02<00:00, 360.50it/s]
Adding requests:  88%|████████▊ | 898/1024 [00:02<00:00, 362.28it/s]
Adding requests:  91%|█████████▏| 935/1024 [00:02<00:00, 354.67it/s]
Adding requests:  95%|█████████▍| 972/1024 [00:02<00:00, 358.08it/s]
Adding requests:  98%|█████████▊| 1008/1024 [00:02<00:00, 353.39it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 350.46it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  34%|███▍      | 353/1024 [00:00<00:00, 3287.55it/s, est. speed input: 3366679.37 toks/s, output: 3287.60 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:02<00:01, 205.59it/s, est. speed input: 246393.95 toks/s, output: 240.62 toks/s]   
Processed prompts:  81%|████████  | 825/1024 [00:04<00:01, 173.01it/s, est. speed input: 209608.66 toks/s, output: 204.70 toks/s]
Processed prompts:  89%|████████▊ | 908/1024 [00:04<00:00, 161.89it/s, est. speed input: 198048.43 toks/s, output: 193.41 toks/s]
Processed prompts:  94%|█████████▍| 964/1024 [00:05<00:00, 154.04it/s, est. speed input: 191270.32 toks/s, output: 186.79 toks/s]
Processed prompts:  98%|█████████▊| 1004/1024 [00:05<00:00, 148.69it/s, est. speed input: 187187.11 toks/s, output: 182.80 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 148.69it/s, est. speed input: 186445.80 toks/s, output: 182.08 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 182.07it/s, est. speed input: 186445.80 toks/s, output: 182.08 toks/s]
[rank0]:[W126 16:24:16.981116637 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.1s

测试结果:
  Requests/s:   119.80
  Tokens/s:     122799.08
  Total Reqs:   1024
  Elapsed:      8.55s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     122679.27

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:24:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=756726) WARNING 01-26 16:24:53 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=756726) WARNING 01-26 16:25:03 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 125.55 requests/s, 128688.35 total tokens/s, 125.55 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 16:24:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:24:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:24:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:24:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:24:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:24:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:24:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:24:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:24:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:24:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:24:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:24:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:24:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:24:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:24:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:24:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:24:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:24:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:24:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:24:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:24:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:24:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:24:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:24:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:24:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:24:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:24:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:24:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=756726) [2026-01-26 16:24:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=756726) [2026-01-26 16:24:45] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=756726) [2026-01-26 16:24:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=756726) [2026-01-26 16:24:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=756726) [2026-01-26 16:24:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=756726) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=756726) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.41it/s]
(EngineCore_DP0 pid=756726) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.13it/s]
(EngineCore_DP0 pid=756726) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.17it/s]
(EngineCore_DP0 pid=756726) 
(EngineCore_DP0 pid=756726) [rank0]:W0126 16:24:58.286000 756726 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=756726) [rank0]:W0126 16:24:58.368000 756726 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=756726) [rank0]:W0126 16:24:59.309000 756726 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=756726) [rank0]:W0126 16:24:59.439000 756726 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=756726) 2026-01-26 16:25:03,122 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=756726) 2026-01-26 16:25:03,149 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=756726) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 15.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.91it/s]
(EngineCore_DP0 pid=756726) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 17.18it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 15.84it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 15.96it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 276.23it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 316.88it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 314.95it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:05, 322.33it/s]
Adding requests:   8%|▊         | 164/2048 [00:00<00:05, 332.07it/s]
Adding requests:  10%|▉         | 201/2048 [00:00<00:05, 342.69it/s]
Adding requests:  12%|█▏        | 237/2048 [00:00<00:05, 347.63it/s]
Adding requests:  13%|█▎        | 272/2048 [00:00<00:05, 344.95it/s]
Adding requests:  15%|█▌        | 309/2048 [00:00<00:04, 351.10it/s]
Adding requests:  17%|█▋        | 347/2048 [00:01<00:04, 357.22it/s]
Adding requests:  19%|█▉        | 384/2048 [00:01<00:04, 359.98it/s]
Adding requests:  21%|██        | 423/2048 [00:01<00:04, 367.61it/s]
Adding requests:  22%|██▏       | 460/2048 [00:01<00:04, 362.19it/s]
Adding requests:  24%|██▍       | 500/2048 [00:01<00:04, 371.11it/s]
Adding requests:  26%|██▋       | 539/2048 [00:01<00:04, 375.11it/s]
Adding requests:  28%|██▊       | 577/2048 [00:01<00:03, 374.00it/s]
Adding requests:  30%|███       | 615/2048 [00:01<00:03, 362.27it/s]
Adding requests:  32%|███▏      | 652/2048 [00:01<00:03, 355.95it/s]
Adding requests:  34%|███▎      | 690/2048 [00:01<00:03, 360.93it/s]
Adding requests:  35%|███▌      | 727/2048 [00:02<00:03, 349.75it/s]
Adding requests:  37%|███▋      | 763/2048 [00:02<00:03, 350.78it/s]
Adding requests:  39%|███▉      | 800/2048 [00:02<00:03, 353.27it/s]
Adding requests:  41%|████      | 837/2048 [00:02<00:03, 357.71it/s]
Adding requests:  43%|████▎     | 875/2048 [00:02<00:03, 360.79it/s]
Adding requests:  45%|████▍     | 912/2048 [00:02<00:03, 360.32it/s]
Adding requests:  46%|████▋     | 949/2048 [00:02<00:03, 354.98it/s]
Adding requests:  48%|████▊     | 986/2048 [00:02<00:02, 358.00it/s]
Adding requests:  50%|████▉     | 1022/2048 [00:02<00:02, 354.56it/s]
Adding requests:  52%|█████▏    | 1058/2048 [00:02<00:02, 352.43it/s]
Adding requests:  53%|█████▎    | 1094/2048 [00:03<00:02, 350.23it/s]
Adding requests:  55%|█████▌    | 1133/2048 [00:03<00:02, 358.56it/s]
Adding requests:  57%|█████▋    | 1169/2048 [00:03<00:02, 353.76it/s]
Adding requests:  59%|█████▉    | 1206/2048 [00:03<00:02, 357.39it/s]
Adding requests:  61%|██████    | 1243/2048 [00:03<00:02, 360.00it/s]
Adding requests:  62%|██████▎   | 1280/2048 [00:03<00:02, 355.18it/s]
Adding requests:  64%|██████▍   | 1316/2048 [00:03<00:02, 355.47it/s]
Adding requests:  66%|██████▌   | 1354/2048 [00:03<00:01, 360.97it/s]
Adding requests:  68%|██████▊   | 1391/2048 [00:03<00:01, 360.69it/s]
Adding requests:  70%|██████▉   | 1428/2048 [00:04<00:01, 355.62it/s]
Adding requests:  72%|███████▏  | 1465/2048 [00:04<00:01, 357.90it/s]
Adding requests:  73%|███████▎  | 1502/2048 [00:04<00:01, 360.96it/s]
Adding requests:  75%|███████▌  | 1539/2048 [00:04<00:01, 359.99it/s]
Adding requests:  77%|███████▋  | 1576/2048 [00:04<00:01, 351.66it/s]
Adding requests:  79%|███████▊  | 1612/2048 [00:04<00:01, 353.53it/s]
Adding requests:  80%|████████  | 1648/2048 [00:04<00:01, 343.71it/s]
Adding requests:  82%|████████▏ | 1683/2048 [00:04<00:01, 343.37it/s]
Adding requests:  84%|████████▍ | 1720/2048 [00:04<00:00, 349.09it/s]
Adding requests:  86%|████████▌ | 1756/2048 [00:04<00:00, 351.78it/s]
Adding requests:  88%|████████▊ | 1792/2048 [00:05<00:00, 353.92it/s]
Adding requests:  89%|████████▉ | 1829/2048 [00:05<00:00, 356.56it/s]
Adding requests:  91%|█████████ | 1865/2048 [00:05<00:00, 347.07it/s]
Adding requests:  93%|█████████▎| 1901/2048 [00:05<00:00, 349.05it/s]
Adding requests:  95%|█████████▍| 1939/2048 [00:05<00:00, 357.98it/s]
Adding requests:  96%|█████████▋| 1975/2048 [00:05<00:00, 355.84it/s]
Adding requests:  98%|█████████▊| 2011/2048 [00:05<00:00, 352.20it/s]
Adding requests: 100%|█████████▉| 2047/2048 [00:05<00:00, 347.53it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 353.42it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 733/2048 [00:00<00:00, 3490.58it/s, est. speed input: 3574498.58 toks/s, output: 3490.62 toks/s]
Processed prompts:  53%|█████▎    | 1083/2048 [00:02<00:03, 305.62it/s, est. speed input: 384124.50 toks/s, output: 375.12 toks/s]  
Processed prompts:  60%|██████    | 1235/2048 [00:04<00:03, 233.10it/s, est. speed input: 304427.51 toks/s, output: 297.29 toks/s]
Processed prompts:  65%|██████▍   | 1323/2048 [00:04<00:03, 212.24it/s, est. speed input: 282710.27 toks/s, output: 276.08 toks/s]
Processed prompts:  67%|██████▋   | 1382/2048 [00:05<00:03, 192.76it/s, est. speed input: 266820.82 toks/s, output: 260.57 toks/s]
Processed prompts:  70%|██████▉   | 1424/2048 [00:05<00:03, 178.10it/s, est. speed input: 256429.04 toks/s, output: 250.42 toks/s]
Processed prompts:  71%|███████   | 1456/2048 [00:05<00:03, 171.55it/s, est. speed input: 251385.38 toks/s, output: 245.49 toks/s]
Processed prompts:  72%|███████▏  | 1482/2048 [00:06<00:03, 174.63it/s, est. speed input: 250503.23 toks/s, output: 244.63 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:06<00:03, 157.47it/s, est. speed input: 244235.54 toks/s, output: 238.51 toks/s]
Processed prompts:  75%|███████▍  | 1526/2048 [00:06<00:03, 157.94it/s, est. speed input: 242707.23 toks/s, output: 237.02 toks/s]
Processed prompts:  75%|███████▌  | 1545/2048 [00:06<00:03, 156.31it/s, est. speed input: 240917.08 toks/s, output: 235.27 toks/s]
Processed prompts:  76%|███████▋  | 1563/2048 [00:06<00:03, 154.10it/s, est. speed input: 239174.67 toks/s, output: 233.57 toks/s]
Processed prompts:  77%|███████▋  | 1580/2048 [00:06<00:03, 150.66it/s, est. speed input: 237381.66 toks/s, output: 231.82 toks/s]
Processed prompts:  78%|███████▊  | 1596/2048 [00:06<00:03, 145.00it/s, est. speed input: 235401.37 toks/s, output: 229.88 toks/s]
Processed prompts:  79%|███████▊  | 1611/2048 [00:07<00:03, 137.73it/s, est. speed input: 233269.65 toks/s, output: 227.80 toks/s]
Processed prompts:  79%|███████▉  | 1625/2048 [00:07<00:03, 131.90it/s, est. speed input: 231303.19 toks/s, output: 225.88 toks/s]
Processed prompts:  80%|████████  | 1639/2048 [00:07<00:03, 126.00it/s, est. speed input: 229251.28 toks/s, output: 223.88 toks/s]
Processed prompts:  81%|████████  | 1652/2048 [00:07<00:03, 118.81it/s, est. speed input: 227064.80 toks/s, output: 221.74 toks/s]
Processed prompts:  81%|████████▏ | 1664/2048 [00:07<00:03, 111.70it/s, est. speed input: 224860.25 toks/s, output: 219.59 toks/s]
Processed prompts:  82%|████████▏ | 1677/2048 [00:07<00:03, 108.63it/s, est. speed input: 222838.20 toks/s, output: 217.62 toks/s]
Processed prompts:  83%|████████▎ | 1693/2048 [00:07<00:03, 113.20it/s, est. speed input: 221263.76 toks/s, output: 216.08 toks/s]
Processed prompts:  83%|████████▎ | 1709/2048 [00:07<00:02, 116.83it/s, est. speed input: 219780.19 toks/s, output: 214.63 toks/s]
Processed prompts:  84%|████████▍ | 1725/2048 [00:08<00:02, 119.45it/s, est. speed input: 218347.35 toks/s, output: 213.23 toks/s]
Processed prompts:  85%|████████▌ | 1741/2048 [00:08<00:02, 125.41it/s, est. speed input: 217329.73 toks/s, output: 212.24 toks/s]
Processed prompts:  86%|████████▌ | 1757/2048 [00:08<00:02, 126.58it/s, est. speed input: 216068.44 toks/s, output: 211.00 toks/s]
Processed prompts:  87%|████████▋ | 1773/2048 [00:08<00:02, 126.39it/s, est. speed input: 214759.94 toks/s, output: 209.73 toks/s]
Processed prompts:  87%|████████▋ | 1789/2048 [00:08<00:02, 127.83it/s, est. speed input: 213618.25 toks/s, output: 208.61 toks/s]
Processed prompts:  88%|████████▊ | 1805/2048 [00:08<00:01, 127.34it/s, est. speed input: 212389.24 toks/s, output: 207.41 toks/s]
Processed prompts:  89%|████████▉ | 1821/2048 [00:08<00:01, 126.33it/s, est. speed input: 211142.81 toks/s, output: 206.19 toks/s]
Processed prompts:  90%|████████▉ | 1837/2048 [00:08<00:01, 126.62it/s, est. speed input: 210008.62 toks/s, output: 205.09 toks/s]
Processed prompts:  90%|█████████ | 1853/2048 [00:09<00:01, 125.89it/s, est. speed input: 208835.09 toks/s, output: 203.94 toks/s]
Processed prompts:  91%|█████████▏| 1869/2048 [00:09<00:01, 125.35it/s, est. speed input: 207691.20 toks/s, output: 202.82 toks/s]
Processed prompts:  92%|█████████▏| 1885/2048 [00:09<00:01, 127.01it/s, est. speed input: 206730.22 toks/s, output: 201.88 toks/s]
Processed prompts:  93%|█████████▎| 1901/2048 [00:09<00:01, 126.30it/s, est. speed input: 205658.52 toks/s, output: 200.84 toks/s]
Processed prompts:  94%|█████████▎| 1917/2048 [00:09<00:01, 128.07it/s, est. speed input: 204774.55 toks/s, output: 199.98 toks/s]
Processed prompts:  94%|█████████▍| 1933/2048 [00:09<00:00, 128.77it/s, est. speed input: 203875.22 toks/s, output: 199.10 toks/s]
Processed prompts:  95%|█████████▌| 1949/2048 [00:09<00:00, 127.88it/s, est. speed input: 202905.81 toks/s, output: 198.15 toks/s]
Processed prompts:  96%|█████████▌| 1965/2048 [00:09<00:00, 126.99it/s, est. speed input: 201942.72 toks/s, output: 197.21 toks/s]
Processed prompts:  97%|█████████▋| 1981/2048 [00:10<00:00, 128.02it/s, est. speed input: 201111.81 toks/s, output: 196.40 toks/s]
Processed prompts:  98%|█████████▊| 1997/2048 [00:10<00:00, 127.11it/s, est. speed input: 200196.80 toks/s, output: 195.50 toks/s]
Processed prompts:  98%|█████████▊| 2013/2048 [00:10<00:00, 126.23it/s, est. speed input: 199287.68 toks/s, output: 194.62 toks/s]
Processed prompts:  99%|█████████▉| 2029/2048 [00:10<00:00, 125.92it/s, est. speed input: 198420.20 toks/s, output: 193.77 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:10<00:00, 125.92it/s, est. speed input: 199424.51 toks/s, output: 194.75 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:10<00:00, 194.75it/s, est. speed input: 199424.51 toks/s, output: 194.75 toks/s]
[rank0]:[W126 16:25:21.190317313 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 65.2s

测试结果:
  Requests/s:   125.55
  Tokens/s:     128688.35
  Total Reqs:   2048
  Elapsed:      16.31s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     128562.80

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:25:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=758339) WARNING 01-26 16:26:10 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=758339) WARNING 01-26 16:26:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 128.28 requests/s, 131489.56 total tokens/s, 128.28 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 16:25:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:25:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:25:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:25:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:25:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:25:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:25:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:25:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:25:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:25:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:25:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:25:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:25:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:25:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:26:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:26:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:26:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:26:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:26:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:26:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:26:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:26:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:26:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:26:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:26:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:26:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:26:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:26:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=758339) [2026-01-26 16:26:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=758339) [2026-01-26 16:26:02] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=758339) [2026-01-26 16:26:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=758339) [2026-01-26 16:26:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=758339) [2026-01-26 16:26:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=758339) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=758339) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.43it/s]
(EngineCore_DP0 pid=758339) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
(EngineCore_DP0 pid=758339) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=758339) 
(EngineCore_DP0 pid=758339) [rank0]:W0126 16:26:16.273000 758339 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=758339) [rank0]:W0126 16:26:16.355000 758339 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=758339) [rank0]:W0126 16:26:17.310000 758339 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=758339) [rank0]:W0126 16:26:17.440000 758339 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=758339) 2026-01-26 16:26:21,201 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=758339) 2026-01-26 16:26:21,253 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=758339) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 15.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 16.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 16.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 17.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 16.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 15.42it/s]
(EngineCore_DP0 pid=758339) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 15.67it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.64it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 17.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 16.89it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 277.41it/s]
Adding requests:   1%|▏         | 58/4096 [00:00<00:13, 289.20it/s]
Adding requests:   2%|▏         | 91/4096 [00:00<00:13, 304.17it/s]
Adding requests:   3%|▎         | 125/4096 [00:00<00:12, 315.49it/s]
Adding requests:   4%|▍         | 160/4096 [00:00<00:12, 325.55it/s]
Adding requests:   5%|▍         | 197/4096 [00:00<00:11, 338.10it/s]
Adding requests:   6%|▌         | 233/4096 [00:00<00:11, 342.87it/s]
Adding requests:   7%|▋         | 268/4096 [00:00<00:11, 343.58it/s]
Adding requests:   7%|▋         | 304/4096 [00:00<00:10, 347.49it/s]
Adding requests:   8%|▊         | 342/4096 [00:01<00:10, 354.49it/s]
Adding requests:   9%|▉         | 379/4096 [00:01<00:10, 357.93it/s]
Adding requests:  10%|█         | 418/4096 [00:01<00:10, 365.93it/s]
Adding requests:  11%|█         | 455/4096 [00:01<00:10, 359.56it/s]
Adding requests:  12%|█▏        | 495/4096 [00:01<00:09, 370.18it/s]
Adding requests:  13%|█▎        | 535/4096 [00:01<00:09, 375.68it/s]
Adding requests:  14%|█▍        | 573/4096 [00:01<00:09, 374.80it/s]
Adding requests:  15%|█▍        | 611/4096 [00:01<00:09, 361.58it/s]
Adding requests:  16%|█▌        | 648/4096 [00:01<00:09, 358.18it/s]
Adding requests:  17%|█▋        | 684/4096 [00:01<00:09, 354.91it/s]
Adding requests:  18%|█▊        | 720/4096 [00:02<00:09, 354.79it/s]
Adding requests:  18%|█▊        | 756/4096 [00:02<00:09, 353.62it/s]
Adding requests:  19%|█▉        | 792/4096 [00:02<00:09, 353.08it/s]
Adding requests:  20%|██        | 830/4096 [00:02<00:09, 360.85it/s]
Adding requests:  21%|██        | 867/4096 [00:02<00:08, 361.18it/s]
Adding requests:  22%|██▏       | 904/4096 [00:02<00:08, 360.17it/s]
Adding requests:  23%|██▎       | 941/4096 [00:02<00:09, 348.44it/s]
Adding requests:  24%|██▍       | 977/4096 [00:02<00:08, 349.82it/s]
Adding requests:  25%|██▍       | 1013/4096 [00:02<00:08, 348.18it/s]
Adding requests:  26%|██▌       | 1049/4096 [00:02<00:08, 350.52it/s]
Adding requests:  26%|██▋       | 1085/4096 [00:03<00:08, 350.73it/s]
Adding requests:  27%|██▋       | 1121/4096 [00:03<00:08, 351.33it/s]
Adding requests:  28%|██▊       | 1157/4096 [00:03<00:08, 351.90it/s]
Adding requests:  29%|██▉       | 1193/4096 [00:03<00:08, 353.54it/s]
Adding requests:  30%|███       | 1231/4096 [00:03<00:07, 360.61it/s]
Adding requests:  31%|███       | 1268/4096 [00:03<00:07, 357.14it/s]
Adding requests:  32%|███▏      | 1304/4096 [00:03<00:07, 356.52it/s]
Adding requests:  33%|███▎      | 1340/4096 [00:03<00:07, 356.34it/s]
Adding requests:  34%|███▎      | 1378/4096 [00:03<00:07, 360.87it/s]
Adding requests:  35%|███▍      | 1415/4096 [00:04<00:07, 357.36it/s]
Adding requests:  35%|███▌      | 1451/4096 [00:04<00:07, 357.27it/s]
Adding requests:  36%|███▋      | 1489/4096 [00:04<00:07, 361.94it/s]
Adding requests:  37%|███▋      | 1526/4096 [00:04<00:07, 363.67it/s]
Adding requests:  38%|███▊      | 1563/4096 [00:04<00:07, 357.41it/s]
Adding requests:  39%|███▉      | 1599/4096 [00:04<00:07, 352.17it/s]
Adding requests:  40%|███▉      | 1635/4096 [00:04<00:07, 343.93it/s]
Adding requests:  41%|████      | 1670/4096 [00:04<00:07, 334.99it/s]
Adding requests:  42%|████▏     | 1707/4096 [00:04<00:06, 344.59it/s]
Adding requests:  43%|████▎     | 1742/4096 [00:04<00:06, 344.87it/s]
Adding requests:  43%|████▎     | 1780/4096 [00:05<00:06, 353.50it/s]
Adding requests:  44%|████▍     | 1816/4096 [00:05<00:06, 350.49it/s]
Adding requests:  45%|████▌     | 1853/4096 [00:05<00:06, 354.23it/s]
Adding requests:  46%|████▌     | 1890/4096 [00:05<00:06, 357.06it/s]
Adding requests:  47%|████▋     | 1928/4096 [00:05<00:05, 361.73it/s]
Adding requests:  48%|████▊     | 1965/4096 [00:05<00:05, 363.10it/s]
Adding requests:  49%|████▉     | 2002/4096 [00:05<00:06, 345.06it/s]
Adding requests:  50%|████▉     | 2037/4096 [00:05<00:06, 340.55it/s]
Adding requests:  51%|█████     | 2072/4096 [00:05<00:05, 339.02it/s]
Adding requests:  51%|█████▏    | 2109/4096 [00:06<00:05, 346.26it/s]
Adding requests:  52%|█████▏    | 2144/4096 [00:06<00:05, 347.03it/s]
Adding requests:  53%|█████▎    | 2179/4096 [00:06<00:05, 341.69it/s]
Adding requests:  54%|█████▍    | 2214/4096 [00:06<00:05, 342.70it/s]
Adding requests:  55%|█████▍    | 2250/4096 [00:06<00:05, 347.26it/s]
Adding requests:  56%|█████▌    | 2288/4096 [00:06<00:05, 354.68it/s]
Adding requests:  57%|█████▋    | 2325/4096 [00:06<00:04, 358.77it/s]
Adding requests:  58%|█████▊    | 2362/4096 [00:06<00:04, 360.47it/s]
Adding requests:  59%|█████▊    | 2401/4096 [00:06<00:04, 366.28it/s]
Adding requests:  60%|█████▉    | 2438/4096 [00:06<00:04, 365.97it/s]
Adding requests:  60%|██████    | 2475/4096 [00:07<00:04, 365.96it/s]
Adding requests:  61%|██████▏   | 2513/4096 [00:07<00:04, 368.27it/s]
Adding requests:  62%|██████▏   | 2551/4096 [00:07<00:04, 371.61it/s]
Adding requests:  63%|██████▎   | 2590/4096 [00:07<00:04, 375.46it/s]
Adding requests:  64%|██████▍   | 2628/4096 [00:07<00:03, 368.80it/s]
Adding requests:  65%|██████▌   | 2665/4096 [00:07<00:03, 360.59it/s]
Adding requests:  66%|██████▌   | 2702/4096 [00:07<00:03, 357.44it/s]
Adding requests:  67%|██████▋   | 2738/4096 [00:07<00:03, 358.17it/s]
Adding requests:  68%|██████▊   | 2776/4096 [00:07<00:03, 363.29it/s]
Adding requests:  69%|██████▊   | 2815/4096 [00:07<00:03, 370.48it/s]
Adding requests:  70%|██████▉   | 2853/4096 [00:08<00:03, 368.52it/s]
Adding requests:  71%|███████   | 2890/4096 [00:08<00:03, 366.12it/s]
Adding requests:  71%|███████▏  | 2927/4096 [00:08<00:03, 366.63it/s]
Adding requests:  72%|███████▏  | 2965/4096 [00:08<00:03, 368.07it/s]
Adding requests:  73%|███████▎  | 3002/4096 [00:08<00:02, 367.37it/s]
Adding requests:  74%|███████▍  | 3042/4096 [00:08<00:02, 374.10it/s]
Adding requests:  75%|███████▌  | 3080/4096 [00:08<00:02, 371.66it/s]
Adding requests:  76%|███████▌  | 3119/4096 [00:08<00:02, 375.66it/s]
Adding requests:  77%|███████▋  | 3157/4096 [00:08<00:02, 372.39it/s]
Adding requests:  78%|███████▊  | 3195/4096 [00:08<00:02, 365.06it/s]
Adding requests:  79%|███████▉  | 3233/4096 [00:09<00:02, 369.36it/s]
Adding requests:  80%|███████▉  | 3270/4096 [00:09<00:02, 363.07it/s]
Adding requests:  81%|████████  | 3307/4096 [00:09<00:02, 345.14it/s]
Adding requests:  82%|████████▏ | 3343/4096 [00:09<00:02, 348.56it/s]
Adding requests:  83%|████████▎ | 3380/4096 [00:09<00:02, 353.71it/s]
Adding requests:  83%|████████▎ | 3416/4096 [00:09<00:01, 345.41it/s]
Adding requests:  84%|████████▍ | 3453/4096 [00:09<00:01, 350.60it/s]
Adding requests:  85%|████████▌ | 3489/4096 [00:09<00:01, 350.00it/s]
Adding requests:  86%|████████▌ | 3528/4096 [00:09<00:01, 360.88it/s]
Adding requests:  87%|████████▋ | 3567/4096 [00:10<00:01, 366.21it/s]
Adding requests:  88%|████████▊ | 3604/4096 [00:10<00:01, 362.71it/s]
Adding requests:  89%|████████▉ | 3642/4096 [00:10<00:01, 365.63it/s]
Adding requests:  90%|████████▉ | 3679/4096 [00:10<00:01, 358.70it/s]
Adding requests:  91%|█████████ | 3716/4096 [00:10<00:01, 360.61it/s]
Adding requests:  92%|█████████▏| 3753/4096 [00:10<00:00, 353.58it/s]
Adding requests:  93%|█████████▎| 3789/4096 [00:10<00:00, 344.25it/s]
Adding requests:  93%|█████████▎| 3824/4096 [00:10<00:00, 342.68it/s]
Adding requests:  94%|█████████▍| 3860/4096 [00:10<00:00, 346.44it/s]
Adding requests:  95%|█████████▌| 3895/4096 [00:10<00:00, 345.62it/s]
Adding requests:  96%|█████████▌| 3930/4096 [00:11<00:00, 342.65it/s]
Adding requests:  97%|█████████▋| 3966/4096 [00:11<00:00, 346.26it/s]
Adding requests:  98%|█████████▊| 4001/4096 [00:11<00:00, 344.61it/s]
Adding requests:  99%|█████████▊| 4038/4096 [00:11<00:00, 350.10it/s]
Adding requests:  99%|█████████▉| 4074/4096 [00:11<00:00, 345.88it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 354.78it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 1479/4096 [00:00<00:00, 4243.35it/s, est. speed input: 4345304.07 toks/s, output: 4243.38 toks/s]
Processed prompts:  46%|████▋     | 1904/4096 [00:03<00:05, 422.60it/s, est. speed input: 547657.74 toks/s, output: 534.82 toks/s]   
Processed prompts:  51%|█████     | 2087/4096 [00:05<00:06, 308.91it/s, est. speed input: 423796.91 toks/s, output: 413.86 toks/s]
Processed prompts:  54%|█████▎    | 2192/4096 [00:05<00:06, 272.94it/s, est. speed input: 387847.71 toks/s, output: 378.76 toks/s]
Processed prompts:  55%|█████▌    | 2261/4096 [00:06<00:07, 249.74it/s, est. speed input: 368106.94 toks/s, output: 359.48 toks/s]
Processed prompts:  56%|█████▋    | 2311/4096 [00:06<00:08, 220.42it/s, est. speed input: 348530.88 toks/s, output: 340.36 toks/s]
Processed prompts:  57%|█████▋    | 2347/4096 [00:07<00:08, 212.22it/s, est. speed input: 341936.57 toks/s, output: 333.92 toks/s]
Processed prompts:  58%|█████▊    | 2377/4096 [00:07<00:08, 199.22it/s, est. speed input: 334728.15 toks/s, output: 326.88 toks/s]
Processed prompts:  59%|█████▉    | 2407/4096 [00:07<00:09, 184.88it/s, est. speed input: 327577.09 toks/s, output: 319.90 toks/s]
Processed prompts:  60%|█████▉    | 2439/4096 [00:07<00:09, 173.07it/s, est. speed input: 321154.34 toks/s, output: 313.63 toks/s]
Processed prompts:  60%|██████    | 2471/4096 [00:08<00:09, 164.86it/s, est. speed input: 315727.22 toks/s, output: 308.33 toks/s]
Processed prompts:  61%|██████    | 2503/4096 [00:08<00:10, 155.48it/s, est. speed input: 310042.94 toks/s, output: 302.78 toks/s]
Processed prompts:  62%|██████▏   | 2535/4096 [00:08<00:10, 149.20it/s, est. speed input: 304977.61 toks/s, output: 297.83 toks/s]
Processed prompts:  63%|██████▎   | 2567/4096 [00:08<00:10, 143.40it/s, est. speed input: 299999.76 toks/s, output: 292.97 toks/s]
Processed prompts:  63%|██████▎   | 2599/4096 [00:09<00:10, 139.64it/s, est. speed input: 295424.70 toks/s, output: 288.50 toks/s]
Processed prompts:  64%|██████▍   | 2631/4096 [00:09<00:10, 137.08it/s, est. speed input: 291140.12 toks/s, output: 284.32 toks/s]
Processed prompts:  65%|██████▌   | 2663/4096 [00:09<00:10, 134.18it/s, est. speed input: 286878.16 toks/s, output: 280.15 toks/s]
Processed prompts:  66%|██████▌   | 2695/4096 [00:09<00:10, 132.15it/s, est. speed input: 282844.36 toks/s, output: 276.21 toks/s]
Processed prompts:  67%|██████▋   | 2727/4096 [00:10<00:10, 131.30it/s, est. speed input: 279120.85 toks/s, output: 272.58 toks/s]
Processed prompts:  67%|██████▋   | 2759/4096 [00:10<00:10, 130.04it/s, est. speed input: 275463.07 toks/s, output: 269.01 toks/s]
Processed prompts:  68%|██████▊   | 2791/4096 [00:10<00:10, 129.00it/s, est. speed input: 271953.26 toks/s, output: 265.58 toks/s]
Processed prompts:  69%|██████▉   | 2823/4096 [00:10<00:09, 128.43it/s, est. speed input: 268636.11 toks/s, output: 262.34 toks/s]
Processed prompts:  70%|██████▉   | 2855/4096 [00:11<00:09, 128.07it/s, est. speed input: 265476.34 toks/s, output: 259.25 toks/s]
Processed prompts:  70%|███████   | 2887/4096 [00:11<00:09, 131.30it/s, est. speed input: 262976.91 toks/s, output: 256.81 toks/s]
Processed prompts:  71%|███████▏  | 2919/4096 [00:11<00:08, 130.87it/s, est. speed input: 260188.99 toks/s, output: 254.09 toks/s]
Processed prompts:  72%|███████▏  | 2951/4096 [00:11<00:08, 130.51it/s, est. speed input: 257509.91 toks/s, output: 251.47 toks/s]
Processed prompts:  73%|███████▎  | 2983/4096 [00:11<00:08, 130.77it/s, est. speed input: 255008.68 toks/s, output: 249.03 toks/s]
Processed prompts:  74%|███████▎  | 3015/4096 [00:12<00:08, 129.61it/s, est. speed input: 252433.68 toks/s, output: 246.52 toks/s]
Processed prompts:  74%|███████▍  | 3047/4096 [00:12<00:08, 128.93it/s, est. speed input: 249977.68 toks/s, output: 244.12 toks/s]
Processed prompts:  75%|███████▌  | 3079/4096 [00:12<00:07, 128.49it/s, est. speed input: 247622.41 toks/s, output: 241.82 toks/s]
Processed prompts:  76%|███████▌  | 3111/4096 [00:12<00:07, 128.28it/s, est. speed input: 245370.46 toks/s, output: 239.62 toks/s]
Processed prompts:  77%|███████▋  | 3143/4096 [00:13<00:07, 127.62it/s, est. speed input: 243142.48 toks/s, output: 237.44 toks/s]
Processed prompts:  78%|███████▊  | 3175/4096 [00:13<00:07, 128.39it/s, est. speed input: 241141.49 toks/s, output: 235.49 toks/s]
Processed prompts:  78%|███████▊  | 3207/4096 [00:13<00:06, 127.75it/s, est. speed input: 239078.02 toks/s, output: 233.47 toks/s]
Processed prompts:  79%|███████▉  | 3239/4096 [00:13<00:06, 128.21it/s, est. speed input: 237189.63 toks/s, output: 231.63 toks/s]
Processed prompts:  80%|███████▉  | 3271/4096 [00:14<00:06, 128.02it/s, est. speed input: 235311.90 toks/s, output: 229.80 toks/s]
Processed prompts:  81%|████████  | 3303/4096 [00:14<00:06, 128.03it/s, est. speed input: 233514.44 toks/s, output: 228.04 toks/s]
Processed prompts:  81%|████████▏ | 3335/4096 [00:14<00:05, 127.28it/s, est. speed input: 231699.79 toks/s, output: 226.27 toks/s]
Processed prompts:  82%|████████▏ | 3367/4096 [00:14<00:05, 127.12it/s, est. speed input: 229983.40 toks/s, output: 224.59 toks/s]
Processed prompts:  83%|████████▎ | 3399/4096 [00:15<00:05, 127.68it/s, est. speed input: 228390.51 toks/s, output: 223.04 toks/s]
Processed prompts:  84%|████████▍ | 3431/4096 [00:15<00:05, 127.45it/s, est. speed input: 226788.48 toks/s, output: 221.47 toks/s]
Processed prompts:  85%|████████▍ | 3463/4096 [00:15<00:04, 126.94it/s, est. speed input: 225204.53 toks/s, output: 219.93 toks/s]
Processed prompts:  85%|████████▌ | 3495/4096 [00:15<00:04, 127.28it/s, est. speed input: 223735.97 toks/s, output: 218.49 toks/s]
Processed prompts:  86%|████████▌ | 3527/4096 [00:16<00:04, 127.20it/s, est. speed input: 222282.91 toks/s, output: 217.07 toks/s]
Processed prompts:  87%|████████▋ | 3559/4096 [00:16<00:04, 128.71it/s, est. speed input: 221012.15 toks/s, output: 215.83 toks/s]
Processed prompts:  88%|████████▊ | 3591/4096 [00:16<00:03, 128.16it/s, est. speed input: 219639.76 toks/s, output: 214.49 toks/s]
Processed prompts:  88%|████████▊ | 3623/4096 [00:16<00:03, 128.75it/s, est. speed input: 218389.77 toks/s, output: 213.27 toks/s]
Processed prompts:  89%|████████▉ | 3655/4096 [00:17<00:03, 128.22it/s, est. speed input: 217098.90 toks/s, output: 212.01 toks/s]
Processed prompts:  90%|█████████ | 3687/4096 [00:17<00:03, 129.51it/s, est. speed input: 215977.02 toks/s, output: 210.91 toks/s]
Processed prompts:  91%|█████████ | 3719/4096 [00:17<00:02, 129.83it/s, est. speed input: 214839.35 toks/s, output: 209.80 toks/s]
Processed prompts:  92%|█████████▏| 3751/4096 [00:17<00:02, 128.37it/s, est. speed input: 213604.98 toks/s, output: 208.60 toks/s]
Processed prompts:  92%|█████████▏| 3783/4096 [00:18<00:02, 127.76it/s, est. speed input: 212435.95 toks/s, output: 207.46 toks/s]
Processed prompts:  93%|█████████▎| 3815/4096 [00:18<00:02, 127.94it/s, est. speed input: 211343.62 toks/s, output: 206.39 toks/s]
Processed prompts:  94%|█████████▍| 3847/4096 [00:18<00:01, 127.80it/s, est. speed input: 210260.41 toks/s, output: 205.33 toks/s]
Processed prompts:  95%|█████████▍| 3879/4096 [00:18<00:01, 127.50it/s, est. speed input: 209192.08 toks/s, output: 204.29 toks/s]
Processed prompts:  95%|█████████▌| 3911/4096 [00:19<00:01, 129.42it/s, est. speed input: 208300.54 toks/s, output: 203.42 toks/s]
Processed prompts:  96%|█████████▋| 3943/4096 [00:19<00:01, 131.01it/s, est. speed input: 207444.79 toks/s, output: 202.58 toks/s]
Processed prompts:  97%|█████████▋| 3975/4096 [00:19<00:00, 129.37it/s, est. speed input: 206428.33 toks/s, output: 201.59 toks/s]
Processed prompts:  98%|█████████▊| 4007/4096 [00:19<00:00, 129.74it/s, est. speed input: 205536.69 toks/s, output: 200.72 toks/s]
Processed prompts:  99%|█████████▊| 4039/4096 [00:20<00:00, 132.36it/s, est. speed input: 204814.34 toks/s, output: 200.01 toks/s]
Processed prompts:  99%|█████████▉| 4071/4096 [00:20<00:00, 141.78it/s, est. speed input: 204530.78 toks/s, output: 199.74 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:20<00:00, 141.78it/s, est. speed input: 205783.54 toks/s, output: 200.96 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:20<00:00, 200.96it/s, est. speed input: 205783.54 toks/s, output: 200.96 toks/s]
[rank0]:[W126 16:26:56.493954999 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 94.5s

测试结果:
  Requests/s:   128.28
  Tokens/s:     131489.56
  Total Reqs:   4096
  Elapsed:      31.93s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     131361.28

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:27:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=760511) WARNING 01-26 16:28:09 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     def forward(
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     raise e
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/tmp/torchinductor_root/fa/cfalzfvmybtpgcm7jvh2oi6qiun3omrxyes7f4cv2lkfzyzrjpwd.py", line 1090, in call
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     triton_poi_fused_mul_quant_only_fp8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_only_fp8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     torch.cuda.synchronize()
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866] Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=760511) ERROR 01-26 16:28:16 [core.py:866] 


─── STDERR ───
[2026-01-26 16:27:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:27:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:27:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:27:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:27:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:27:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:27:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:27:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:27:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:27:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:27:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:27:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:27:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:27:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:28:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:28:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:28:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:28:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:28:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:28:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:28:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:28:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=760511) [2026-01-26 16:28:01] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=760511) [2026-01-26 16:28:01] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=760511) [2026-01-26 16:28:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=760511) [2026-01-26 16:28:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=760511) [2026-01-26 16:28:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=760511) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=760511) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=760511) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]
(EngineCore_DP0 pid=760511) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=760511) 
(EngineCore_DP0 pid=760511) [rank0]:W0126 16:28:14.670000 760511 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=760511) [rank0]:W0126 16:28:14.750000 760511 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=760511) [2026-01-26 16:28:14] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=4608, K=3584), falling back to default heuristic
(EngineCore_DP0 pid=760511) [rank0]:W0126 16:28:15.675000 760511 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=760511) [rank0]:W0126 16:28:15.798000 760511 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=760511) [2026-01-26 16:28:16] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3584, K=3584), falling back to default heuristic
(EngineCore_DP0 pid=760511) [2026-01-26 16:28:16] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=37888, K=3584), falling back to default heuristic
(EngineCore_DP0 pid=760511) Process EngineCore_DP0:
(EngineCore_DP0 pid=760511) Traceback (most recent call last):
(EngineCore_DP0 pid=760511)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=760511)     self.run()
(EngineCore_DP0 pid=760511)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=760511)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=760511)     raise e
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=760511)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=760511)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=760511)     super().__init__(
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=760511)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=760511)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=760511)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=760511)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=760511)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=760511)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=760511)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=760511)     return func(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=760511)     return func(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=760511)     self.model_runner.profile_run()
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=760511)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=760511)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=760511)     return func(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=760511)     outputs = self.model(
(EngineCore_DP0 pid=760511)               ^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=760511)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=760511)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=760511)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=760511)     hidden_states = self.model(
(EngineCore_DP0 pid=760511)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=760511)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=760511)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=760511)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=760511)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=760511)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=760511)     def forward(
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=760511)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=760511)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=760511)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=760511)     raise e
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=760511)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=760511)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=760511)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=760511)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=760511)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=760511)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=760511)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=760511)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=760511)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=760511)     return compiled_fn(full_args)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=760511)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=760511)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=760511)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=760511)                             ^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=760511)     outs = compiled_fn(args)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=760511)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=760511)     return self.current_callable(inputs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=760511)     out = model(new_inputs)
(EngineCore_DP0 pid=760511)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/tmp/torchinductor_root/fa/cfalzfvmybtpgcm7jvh2oi6qiun3omrxyes7f4cv2lkfzyzrjpwd.py", line 1090, in call
(EngineCore_DP0 pid=760511)     triton_poi_fused_mul_quant_only_fp8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_only_fp8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=760511)     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=760511)     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=760511)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=760511)     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=760511)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=760511)     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=760511)     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=760511)     torch.cuda.synchronize()
(EngineCore_DP0 pid=760511)   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=760511)     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=760511)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=760511) torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=760511) Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=760511) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=760511) For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=760511) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=760511) 
[rank0]:[W126 16:28:16.069651703 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,36.0410,18489.0542,3.5515
1024,1024,1,128,128,37.6827,38624.7752,3.3968
2048,1024,2,256,128,69.4761,71213.0088,3.6847
4096,1024,4,512,128,113.9535,116802.3172,4.4931
8192,1024,8,1024,128,119.8040,122799.0758,8.5473
16384,1024,16,2048,128,125.5496,128688.3481,16.3123
32768,1024,32,4096,128,128.2825,131489.5623,31.9295
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-FP8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:28:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=761512) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=761512) WARNING 01-26 16:28:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=761512) WARNING 01-26 16:28:52 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 38.29 requests/s, 19643.27 total tokens/s, 38.29 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:28:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:28:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:28:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:28:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:28:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:28:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:28:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:28:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:28:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:28:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:28:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:28:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:28:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:28:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:28:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:28:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:33] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:33] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:33] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:33] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:33] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=761512) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=761512) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=761512) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]
(EngineCore_DP0 pid=761512) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.24it/s]
(EngineCore_DP0 pid=761512) 
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=761512) 2026-01-26 16:28:52,170 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=761512) 2026-01-26 16:28:52,193 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=761512) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.16it/s]
(EngineCore_DP0 pid=761512) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.63it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 509.14it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 256.75it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 304.54it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:00, 138.29it/s, est. speed input: 70808.46 toks/s, output: 138.29 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 57.75it/s, est. speed input: 32508.32 toks/s, output: 63.49 toks/s]  
Processed prompts:  30%|██▉       | 38/128 [00:00<00:01, 50.22it/s, est. speed input: 28591.11 toks/s, output: 55.84 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 46.91it/s, est. speed input: 26933.09 toks/s, output: 52.60 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 45.03it/s, est. speed input: 25996.07 toks/s, output: 50.77 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 43.80it/s, est. speed input: 25395.62 toks/s, output: 49.60 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 42.79it/s, est. speed input: 24907.20 toks/s, output: 48.65 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 42.07it/s, est. speed input: 24515.20 toks/s, output: 47.88 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 41.51it/s, est. speed input: 24183.43 toks/s, output: 47.23 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 41.13it/s, est. speed input: 23906.82 toks/s, output: 46.69 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 40.89it/s, est. speed input: 23675.13 toks/s, output: 46.24 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:01, 40.68it/s, est. speed input: 23469.75 toks/s, output: 45.84 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 40.51it/s, est. speed input: 23286.71 toks/s, output: 45.48 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 40.43it/s, est. speed input: 23129.42 toks/s, output: 45.17 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 40.36it/s, est. speed input: 22988.35 toks/s, output: 44.90 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.29it/s, est. speed input: 22860.39 toks/s, output: 44.65 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.27it/s, est. speed input: 22747.08 toks/s, output: 44.43 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 40.26it/s, est. speed input: 22645.72 toks/s, output: 44.23 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 40.23it/s, est. speed input: 22551.65 toks/s, output: 44.05 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 40.18it/s, est. speed input: 22463.10 toks/s, output: 43.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.18it/s, est. speed input: 22432.72 toks/s, output: 43.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.81it/s, est. speed input: 22432.72 toks/s, output: 43.81 toks/s]
[rank0]:[W126 16:28:57.098468324 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.8s

测试结果:
  Requests/s:   38.29
  Tokens/s:     19643.27
  Total Reqs:   128
  Elapsed:      3.34s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     19604.98

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:29:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=762726) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=762726) WARNING 01-26 16:29:23 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=762726) WARNING 01-26 16:29:32 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.82 requests/s, 40815.57 total tokens/s, 39.82 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:29:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:29:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:29:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:29:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:29:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:29:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:29:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:29:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:29:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:29:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:29:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:29:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:15] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:15] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:15] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:15] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:15] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=762726) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=762726) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.56it/s]
(EngineCore_DP0 pid=762726) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.30it/s]
(EngineCore_DP0 pid=762726) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]
(EngineCore_DP0 pid=762726) 
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=762726) 2026-01-26 16:29:32,827 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=762726) 2026-01-26 16:29:32,849 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=762726) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.96it/s]
(EngineCore_DP0 pid=762726) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.37it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  21%|██        | 27/128 [00:00<00:00, 267.23it/s]
Adding requests:  45%|████▍     | 57/128 [00:00<00:00, 282.00it/s]
Adding requests:  69%|██████▉   | 88/128 [00:00<00:00, 291.87it/s]
Adding requests:  95%|█████████▍| 121/128 [00:00<00:00, 303.03it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 296.79it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:00, 164.52it/s, est. speed input: 168477.99 toks/s, output: 164.52 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 61.53it/s, est. speed input: 70130.39 toks/s, output: 68.49 toks/s]   
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 52.87it/s, est. speed input: 61142.90 toks/s, output: 59.71 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:00<00:01, 49.24it/s, est. speed input: 57581.26 toks/s, output: 56.23 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 47.00it/s, est. speed input: 55464.16 toks/s, output: 54.16 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 45.30it/s, est. speed input: 53853.76 toks/s, output: 52.59 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 44.18it/s, est. speed input: 52769.71 toks/s, output: 51.53 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 43.23it/s, est. speed input: 51843.93 toks/s, output: 50.63 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 42.53it/s, est. speed input: 51070.09 toks/s, output: 49.87 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:01, 41.99it/s, est. speed input: 50398.54 toks/s, output: 49.22 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 41.61it/s, est. speed input: 49821.80 toks/s, output: 48.65 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 41.38it/s, est. speed input: 49328.33 toks/s, output: 48.17 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 41.15it/s, est. speed input: 48876.42 toks/s, output: 47.73 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.96it/s, est. speed input: 48469.50 toks/s, output: 47.33 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.82it/s, est. speed input: 48103.20 toks/s, output: 46.98 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 40.76it/s, est. speed input: 47781.28 toks/s, output: 46.66 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 40.73it/s, est. speed input: 47491.94 toks/s, output: 46.38 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 40.66it/s, est. speed input: 47218.40 toks/s, output: 46.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.66it/s, est. speed input: 47117.41 toks/s, output: 46.01 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 46.01it/s, est. speed input: 47117.41 toks/s, output: 46.01 toks/s]
[rank0]:[W126 16:29:37.955922062 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.8s

测试结果:
  Requests/s:   39.82
  Tokens/s:     40815.57
  Total Reqs:   128
  Elapsed:      3.21s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     40775.75

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:29:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=763845) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=763845) WARNING 01-26 16:30:03 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=763845) WARNING 01-26 16:30:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 72.76 requests/s, 74576.67 total tokens/s, 72.76 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 16:29:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:29:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:29:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:29:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:29:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:29:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:29:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:29:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:29:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:29:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:29:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:29:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=763845) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=763845) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.64it/s]
(EngineCore_DP0 pid=763845) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.36it/s]
(EngineCore_DP0 pid=763845) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=763845) 
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=763845) 2026-01-26 16:30:13,066 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=763845) 2026-01-26 16:30:13,089 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=763845) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.57it/s]
(EngineCore_DP0 pid=763845) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.87it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:50,  5.07it/s]
Adding requests:  11%|█         | 28/256 [00:00<00:01, 115.97it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 198.30it/s]
Adding requests:  36%|███▋      | 93/256 [00:00<00:00, 234.42it/s]
Adding requests:  49%|████▉     | 126/256 [00:00<00:00, 264.63it/s]
Adding requests:  62%|██████▎   | 160/256 [00:00<00:00, 286.58it/s]
Adding requests:  76%|███████▌  | 195/256 [00:00<00:00, 305.30it/s]
Adding requests:  90%|████████▉ | 230/256 [00:00<00:00, 318.92it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 258.96it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:00<00:00, 611.34it/s, est. speed input: 626054.67 toks/s, output: 611.35 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:00<00:01, 124.98it/s, est. speed input: 145323.35 toks/s, output: 141.92 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:01<00:00, 107.66it/s, est. speed input: 126422.87 toks/s, output: 123.46 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:01<00:00, 100.29it/s, est. speed input: 119071.75 toks/s, output: 116.28 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:01<00:00, 95.59it/s, est. speed input: 114745.95 toks/s, output: 112.06 toks/s] 
Processed prompts:  79%|███████▉  | 203/256 [00:01<00:00, 92.56it/s, est. speed input: 112037.73 toks/s, output: 109.41 toks/s]
Processed prompts:  84%|████████▍ | 215/256 [00:02<00:00, 89.78it/s, est. speed input: 109786.21 toks/s, output: 107.21 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:02<00:00, 85.89it/s, est. speed input: 107395.14 toks/s, output: 104.88 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 84.65it/s, est. speed input: 106021.00 toks/s, output: 103.54 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:02<00:00, 83.53it/s, est. speed input: 104770.26 toks/s, output: 102.31 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 82.75it/s, est. speed input: 103669.87 toks/s, output: 101.24 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 82.75it/s, est. speed input: 103669.87 toks/s, output: 101.24 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 101.24it/s, est. speed input: 103669.87 toks/s, output: 101.24 toks/s]
[rank0]:[W126 16:30:18.603620856 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.6s

测试结果:
  Requests/s:   72.76
  Tokens/s:     74576.67
  Total Reqs:   256
  Elapsed:      3.52s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     74503.91

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:30:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=764978) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=764978) WARNING 01-26 16:30:45 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=764978) WARNING 01-26 16:30:55 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 136.05 requests/s, 139453.88 total tokens/s, 136.05 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 16:30:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:30:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:30:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:30:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:30:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:30:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:30:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:30:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:30:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:30:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:30:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:30:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:30:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:30:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:30:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:30:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=764978) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=764978) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.77it/s]
(EngineCore_DP0 pid=764978) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.41it/s]
(EngineCore_DP0 pid=764978) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.46it/s]
(EngineCore_DP0 pid=764978) 
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=764978) 2026-01-26 16:30:55,819 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=764978) 2026-01-26 16:30:55,841 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=764978) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.30it/s]
(EngineCore_DP0 pid=764978) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 16.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 17.53it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 27/512 [00:00<00:01, 263.98it/s]
Adding requests:  12%|█▏        | 62/512 [00:00<00:01, 311.58it/s]
Adding requests:  18%|█▊        | 94/512 [00:00<00:01, 310.93it/s]
Adding requests:  25%|██▌       | 128/512 [00:00<00:01, 320.65it/s]
Adding requests:  32%|███▏      | 164/512 [00:00<00:01, 330.99it/s]
Adding requests:  39%|███▉      | 199/512 [00:00<00:00, 332.32it/s]
Adding requests:  46%|████▌     | 234/512 [00:00<00:00, 336.51it/s]
Adding requests:  53%|█████▎    | 269/512 [00:00<00:00, 338.56it/s]
Adding requests:  60%|█████▉    | 305/512 [00:00<00:00, 344.67it/s]
Adding requests:  67%|██████▋   | 343/512 [00:01<00:00, 353.13it/s]
Adding requests:  74%|███████▍  | 379/512 [00:01<00:00, 353.47it/s]
Adding requests:  82%|████████▏ | 418/512 [00:01<00:00, 363.18it/s]
Adding requests:  89%|████████▉ | 455/512 [00:01<00:00, 359.78it/s]
Adding requests:  97%|█████████▋| 495/512 [00:01<00:00, 370.75it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 346.39it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  41%|████      | 210/512 [00:00<00:00, 1757.39it/s, est. speed input: 1799675.02 toks/s, output: 1757.42 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:01<00:00, 238.74it/s, est. speed input: 284611.59 toks/s, output: 277.94 toks/s]   
Processed prompts:  91%|█████████ | 467/512 [00:01<00:00, 202.04it/s, est. speed input: 243772.18 toks/s, output: 238.06 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 202.04it/s, est. speed input: 229556.34 toks/s, output: 224.18 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 224.17it/s, est. speed input: 229556.34 toks/s, output: 224.18 toks/s]
[rank0]:[W126 16:31:01.911865543 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.2s

测试结果:
  Requests/s:   136.05
  Tokens/s:     139453.88
  Total Reqs:   512
  Elapsed:      3.76s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     139317.83

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:31:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=766189) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=766189) WARNING 01-26 16:31:32 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=766189) WARNING 01-26 16:31:41 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 147.68 requests/s, 151372.08 total tokens/s, 147.68 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 16:31:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:31:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:31:16] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:31:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:31:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:31:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:31:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:31:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:31:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:31:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:31:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:31:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:31:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:31:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:31:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:31:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:24] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:24] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:24] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:24] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=766189) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=766189) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.76it/s]
(EngineCore_DP0 pid=766189) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.41it/s]
(EngineCore_DP0 pid=766189) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]
(EngineCore_DP0 pid=766189) 
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=766189) 2026-01-26 16:31:41,911 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=766189) 2026-01-26 16:31:41,935 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=766189) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 17.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 16.16it/s]
(EngineCore_DP0 pid=766189) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 18.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.98it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 276.10it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 318.02it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:03, 300.35it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 309.68it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 321.84it/s]
Adding requests:  19%|█▉        | 199/1024 [00:00<00:02, 332.43it/s]
Adding requests:  23%|██▎       | 235/1024 [00:00<00:02, 339.86it/s]
Adding requests:  26%|██▋       | 270/1024 [00:00<00:02, 340.13it/s]
Adding requests:  30%|██▉       | 306/1024 [00:00<00:02, 345.06it/s]
Adding requests:  33%|███▎      | 343/1024 [00:01<00:01, 351.77it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:01, 355.36it/s]
Adding requests:  41%|████      | 418/1024 [00:01<00:01, 360.81it/s]
Adding requests:  44%|████▍     | 455/1024 [00:01<00:01, 358.08it/s]
Adding requests:  48%|████▊     | 495/1024 [00:01<00:01, 368.97it/s]
Adding requests:  52%|█████▏    | 532/1024 [00:01<00:01, 367.60it/s]
Adding requests:  56%|█████▌    | 569/1024 [00:01<00:01, 365.81it/s]
Adding requests:  59%|█████▉    | 606/1024 [00:01<00:01, 354.95it/s]
Adding requests:  63%|██████▎   | 642/1024 [00:01<00:01, 353.27it/s]
Adding requests:  66%|██████▌   | 678/1024 [00:01<00:01, 342.73it/s]
Adding requests:  70%|██████▉   | 715/1024 [00:02<00:00, 348.55it/s]
Adding requests:  73%|███████▎  | 750/1024 [00:02<00:00, 344.79it/s]
Adding requests:  77%|███████▋  | 786/1024 [00:02<00:00, 348.06it/s]
Adding requests:  80%|████████  | 821/1024 [00:02<00:00, 347.94it/s]
Adding requests:  84%|████████▍ | 859/1024 [00:02<00:00, 355.39it/s]
Adding requests:  88%|████████▊ | 896/1024 [00:02<00:00, 358.43it/s]
Adding requests:  91%|█████████ | 932/1024 [00:02<00:00, 351.58it/s]
Adding requests:  95%|█████████▍| 969/1024 [00:02<00:00, 355.78it/s]
Adding requests:  98%|█████████▊| 1005/1024 [00:02<00:00, 349.82it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 347.50it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:00<00:00, 2981.06it/s, est. speed input: 3052757.46 toks/s, output: 2981.10 toks/s]
Processed prompts:  72%|███████▏  | 741/1024 [00:02<00:00, 293.56it/s, est. speed input: 358428.69 toks/s, output: 350.03 toks/s]   
Processed prompts:  85%|████████▌ | 873/1024 [00:02<00:00, 242.06it/s, est. speed input: 300899.15 toks/s, output: 293.85 toks/s]
Processed prompts:  93%|█████████▎| 952/1024 [00:03<00:00, 218.80it/s, est. speed input: 278252.38 toks/s, output: 271.73 toks/s]
Processed prompts:  98%|█████████▊| 1006/1024 [00:03<00:00, 204.04it/s, est. speed input: 265722.30 toks/s, output: 259.49 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:03<00:00, 204.04it/s, est. speed input: 263075.55 toks/s, output: 256.91 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:03<00:00, 256.90it/s, est. speed input: 263075.55 toks/s, output: 256.91 toks/s]
[rank0]:[W126 16:31:50.217091250 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.4s

测试结果:
  Requests/s:   147.68
  Tokens/s:     151372.08
  Total Reqs:   1024
  Elapsed:      6.93s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     151224.40

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:32:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=767522) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=767522) WARNING 01-26 16:32:27 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=767522) WARNING 01-26 16:32:37 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 156.73 requests/s, 160646.32 total tokens/s, 156.73 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 16:32:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:32:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:32:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:32:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:32:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:32:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:32:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:32:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:32:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:32:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:32:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:32:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:32:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:32:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:32:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:32:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:19] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=767522) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=767522) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.75it/s]
(EngineCore_DP0 pid=767522) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.40it/s]
(EngineCore_DP0 pid=767522) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.44it/s]
(EngineCore_DP0 pid=767522) 
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=767522) [rank0]:W0126 16:32:32.810000 767522 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=767522) [rank0]:W0126 16:32:32.893000 767522 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=767522) [rank0]:W0126 16:32:33.863000 767522 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=767522) [rank0]:W0126 16:32:33.995000 767522 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=767522) 2026-01-26 16:32:37,676 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=767522) 2026-01-26 16:32:37,702 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=767522) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 15.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 16.14it/s]
(EngineCore_DP0 pid=767522) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.23it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.62it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.50it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 279.06it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 318.68it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 316.10it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:05, 323.35it/s]
Adding requests:   8%|▊         | 164/2048 [00:00<00:05, 331.22it/s]
Adding requests:  10%|▉         | 201/2048 [00:00<00:05, 342.45it/s]
Adding requests:  12%|█▏        | 238/2048 [00:00<00:05, 348.15it/s]
Adding requests:  13%|█▎        | 273/2048 [00:00<00:05, 345.30it/s]
Adding requests:  15%|█▌        | 310/2048 [00:00<00:04, 352.08it/s]
Adding requests:  17%|█▋        | 348/2048 [00:01<00:04, 358.29it/s]
Adding requests:  19%|█▉        | 386/2048 [00:01<00:04, 361.15it/s]
Adding requests:  21%|██        | 425/2048 [00:01<00:04, 367.86it/s]
Adding requests:  23%|██▎       | 462/2048 [00:01<00:04, 363.96it/s]
Adding requests:  25%|██▍       | 502/2048 [00:01<00:04, 371.55it/s]
Adding requests:  26%|██▋       | 541/2048 [00:01<00:04, 373.94it/s]
Adding requests:  28%|██▊       | 579/2048 [00:01<00:03, 372.26it/s]
Adding requests:  30%|███       | 617/2048 [00:01<00:04, 357.04it/s]
Adding requests:  32%|███▏      | 653/2048 [00:01<00:03, 351.20it/s]
Adding requests:  34%|███▎      | 691/2048 [00:01<00:03, 357.89it/s]
Adding requests:  35%|███▌      | 727/2048 [00:02<00:03, 351.82it/s]
Adding requests:  37%|███▋      | 763/2048 [00:02<00:03, 352.14it/s]
Adding requests:  39%|███▉      | 800/2048 [00:02<00:03, 354.42it/s]
Adding requests:  41%|████      | 837/2048 [00:02<00:03, 358.81it/s]
Adding requests:  43%|████▎     | 875/2048 [00:02<00:03, 361.62it/s]
Adding requests:  45%|████▍     | 912/2048 [00:02<00:03, 361.14it/s]
Adding requests:  46%|████▋     | 949/2048 [00:02<00:03, 356.37it/s]
Adding requests:  48%|████▊     | 986/2048 [00:02<00:02, 359.22it/s]
Adding requests:  50%|████▉     | 1022/2048 [00:02<00:02, 355.78it/s]
Adding requests:  52%|█████▏    | 1058/2048 [00:02<00:02, 353.76it/s]
Adding requests:  53%|█████▎    | 1094/2048 [00:03<00:02, 351.25it/s]
Adding requests:  55%|█████▌    | 1133/2048 [00:03<00:02, 359.40it/s]
Adding requests:  57%|█████▋    | 1169/2048 [00:03<00:02, 354.24it/s]
Adding requests:  59%|█████▉    | 1206/2048 [00:03<00:02, 357.85it/s]
Adding requests:  61%|██████    | 1243/2048 [00:03<00:02, 360.36it/s]
Adding requests:  62%|██████▎   | 1280/2048 [00:03<00:02, 355.54it/s]
Adding requests:  64%|██████▍   | 1316/2048 [00:03<00:02, 356.02it/s]
Adding requests:  66%|██████▌   | 1354/2048 [00:03<00:01, 361.24it/s]
Adding requests:  68%|██████▊   | 1391/2048 [00:03<00:01, 348.07it/s]
Adding requests:  70%|██████▉   | 1426/2048 [00:04<00:01, 346.57it/s]
Adding requests:  71%|███████▏  | 1463/2048 [00:04<00:01, 351.85it/s]
Adding requests:  73%|███████▎  | 1500/2048 [00:04<00:01, 356.60it/s]
Adding requests:  75%|███████▌  | 1536/2048 [00:04<00:01, 355.14it/s]
Adding requests:  77%|███████▋  | 1572/2048 [00:04<00:01, 350.45it/s]
Adding requests:  79%|███████▊  | 1608/2048 [00:04<00:01, 352.33it/s]
Adding requests:  80%|████████  | 1644/2048 [00:04<00:01, 334.26it/s]
Adding requests:  82%|████████▏ | 1678/2048 [00:04<00:01, 334.87it/s]
Adding requests:  84%|████████▎ | 1715/2048 [00:04<00:00, 344.87it/s]
Adding requests:  86%|████████▌ | 1752/2048 [00:04<00:00, 350.38it/s]
Adding requests:  87%|████████▋ | 1788/2048 [00:05<00:00, 352.69it/s]
Adding requests:  89%|████████▉ | 1824/2048 [00:05<00:00, 353.28it/s]
Adding requests:  91%|█████████ | 1861/2048 [00:05<00:00, 355.46it/s]
Adding requests:  93%|█████████▎| 1897/2048 [00:05<00:00, 355.71it/s]
Adding requests:  94%|█████████▍| 1935/2048 [00:05<00:00, 362.24it/s]
Adding requests:  96%|█████████▋| 1973/2048 [00:05<00:00, 364.21it/s]
Adding requests:  98%|█████████▊| 2010/2048 [00:05<00:00, 356.50it/s]
Adding requests: 100%|█████████▉| 2046/2048 [00:05<00:00, 352.92it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 353.29it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  44%|████▍     | 909/2048 [00:00<00:00, 6450.89it/s, est. speed input: 6606043.17 toks/s, output: 6450.98 toks/s]
Processed prompts:  76%|███████▌  | 1555/2048 [00:04<00:01, 308.72it/s, est. speed input: 379494.72 toks/s, output: 370.60 toks/s]  
Processed prompts:  89%|████████▉ | 1830/2048 [00:05<00:00, 253.98it/s, est. speed input: 317327.53 toks/s, output: 309.89 toks/s]
Processed prompts:  97%|█████████▋| 1987/2048 [00:06<00:00, 230.41it/s, est. speed input: 294146.49 toks/s, output: 287.25 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:07<00:00, 230.41it/s, est. speed input: 288520.68 toks/s, output: 281.76 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:07<00:00, 281.75it/s, est. speed input: 288520.68 toks/s, output: 281.76 toks/s]
[rank0]:[W126 16:32:53.481675395 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 62.3s

测试结果:
  Requests/s:   156.73
  Tokens/s:     160646.32
  Total Reqs:   2048
  Elapsed:      13.07s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     160489.59

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:33:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=769091) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=769091) WARNING 01-26 16:33:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=769091) WARNING 01-26 16:33:52 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 162.05 requests/s, 166103.86 total tokens/s, 162.05 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 16:33:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:33:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:33:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:33:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:33:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:33:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:33:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:33:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:33:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:33:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:33:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:33:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:33:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:33:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:33:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:33:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=769091) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=769091) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.75it/s]
(EngineCore_DP0 pid=769091) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.40it/s]
(EngineCore_DP0 pid=769091) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.44it/s]
(EngineCore_DP0 pid=769091) 
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=769091) [rank0]:W0126 16:33:47.146000 769091 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=769091) [rank0]:W0126 16:33:47.228000 769091 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=769091) [rank0]:W0126 16:33:48.171000 769091 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=769091) [rank0]:W0126 16:33:48.298000 769091 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=769091) 2026-01-26 16:33:52,167 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=769091) 2026-01-26 16:33:52,192 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=769091) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 13.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 12.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 14.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.40it/s]
(EngineCore_DP0 pid=769091) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.90it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.11it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 18.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.28it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 276.55it/s]
Adding requests:   2%|▏         | 64/4096 [00:00<00:12, 319.59it/s]
Adding requests:   2%|▏         | 96/4096 [00:00<00:12, 316.97it/s]
Adding requests:   3%|▎         | 130/4096 [00:00<00:12, 322.98it/s]
Adding requests:   4%|▍         | 166/4096 [00:00<00:11, 332.79it/s]
Adding requests:   5%|▍         | 203/4096 [00:00<00:11, 343.27it/s]
Adding requests:   6%|▌         | 239/4096 [00:00<00:11, 347.29it/s]
Adding requests:   7%|▋         | 274/4096 [00:00<00:11, 346.04it/s]
Adding requests:   8%|▊         | 311/4096 [00:00<00:10, 352.05it/s]
Adding requests:   9%|▊         | 349/4096 [00:01<00:10, 357.57it/s]
Adding requests:   9%|▉         | 386/4096 [00:01<00:10, 361.19it/s]
Adding requests:  10%|█         | 425/4096 [00:01<00:09, 367.36it/s]
Adding requests:  11%|█▏        | 462/4096 [00:01<00:09, 364.60it/s]
Adding requests:  12%|█▏        | 502/4096 [00:01<00:09, 371.75it/s]
Adding requests:  13%|█▎        | 542/4096 [00:01<00:09, 378.29it/s]
Adding requests:  14%|█▍        | 580/4096 [00:01<00:09, 374.81it/s]
Adding requests:  15%|█▌        | 618/4096 [00:01<00:09, 365.80it/s]
Adding requests:  16%|█▌        | 655/4096 [00:01<00:09, 358.63it/s]
Adding requests:  17%|█▋        | 693/4096 [00:01<00:09, 362.81it/s]
Adding requests:  18%|█▊        | 730/4096 [00:02<00:09, 356.34it/s]
Adding requests:  19%|█▊        | 766/4096 [00:02<00:09, 356.95it/s]
Adding requests:  20%|█▉        | 802/4096 [00:02<00:09, 355.82it/s]
Adding requests:  21%|██        | 840/4096 [00:02<00:08, 362.03it/s]
Adding requests:  21%|██▏       | 877/4096 [00:02<00:08, 362.18it/s]
Adding requests:  22%|██▏       | 914/4096 [00:02<00:08, 363.16it/s]
Adding requests:  23%|██▎       | 951/4096 [00:02<00:08, 359.41it/s]
Adding requests:  24%|██▍       | 987/4096 [00:02<00:08, 357.98it/s]
Adding requests:  25%|██▍       | 1023/4096 [00:02<00:08, 354.28it/s]
Adding requests:  26%|██▌       | 1059/4096 [00:02<00:08, 352.44it/s]
Adding requests:  27%|██▋       | 1095/4096 [00:03<00:08, 350.17it/s]
Adding requests:  28%|██▊       | 1134/4096 [00:03<00:08, 359.54it/s]
Adding requests:  29%|██▊       | 1170/4096 [00:03<00:08, 352.72it/s]
Adding requests:  29%|██▉       | 1206/4096 [00:03<00:08, 351.38it/s]
Adding requests:  30%|███       | 1243/4096 [00:03<00:08, 355.40it/s]
Adding requests:  31%|███       | 1279/4096 [00:03<00:08, 351.68it/s]
Adding requests:  32%|███▏      | 1315/4096 [00:03<00:07, 352.23it/s]
Adding requests:  33%|███▎      | 1353/4096 [00:03<00:07, 357.12it/s]
Adding requests:  34%|███▍      | 1390/4096 [00:03<00:07, 359.99it/s]
Adding requests:  35%|███▍      | 1427/4096 [00:04<00:07, 354.81it/s]
Adding requests:  36%|███▌      | 1464/4096 [00:04<00:07, 357.53it/s]
Adding requests:  37%|███▋      | 1502/4096 [00:04<00:07, 361.49it/s]
Adding requests:  38%|███▊      | 1539/4096 [00:04<00:07, 361.10it/s]
Adding requests:  38%|███▊      | 1576/4096 [00:04<00:07, 353.58it/s]
Adding requests:  39%|███▉      | 1612/4096 [00:04<00:07, 354.35it/s]
Adding requests:  40%|████      | 1648/4096 [00:04<00:07, 344.66it/s]
Adding requests:  41%|████      | 1683/4096 [00:04<00:06, 345.01it/s]
Adding requests:  42%|████▏     | 1720/4096 [00:04<00:06, 350.02it/s]
Adding requests:  43%|████▎     | 1757/4096 [00:04<00:06, 353.15it/s]
Adding requests:  44%|████▍     | 1794/4096 [00:05<00:06, 355.99it/s]
Adding requests:  45%|████▍     | 1831/4096 [00:05<00:06, 358.39it/s]
Adding requests:  46%|████▌     | 1867/4096 [00:05<00:06, 356.67it/s]
Adding requests:  46%|████▋     | 1904/4096 [00:05<00:06, 358.44it/s]
Adding requests:  47%|████▋     | 1940/4096 [00:05<00:06, 352.67it/s]
Adding requests:  48%|████▊     | 1976/4096 [00:05<00:05, 353.42it/s]
Adding requests:  49%|████▉     | 2012/4096 [00:05<00:05, 351.88it/s]
Adding requests:  50%|█████     | 2048/4096 [00:05<00:05, 348.34it/s]
Adding requests:  51%|█████     | 2083/4096 [00:05<00:06, 330.98it/s]
Adding requests:  52%|█████▏    | 2121/4096 [00:05<00:05, 344.20it/s]
Adding requests:  53%|█████▎    | 2156/4096 [00:06<00:05, 342.57it/s]
Adding requests:  53%|█████▎    | 2191/4096 [00:06<00:05, 339.88it/s]
Adding requests:  54%|█████▍    | 2226/4096 [00:06<00:05, 342.44it/s]
Adding requests:  55%|█████▌    | 2263/4096 [00:06<00:05, 347.45it/s]
Adding requests:  56%|█████▌    | 2301/4096 [00:06<00:05, 356.69it/s]
Adding requests:  57%|█████▋    | 2338/4096 [00:06<00:04, 359.48it/s]
Adding requests:  58%|█████▊    | 2375/4096 [00:06<00:04, 360.67it/s]
Adding requests:  59%|█████▉    | 2413/4096 [00:06<00:04, 366.37it/s]
Adding requests:  60%|█████▉    | 2450/4096 [00:06<00:04, 365.13it/s]
Adding requests:  61%|██████    | 2487/4096 [00:07<00:04, 366.18it/s]
Adding requests:  62%|██████▏   | 2524/4096 [00:07<00:04, 366.08it/s]
Adding requests:  63%|██████▎   | 2565/4096 [00:07<00:04, 378.62it/s]
Adding requests:  64%|██████▎   | 2603/4096 [00:07<00:03, 376.36it/s]
Adding requests:  64%|██████▍   | 2641/4096 [00:07<00:03, 364.33it/s]
Adding requests:  65%|██████▌   | 2678/4096 [00:07<00:03, 357.59it/s]
Adding requests:  66%|██████▋   | 2714/4096 [00:07<00:03, 352.67it/s]
Adding requests:  67%|██████▋   | 2751/4096 [00:07<00:03, 357.42it/s]
Adding requests:  68%|██████▊   | 2791/4096 [00:07<00:03, 366.77it/s]
Adding requests:  69%|██████▉   | 2828/4096 [00:07<00:03, 367.52it/s]
Adding requests:  70%|██████▉   | 2865/4096 [00:08<00:03, 365.38it/s]
Adding requests:  71%|███████   | 2902/4096 [00:08<00:03, 365.37it/s]
Adding requests:  72%|███████▏  | 2940/4096 [00:08<00:03, 367.62it/s]
Adding requests:  73%|███████▎  | 2977/4096 [00:08<00:03, 367.28it/s]
Adding requests:  74%|███████▎  | 3015/4096 [00:08<00:02, 370.28it/s]
Adding requests:  75%|███████▍  | 3053/4096 [00:08<00:02, 371.83it/s]
Adding requests:  75%|███████▌  | 3091/4096 [00:08<00:02, 370.55it/s]
Adding requests:  76%|███████▋  | 3130/4096 [00:08<00:02, 374.34it/s]
Adding requests:  77%|███████▋  | 3168/4096 [00:08<00:02, 368.78it/s]
Adding requests:  78%|███████▊  | 3205/4096 [00:08<00:02, 364.83it/s]
Adding requests:  79%|███████▉  | 3243/4096 [00:09<00:02, 366.65it/s]
Adding requests:  80%|████████  | 3280/4096 [00:09<00:02, 359.88it/s]
Adding requests:  81%|████████  | 3317/4096 [00:09<00:02, 354.49it/s]
Adding requests:  82%|████████▏ | 3354/4096 [00:09<00:02, 356.92it/s]
Adding requests:  83%|████████▎ | 3390/4096 [00:09<00:02, 351.05it/s]
Adding requests:  84%|████████▎ | 3427/4096 [00:09<00:01, 355.06it/s]
Adding requests:  85%|████████▍ | 3464/4096 [00:09<00:01, 358.44it/s]
Adding requests:  85%|████████▌ | 3500/4096 [00:09<00:01, 357.67it/s]
Adding requests:  86%|████████▋ | 3540/4096 [00:09<00:01, 368.49it/s]
Adding requests:  87%|████████▋ | 3577/4096 [00:10<00:01, 366.80it/s]
Adding requests:  88%|████████▊ | 3614/4096 [00:10<00:01, 367.56it/s]
Adding requests:  89%|████████▉ | 3651/4096 [00:10<00:01, 367.36it/s]
Adding requests:  90%|█████████ | 3688/4096 [00:10<00:01, 357.66it/s]
Adding requests:  91%|█████████ | 3725/4096 [00:10<00:01, 361.08it/s]
Adding requests:  92%|█████████▏| 3762/4096 [00:10<00:00, 353.69it/s]
Adding requests:  93%|█████████▎| 3798/4096 [00:10<00:00, 342.89it/s]
Adding requests:  94%|█████████▎| 3833/4096 [00:10<00:00, 342.26it/s]
Adding requests:  94%|█████████▍| 3870/4096 [00:10<00:00, 349.78it/s]
Adding requests:  95%|█████████▌| 3906/4096 [00:10<00:00, 343.91it/s]
Adding requests:  96%|█████████▌| 3942/4096 [00:11<00:00, 343.77it/s]
Adding requests:  97%|█████████▋| 3978/4096 [00:11<00:00, 345.22it/s]
Adding requests:  98%|█████████▊| 4015/4096 [00:11<00:00, 350.00it/s]
Adding requests:  99%|█████████▉| 4051/4096 [00:11<00:00, 345.38it/s]
Adding requests: 100%|█████████▉| 4087/4096 [00:11<00:00, 347.40it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 356.25it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  45%|████▌     | 1862/4096 [00:00<00:00, 6647.97it/s, est. speed input: 6807716.76 toks/s, output: 6648.03 toks/s]
Processed prompts:  62%|██████▏   | 2527/4096 [00:04<00:03, 478.27it/s, est. speed input: 616154.44 toks/s, output: 601.71 toks/s]   
Processed prompts:  69%|██████▊   | 2811/4096 [00:05<00:03, 357.32it/s, est. speed input: 482178.67 toks/s, output: 470.88 toks/s]
Processed prompts:  73%|███████▎  | 2973/4096 [00:06<00:03, 314.06it/s, est. speed input: 439048.48 toks/s, output: 428.76 toks/s]
Processed prompts:  75%|███████▌  | 3078/4096 [00:07<00:03, 275.73it/s, est. speed input: 407990.06 toks/s, output: 398.43 toks/s]
Processed prompts:  77%|███████▋  | 3150/4096 [00:08<00:03, 263.24it/s, est. speed input: 397080.20 toks/s, output: 387.77 toks/s]
Processed prompts:  78%|███████▊  | 3204/4096 [00:08<00:03, 264.24it/s, est. speed input: 394356.29 toks/s, output: 385.11 toks/s]
Processed prompts:  79%|███████▉  | 3249/4096 [00:08<00:03, 234.46it/s, est. speed input: 381664.26 toks/s, output: 372.72 toks/s]
Processed prompts:  80%|████████  | 3284/4096 [00:08<00:03, 227.25it/s, est. speed input: 377178.37 toks/s, output: 368.34 toks/s]
Processed prompts:  81%|████████  | 3314/4096 [00:09<00:03, 215.77it/s, est. speed input: 372314.09 toks/s, output: 363.59 toks/s]
Processed prompts:  82%|████████▏ | 3339/4096 [00:09<00:03, 199.95it/s, est. speed input: 367131.61 toks/s, output: 358.53 toks/s]
Processed prompts:  82%|████████▏ | 3366/4096 [00:09<00:03, 186.95it/s, est. speed input: 362348.20 toks/s, output: 353.86 toks/s]
Processed prompts:  83%|████████▎ | 3398/4096 [00:09<00:03, 182.00it/s, est. speed input: 358459.10 toks/s, output: 350.06 toks/s]
Processed prompts:  84%|████████▎ | 3430/4096 [00:09<00:03, 177.05it/s, est. speed input: 354599.39 toks/s, output: 346.29 toks/s]
Processed prompts:  85%|████████▍ | 3462/4096 [00:10<00:03, 172.81it/s, est. speed input: 350849.68 toks/s, output: 342.63 toks/s]
Processed prompts:  85%|████████▌ | 3494/4096 [00:10<00:03, 170.06it/s, est. speed input: 347324.70 toks/s, output: 339.18 toks/s]
Processed prompts:  86%|████████▌ | 3526/4096 [00:10<00:03, 167.18it/s, est. speed input: 343820.21 toks/s, output: 335.76 toks/s]
Processed prompts:  87%|████████▋ | 3558/4096 [00:10<00:03, 167.51it/s, est. speed input: 340773.25 toks/s, output: 332.79 toks/s]
Processed prompts:  88%|████████▊ | 3590/4096 [00:10<00:03, 165.95it/s, est. speed input: 337605.27 toks/s, output: 329.69 toks/s]
Processed prompts:  88%|████████▊ | 3622/4096 [00:11<00:02, 165.86it/s, est. speed input: 334676.90 toks/s, output: 326.83 toks/s]
Processed prompts:  89%|████████▉ | 3654/4096 [00:11<00:02, 164.48it/s, est. speed input: 331692.94 toks/s, output: 323.92 toks/s]
Processed prompts:  90%|████████▉ | 3686/4096 [00:11<00:02, 166.45it/s, est. speed input: 329149.76 toks/s, output: 321.44 toks/s]
Processed prompts:  91%|█████████ | 3718/4096 [00:11<00:02, 166.72it/s, est. speed input: 326562.81 toks/s, output: 318.91 toks/s]
Processed prompts:  92%|█████████▏| 3750/4096 [00:11<00:02, 164.51it/s, est. speed input: 323801.05 toks/s, output: 316.21 toks/s]
Processed prompts:  92%|█████████▏| 3782/4096 [00:12<00:01, 162.97it/s, est. speed input: 321130.08 toks/s, output: 313.60 toks/s]
Processed prompts:  93%|█████████▎| 3814/4096 [00:12<00:01, 162.34it/s, est. speed input: 318592.55 toks/s, output: 311.13 toks/s]
Processed prompts:  94%|█████████▍| 3846/4096 [00:12<00:01, 161.65it/s, est. speed input: 316110.25 toks/s, output: 308.70 toks/s]
Processed prompts:  95%|█████████▍| 3878/4096 [00:12<00:01, 161.63it/s, est. speed input: 313752.25 toks/s, output: 306.40 toks/s]
Processed prompts:  95%|█████████▌| 3910/4096 [00:12<00:01, 164.13it/s, est. speed input: 311713.92 toks/s, output: 304.41 toks/s]
Processed prompts:  96%|█████████▌| 3942/4096 [00:13<00:00, 165.12it/s, est. speed input: 309658.35 toks/s, output: 302.40 toks/s]
Processed prompts:  97%|█████████▋| 3974/4096 [00:13<00:00, 163.78it/s, est. speed input: 307476.53 toks/s, output: 300.27 toks/s]
Processed prompts:  98%|█████████▊| 4006/4096 [00:13<00:00, 163.74it/s, est. speed input: 305439.40 toks/s, output: 298.28 toks/s]
Processed prompts:  99%|█████████▊| 4038/4096 [00:13<00:00, 166.92it/s, est. speed input: 303740.33 toks/s, output: 296.62 toks/s]
Processed prompts:  99%|█████████▉| 4070/4096 [00:13<00:00, 174.96it/s, est. speed input: 302539.91 toks/s, output: 295.45 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:13<00:00, 174.96it/s, est. speed input: 304465.43 toks/s, output: 297.33 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:13<00:00, 297.33it/s, est. speed input: 304465.43 toks/s, output: 297.33 toks/s]
[rank0]:[W126 16:34:20.790603803 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 87.4s

测试结果:
  Requests/s:   162.05
  Tokens/s:     166103.86
  Total Reqs:   4096
  Elapsed:      25.28s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     165941.80

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:35:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=771169) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=771169) WARNING 01-26 16:35:33 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     def forward(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     raise e
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/tmp/torchinductor_root/xo/cxohmxvb3s5i5om5eclj5heiwlbja3nfrkpkl67ukch2tdr36mj6.py", line 1090, in call
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     triton_poi_fused_mul_quant_slide_fp8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_slide_fp8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     torch.cuda.synchronize()
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] 


─── STDERR ───
[2026-01-26 16:35:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:35:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:35:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:35:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:35:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:35:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:35:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:35:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:35:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:35:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:35:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:35:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=771169) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=771169) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.77it/s]
(EngineCore_DP0 pid=771169) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.42it/s]
(EngineCore_DP0 pid=771169) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.46it/s]
(EngineCore_DP0 pid=771169) 
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=771169) [rank0]:W0126 16:35:38.594000 771169 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=771169) [rank0]:W0126 16:35:38.677000 771169 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=771169) [rank0]:W0126 16:35:39.794000 771169 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=771169) [rank0]:W0126 16:35:39.919000 771169 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=771169) Process EngineCore_DP0:
(EngineCore_DP0 pid=771169) Traceback (most recent call last):
(EngineCore_DP0 pid=771169)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=771169)     self.run()
(EngineCore_DP0 pid=771169)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=771169)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=771169)     raise e
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=771169)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=771169)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=771169)     super().__init__(
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=771169)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=771169)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=771169)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=771169)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=771169)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=771169)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=771169)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=771169)     return func(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=771169)     return func(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=771169)     self.model_runner.profile_run()
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=771169)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=771169)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=771169)     return func(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=771169)     outputs = self.model(
(EngineCore_DP0 pid=771169)               ^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=771169)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=771169)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=771169)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=771169)     hidden_states = self.model(
(EngineCore_DP0 pid=771169)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=771169)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=771169)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=771169)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=771169)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=771169)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=771169)     def forward(
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=771169)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=771169)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=771169)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=771169)     raise e
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=771169)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=771169)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=771169)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=771169)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=771169)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=771169)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=771169)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=771169)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=771169)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=771169)     return compiled_fn(full_args)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=771169)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=771169)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=771169)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=771169)                             ^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=771169)     outs = compiled_fn(args)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=771169)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=771169)     return self.current_callable(inputs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=771169)     out = model(new_inputs)
(EngineCore_DP0 pid=771169)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/tmp/torchinductor_root/xo/cxohmxvb3s5i5om5eclj5heiwlbja3nfrkpkl67ukch2tdr36mj6.py", line 1090, in call
(EngineCore_DP0 pid=771169)     triton_poi_fused_mul_quant_slide_fp8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_slide_fp8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=771169)     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=771169)     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=771169)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=771169)     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=771169)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=771169)     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=771169)     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=771169)     torch.cuda.synchronize()
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=771169)     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=771169) Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=771169) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=771169) For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=771169) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=771169) 
[rank0]:[W126 16:35:40.217689091 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,38.2910,19643.2668,3.3428
1024,1024,1,128,128,39.8201,40815.5727,3.2145
2048,1024,2,256,128,72.7577,74576.6711,3.5185
4096,1024,4,512,128,136.0526,139453.8787,3.7633
8192,1024,8,1024,128,147.6801,151372.0832,6.9339
16384,1024,16,2048,128,156.7281,160646.3171,13.0672
32768,1024,32,4096,128,162.0525,166103.8555,25.2758
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-FP8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:35:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=772167) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=772167) WARNING 01-26 16:36:06 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=772167) WARNING 01-26 16:36:16 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 38.97 requests/s, 19992.58 total tokens/s, 38.97 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:35:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:35:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:35:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:35:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:35:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:35:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:35:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:35:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:35:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:35:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:35:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:35:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=772167) [2026-01-26 16:35:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=772167) [2026-01-26 16:35:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=772167) [2026-01-26 16:35:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=772167) [2026-01-26 16:35:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=772167) [2026-01-26 16:35:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=772167) [2026-01-26 16:35:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=772167) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=772167) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.13it/s]
(EngineCore_DP0 pid=772167) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.11s/it]
(EngineCore_DP0 pid=772167) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.08s/it]
(EngineCore_DP0 pid=772167) 
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=772167) [2026-01-26 16:36:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=772167) 2026-01-26 16:36:16,292 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=772167) 2026-01-26 16:36:16,315 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=772167) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.33it/s]
(EngineCore_DP0 pid=772167) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.34it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 513.17it/s]
Adding requests:  88%|████████▊ | 112/128 [00:00<00:00, 561.17it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 559.44it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:01, 65.17it/s, est. speed input: 33367.98 toks/s, output: 65.17 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 48.91it/s, est. speed input: 26083.87 toks/s, output: 50.94 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 45.26it/s, est. speed input: 24350.33 toks/s, output: 47.56 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 43.81it/s, est. speed input: 23632.80 toks/s, output: 46.16 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 42.90it/s, est. speed input: 23169.35 toks/s, output: 45.25 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 42.26it/s, est. speed input: 22832.01 toks/s, output: 44.59 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:02, 41.90it/s, est. speed input: 22598.31 toks/s, output: 44.14 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:01, 41.58it/s, est. speed input: 22404.94 toks/s, output: 43.76 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 41.38it/s, est. speed input: 22253.59 toks/s, output: 43.46 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 41.13it/s, est. speed input: 22112.24 toks/s, output: 43.19 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 41.06it/s, est. speed input: 22010.85 toks/s, output: 42.99 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 41.02it/s, est. speed input: 21926.95 toks/s, output: 42.83 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 41.00it/s, est. speed input: 21857.40 toks/s, output: 42.69 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 40.98it/s, est. speed input: 21795.70 toks/s, output: 42.57 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 40.96it/s, est. speed input: 21740.98 toks/s, output: 42.46 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 40.99it/s, est. speed input: 21697.68 toks/s, output: 42.38 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 40.98it/s, est. speed input: 21656.69 toks/s, output: 42.30 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 40.94it/s, est. speed input: 21616.38 toks/s, output: 42.22 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 40.95it/s, est. speed input: 21583.65 toks/s, output: 42.16 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.97it/s, est. speed input: 21555.58 toks/s, output: 42.10 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.94it/s, est. speed input: 21526.32 toks/s, output: 42.04 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 40.93it/s, est. speed input: 21500.96 toks/s, output: 41.99 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 40.97it/s, est. speed input: 21480.67 toks/s, output: 41.95 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 40.99it/s, est. speed input: 21461.74 toks/s, output: 41.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.99it/s, est. speed input: 21454.62 toks/s, output: 41.90 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.90it/s, est. speed input: 21454.62 toks/s, output: 41.90 toks/s]
[rank0]:[W126 16:36:21.116924268 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.6s

测试结果:
  Requests/s:   38.97
  Tokens/s:     19992.58
  Total Reqs:   128
  Elapsed:      3.28s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     19953.60

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:36:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=773356) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=773356) WARNING 01-26 16:36:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=773356) WARNING 01-26 16:36:57 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.78 requests/s, 40776.04 total tokens/s, 39.78 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:36:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:36:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:36:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:36:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:36:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:36:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:36:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:36:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:36:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:36:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:36:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:36:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:36:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:36:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:36:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:36:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:36:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=773356) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=773356) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.21it/s]
(EngineCore_DP0 pid=773356) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.02s/it]
(EngineCore_DP0 pid=773356) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=773356) 
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=773356) [2026-01-26 16:36:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=773356) 2026-01-26 16:36:57,102 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=773356) 2026-01-26 16:36:57,125 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=773356) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.88it/s]
(EngineCore_DP0 pid=773356) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.33it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 275.68it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 318.83it/s]
Adding requests:  74%|███████▍  | 95/128 [00:00<00:00, 315.81it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 320.10it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 315.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:00, 153.22it/s, est. speed input: 156902.83 toks/s, output: 153.22 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:01, 60.95it/s, est. speed input: 69205.32 toks/s, output: 67.58 toks/s]   
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 52.84it/s, est. speed input: 60900.20 toks/s, output: 59.47 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:01, 49.09it/s, est. speed input: 57243.39 toks/s, output: 55.90 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 46.76it/s, est. speed input: 55048.97 toks/s, output: 53.76 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 45.07it/s, est. speed input: 53421.65 toks/s, output: 52.17 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 43.96it/s, est. speed input: 52329.39 toks/s, output: 51.10 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 43.05it/s, est. speed input: 51408.39 toks/s, output: 50.20 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 42.37it/s, est. speed input: 50637.37 toks/s, output: 49.45 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:01, 41.84it/s, est. speed input: 49970.27 toks/s, output: 48.80 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 41.37it/s, est. speed input: 49371.41 toks/s, output: 48.21 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:01<00:00, 41.09it/s, est. speed input: 48864.18 toks/s, output: 47.72 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 40.98it/s, est. speed input: 48439.27 toks/s, output: 47.30 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 40.91it/s, est. speed input: 48063.33 toks/s, output: 46.94 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 40.81it/s, est. speed input: 47716.09 toks/s, output: 46.60 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 40.75it/s, est. speed input: 47406.04 toks/s, output: 46.29 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 40.69it/s, est. speed input: 47124.06 toks/s, output: 46.02 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 40.64it/s, est. speed input: 46866.22 toks/s, output: 45.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.59it/s, est. speed input: 46627.03 toks/s, output: 45.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.59it/s, est. speed input: 46627.03 toks/s, output: 45.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.53it/s, est. speed input: 46627.03 toks/s, output: 45.53 toks/s]
[rank0]:[W126 16:37:02.316692195 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.4s

测试结果:
  Requests/s:   39.78
  Tokens/s:     40776.04
  Total Reqs:   128
  Elapsed:      3.22s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     40736.26

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:37:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=774468) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=774468) WARNING 01-26 16:37:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=774468) WARNING 01-26 16:37:38 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 74.99 requests/s, 76860.18 total tokens/s, 74.99 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 16:37:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:37:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:37:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:37:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:37:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:37:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:37:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:37:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:37:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:37:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:37:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:37:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:37:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:37:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:37:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:37:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=774468) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=774468) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.37it/s]
(EngineCore_DP0 pid=774468) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.11it/s]
(EngineCore_DP0 pid=774468) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
(EngineCore_DP0 pid=774468) 
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=774468) [2026-01-26 16:37:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=774468) 2026-01-26 16:37:38,161 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=774468) 2026-01-26 16:37:38,184 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=774468) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.17it/s]
(EngineCore_DP0 pid=774468) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.40it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█         | 27/256 [00:00<00:00, 261.73it/s]
Adding requests:  23%|██▎       | 60/256 [00:00<00:00, 299.98it/s]
Adding requests:  36%|███▌      | 91/256 [00:00<00:00, 300.40it/s]
Adding requests:  48%|████▊     | 122/256 [00:00<00:00, 302.68it/s]
Adding requests:  60%|██████    | 154/256 [00:00<00:00, 308.44it/s]
Adding requests:  74%|███████▍  | 189/256 [00:00<00:00, 321.89it/s]
Adding requests:  88%|████████▊ | 224/256 [00:00<00:00, 327.65it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 315.47it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:00<00:00, 579.47it/s, est. speed input: 593412.60 toks/s, output: 579.48 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:00<00:01, 123.97it/s, est. speed input: 144561.15 toks/s, output: 141.17 toks/s]
Processed prompts:  58%|█████▊    | 149/256 [00:01<00:00, 107.43it/s, est. speed input: 126293.80 toks/s, output: 123.33 toks/s]
Processed prompts:  66%|██████▌   | 169/256 [00:01<00:00, 99.06it/s, est. speed input: 118122.53 toks/s, output: 115.35 toks/s] 
Processed prompts:  72%|███████▏  | 184/256 [00:01<00:00, 92.83it/s, est. speed input: 112872.26 toks/s, output: 110.23 toks/s]
Processed prompts:  77%|███████▋  | 197/256 [00:01<00:00, 91.16it/s, est. speed input: 110675.59 toks/s, output: 108.08 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:01<00:00, 85.38it/s, est. speed input: 107209.97 toks/s, output: 104.70 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:02<00:00, 83.83it/s, est. speed input: 105556.58 toks/s, output: 103.08 toks/s]
Processed prompts:  89%|████████▊ | 227/256 [00:02<00:00, 84.62it/s, est. speed input: 104837.85 toks/s, output: 102.38 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 80.94it/s, est. speed input: 103046.81 toks/s, output: 100.63 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:02<00:00, 82.47it/s, est. speed input: 102488.87 toks/s, output: 100.09 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:02<00:00, 78.93it/s, est. speed input: 100968.72 toks/s, output: 98.60 toks/s] 
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 78.93it/s, est. speed input: 100771.69 toks/s, output: 98.41 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 98.41it/s, est. speed input: 100771.69 toks/s, output: 98.41 toks/s]
[rank0]:[W126 16:37:43.633944794 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.3s

测试结果:
  Requests/s:   74.99
  Tokens/s:     76860.18
  Total Reqs:   256
  Elapsed:      3.41s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     76785.19

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:37:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=775625) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=775625) WARNING 01-26 16:38:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=775625) WARNING 01-26 16:38:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 115.57 requests/s, 118454.63 total tokens/s, 115.57 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 16:37:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:37:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:37:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:37:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:37:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:37:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:37:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:37:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:37:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:38:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:38:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:38:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:38:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:38:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:38:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:38:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:38:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:02] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=775625) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=775625) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=775625) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.13it/s]
(EngineCore_DP0 pid=775625) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
(EngineCore_DP0 pid=775625) 
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=775625) [2026-01-26 16:38:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=775625) 2026-01-26 16:38:21,312 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=775625) 2026-01-26 16:38:21,335 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=775625) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 12.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.54it/s]
(EngineCore_DP0 pid=775625) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.69it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 17.80it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▍         | 25/512 [00:00<00:01, 248.59it/s]
Adding requests:  11%|█▏        | 58/512 [00:00<00:01, 296.12it/s]
Adding requests:  18%|█▊        | 90/512 [00:00<00:01, 302.48it/s]
Adding requests:  24%|██▍       | 124/512 [00:00<00:01, 314.26it/s]
Adding requests:  31%|███       | 159/512 [00:00<00:01, 324.95it/s]
Adding requests:  38%|███▊      | 195/512 [00:00<00:00, 335.87it/s]
Adding requests:  45%|████▌     | 231/512 [00:00<00:00, 343.46it/s]
Adding requests:  52%|█████▏    | 266/512 [00:00<00:00, 343.63it/s]
Adding requests:  59%|█████▉    | 302/512 [00:00<00:00, 348.12it/s]
Adding requests:  66%|██████▌   | 339/512 [00:01<00:00, 354.10it/s]
Adding requests:  73%|███████▎  | 376/512 [00:01<00:00, 357.64it/s]
Adding requests:  81%|████████  | 414/512 [00:01<00:00, 363.29it/s]
Adding requests:  88%|████████▊ | 451/512 [00:01<00:00, 358.80it/s]
Adding requests:  96%|█████████▌| 491/512 [00:01<00:00, 370.82it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 346.49it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:00<00:00, 1492.16it/s, est. speed input: 1528060.96 toks/s, output: 1492.18 toks/s]
Processed prompts:  64%|██████▍   | 328/512 [00:01<00:00, 203.63it/s, est. speed input: 242629.57 toks/s, output: 236.94 toks/s]   
Processed prompts:  78%|███████▊  | 397/512 [00:01<00:00, 171.43it/s, est. speed input: 206978.96 toks/s, output: 202.13 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [00:02<00:00, 157.34it/s, est. speed input: 193128.90 toks/s, output: 188.60 toks/s]
Processed prompts:  92%|█████████▏| 471/512 [00:02<00:00, 147.87it/s, est. speed input: 185006.86 toks/s, output: 180.67 toks/s]
Processed prompts:  97%|█████████▋| 495/512 [00:02<00:00, 142.00it/s, est. speed input: 180193.16 toks/s, output: 175.97 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 142.00it/s, est. speed input: 177639.91 toks/s, output: 173.48 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 173.47it/s, est. speed input: 177639.91 toks/s, output: 173.48 toks/s]
[rank0]:[W126 16:38:28.438211260 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.8s

测试结果:
  Requests/s:   115.57
  Tokens/s:     118454.63
  Total Reqs:   512
  Elapsed:      4.43s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     118339.07

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:38:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=776844) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=776844) WARNING 01-26 16:38:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=776844) WARNING 01-26 16:39:08 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 122.05 requests/s, 125105.08 total tokens/s, 122.05 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 16:38:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:38:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:38:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:38:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:38:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:38:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:38:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:38:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:38:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:38:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:38:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:38:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:38:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:38:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:38:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:38:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:38:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:50] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=776844) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=776844) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=776844) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]
(EngineCore_DP0 pid=776844) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
(EngineCore_DP0 pid=776844) 
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=776844) [2026-01-26 16:38:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=776844) 2026-01-26 16:39:08,818 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=776844) 2026-01-26 16:39:08,842 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=776844) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 13.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 16.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 14.60it/s]
(EngineCore_DP0 pid=776844) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 17.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.07it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 279.22it/s]
Adding requests:   6%|▋         | 64/1024 [00:00<00:02, 320.53it/s]
Adding requests:   9%|▉         | 97/1024 [00:00<00:02, 318.24it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 322.78it/s]
Adding requests:  16%|█▌        | 166/1024 [00:00<00:02, 333.31it/s]
Adding requests:  20%|█▉        | 203/1024 [00:00<00:02, 343.87it/s]
Adding requests:  23%|██▎       | 239/1024 [00:00<00:02, 348.81it/s]
Adding requests:  27%|██▋       | 274/1024 [00:00<00:02, 348.04it/s]
Adding requests:  30%|███       | 312/1024 [00:00<00:02, 353.16it/s]
Adding requests:  34%|███▍      | 349/1024 [00:01<00:01, 358.17it/s]
Adding requests:  38%|███▊      | 387/1024 [00:01<00:01, 362.56it/s]
Adding requests:  42%|████▏     | 425/1024 [00:01<00:01, 367.58it/s]
Adding requests:  45%|████▌     | 462/1024 [00:01<00:01, 364.90it/s]
Adding requests:  49%|████▉     | 502/1024 [00:01<00:01, 372.10it/s]
Adding requests:  53%|█████▎    | 542/1024 [00:01<00:01, 378.85it/s]
Adding requests:  57%|█████▋    | 580/1024 [00:01<00:01, 374.36it/s]
Adding requests:  60%|██████    | 618/1024 [00:01<00:01, 366.04it/s]
Adding requests:  64%|██████▍   | 655/1024 [00:01<00:01, 358.98it/s]
Adding requests:  68%|██████▊   | 693/1024 [00:01<00:00, 363.05it/s]
Adding requests:  71%|███████▏  | 730/1024 [00:02<00:00, 356.53it/s]
Adding requests:  75%|███████▍  | 766/1024 [00:02<00:00, 357.19it/s]
Adding requests:  78%|███████▊  | 802/1024 [00:02<00:00, 355.92it/s]
Adding requests:  82%|████████▏ | 840/1024 [00:02<00:00, 362.30it/s]
Adding requests:  86%|████████▌ | 877/1024 [00:02<00:00, 361.90it/s]
Adding requests:  89%|████████▉ | 914/1024 [00:02<00:00, 363.16it/s]
Adding requests:  93%|█████████▎| 951/1024 [00:02<00:00, 359.77it/s]
Adding requests:  96%|█████████▋| 988/1024 [00:02<00:00, 360.89it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 356.06it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:00<00:00, 3249.60it/s, est. speed input: 3327872.50 toks/s, output: 3249.67 toks/s]
Processed prompts:  66%|██████▋   | 679/1024 [00:02<00:01, 214.01it/s, est. speed input: 256647.98 toks/s, output: 250.63 toks/s]   
Processed prompts:  80%|████████  | 821/1024 [00:03<00:01, 178.38it/s, est. speed input: 216702.42 toks/s, output: 211.62 toks/s]
Processed prompts:  88%|████████▊ | 904/1024 [00:04<00:00, 166.48it/s, est. speed input: 204334.92 toks/s, output: 199.55 toks/s]
Processed prompts:  94%|█████████▎| 959/1024 [00:04<00:00, 157.76it/s, est. speed input: 196934.28 toks/s, output: 192.32 toks/s]
Processed prompts:  98%|█████████▊| 999/1024 [00:05<00:00, 152.11it/s, est. speed input: 192578.48 toks/s, output: 188.06 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 152.11it/s, est. speed input: 190227.36 toks/s, output: 185.77 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 185.77it/s, est. speed input: 190227.36 toks/s, output: 185.77 toks/s]
[rank0]:[W126 16:39:19.023682633 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.6s

测试结果:
  Requests/s:   122.05
  Tokens/s:     125105.08
  Total Reqs:   1024
  Elapsed:      8.39s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     124983.02

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:39:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=778215) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=778215) WARNING 01-26 16:39:56 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=778215) WARNING 01-26 16:40:06 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 129.66 requests/s, 132904.88 total tokens/s, 129.66 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 16:39:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:39:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:39:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:39:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:39:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:39:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:39:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:39:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:39:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:39:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:39:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:39:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:39:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:39:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:39:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:39:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:39:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=778215) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=778215) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=778215) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]
(EngineCore_DP0 pid=778215) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
(EngineCore_DP0 pid=778215) 
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=778215) [2026-01-26 16:39:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=778215) [rank0]:W0126 16:40:01.981000 778215 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=778215) [rank0]:W0126 16:40:02.063000 778215 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=778215) [rank0]:W0126 16:40:03.034000 778215 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=778215) [rank0]:W0126 16:40:03.165000 778215 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=778215) 2026-01-26 16:40:06,641 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=778215) 2026-01-26 16:40:06,668 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=778215) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 16.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.45it/s]
(EngineCore_DP0 pid=778215) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 14.66it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 15.85it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.10it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 275.09it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 317.78it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 316.11it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:05, 323.58it/s]
Adding requests:   8%|▊         | 164/2048 [00:00<00:05, 330.74it/s]
Adding requests:  10%|▉         | 201/2048 [00:00<00:05, 342.61it/s]
Adding requests:  12%|█▏        | 238/2048 [00:00<00:05, 348.59it/s]
Adding requests:  13%|█▎        | 273/2048 [00:00<00:05, 346.11it/s]
Adding requests:  15%|█▌        | 310/2048 [00:00<00:04, 352.87it/s]
Adding requests:  17%|█▋        | 348/2048 [00:01<00:04, 359.41it/s]
Adding requests:  19%|█▉        | 386/2048 [00:01<00:04, 362.38it/s]
Adding requests:  21%|██        | 423/2048 [00:01<00:04, 361.40it/s]
Adding requests:  22%|██▏       | 460/2048 [00:01<00:04, 357.89it/s]
Adding requests:  24%|██▍       | 500/2048 [00:01<00:04, 368.50it/s]
Adding requests:  26%|██▋       | 538/2048 [00:01<00:04, 371.06it/s]
Adding requests:  28%|██▊       | 576/2048 [00:01<00:03, 372.05it/s]
Adding requests:  30%|██▉       | 614/2048 [00:01<00:04, 348.70it/s]
Adding requests:  32%|███▏      | 650/2048 [00:01<00:04, 345.74it/s]
Adding requests:  34%|███▎      | 687/2048 [00:01<00:03, 352.11it/s]
Adding requests:  35%|███▌      | 723/2048 [00:02<00:03, 350.19it/s]
Adding requests:  37%|███▋      | 759/2048 [00:02<00:03, 349.16it/s]
Adding requests:  39%|███▉      | 795/2048 [00:02<00:03, 349.90it/s]
Adding requests:  41%|████      | 832/2048 [00:02<00:03, 354.08it/s]
Adding requests:  42%|████▏     | 868/2048 [00:02<00:03, 355.23it/s]
Adding requests:  44%|████▍     | 906/2048 [00:02<00:03, 361.77it/s]
Adding requests:  46%|████▌     | 943/2048 [00:02<00:03, 353.28it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:03, 354.60it/s]
Adding requests:  50%|████▉     | 1015/2048 [00:02<00:02, 349.15it/s]
Adding requests:  51%|█████▏    | 1051/2048 [00:02<00:02, 351.03it/s]
Adding requests:  53%|█████▎    | 1087/2048 [00:03<00:02, 351.56it/s]
Adding requests:  55%|█████▍    | 1123/2048 [00:03<00:02, 353.20it/s]
Adding requests:  57%|█████▋    | 1159/2048 [00:03<00:02, 352.01it/s]
Adding requests:  58%|█████▊    | 1195/2048 [00:03<00:02, 354.11it/s]
Adding requests:  60%|██████    | 1233/2048 [00:03<00:02, 361.43it/s]
Adding requests:  62%|██████▏   | 1270/2048 [00:03<00:02, 356.68it/s]
Adding requests:  64%|██████▍   | 1306/2048 [00:03<00:02, 355.75it/s]
Adding requests:  66%|██████▌   | 1342/2048 [00:03<00:01, 355.79it/s]
Adding requests:  67%|██████▋   | 1379/2048 [00:03<00:01, 358.58it/s]
Adding requests:  69%|██████▉   | 1415/2048 [00:04<00:01, 355.25it/s]
Adding requests:  71%|███████   | 1452/2048 [00:04<00:01, 357.09it/s]
Adding requests:  73%|███████▎  | 1490/2048 [00:04<00:01, 361.59it/s]
Adding requests:  75%|███████▍  | 1527/2048 [00:04<00:01, 361.79it/s]
Adding requests:  76%|███████▋  | 1564/2048 [00:04<00:01, 357.07it/s]
Adding requests:  78%|███████▊  | 1600/2048 [00:04<00:01, 352.39it/s]
Adding requests:  80%|███████▉  | 1636/2048 [00:04<00:01, 347.87it/s]
Adding requests:  82%|████████▏ | 1671/2048 [00:04<00:01, 333.60it/s]
Adding requests:  83%|████████▎ | 1708/2048 [00:04<00:00, 343.91it/s]
Adding requests:  85%|████████▌ | 1744/2048 [00:04<00:00, 346.39it/s]
Adding requests:  87%|████████▋ | 1782/2048 [00:05<00:00, 353.80it/s]
Adding requests:  89%|████████▉ | 1818/2048 [00:05<00:00, 351.01it/s]
Adding requests:  91%|█████████ | 1855/2048 [00:05<00:00, 356.52it/s]
Adding requests:  92%|█████████▏| 1892/2048 [00:05<00:00, 357.84it/s]
Adding requests:  94%|█████████▍| 1930/2048 [00:05<00:00, 362.14it/s]
Adding requests:  96%|█████████▌| 1967/2048 [00:05<00:00, 363.34it/s]
Adding requests:  98%|█████████▊| 2004/2048 [00:05<00:00, 356.65it/s]
Adding requests: 100%|█████████▉| 2040/2048 [00:05<00:00, 353.58it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 352.63it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  37%|███▋      | 748/2048 [00:00<00:00, 5954.41it/s, est. speed input: 6097702.50 toks/s, output: 5954.53 toks/s]
Processed prompts:  66%|██████▌   | 1344/2048 [00:04<00:02, 241.57it/s, est. speed input: 294552.24 toks/s, output: 287.65 toks/s]  
Processed prompts:  78%|███████▊  | 1597/2048 [00:06<00:02, 199.23it/s, est. speed input: 246592.45 toks/s, output: 240.81 toks/s]
Processed prompts:  85%|████████▌ | 1741/2048 [00:07<00:01, 183.31it/s, est. speed input: 230522.19 toks/s, output: 225.12 toks/s]
Processed prompts:  90%|████████▉ | 1833/2048 [00:08<00:01, 178.30it/s, est. speed input: 224979.36 toks/s, output: 219.71 toks/s]
Processed prompts:  93%|█████████▎| 1898/2048 [00:08<00:00, 171.57it/s, est. speed input: 220116.99 toks/s, output: 214.96 toks/s]
Processed prompts:  95%|█████████▍| 1945/2048 [00:09<00:00, 165.39it/s, est. speed input: 216520.63 toks/s, output: 211.45 toks/s]
Processed prompts:  97%|█████████▋| 1981/2048 [00:09<00:00, 154.47it/s, est. speed input: 212145.29 toks/s, output: 207.17 toks/s]
Processed prompts:  98%|█████████▊| 2009/2048 [00:09<00:00, 159.92it/s, est. speed input: 212424.54 toks/s, output: 207.45 toks/s]
Processed prompts:  99%|█████████▉| 2036/2048 [00:09<00:00, 151.49it/s, est. speed input: 210005.80 toks/s, output: 205.08 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:09<00:00, 151.49it/s, est. speed input: 210021.79 toks/s, output: 205.10 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:09<00:00, 205.10it/s, est. speed input: 210021.79 toks/s, output: 205.10 toks/s]
[rank0]:[W126 16:40:24.207988852 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 65.2s

测试结果:
  Requests/s:   129.66
  Tokens/s:     132904.88
  Total Reqs:   2048
  Elapsed:      15.79s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     132775.22

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:40:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=779803) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=779803) WARNING 01-26 16:41:13 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=779803) WARNING 01-26 16:41:23 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 133.25 requests/s, 136582.37 total tokens/s, 133.25 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 16:40:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:40:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:40:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:40:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:40:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:40:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:40:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:40:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:40:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:40:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:40:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:40:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:40:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:40:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:41:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:41:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:41:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:41:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:41:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:41:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:41:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:41:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:41:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:41:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:41:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:41:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:41:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:41:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:05] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=779803) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=779803) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.41it/s]
(EngineCore_DP0 pid=779803) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]
(EngineCore_DP0 pid=779803) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]
(EngineCore_DP0 pid=779803) 
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=779803) [2026-01-26 16:41:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=779803) [rank0]:W0126 16:41:18.759000 779803 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=779803) [rank0]:W0126 16:41:18.841000 779803 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=779803) [rank0]:W0126 16:41:19.807000 779803 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=779803) [rank0]:W0126 16:41:19.937000 779803 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=779803) 2026-01-26 16:41:23,487 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=779803) 2026-01-26 16:41:23,513 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=779803) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 13.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 13.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 14.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.53it/s]
(EngineCore_DP0 pid=779803) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.84it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.99it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 18.30it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.22it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 24/4096 [00:00<00:17, 235.86it/s]
Adding requests:   1%|▏         | 57/4096 [00:00<00:14, 285.48it/s]
Adding requests:   2%|▏         | 90/4096 [00:00<00:13, 303.04it/s]
Adding requests:   3%|▎         | 123/4096 [00:00<00:12, 312.17it/s]
Adding requests:   4%|▍         | 157/4096 [00:00<00:12, 319.72it/s]
Adding requests:   5%|▍         | 194/4096 [00:00<00:11, 334.89it/s]
Adding requests:   6%|▌         | 230/4096 [00:00<00:11, 340.54it/s]
Adding requests:   6%|▋         | 265/4096 [00:00<00:11, 338.80it/s]
Adding requests:   7%|▋         | 301/4096 [00:00<00:11, 343.94it/s]
Adding requests:   8%|▊         | 338/4096 [00:01<00:10, 350.21it/s]
Adding requests:   9%|▉         | 375/4096 [00:01<00:10, 353.52it/s]
Adding requests:  10%|█         | 412/4096 [00:01<00:10, 358.37it/s]
Adding requests:  11%|█         | 448/4096 [00:01<00:10, 355.32it/s]
Adding requests:  12%|█▏        | 488/4096 [00:01<00:09, 367.36it/s]
Adding requests:  13%|█▎        | 528/4096 [00:01<00:09, 375.20it/s]
Adding requests:  14%|█▍        | 566/4096 [00:01<00:09, 372.73it/s]
Adding requests:  15%|█▍        | 604/4096 [00:01<00:09, 358.07it/s]
Adding requests:  16%|█▌        | 640/4096 [00:01<00:09, 351.64it/s]
Adding requests:  17%|█▋        | 676/4096 [00:01<00:09, 347.96it/s]
Adding requests:  17%|█▋        | 714/4096 [00:02<00:09, 354.75it/s]
Adding requests:  18%|█▊        | 750/4096 [00:02<00:09, 348.26it/s]
Adding requests:  19%|█▉        | 786/4096 [00:02<00:09, 351.32it/s]
Adding requests:  20%|██        | 822/4096 [00:02<00:09, 351.08it/s]
Adding requests:  21%|██        | 860/4096 [00:02<00:09, 357.60it/s]
Adding requests:  22%|██▏       | 897/4096 [00:02<00:08, 359.65it/s]
Adding requests:  23%|██▎       | 933/4096 [00:02<00:08, 352.41it/s]
Adding requests:  24%|██▎       | 969/4096 [00:02<00:08, 347.80it/s]
Adding requests:  25%|██▍       | 1004/4096 [00:02<00:08, 344.30it/s]
Adding requests:  25%|██▌       | 1041/4096 [00:03<00:08, 348.31it/s]
Adding requests:  26%|██▋       | 1076/4096 [00:03<00:08, 345.51it/s]
Adding requests:  27%|██▋       | 1111/4096 [00:03<00:08, 346.59it/s]
Adding requests:  28%|██▊       | 1148/4096 [00:03<00:08, 350.68it/s]
Adding requests:  29%|██▉       | 1184/4096 [00:03<00:08, 349.26it/s]
Adding requests:  30%|██▉       | 1222/4096 [00:03<00:08, 355.80it/s]
Adding requests:  31%|███       | 1258/4096 [00:03<00:08, 351.76it/s]
Adding requests:  32%|███▏      | 1294/4096 [00:03<00:08, 348.24it/s]
Adding requests:  32%|███▏      | 1331/4096 [00:03<00:07, 351.25it/s]
Adding requests:  33%|███▎      | 1368/4096 [00:03<00:07, 355.46it/s]
Adding requests:  34%|███▍      | 1404/4096 [00:04<00:07, 353.52it/s]
Adding requests:  35%|███▌      | 1440/4096 [00:04<00:07, 354.60it/s]
Adding requests:  36%|███▌      | 1476/4096 [00:04<00:07, 352.35it/s]
Adding requests:  37%|███▋      | 1514/4096 [00:04<00:07, 358.44it/s]
Adding requests:  38%|███▊      | 1550/4096 [00:04<00:07, 355.95it/s]
Adding requests:  39%|███▊      | 1586/4096 [00:04<00:07, 348.52it/s]
Adding requests:  40%|███▉      | 1621/4096 [00:04<00:07, 343.99it/s]
Adding requests:  40%|████      | 1656/4096 [00:04<00:07, 339.87it/s]
Adding requests:  41%|████▏     | 1691/4096 [00:04<00:07, 342.33it/s]
Adding requests:  42%|████▏     | 1728/4096 [00:04<00:06, 347.77it/s]
Adding requests:  43%|████▎     | 1764/4096 [00:05<00:06, 351.32it/s]
Adding requests:  44%|████▍     | 1800/4096 [00:05<00:06, 351.51it/s]
Adding requests:  45%|████▍     | 1836/4096 [00:05<00:06, 349.16it/s]
Adding requests:  46%|████▌     | 1873/4096 [00:05<00:06, 353.59it/s]
Adding requests:  47%|████▋     | 1909/4096 [00:05<00:06, 353.95it/s]
Adding requests:  48%|████▊     | 1947/4096 [00:05<00:05, 360.76it/s]
Adding requests:  48%|████▊     | 1984/4096 [00:05<00:05, 359.68it/s]
Adding requests:  49%|████▉     | 2020/4096 [00:05<00:05, 347.02it/s]
Adding requests:  50%|█████     | 2055/4096 [00:05<00:05, 347.73it/s]
Adding requests:  51%|█████     | 2090/4096 [00:05<00:05, 340.13it/s]
Adding requests:  52%|█████▏    | 2125/4096 [00:06<00:05, 340.93it/s]
Adding requests:  53%|█████▎    | 2160/4096 [00:06<00:05, 339.74it/s]
Adding requests:  54%|█████▎    | 2195/4096 [00:06<00:05, 336.38it/s]
Adding requests:  54%|█████▍    | 2230/4096 [00:06<00:05, 338.88it/s]
Adding requests:  55%|█████▌    | 2267/4096 [00:06<00:05, 347.14it/s]
Adding requests:  56%|█████▋    | 2304/4096 [00:06<00:05, 353.15it/s]
Adding requests:  57%|█████▋    | 2341/4096 [00:06<00:04, 356.54it/s]
Adding requests:  58%|█████▊    | 2377/4096 [00:06<00:04, 355.10it/s]
Adding requests:  59%|█████▉    | 2414/4096 [00:06<00:04, 358.95it/s]
Adding requests:  60%|█████▉    | 2450/4096 [00:07<00:04, 355.25it/s]
Adding requests:  61%|██████    | 2486/4096 [00:07<00:04, 355.90it/s]
Adding requests:  62%|██████▏   | 2524/4096 [00:07<00:04, 359.81it/s]
Adding requests:  63%|██████▎   | 2564/4096 [00:07<00:04, 369.85it/s]
Adding requests:  64%|██████▎   | 2602/4096 [00:07<00:04, 371.08it/s]
Adding requests:  64%|██████▍   | 2640/4096 [00:07<00:04, 357.16it/s]
Adding requests:  65%|██████▌   | 2676/4096 [00:07<00:03, 357.26it/s]
Adding requests:  66%|██████▌   | 2712/4096 [00:07<00:03, 352.18it/s]
Adding requests:  67%|██████▋   | 2749/4096 [00:07<00:03, 356.01it/s]
Adding requests:  68%|██████▊   | 2787/4096 [00:07<00:03, 362.45it/s]
Adding requests:  69%|██████▉   | 2825/4096 [00:08<00:03, 364.62it/s]
Adding requests:  70%|██████▉   | 2862/4096 [00:08<00:03, 362.41it/s]
Adding requests:  71%|███████   | 2899/4096 [00:08<00:03, 361.43it/s]
Adding requests:  72%|███████▏  | 2937/4096 [00:08<00:03, 365.62it/s]
Adding requests:  73%|███████▎  | 2974/4096 [00:08<00:03, 359.77it/s]
Adding requests:  74%|███████▎  | 3012/4096 [00:08<00:02, 365.68it/s]
Adding requests:  74%|███████▍  | 3049/4096 [00:08<00:02, 364.04it/s]
Adding requests:  75%|███████▌  | 3087/4096 [00:08<00:02, 368.21it/s]
Adding requests:  76%|███████▋  | 3125/4096 [00:08<00:02, 370.89it/s]
Adding requests:  77%|███████▋  | 3163/4096 [00:08<00:02, 366.54it/s]
Adding requests:  78%|███████▊  | 3200/4096 [00:09<00:02, 359.36it/s]
Adding requests:  79%|███████▉  | 3238/4096 [00:09<00:02, 364.43it/s]
Adding requests:  80%|███████▉  | 3275/4096 [00:09<00:02, 358.59it/s]
Adding requests:  81%|████████  | 3311/4096 [00:09<00:02, 347.92it/s]
Adding requests:  82%|████████▏ | 3348/4096 [00:09<00:02, 351.56it/s]
Adding requests:  83%|████████▎ | 3385/4096 [00:09<00:01, 356.17it/s]
Adding requests:  84%|████████▎ | 3421/4096 [00:09<00:01, 348.06it/s]
Adding requests:  84%|████████▍ | 3458/4096 [00:09<00:01, 352.37it/s]
Adding requests:  85%|████████▌ | 3494/4096 [00:09<00:01, 349.84it/s]
Adding requests:  86%|████████▋ | 3533/4096 [00:10<00:01, 360.60it/s]
Adding requests:  87%|████████▋ | 3570/4096 [00:10<00:01, 363.07it/s]
Adding requests:  88%|████████▊ | 3607/4096 [00:10<00:01, 360.59it/s]
Adding requests:  89%|████████▉ | 3645/4096 [00:10<00:01, 363.40it/s]
Adding requests:  90%|████████▉ | 3682/4096 [00:10<00:01, 353.77it/s]
Adding requests:  91%|█████████ | 3719/4096 [00:10<00:01, 358.23it/s]
Adding requests:  92%|█████████▏| 3755/4096 [00:10<00:00, 348.55it/s]
Adding requests:  93%|█████████▎| 3790/4096 [00:10<00:00, 339.64it/s]
Adding requests:  93%|█████████▎| 3825/4096 [00:10<00:00, 337.86it/s]
Adding requests:  94%|█████████▍| 3861/4096 [00:10<00:00, 342.88it/s]
Adding requests:  95%|█████████▌| 3896/4096 [00:11<00:00, 342.08it/s]
Adding requests:  96%|█████████▌| 3931/4096 [00:11<00:00, 338.89it/s]
Adding requests:  97%|█████████▋| 3966/4096 [00:11<00:00, 341.83it/s]
Adding requests:  98%|█████████▊| 4001/4096 [00:11<00:00, 340.06it/s]
Adding requests:  99%|█████████▊| 4037/4096 [00:11<00:00, 345.42it/s]
Adding requests:  99%|█████████▉| 4072/4096 [00:11<00:00, 341.72it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 351.26it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  37%|███▋      | 1535/4096 [00:00<00:00, 8390.85it/s, est. speed input: 8592599.14 toks/s, output: 8390.94 toks/s]
Processed prompts:  58%|█████▊    | 2375/4096 [00:06<00:05, 303.56it/s, est. speed input: 382284.83 toks/s, output: 373.32 toks/s]   
Processed prompts:  67%|██████▋   | 2729/4096 [00:09<00:05, 240.18it/s, est. speed input: 310474.21 toks/s, output: 303.20 toks/s]
Processed prompts:  71%|███████▏  | 2928/4096 [00:10<00:05, 217.70it/s, est. speed input: 287613.79 toks/s, output: 280.87 toks/s]
Processed prompts:  75%|███████▍  | 3054/4096 [00:11<00:05, 202.45it/s, est. speed input: 274750.44 toks/s, output: 268.31 toks/s]
Processed prompts:  77%|███████▋  | 3139/4096 [00:12<00:05, 188.75it/s, est. speed input: 265477.47 toks/s, output: 259.26 toks/s]
Processed prompts:  78%|███████▊  | 3200/4096 [00:12<00:04, 180.61it/s, est. speed input: 260315.12 toks/s, output: 254.21 toks/s]
Processed prompts:  79%|███████▉  | 3245/4096 [00:12<00:04, 181.17it/s, est. speed input: 259027.27 toks/s, output: 252.96 toks/s]
Processed prompts:  80%|████████  | 3282/4096 [00:13<00:04, 178.17it/s, est. speed input: 257158.81 toks/s, output: 251.13 toks/s]
Processed prompts:  81%|████████  | 3312/4096 [00:13<00:04, 170.77it/s, est. speed input: 254787.43 toks/s, output: 248.82 toks/s]
Processed prompts:  81%|████████▏ | 3337/4096 [00:13<00:04, 159.87it/s, est. speed input: 252142.49 toks/s, output: 246.23 toks/s]
Processed prompts:  82%|████████▏ | 3359/4096 [00:13<00:05, 146.84it/s, est. speed input: 249352.28 toks/s, output: 243.51 toks/s]
Processed prompts:  83%|████████▎ | 3391/4096 [00:14<00:04, 143.77it/s, est. speed input: 247392.54 toks/s, output: 241.59 toks/s]
Processed prompts:  84%|████████▎ | 3423/4096 [00:14<00:04, 141.78it/s, est. speed input: 245581.31 toks/s, output: 239.83 toks/s]
Processed prompts:  84%|████████▍ | 3455/4096 [00:14<00:04, 139.53it/s, est. speed input: 243761.29 toks/s, output: 238.05 toks/s]
Processed prompts:  85%|████████▌ | 3487/4096 [00:14<00:04, 137.49it/s, est. speed input: 241971.75 toks/s, output: 236.30 toks/s]
Processed prompts:  86%|████████▌ | 3519/4096 [00:14<00:04, 136.06it/s, est. speed input: 240253.01 toks/s, output: 234.62 toks/s]
Processed prompts:  87%|████████▋ | 3551/4096 [00:15<00:03, 136.77it/s, est. speed input: 238764.30 toks/s, output: 233.17 toks/s]
Processed prompts:  87%|████████▋ | 3583/4096 [00:15<00:03, 135.74it/s, est. speed input: 237174.80 toks/s, output: 231.62 toks/s]
Processed prompts:  88%|████████▊ | 3615/4096 [00:15<00:03, 135.70it/s, est. speed input: 235697.22 toks/s, output: 230.17 toks/s]
Processed prompts:  89%|████████▉ | 3647/4096 [00:15<00:03, 134.77it/s, est. speed input: 234184.31 toks/s, output: 228.70 toks/s]
Processed prompts:  90%|████████▉ | 3679/4096 [00:16<00:03, 136.49it/s, est. speed input: 232919.70 toks/s, output: 227.46 toks/s]
Processed prompts:  91%|█████████ | 3711/4096 [00:16<00:02, 136.04it/s, est. speed input: 231551.76 toks/s, output: 226.12 toks/s]
Processed prompts:  91%|█████████▏| 3743/4096 [00:16<00:02, 134.73it/s, est. speed input: 230141.50 toks/s, output: 224.75 toks/s]
Processed prompts:  92%|█████████▏| 3775/4096 [00:16<00:02, 134.22it/s, est. speed input: 228804.17 toks/s, output: 223.44 toks/s]
Processed prompts:  93%|█████████▎| 3807/4096 [00:17<00:02, 133.57it/s, est. speed input: 227481.41 toks/s, output: 222.15 toks/s]
Processed prompts:  94%|█████████▎| 3839/4096 [00:17<00:01, 133.31it/s, est. speed input: 226210.64 toks/s, output: 220.91 toks/s]
Processed prompts:  95%|█████████▍| 3871/4096 [00:17<00:01, 132.79it/s, est. speed input: 224948.20 toks/s, output: 219.68 toks/s]
Processed prompts:  95%|█████████▌| 3903/4096 [00:17<00:01, 134.37it/s, est. speed input: 223866.86 toks/s, output: 218.62 toks/s]
Processed prompts:  96%|█████████▌| 3935/4096 [00:18<00:01, 136.08it/s, est. speed input: 222853.97 toks/s, output: 217.63 toks/s]
Processed prompts:  97%|█████████▋| 3967/4096 [00:18<00:00, 134.58it/s, est. speed input: 221675.68 toks/s, output: 216.48 toks/s]
Processed prompts:  98%|█████████▊| 3999/4096 [00:18<00:00, 135.08it/s, est. speed input: 220636.77 toks/s, output: 215.47 toks/s]
Processed prompts:  98%|█████████▊| 4031/4096 [00:18<00:00, 136.06it/s, est. speed input: 219665.33 toks/s, output: 214.52 toks/s]
Processed prompts:  99%|█████████▉| 4063/4096 [00:19<00:00, 138.42it/s, est. speed input: 218825.92 toks/s, output: 213.70 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:19<00:00, 138.42it/s, est. speed input: 219875.32 toks/s, output: 214.72 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:19<00:00, 214.72it/s, est. speed input: 219875.32 toks/s, output: 214.72 toks/s]
[rank0]:[W126 16:41:57.599308724 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 92.5s

测试结果:
  Requests/s:   133.25
  Tokens/s:     136582.37
  Total Reqs:   4096
  Elapsed:      30.74s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     136449.12

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:42:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=781956) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=781956) WARNING 01-26 16:43:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     def forward(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     raise e
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/tmp/torchinductor_root/nw/cnwig33mlgcqa6ziknfxcunkmni5hxs2vdbr3vskyaf4e4mk25xu.py", line 1093, in call
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 6)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 221, in quant_slide_fp8_triton
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) ERROR 01-26 16:43:17 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered


─── STDERR ───
[2026-01-26 16:42:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:42:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:42:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:42:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:42:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:42:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:42:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:42:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:42:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:42:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:42:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:42:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:42:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:42:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:43:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:43:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:43:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:43:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:43:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:43:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:43:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:43:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:02] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=781956) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=781956) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.41it/s]
(EngineCore_DP0 pid=781956) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]
(EngineCore_DP0 pid=781956) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]
(EngineCore_DP0 pid=781956) 
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=781956) [2026-01-26 16:43:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=781956) [rank0]:W0126 16:43:15.924000 781956 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=781956) [rank0]:W0126 16:43:16.006000 781956 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=781956) [rank0]:W0126 16:43:17.090000 781956 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=781956) [rank0]:W0126 16:43:17.219000 781956 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=781956) Process EngineCore_DP0:
(EngineCore_DP0 pid=781956) Traceback (most recent call last):
(EngineCore_DP0 pid=781956)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=781956)     self.run()
(EngineCore_DP0 pid=781956)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=781956)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=781956)     raise e
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=781956)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=781956)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=781956)     super().__init__(
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=781956)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=781956)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=781956)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=781956)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=781956)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=781956)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=781956)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=781956)     return func(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=781956)     return func(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=781956)     self.model_runner.profile_run()
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=781956)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=781956)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=781956)     return func(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=781956)     outputs = self.model(
(EngineCore_DP0 pid=781956)               ^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=781956)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=781956)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=781956)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=781956)     hidden_states = self.model(
(EngineCore_DP0 pid=781956)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=781956)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=781956)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=781956)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=781956)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=781956)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=781956)     def forward(
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=781956)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=781956)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=781956)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=781956)     raise e
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=781956)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=781956)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=781956)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=781956)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=781956)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=781956)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=781956)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=781956)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=781956)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=781956)     return compiled_fn(full_args)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=781956)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=781956)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=781956)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=781956)                             ^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=781956)     outs = compiled_fn(args)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=781956)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=781956)     return self.current_callable(inputs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=781956)     out = model(new_inputs)
(EngineCore_DP0 pid=781956)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/tmp/torchinductor_root/nw/cnwig33mlgcqa6ziknfxcunkmni5hxs2vdbr3vskyaf4e4mk25xu.py", line 1093, in call
(EngineCore_DP0 pid=781956)     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 6)
(EngineCore_DP0 pid=781956)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=781956)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=781956)     return fn(input, L)
(EngineCore_DP0 pid=781956)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 221, in quant_slide_fp8_triton
(EngineCore_DP0 pid=781956)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=781956)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=781956)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=781956)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=781956)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=781956)     self._init_handles()
(EngineCore_DP0 pid=781956)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=781956)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=781956)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=781956) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 16:43:18.315825035 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,38.9719,19992.5754,3.2844
1024,1024,1,128,128,39.7815,40776.0415,3.2176
2048,1024,2,256,128,74.9855,76860.1752,3.4140
4096,1024,4,512,128,115.5655,118454.6338,4.4304
8192,1024,8,1024,128,122.0537,125105.0780,8.3897
16384,1024,16,2048,128,129.6633,132904.8802,15.7948
32768,1024,32,4096,128,133.2511,136582.3697,30.7390
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-FP8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:43:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=782950) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=782950) WARNING 01-26 16:43:43 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=782950) WARNING 01-26 16:43:53 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.68 requests/s, 19330.87 total tokens/s, 37.68 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:43:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:43:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:43:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:43:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:43:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:43:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:43:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:43:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:43:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:43:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:43:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:43:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:43:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:43:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:43:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:43:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=782950) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=782950) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.21it/s]
(EngineCore_DP0 pid=782950) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.05s/it]
(EngineCore_DP0 pid=782950) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.02s/it]
(EngineCore_DP0 pid=782950) 
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=782950) 2026-01-26 16:43:53,524 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=782950) 2026-01-26 16:43:53,547 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=782950) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.26it/s]
(EngineCore_DP0 pid=782950) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.80it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 509.87it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 262.65it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 309.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:00, 122.75it/s, est. speed input: 62849.13 toks/s, output: 122.75 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:01, 56.74it/s, est. speed input: 31800.17 toks/s, output: 62.11 toks/s]  
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 49.72it/s, est. speed input: 28241.48 toks/s, output: 55.16 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 46.60it/s, est. speed input: 26730.66 toks/s, output: 52.21 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 44.44it/s, est. speed input: 25696.33 toks/s, output: 50.19 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 43.15it/s, est. speed input: 25064.26 toks/s, output: 48.95 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 42.16it/s, est. speed input: 24560.70 toks/s, output: 47.97 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 41.45it/s, est. speed input: 24157.72 toks/s, output: 47.18 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 40.88it/s, est. speed input: 23814.83 toks/s, output: 46.51 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 40.52it/s, est. speed input: 23534.99 toks/s, output: 45.97 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 40.22it/s, est. speed input: 23291.21 toks/s, output: 45.49 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:01, 39.94it/s, est. speed input: 23070.92 toks/s, output: 45.06 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:01, 39.76it/s, est. speed input: 22914.67 toks/s, output: 44.76 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 39.71it/s, est. speed input: 22783.41 toks/s, output: 44.50 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 39.59it/s, est. speed input: 22656.94 toks/s, output: 44.25 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 39.52it/s, est. speed input: 22543.11 toks/s, output: 44.03 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 39.51it/s, est. speed input: 22442.99 toks/s, output: 43.83 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 39.53it/s, est. speed input: 22352.62 toks/s, output: 43.66 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 39.56it/s, est. speed input: 22271.01 toks/s, output: 43.50 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 39.58it/s, est. speed input: 22195.84 toks/s, output: 43.35 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 39.46it/s, est. speed input: 22116.98 toks/s, output: 43.20 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 39.51it/s, est. speed input: 22052.15 toks/s, output: 43.07 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 39.46it/s, est. speed input: 21986.40 toks/s, output: 42.94 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 39.46it/s, est. speed input: 21972.91 toks/s, output: 42.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 42.91it/s, est. speed input: 21972.91 toks/s, output: 42.92 toks/s]
[rank0]:[W126 16:43:59.516333881 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.8s

测试结果:
  Requests/s:   37.68
  Tokens/s:     19330.87
  Total Reqs:   128
  Elapsed:      3.40s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     19293.18

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:44:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=784136) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=784136) WARNING 01-26 16:44:24 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=784136) WARNING 01-26 16:44:34 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.80 requests/s, 40793.30 total tokens/s, 39.80 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:44:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:44:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:44:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:44:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:44:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:44:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:44:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:44:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:44:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:44:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:44:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:44:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:15] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:15] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:15] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:15] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:15] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=784136) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=784136) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.31it/s]
(EngineCore_DP0 pid=784136) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
(EngineCore_DP0 pid=784136) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.06it/s]
(EngineCore_DP0 pid=784136) 
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=784136) 2026-01-26 16:44:34,106 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=784136) 2026-01-26 16:44:34,129 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=784136) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.82it/s]
(EngineCore_DP0 pid=784136) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.20it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 273.27it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 319.45it/s]
Adding requests:  75%|███████▌  | 96/128 [00:00<00:00, 316.95it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 317.42it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:00, 176.40it/s, est. speed input: 180666.61 toks/s, output: 176.42 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 59.33it/s, est. speed input: 67468.40 toks/s, output: 65.89 toks/s]   
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 51.75it/s, est. speed input: 59438.96 toks/s, output: 58.05 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:00<00:01, 48.22it/s, est. speed input: 55879.79 toks/s, output: 54.57 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 46.26it/s, est. speed input: 54007.35 toks/s, output: 52.74 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 44.77it/s, est. speed input: 52581.29 toks/s, output: 51.35 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 43.81it/s, est. speed input: 51638.39 toks/s, output: 50.43 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 42.98it/s, est. speed input: 50824.10 toks/s, output: 49.63 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 42.34it/s, est. speed input: 50134.26 toks/s, output: 48.96 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:01, 41.86it/s, est. speed input: 49541.58 toks/s, output: 48.38 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 41.43it/s, est. speed input: 49005.88 toks/s, output: 47.86 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 41.21it/s, est. speed input: 48558.86 toks/s, output: 47.42 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 41.03it/s, est. speed input: 48158.44 toks/s, output: 47.03 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.93it/s, est. speed input: 47805.73 toks/s, output: 46.69 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.83it/s, est. speed input: 47484.64 toks/s, output: 46.37 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 40.73it/s, est. speed input: 47190.63 toks/s, output: 46.08 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 40.70it/s, est. speed input: 46929.15 toks/s, output: 45.83 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 40.70it/s, est. speed input: 46696.04 toks/s, output: 45.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.70it/s, est. speed input: 46612.95 toks/s, output: 45.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.52it/s, est. speed input: 46612.95 toks/s, output: 45.52 toks/s]
[rank0]:[W126 16:44:38.212888621 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.9s

测试结果:
  Requests/s:   39.80
  Tokens/s:     40793.30
  Total Reqs:   128
  Elapsed:      3.22s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     40753.50

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:44:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=785250) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=785250) WARNING 01-26 16:45:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=785250) WARNING 01-26 16:45:15 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 74.01 requests/s, 75860.26 total tokens/s, 74.01 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 16:44:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:44:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:44:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:44:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:44:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:44:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:44:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:44:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:44:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:44:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:44:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:44:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=785250) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=785250) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.08it/s]
(EngineCore_DP0 pid=785250) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]
(EngineCore_DP0 pid=785250) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.04s/it]
(EngineCore_DP0 pid=785250) 
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=785250) 2026-01-26 16:45:14,979 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=785250) 2026-01-26 16:45:15,001 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=785250) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.68it/s]
(EngineCore_DP0 pid=785250) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.92it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:48,  5.29it/s]
Adding requests:  11%|█▏        | 29/256 [00:00<00:01, 123.48it/s]
Adding requests:  25%|██▌       | 64/256 [00:00<00:00, 209.61it/s]
Adding requests:  38%|███▊      | 96/256 [00:00<00:00, 246.18it/s]
Adding requests:  51%|█████     | 130/256 [00:00<00:00, 275.64it/s]
Adding requests:  64%|██████▍   | 164/256 [00:00<00:00, 295.97it/s]
Adding requests:  78%|███████▊  | 199/256 [00:00<00:00, 311.86it/s]
Adding requests:  91%|█████████▏| 234/256 [00:00<00:00, 321.97it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 265.38it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:00<00:00, 598.14it/s, est. speed input: 612532.23 toks/s, output: 598.15 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:00<00:01, 127.78it/s, est. speed input: 148677.45 toks/s, output: 145.19 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:01<00:00, 109.58it/s, est. speed input: 128835.61 toks/s, output: 125.82 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:00, 101.92it/s, est. speed input: 121175.07 toks/s, output: 118.33 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:01<00:00, 97.12it/s, est. speed input: 116720.09 toks/s, output: 113.98 toks/s] 
Processed prompts:  79%|███████▊  | 201/256 [00:01<00:00, 95.36it/s, est. speed input: 114580.86 toks/s, output: 111.90 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:01<00:00, 90.64it/s, est. speed input: 111556.69 toks/s, output: 108.94 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:02<00:00, 86.89it/s, est. speed input: 109127.99 toks/s, output: 106.57 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:02<00:00, 85.73it/s, est. speed input: 107724.02 toks/s, output: 105.20 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 84.75it/s, est. speed input: 106467.43 toks/s, output: 103.97 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:02<00:00, 84.02it/s, est. speed input: 105347.84 toks/s, output: 102.88 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 84.02it/s, est. speed input: 105142.45 toks/s, output: 102.68 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.67it/s, est. speed input: 105142.45 toks/s, output: 102.68 toks/s]
[rank0]:[W126 16:45:20.491756672 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.1s

测试结果:
  Requests/s:   74.01
  Tokens/s:     75860.26
  Total Reqs:   256
  Elapsed:      3.46s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     75786.25

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:45:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=786395) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=786395) WARNING 01-26 16:45:48 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=786395) WARNING 01-26 16:45:58 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 111.11 requests/s, 113883.59 total tokens/s, 111.11 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 16:45:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:45:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:45:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:45:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:45:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:45:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:45:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:45:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:45:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:45:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:45:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:45:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:45:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:45:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:45:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:45:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=786395) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=786395) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.18it/s]
(EngineCore_DP0 pid=786395) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.02s/it]
(EngineCore_DP0 pid=786395) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=786395) 
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=786395) 2026-01-26 16:45:58,096 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=786395) 2026-01-26 16:45:58,119 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=786395) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.43it/s]
(EngineCore_DP0 pid=786395) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 17.13it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 276.83it/s]
Adding requests:  12%|█▏        | 63/512 [00:00<00:01, 318.33it/s]
Adding requests:  19%|█▊        | 95/512 [00:00<00:01, 315.47it/s]
Adding requests:  25%|██▍       | 127/512 [00:00<00:01, 316.39it/s]
Adding requests:  32%|███▏      | 162/512 [00:00<00:01, 326.21it/s]
Adding requests:  39%|███▊      | 198/512 [00:00<00:00, 337.57it/s]
Adding requests:  46%|████▌     | 234/512 [00:00<00:00, 343.21it/s]
Adding requests:  53%|█████▎    | 269/512 [00:00<00:00, 343.55it/s]
Adding requests:  60%|█████▉    | 305/512 [00:00<00:00, 347.50it/s]
Adding requests:  67%|██████▋   | 343/512 [00:01<00:00, 353.83it/s]
Adding requests:  74%|███████▍  | 380/512 [00:01<00:00, 356.98it/s]
Adding requests:  82%|████████▏ | 419/512 [00:01<00:00, 365.37it/s]
Adding requests:  89%|████████▉ | 456/512 [00:01<00:00, 361.22it/s]
Adding requests:  97%|█████████▋| 496/512 [00:01<00:00, 371.42it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 348.68it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:00<00:00, 1633.62it/s, est. speed input: 1672939.58 toks/s, output: 1633.65 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:01<00:00, 186.39it/s, est. speed input: 220725.13 toks/s, output: 215.55 toks/s]   
Processed prompts:  80%|███████▉  | 408/512 [00:02<00:00, 159.69it/s, est. speed input: 190713.80 toks/s, output: 186.24 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:02<00:00, 146.69it/s, est. speed input: 178122.71 toks/s, output: 173.95 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:02<00:00, 139.72it/s, est. speed input: 171890.34 toks/s, output: 167.86 toks/s]
Processed prompts: 100%|█████████▉| 511/512 [00:03<00:00, 132.42it/s, est. speed input: 166717.75 toks/s, output: 162.81 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 132.42it/s, est. speed input: 167040.54 toks/s, output: 163.13 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 163.12it/s, est. speed input: 167040.54 toks/s, output: 163.13 toks/s]
[rank0]:[W126 16:46:05.276036441 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.8s

测试结果:
  Requests/s:   111.11
  Tokens/s:     113883.59
  Total Reqs:   512
  Elapsed:      4.61s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     113772.49

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:46:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=787629) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=787629) WARNING 01-26 16:46:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=787629) WARNING 01-26 16:46:45 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 120.00 requests/s, 122998.03 total tokens/s, 120.00 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 16:46:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:46:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:46:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:46:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:46:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:46:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:46:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:46:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:46:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:46:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:46:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:46:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:46:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:46:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:46:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:46:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:27] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=787629) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=787629) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=787629) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.00it/s]
(EngineCore_DP0 pid=787629) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
(EngineCore_DP0 pid=787629) 
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=787629) 2026-01-26 16:46:45,786 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=787629) 2026-01-26 16:46:45,854 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=787629) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 12.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 14.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 13.39it/s]
(EngineCore_DP0 pid=787629) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 18.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.43it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 278.93it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:02, 320.48it/s]
Adding requests:   9%|▉         | 96/1024 [00:00<00:02, 316.79it/s]
Adding requests:  13%|█▎        | 129/1024 [00:00<00:02, 321.47it/s]
Adding requests:  16%|█▌        | 165/1024 [00:00<00:02, 331.81it/s]
Adding requests:  20%|█▉        | 202/1024 [00:00<00:02, 342.32it/s]
Adding requests:  23%|██▎       | 238/1024 [00:00<00:02, 347.15it/s]
Adding requests:  27%|██▋       | 273/1024 [00:00<00:02, 345.15it/s]
Adding requests:  30%|███       | 310/1024 [00:00<00:02, 351.54it/s]
Adding requests:  34%|███▍      | 347/1024 [00:01<00:01, 356.41it/s]
Adding requests:  38%|███▊      | 384/1024 [00:01<00:01, 360.15it/s]
Adding requests:  41%|████▏     | 423/1024 [00:01<00:01, 367.17it/s]
Adding requests:  45%|████▍     | 460/1024 [00:01<00:01, 363.26it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 372.21it/s]
Adding requests:  53%|█████▎    | 540/1024 [00:01<00:01, 378.49it/s]
Adding requests:  56%|█████▋    | 578/1024 [00:01<00:01, 374.85it/s]
Adding requests:  60%|██████    | 616/1024 [00:01<00:01, 365.74it/s]
Adding requests:  64%|██████▍   | 653/1024 [00:01<00:01, 358.36it/s]
Adding requests:  67%|██████▋   | 691/1024 [00:01<00:00, 363.56it/s]
Adding requests:  71%|███████   | 728/1024 [00:02<00:00, 356.72it/s]
Adding requests:  75%|███████▍  | 764/1024 [00:02<00:00, 356.85it/s]
Adding requests:  78%|███████▊  | 801/1024 [00:02<00:00, 357.32it/s]
Adding requests:  82%|████████▏ | 839/1024 [00:02<00:00, 363.67it/s]
Adding requests:  86%|████████▌ | 876/1024 [00:02<00:00, 363.09it/s]
Adding requests:  89%|████████▉ | 913/1024 [00:02<00:00, 360.01it/s]
Adding requests:  93%|█████████▎| 950/1024 [00:02<00:00, 356.55it/s]
Adding requests:  96%|█████████▋| 987/1024 [00:02<00:00, 359.80it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 355.58it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 355.18it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:00<00:00, 2196.30it/s, est. speed input: 2249106.41 toks/s, output: 2196.32 toks/s]
Processed prompts:  56%|█████▌    | 574/1024 [00:01<00:01, 246.90it/s, est. speed input: 302505.50 toks/s, output: 295.41 toks/s]   
Processed prompts:  66%|██████▌   | 672/1024 [00:02<00:01, 200.60it/s, est. speed input: 251352.40 toks/s, output: 245.46 toks/s]
Processed prompts:  71%|███████▏  | 731/1024 [00:03<00:01, 177.21it/s, est. speed input: 229154.17 toks/s, output: 223.78 toks/s]
Processed prompts:  75%|███████▌  | 771/1024 [00:03<00:01, 166.55it/s, est. speed input: 219491.27 toks/s, output: 214.35 toks/s]
Processed prompts:  78%|███████▊  | 801/1024 [00:03<00:01, 164.95it/s, est. speed input: 216382.08 toks/s, output: 211.31 toks/s]
Processed prompts:  81%|████████  | 827/1024 [00:04<00:01, 150.87it/s, est. speed input: 208832.14 toks/s, output: 203.94 toks/s]
Processed prompts:  83%|████████▎ | 848/1024 [00:04<00:01, 151.93it/s, est. speed input: 207378.99 toks/s, output: 202.52 toks/s]
Processed prompts:  85%|████████▍ | 867/1024 [00:04<00:01, 139.72it/s, est. speed input: 202465.93 toks/s, output: 197.72 toks/s]
Processed prompts:  86%|████████▌ | 883/1024 [00:04<00:01, 136.44it/s, est. speed input: 200154.19 toks/s, output: 195.46 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:04<00:00, 131.92it/s, est. speed input: 197772.23 toks/s, output: 193.14 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:04<00:00, 129.47it/s, est. speed input: 195721.89 toks/s, output: 191.13 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:04<00:00, 127.20it/s, est. speed input: 193748.18 toks/s, output: 189.21 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:05<00:00, 125.51it/s, est. speed input: 191900.98 toks/s, output: 187.40 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:05<00:00, 124.13it/s, est. speed input: 190139.40 toks/s, output: 185.68 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:05<00:00, 123.08it/s, est. speed input: 188464.52 toks/s, output: 184.05 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:05<00:00, 122.58it/s, est. speed input: 186908.13 toks/s, output: 182.53 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:05<00:00, 122.19it/s, est. speed input: 185422.88 toks/s, output: 181.08 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 122.19it/s, est. speed input: 185616.09 toks/s, output: 181.27 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 181.26it/s, est. speed input: 185616.09 toks/s, output: 181.27 toks/s]
[rank0]:[W126 16:46:56.062125678 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.0s

测试结果:
  Requests/s:   120.00
  Tokens/s:     122998.03
  Total Reqs:   1024
  Elapsed:      8.53s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     122878.03

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:47:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=788983) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=788983) WARNING 01-26 16:47:34 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=788983) WARNING 01-26 16:47:44 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 124.81 requests/s, 127932.04 total tokens/s, 124.81 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 16:47:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:47:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:47:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:47:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:47:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:47:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:47:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:47:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:47:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:47:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:47:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:47:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:47:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:47:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:47:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:47:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=788983) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=788983) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.29it/s]
(EngineCore_DP0 pid=788983) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=788983) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
(EngineCore_DP0 pid=788983) 
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=788983) [rank0]:W0126 16:47:39.578000 788983 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=788983) [rank0]:W0126 16:47:39.659000 788983 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=788983) [rank0]:W0126 16:47:40.604000 788983 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=788983) [rank0]:W0126 16:47:40.732000 788983 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=788983) 2026-01-26 16:47:44,176 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=788983) 2026-01-26 16:47:44,202 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=788983) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 15.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.95it/s]
(EngineCore_DP0 pid=788983) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.07it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 17.64it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 17.43it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 276.11it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 315.33it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 312.00it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:06, 318.88it/s]
Adding requests:   8%|▊         | 163/2048 [00:00<00:05, 328.02it/s]
Adding requests:  10%|▉         | 199/2048 [00:00<00:05, 338.01it/s]
Adding requests:  11%|█▏        | 235/2048 [00:00<00:05, 344.50it/s]
Adding requests:  13%|█▎        | 270/2048 [00:00<00:05, 336.63it/s]
Adding requests:  15%|█▍        | 306/2048 [00:00<00:05, 342.35it/s]
Adding requests:  17%|█▋        | 343/2048 [00:01<00:04, 350.44it/s]
Adding requests:  19%|█▊        | 380/2048 [00:01<00:04, 353.38it/s]
Adding requests:  20%|██        | 419/2048 [00:01<00:04, 361.91it/s]
Adding requests:  22%|██▏       | 456/2048 [00:01<00:04, 357.92it/s]
Adding requests:  24%|██▍       | 496/2048 [00:01<00:04, 368.24it/s]
Adding requests:  26%|██▌       | 535/2048 [00:01<00:04, 372.25it/s]
Adding requests:  28%|██▊       | 573/2048 [00:01<00:03, 370.98it/s]
Adding requests:  30%|██▉       | 611/2048 [00:01<00:04, 356.81it/s]
Adding requests:  32%|███▏      | 647/2048 [00:01<00:04, 349.82it/s]
Adding requests:  33%|███▎      | 683/2048 [00:01<00:03, 352.58it/s]
Adding requests:  35%|███▌      | 719/2048 [00:02<00:03, 353.66it/s]
Adding requests:  37%|███▋      | 755/2048 [00:02<00:03, 350.25it/s]
Adding requests:  39%|███▊      | 791/2048 [00:02<00:03, 352.24it/s]
Adding requests:  40%|████      | 828/2048 [00:02<00:03, 354.11it/s]
Adding requests:  42%|████▏     | 865/2048 [00:02<00:03, 357.85it/s]
Adding requests:  44%|████▍     | 902/2048 [00:02<00:03, 361.01it/s]
Adding requests:  46%|████▌     | 939/2048 [00:02<00:03, 354.82it/s]
Adding requests:  48%|████▊     | 976/2048 [00:02<00:03, 355.90it/s]
Adding requests:  49%|████▉     | 1012/2048 [00:02<00:02, 350.67it/s]
Adding requests:  51%|█████     | 1048/2048 [00:03<00:02, 338.66it/s]
Adding requests:  53%|█████▎    | 1083/2048 [00:03<00:02, 341.40it/s]
Adding requests:  55%|█████▍    | 1118/2048 [00:03<00:02, 341.72it/s]
Adding requests:  56%|█████▋    | 1154/2048 [00:03<00:02, 345.83it/s]
Adding requests:  58%|█████▊    | 1189/2048 [00:03<00:02, 346.23it/s]
Adding requests:  60%|█████▉    | 1227/2048 [00:03<00:02, 354.95it/s]
Adding requests:  62%|██████▏   | 1263/2048 [00:03<00:02, 350.69it/s]
Adding requests:  63%|██████▎   | 1299/2048 [00:03<00:02, 348.17it/s]
Adding requests:  65%|██████▌   | 1335/2048 [00:03<00:02, 349.72it/s]
Adding requests:  67%|██████▋   | 1372/2048 [00:03<00:01, 354.71it/s]
Adding requests:  69%|██████▉   | 1408/2048 [00:04<00:01, 352.82it/s]
Adding requests:  71%|███████   | 1444/2048 [00:04<00:01, 353.64it/s]
Adding requests:  72%|███████▏  | 1480/2048 [00:04<00:01, 354.83it/s]
Adding requests:  74%|███████▍  | 1517/2048 [00:04<00:01, 357.74it/s]
Adding requests:  76%|███████▌  | 1553/2048 [00:04<00:01, 355.27it/s]
Adding requests:  78%|███████▊  | 1589/2048 [00:04<00:01, 350.13it/s]
Adding requests:  79%|███████▉  | 1625/2048 [00:04<00:01, 343.01it/s]
Adding requests:  81%|████████  | 1660/2048 [00:04<00:01, 337.56it/s]
Adding requests:  83%|████████▎ | 1695/2048 [00:04<00:01, 340.58it/s]
Adding requests:  85%|████████▍ | 1732/2048 [00:04<00:00, 348.87it/s]
Adding requests:  86%|████████▋ | 1767/2048 [00:05<00:00, 345.92it/s]
Adding requests:  88%|████████▊ | 1802/2048 [00:05<00:00, 346.03it/s]
Adding requests:  90%|████████▉ | 1837/2048 [00:05<00:00, 346.09it/s]
Adding requests:  91%|█████████▏| 1873/2048 [00:05<00:00, 348.95it/s]
Adding requests:  93%|█████████▎| 1909/2048 [00:05<00:00, 351.21it/s]
Adding requests:  95%|█████████▌| 1947/2048 [00:05<00:00, 358.07it/s]
Adding requests:  97%|█████████▋| 1983/2048 [00:05<00:00, 357.25it/s]
Adding requests:  99%|█████████▊| 2019/2048 [00:05<00:00, 346.17it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 349.13it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 734/2048 [00:00<00:00, 3547.85it/s, est. speed input: 3633133.18 toks/s, output: 3547.90 toks/s]
Processed prompts:  53%|█████▎    | 1089/2048 [00:03<00:03, 293.49it/s, est. speed input: 368974.62 toks/s, output: 360.33 toks/s]  
Processed prompts:  61%|██████    | 1243/2048 [00:04<00:03, 236.91it/s, est. speed input: 305437.34 toks/s, output: 298.28 toks/s]
Processed prompts:  65%|██████▌   | 1333/2048 [00:04<00:03, 206.85it/s, est. speed input: 276901.93 toks/s, output: 270.41 toks/s]
Processed prompts:  68%|██████▊   | 1392/2048 [00:05<00:03, 188.96it/s, est. speed input: 261948.87 toks/s, output: 255.81 toks/s]
Processed prompts:  70%|███████   | 1434/2048 [00:05<00:03, 185.78it/s, est. speed input: 257657.92 toks/s, output: 251.62 toks/s]
Processed prompts:  72%|███████▏  | 1468/2048 [00:05<00:03, 179.24it/s, est. speed input: 252977.28 toks/s, output: 247.05 toks/s]
Processed prompts:  73%|███████▎  | 1495/2048 [00:06<00:03, 166.47it/s, est. speed input: 247070.35 toks/s, output: 241.28 toks/s]
Processed prompts:  74%|███████▍  | 1517/2048 [00:06<00:03, 167.27it/s, est. speed input: 245683.89 toks/s, output: 239.93 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:06<00:03, 147.54it/s, est. speed input: 239639.56 toks/s, output: 234.02 toks/s]
Processed prompts:  76%|███████▌  | 1555/2048 [00:06<00:03, 145.52it/s, est. speed input: 237695.24 toks/s, output: 232.12 toks/s]
Processed prompts:  77%|███████▋  | 1571/2048 [00:06<00:03, 143.07it/s, est. speed input: 235840.55 toks/s, output: 230.31 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:06<00:03, 137.79it/s, est. speed input: 233696.94 toks/s, output: 228.22 toks/s]
Processed prompts:  78%|███████▊  | 1600/2048 [00:07<00:03, 130.97it/s, est. speed input: 231428.48 toks/s, output: 226.00 toks/s]
Processed prompts:  79%|███████▉  | 1614/2048 [00:07<00:03, 127.36it/s, est. speed input: 229519.53 toks/s, output: 224.14 toks/s]
Processed prompts:  80%|███████▉  | 1630/2048 [00:07<00:03, 126.39it/s, est. speed input: 227702.51 toks/s, output: 222.37 toks/s]
Processed prompts:  80%|████████  | 1646/2048 [00:07<00:03, 125.64it/s, est. speed input: 225948.34 toks/s, output: 220.65 toks/s]
Processed prompts:  81%|████████  | 1662/2048 [00:07<00:03, 125.03it/s, est. speed input: 224249.61 toks/s, output: 218.99 toks/s]
Processed prompts:  82%|████████▏ | 1678/2048 [00:07<00:02, 124.89it/s, est. speed input: 222639.30 toks/s, output: 217.42 toks/s]
Processed prompts:  83%|████████▎ | 1694/2048 [00:07<00:02, 125.33it/s, est. speed input: 221136.38 toks/s, output: 215.95 toks/s]
Processed prompts:  83%|████████▎ | 1710/2048 [00:07<00:02, 125.17it/s, est. speed input: 219634.80 toks/s, output: 214.49 toks/s]
Processed prompts:  84%|████████▍ | 1726/2048 [00:08<00:02, 125.30it/s, est. speed input: 218203.54 toks/s, output: 213.09 toks/s]
Processed prompts:  85%|████████▌ | 1742/2048 [00:08<00:02, 127.07it/s, est. speed input: 216967.34 toks/s, output: 211.88 toks/s]
Processed prompts:  86%|████████▌ | 1758/2048 [00:08<00:02, 128.55it/s, est. speed input: 215782.83 toks/s, output: 210.73 toks/s]
Processed prompts:  87%|████████▋ | 1774/2048 [00:08<00:02, 127.25it/s, est. speed input: 214438.40 toks/s, output: 209.41 toks/s]
Processed prompts:  87%|████████▋ | 1790/2048 [00:08<00:02, 128.04it/s, est. speed input: 213271.95 toks/s, output: 208.27 toks/s]
Processed prompts:  88%|████████▊ | 1806/2048 [00:08<00:01, 127.09it/s, est. speed input: 212018.95 toks/s, output: 207.05 toks/s]
Processed prompts:  89%|████████▉ | 1822/2048 [00:08<00:01, 126.16it/s, est. speed input: 210780.01 toks/s, output: 205.84 toks/s]
Processed prompts:  90%|████████▉ | 1838/2048 [00:08<00:01, 125.57it/s, est. speed input: 209580.91 toks/s, output: 204.67 toks/s]
Processed prompts:  91%|█████████ | 1854/2048 [00:09<00:01, 125.18it/s, est. speed input: 208417.48 toks/s, output: 203.53 toks/s]
Processed prompts:  91%|█████████▏| 1870/2048 [00:09<00:01, 125.08it/s, est. speed input: 207299.93 toks/s, output: 202.44 toks/s]
Processed prompts:  92%|█████████▏| 1886/2048 [00:09<00:01, 126.94it/s, est. speed input: 206355.23 toks/s, output: 201.52 toks/s]
Processed prompts:  93%|█████████▎| 1902/2048 [00:09<00:01, 126.91it/s, est. speed input: 205338.15 toks/s, output: 200.53 toks/s]
Processed prompts:  94%|█████████▎| 1918/2048 [00:09<00:01, 125.59it/s, est. speed input: 204255.58 toks/s, output: 199.47 toks/s]
Processed prompts:  94%|█████████▍| 1934/2048 [00:09<00:00, 127.45it/s, est. speed input: 203395.97 toks/s, output: 198.63 toks/s]
Processed prompts:  95%|█████████▌| 1950/2048 [00:09<00:00, 126.37it/s, est. speed input: 202394.45 toks/s, output: 197.65 toks/s]
Processed prompts:  96%|█████████▌| 1966/2048 [00:09<00:00, 125.71it/s, est. speed input: 201424.77 toks/s, output: 196.70 toks/s]
Processed prompts:  97%|█████████▋| 1982/2048 [00:10<00:00, 127.45it/s, est. speed input: 200625.99 toks/s, output: 195.92 toks/s]
Processed prompts:  98%|█████████▊| 1998/2048 [00:10<00:00, 126.44it/s, est. speed input: 199700.65 toks/s, output: 195.02 toks/s]
Processed prompts:  98%|█████████▊| 2014/2048 [00:10<00:00, 126.04it/s, est. speed input: 198818.60 toks/s, output: 194.16 toks/s]
Processed prompts:  99%|█████████▉| 2030/2048 [00:10<00:00, 126.55it/s, est. speed input: 198007.31 toks/s, output: 193.37 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:10<00:00, 126.55it/s, est. speed input: 198948.56 toks/s, output: 194.29 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:10<00:00, 194.28it/s, est. speed input: 198948.56 toks/s, output: 194.29 toks/s]
[rank0]:[W126 16:48:03.251837759 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 66.1s

测试结果:
  Requests/s:   124.81
  Tokens/s:     127932.04
  Total Reqs:   2048
  Elapsed:      16.41s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     127807.23

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:48:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=790596) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=790596) WARNING 01-26 16:48:52 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=790596) WARNING 01-26 16:49:02 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 127.18 requests/s, 130361.69 total tokens/s, 127.18 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 16:48:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:48:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:48:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:48:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:48:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:48:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:48:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:48:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:48:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:48:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:48:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:48:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:48:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:48:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:48:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:48:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:43] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:43] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:43] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:43] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:43] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=790596) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=790596) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.28it/s]
(EngineCore_DP0 pid=790596) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=790596) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
(EngineCore_DP0 pid=790596) 
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=790596) [rank0]:W0126 16:48:57.420000 790596 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=790596) [rank0]:W0126 16:48:57.501000 790596 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=790596) [rank0]:W0126 16:48:58.462000 790596 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=790596) [rank0]:W0126 16:48:58.589000 790596 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=790596) 2026-01-26 16:49:02,127 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=790596) 2026-01-26 16:49:02,154 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=790596) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 15.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 15.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 16.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 17.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 16.00it/s]
(EngineCore_DP0 pid=790596) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.81it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.01it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 17.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 15.61it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 275.53it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 318.39it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 316.02it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:12, 321.81it/s]
Adding requests:   4%|▍         | 165/4096 [00:00<00:11, 331.90it/s]
Adding requests:   5%|▍         | 202/4096 [00:00<00:11, 342.34it/s]
Adding requests:   6%|▌         | 238/4096 [00:00<00:11, 346.96it/s]
Adding requests:   7%|▋         | 273/4096 [00:00<00:11, 344.93it/s]
Adding requests:   8%|▊         | 310/4096 [00:00<00:10, 351.04it/s]
Adding requests:   8%|▊         | 346/4096 [00:01<00:10, 351.52it/s]
Adding requests:   9%|▉         | 383/4096 [00:01<00:10, 356.54it/s]
Adding requests:  10%|█         | 422/4096 [00:01<00:10, 363.57it/s]
Adding requests:  11%|█         | 459/4096 [00:01<00:10, 361.65it/s]
Adding requests:  12%|█▏        | 499/4096 [00:01<00:09, 369.71it/s]
Adding requests:  13%|█▎        | 538/4096 [00:01<00:09, 375.27it/s]
Adding requests:  14%|█▍        | 576/4096 [00:01<00:09, 374.85it/s]
Adding requests:  15%|█▍        | 614/4096 [00:01<00:09, 363.93it/s]
Adding requests:  16%|█▌        | 651/4096 [00:01<00:09, 357.67it/s]
Adding requests:  17%|█▋        | 689/4096 [00:01<00:09, 361.70it/s]
Adding requests:  18%|█▊        | 726/4096 [00:02<00:09, 355.17it/s]
Adding requests:  19%|█▊        | 762/4096 [00:02<00:09, 354.92it/s]
Adding requests:  19%|█▉        | 798/4096 [00:02<00:09, 355.10it/s]
Adding requests:  20%|██        | 836/4096 [00:02<00:09, 361.60it/s]
Adding requests:  21%|██▏       | 874/4096 [00:02<00:08, 364.06it/s]
Adding requests:  22%|██▏       | 911/4096 [00:02<00:08, 363.08it/s]
Adding requests:  23%|██▎       | 948/4096 [00:02<00:08, 357.54it/s]
Adding requests:  24%|██▍       | 984/4096 [00:02<00:08, 353.52it/s]
Adding requests:  25%|██▍       | 1020/4096 [00:02<00:08, 351.86it/s]
Adding requests:  26%|██▌       | 1056/4096 [00:02<00:08, 349.74it/s]
Adding requests:  27%|██▋       | 1092/4096 [00:03<00:08, 350.64it/s]
Adding requests:  28%|██▊       | 1130/4096 [00:03<00:08, 357.59it/s]
Adding requests:  28%|██▊       | 1166/4096 [00:03<00:08, 354.03it/s]
Adding requests:  29%|██▉       | 1202/4096 [00:03<00:08, 354.47it/s]
Adding requests:  30%|███       | 1240/4096 [00:03<00:07, 360.43it/s]
Adding requests:  31%|███       | 1277/4096 [00:03<00:07, 354.93it/s]
Adding requests:  32%|███▏      | 1313/4096 [00:03<00:07, 348.52it/s]
Adding requests:  33%|███▎      | 1348/4096 [00:03<00:07, 347.08it/s]
Adding requests:  34%|███▍      | 1385/4096 [00:03<00:07, 352.06it/s]
Adding requests:  35%|███▍      | 1421/4096 [00:04<00:07, 351.33it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:04<00:07, 353.31it/s]
Adding requests:  36%|███▋      | 1495/4096 [00:04<00:07, 360.10it/s]
Adding requests:  37%|███▋      | 1532/4096 [00:04<00:07, 359.43it/s]
Adding requests:  38%|███▊      | 1568/4096 [00:04<00:07, 353.80it/s]
Adding requests:  39%|███▉      | 1604/4096 [00:04<00:07, 352.62it/s]
Adding requests:  40%|████      | 1640/4096 [00:04<00:07, 345.51it/s]
Adding requests:  41%|████      | 1675/4096 [00:04<00:07, 342.62it/s]
Adding requests:  42%|████▏     | 1712/4096 [00:04<00:06, 350.48it/s]
Adding requests:  43%|████▎     | 1748/4096 [00:04<00:06, 353.09it/s]
Adding requests:  44%|████▎     | 1785/4096 [00:05<00:06, 356.90it/s]
Adding requests:  44%|████▍     | 1821/4096 [00:05<00:06, 352.94it/s]
Adding requests:  45%|████▌     | 1859/4096 [00:05<00:06, 357.83it/s]
Adding requests:  46%|████▋     | 1895/4096 [00:05<00:06, 357.96it/s]
Adding requests:  47%|████▋     | 1933/4096 [00:05<00:05, 363.93it/s]
Adding requests:  48%|████▊     | 1970/4096 [00:05<00:05, 363.78it/s]
Adding requests:  49%|████▉     | 2007/4096 [00:05<00:05, 359.86it/s]
Adding requests:  50%|████▉     | 2044/4096 [00:05<00:05, 354.13it/s]
Adding requests:  51%|█████     | 2080/4096 [00:05<00:05, 345.19it/s]
Adding requests:  52%|█████▏    | 2115/4096 [00:05<00:05, 345.92it/s]
Adding requests:  53%|█████▎    | 2151/4096 [00:06<00:05, 347.76it/s]
Adding requests:  53%|█████▎    | 2186/4096 [00:06<00:05, 341.78it/s]
Adding requests:  54%|█████▍    | 2221/4096 [00:06<00:05, 343.38it/s]
Adding requests:  55%|█████▌    | 2258/4096 [00:06<00:05, 348.42it/s]
Adding requests:  56%|█████▌    | 2295/4096 [00:06<00:05, 354.56it/s]
Adding requests:  57%|█████▋    | 2331/4096 [00:06<00:05, 349.65it/s]
Adding requests:  58%|█████▊    | 2368/4096 [00:06<00:04, 354.33it/s]
Adding requests:  59%|█████▊    | 2406/4096 [00:06<00:04, 361.38it/s]
Adding requests:  60%|█████▉    | 2443/4096 [00:06<00:04, 363.36it/s]
Adding requests:  61%|██████    | 2480/4096 [00:07<00:04, 362.10it/s]
Adding requests:  61%|██████▏   | 2518/4096 [00:07<00:04, 365.66it/s]
Adding requests:  62%|██████▏   | 2558/4096 [00:07<00:04, 373.08it/s]
Adding requests:  63%|██████▎   | 2597/4096 [00:07<00:03, 377.83it/s]
Adding requests:  64%|██████▍   | 2635/4096 [00:07<00:04, 364.79it/s]
Adding requests:  65%|██████▌   | 2672/4096 [00:07<00:03, 359.49it/s]
Adding requests:  66%|██████▌   | 2709/4096 [00:07<00:03, 355.20it/s]
Adding requests:  67%|██████▋   | 2747/4096 [00:07<00:03, 360.48it/s]
Adding requests:  68%|██████▊   | 2785/4096 [00:07<00:03, 365.19it/s]
Adding requests:  69%|██████▉   | 2823/4096 [00:07<00:03, 367.43it/s]
Adding requests:  70%|██████▉   | 2860/4096 [00:08<00:03, 367.62it/s]
Adding requests:  71%|███████   | 2897/4096 [00:08<00:03, 364.93it/s]
Adding requests:  72%|███████▏  | 2935/4096 [00:08<00:03, 367.32it/s]
Adding requests:  73%|███████▎  | 2972/4096 [00:08<00:03, 367.25it/s]
Adding requests:  73%|███████▎  | 3010/4096 [00:08<00:02, 369.54it/s]
Adding requests:  74%|███████▍  | 3048/4096 [00:08<00:02, 369.67it/s]
Adding requests:  75%|███████▌  | 3087/4096 [00:08<00:02, 373.95it/s]
Adding requests:  76%|███████▋  | 3125/4096 [00:08<00:02, 375.20it/s]
Adding requests:  77%|███████▋  | 3163/4096 [00:08<00:02, 370.83it/s]
Adding requests:  78%|███████▊  | 3201/4096 [00:08<00:02, 363.42it/s]
Adding requests:  79%|███████▉  | 3239/4096 [00:09<00:02, 368.19it/s]
Adding requests:  80%|███████▉  | 3276/4096 [00:09<00:02, 362.30it/s]
Adding requests:  81%|████████  | 3313/4096 [00:09<00:02, 353.96it/s]
Adding requests:  82%|████████▏ | 3349/4096 [00:09<00:02, 355.59it/s]
Adding requests:  83%|████████▎ | 3387/4096 [00:09<00:01, 361.25it/s]
Adding requests:  84%|████████▎ | 3424/4096 [00:09<00:01, 350.67it/s]
Adding requests:  84%|████████▍ | 3461/4096 [00:09<00:01, 355.25it/s]
Adding requests:  85%|████████▌ | 3497/4096 [00:09<00:01, 354.41it/s]
Adding requests:  86%|████████▋ | 3537/4096 [00:09<00:01, 367.36it/s]
Adding requests:  87%|████████▋ | 3574/4096 [00:10<00:01, 365.34it/s]
Adding requests:  88%|████████▊ | 3612/4096 [00:10<00:01, 367.06it/s]
Adding requests:  89%|████████▉ | 3649/4096 [00:10<00:01, 367.64it/s]
Adding requests:  90%|████████▉ | 3686/4096 [00:10<00:01, 357.60it/s]
Adding requests:  91%|█████████ | 3724/4096 [00:10<00:01, 361.02it/s]
Adding requests:  92%|█████████▏| 3761/4096 [00:10<00:00, 353.51it/s]
Adding requests:  93%|█████████▎| 3797/4096 [00:10<00:00, 343.81it/s]
Adding requests:  94%|█████████▎| 3832/4096 [00:10<00:00, 342.48it/s]
Adding requests:  94%|█████████▍| 3869/4096 [00:10<00:00, 349.14it/s]
Adding requests:  95%|█████████▌| 3904/4096 [00:10<00:00, 345.87it/s]
Adding requests:  96%|█████████▌| 3939/4096 [00:11<00:00, 344.29it/s]
Adding requests:  97%|█████████▋| 3974/4096 [00:11<00:00, 344.75it/s]
Adding requests:  98%|█████████▊| 4010/4096 [00:11<00:00, 348.08it/s]
Adding requests:  99%|█████████▉| 4045/4096 [00:11<00:00, 346.27it/s]
Adding requests: 100%|█████████▉| 4080/4096 [00:11<00:00, 346.99it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 355.92it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▌      | 1454/4096 [00:00<00:00, 5995.85it/s, est. speed input: 6140005.02 toks/s, output: 5995.92 toks/s]
Processed prompts:  50%|█████     | 2054/4096 [00:04<00:05, 345.27it/s, est. speed input: 442018.22 toks/s, output: 431.66 toks/s]   
Processed prompts:  56%|█████▋    | 2309/4096 [00:06<00:06, 262.36it/s, est. speed input: 349413.86 toks/s, output: 341.22 toks/s]
Processed prompts:  60%|█████▉    | 2454/4096 [00:08<00:07, 225.64it/s, est. speed input: 313650.58 toks/s, output: 306.30 toks/s]
Processed prompts:  62%|██████▏   | 2546/4096 [00:08<00:07, 207.36it/s, est. speed input: 297587.23 toks/s, output: 290.61 toks/s]
Processed prompts:  64%|██████▎   | 2610/4096 [00:09<00:07, 196.02it/s, est. speed input: 288690.81 toks/s, output: 281.92 toks/s]
Processed prompts:  65%|██████▍   | 2657/4096 [00:09<00:07, 195.30it/s, est. speed input: 286176.26 toks/s, output: 279.47 toks/s]
Processed prompts:  66%|██████▌   | 2695/4096 [00:09<00:07, 190.24it/s, est. speed input: 282851.34 toks/s, output: 276.22 toks/s]
Processed prompts:  67%|██████▋   | 2726/4096 [00:10<00:07, 180.39it/s, est. speed input: 278884.40 toks/s, output: 272.35 toks/s]
Processed prompts:  67%|██████▋   | 2751/4096 [00:10<00:08, 166.50it/s, est. speed input: 274551.07 toks/s, output: 268.12 toks/s]
Processed prompts:  68%|██████▊   | 2772/4096 [00:10<00:08, 150.24it/s, est. speed input: 270050.30 toks/s, output: 263.72 toks/s]
Processed prompts:  68%|██████▊   | 2798/4096 [00:10<00:09, 139.85it/s, est. speed input: 266195.83 toks/s, output: 259.96 toks/s]
Processed prompts:  69%|██████▉   | 2830/4096 [00:11<00:09, 137.04it/s, est. speed input: 263130.59 toks/s, output: 256.96 toks/s]
Processed prompts:  70%|██████▉   | 2862/4096 [00:11<00:09, 133.99it/s, est. speed input: 260070.56 toks/s, output: 253.97 toks/s]
Processed prompts:  71%|███████   | 2894/4096 [00:11<00:08, 134.44it/s, est. speed input: 257590.12 toks/s, output: 251.55 toks/s]
Processed prompts:  71%|███████▏  | 2926/4096 [00:11<00:08, 132.84it/s, est. speed input: 254927.51 toks/s, output: 248.95 toks/s]
Processed prompts:  72%|███████▏  | 2958/4096 [00:12<00:08, 131.67it/s, est. speed input: 252376.56 toks/s, output: 246.46 toks/s]
Processed prompts:  73%|███████▎  | 2990/4096 [00:12<00:08, 130.66it/s, est. speed input: 249908.12 toks/s, output: 244.05 toks/s]
Processed prompts:  74%|███████▍  | 3022/4096 [00:12<00:08, 129.37it/s, est. speed input: 247464.28 toks/s, output: 241.66 toks/s]
Processed prompts:  75%|███████▍  | 3054/4096 [00:12<00:08, 127.81it/s, est. speed input: 245035.39 toks/s, output: 239.29 toks/s]
Processed prompts:  75%|███████▌  | 3086/4096 [00:13<00:07, 127.87it/s, est. speed input: 242846.67 toks/s, output: 237.15 toks/s]
Processed prompts:  76%|███████▌  | 3118/4096 [00:13<00:07, 127.32it/s, est. speed input: 240668.09 toks/s, output: 235.03 toks/s]
Processed prompts:  77%|███████▋  | 3150/4096 [00:13<00:07, 128.15it/s, est. speed input: 238714.69 toks/s, output: 233.12 toks/s]
Processed prompts:  78%|███████▊  | 3182/4096 [00:13<00:07, 127.87it/s, est. speed input: 236732.52 toks/s, output: 231.18 toks/s]
Processed prompts:  78%|███████▊  | 3214/4096 [00:14<00:06, 127.49it/s, est. speed input: 234801.77 toks/s, output: 229.30 toks/s]
Processed prompts:  79%|███████▉  | 3246/4096 [00:14<00:06, 127.02it/s, est. speed input: 232917.23 toks/s, output: 227.46 toks/s]
Processed prompts:  80%|████████  | 3278/4096 [00:14<00:06, 126.65it/s, est. speed input: 231093.28 toks/s, output: 225.68 toks/s]
Processed prompts:  81%|████████  | 3310/4096 [00:14<00:06, 126.62it/s, est. speed input: 229356.97 toks/s, output: 223.98 toks/s]
Processed prompts:  82%|████████▏ | 3342/4096 [00:15<00:05, 126.30it/s, est. speed input: 227648.54 toks/s, output: 222.31 toks/s]
Processed prompts:  82%|████████▏ | 3374/4096 [00:15<00:05, 125.76it/s, est. speed input: 225965.35 toks/s, output: 220.67 toks/s]
Processed prompts:  83%|████████▎ | 3406/4096 [00:15<00:05, 126.56it/s, est. speed input: 224451.61 toks/s, output: 219.19 toks/s]
Processed prompts:  84%|████████▍ | 3438/4096 [00:15<00:05, 126.81it/s, est. speed input: 222956.49 toks/s, output: 217.73 toks/s]
Processed prompts:  85%|████████▍ | 3470/4096 [00:16<00:04, 126.65it/s, est. speed input: 221476.98 toks/s, output: 216.29 toks/s]
Processed prompts:  85%|████████▌ | 3502/4096 [00:16<00:04, 126.73it/s, est. speed input: 220061.15 toks/s, output: 214.90 toks/s]
Processed prompts:  86%|████████▋ | 3534/4096 [00:16<00:04, 125.79it/s, est. speed input: 218600.10 toks/s, output: 213.48 toks/s]
Processed prompts:  87%|████████▋ | 3566/4096 [00:16<00:04, 127.53it/s, est. speed input: 217391.24 toks/s, output: 212.30 toks/s]
Processed prompts:  88%|████████▊ | 3598/4096 [00:17<00:03, 127.06it/s, est. speed input: 216074.33 toks/s, output: 211.01 toks/s]
Processed prompts:  89%|████████▊ | 3630/4096 [00:17<00:03, 127.47it/s, est. speed input: 214856.52 toks/s, output: 209.82 toks/s]
Processed prompts:  89%|████████▉ | 3662/4096 [00:17<00:03, 127.73it/s, est. speed input: 213671.31 toks/s, output: 208.66 toks/s]
Processed prompts:  90%|█████████ | 3694/4096 [00:17<00:03, 128.33it/s, est. speed input: 212551.08 toks/s, output: 207.57 toks/s]
Processed prompts:  91%|█████████ | 3726/4096 [00:18<00:02, 128.25it/s, est. speed input: 211424.35 toks/s, output: 206.47 toks/s]
Processed prompts:  92%|█████████▏| 3758/4096 [00:18<00:02, 127.79it/s, est. speed input: 210297.45 toks/s, output: 205.37 toks/s]
Processed prompts:  93%|█████████▎| 3790/4096 [00:18<00:02, 126.95it/s, est. speed input: 209162.45 toks/s, output: 204.26 toks/s]
Processed prompts:  93%|█████████▎| 3822/4096 [00:18<00:02, 126.14it/s, est. speed input: 208041.92 toks/s, output: 203.17 toks/s]
Processed prompts:  94%|█████████▍| 3854/4096 [00:19<00:01, 125.72it/s, est. speed input: 206962.04 toks/s, output: 202.11 toks/s]
Processed prompts:  95%|█████████▍| 3886/4096 [00:19<00:01, 124.97it/s, est. speed input: 205877.11 toks/s, output: 201.05 toks/s]
Processed prompts:  96%|█████████▌| 3918/4096 [00:19<00:01, 128.53it/s, est. speed input: 205106.39 toks/s, output: 200.30 toks/s]
Processed prompts:  96%|█████████▋| 3950/4096 [00:19<00:01, 129.00it/s, est. speed input: 204213.60 toks/s, output: 199.43 toks/s]
Processed prompts:  97%|█████████▋| 3982/4096 [00:20<00:00, 128.97it/s, est. speed input: 203319.51 toks/s, output: 198.55 toks/s]
Processed prompts:  98%|█████████▊| 4014/4096 [00:20<00:00, 128.03it/s, est. speed input: 202388.42 toks/s, output: 197.64 toks/s]
Processed prompts:  99%|█████████▉| 4046/4096 [00:20<00:00, 130.57it/s, est. speed input: 201680.69 toks/s, output: 196.95 toks/s]
Processed prompts: 100%|█████████▉| 4078/4096 [00:20<00:00, 147.34it/s, est. speed input: 201781.90 toks/s, output: 197.05 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:20<00:00, 147.34it/s, est. speed input: 202670.05 toks/s, output: 197.92 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:20<00:00, 197.92it/s, est. speed input: 202670.05 toks/s, output: 197.92 toks/s]
[rank0]:[W126 16:49:37.694390783 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 94.6s

测试结果:
  Requests/s:   127.18
  Tokens/s:     130361.69
  Total Reqs:   4096
  Elapsed:      32.21s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     130234.51

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:50:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=792769) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=792769) WARNING 01-26 16:50:51 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     def forward(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     raise e
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/tmp/torchinductor_root/m5/cm5haedjxfph5usng6bbkwlykhb54lapizg4zoxkgvftcobafk7o.py", line 1093, in call
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 8)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 221, in quant_slide_fp8_triton
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered


─── STDERR ───
[2026-01-26 16:50:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:50:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:50:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:50:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:50:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:50:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:50:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:50:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:50:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:50:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:50:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:50:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:50:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:50:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:50:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:50:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:42] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:42] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:42] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:42] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:42] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=792769) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=792769) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.28it/s]
(EngineCore_DP0 pid=792769) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=792769) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
(EngineCore_DP0 pid=792769) 
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=792769) [rank0]:W0126 16:50:56.209000 792769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=792769) [rank0]:W0126 16:50:56.290000 792769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=792769) [rank0]:W0126 16:50:57.356000 792769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=792769) [rank0]:W0126 16:50:57.479000 792769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=792769) Process EngineCore_DP0:
(EngineCore_DP0 pid=792769) Traceback (most recent call last):
(EngineCore_DP0 pid=792769)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=792769)     self.run()
(EngineCore_DP0 pid=792769)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=792769)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=792769)     raise e
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=792769)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=792769)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=792769)     super().__init__(
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=792769)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=792769)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=792769)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=792769)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=792769)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=792769)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=792769)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=792769)     return func(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=792769)     return func(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=792769)     self.model_runner.profile_run()
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=792769)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=792769)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=792769)     return func(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=792769)     outputs = self.model(
(EngineCore_DP0 pid=792769)               ^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=792769)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=792769)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=792769)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=792769)     hidden_states = self.model(
(EngineCore_DP0 pid=792769)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=792769)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=792769)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=792769)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=792769)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=792769)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=792769)     def forward(
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=792769)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=792769)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=792769)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=792769)     raise e
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=792769)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=792769)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=792769)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=792769)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=792769)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=792769)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=792769)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=792769)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=792769)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=792769)     return compiled_fn(full_args)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=792769)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=792769)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=792769)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=792769)                             ^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=792769)     outs = compiled_fn(args)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=792769)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=792769)     return self.current_callable(inputs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=792769)     out = model(new_inputs)
(EngineCore_DP0 pid=792769)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/tmp/torchinductor_root/m5/cm5haedjxfph5usng6bbkwlykhb54lapizg4zoxkgvftcobafk7o.py", line 1093, in call
(EngineCore_DP0 pid=792769)     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 8)
(EngineCore_DP0 pid=792769)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=792769)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=792769)     return fn(input, L)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 221, in quant_slide_fp8_triton
(EngineCore_DP0 pid=792769)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=792769)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=792769)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=792769)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=792769)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=792769)     self._init_handles()
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=792769)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=792769)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 16:50:58.576229254 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,37.6820,19330.8666,3.3968
1024,1024,1,128,128,39.7983,40793.3016,3.2162
2048,1024,2,256,128,74.0100,75860.2609,3.4590
4096,1024,4,512,128,111.1059,113883.5919,4.6082
8192,1024,8,1024,128,119.9981,122998.0325,8.5335
16384,1024,16,2048,128,124.8118,127932.0445,16.4087
32768,1024,32,4096,128,127.1821,130361.6931,32.2058
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-FP8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:51:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=793771) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=793771) WARNING 01-26 16:51:24 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=793771) WARNING 01-26 16:51:34 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.72 requests/s, 19349.99 total tokens/s, 37.72 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:51:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:51:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:51:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:51:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:51:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:51:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:51:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:51:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:51:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:51:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:51:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:51:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:15] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:15] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:15] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:15] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:15] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=793771) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=793771) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.01s/it]
(EngineCore_DP0 pid=793771) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.19s/it]
(EngineCore_DP0 pid=793771) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.16s/it]
(EngineCore_DP0 pid=793771) 
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=793771) 2026-01-26 16:51:34,207 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=793771) 2026-01-26 16:51:34,231 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=793771) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.16it/s]
(EngineCore_DP0 pid=793771) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.06it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████▏     | 53/128 [00:00<00:00, 521.42it/s]
Adding requests:  88%|████████▊ | 113/128 [00:00<00:00, 566.71it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 565.28it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:02, 58.76it/s, est. speed input: 30088.18 toks/s, output: 58.76 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:02, 46.35it/s, est. speed input: 24571.64 toks/s, output: 47.99 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 43.53it/s, est. speed input: 23248.87 toks/s, output: 45.41 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 41.98it/s, est. speed input: 22516.17 toks/s, output: 43.98 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 41.17it/s, est. speed input: 22087.99 toks/s, output: 43.14 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 40.69it/s, est. speed input: 21806.69 toks/s, output: 42.59 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:02, 40.35it/s, est. speed input: 21595.40 toks/s, output: 42.18 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 40.24it/s, est. speed input: 21458.04 toks/s, output: 41.91 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:01, 40.04it/s, est. speed input: 21329.42 toks/s, output: 41.66 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 39.81it/s, est. speed input: 21208.29 toks/s, output: 41.42 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 39.72it/s, est. speed input: 21135.04 toks/s, output: 41.28 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 39.69it/s, est. speed input: 21076.73 toks/s, output: 41.17 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 39.66it/s, est. speed input: 21025.53 toks/s, output: 41.07 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 39.61it/s, est. speed input: 20976.17 toks/s, output: 40.97 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 39.66it/s, est. speed input: 20942.68 toks/s, output: 40.90 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 39.71it/s, est. speed input: 20912.71 toks/s, output: 40.84 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 39.74it/s, est. speed input: 20886.51 toks/s, output: 40.79 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 39.81it/s, est. speed input: 20866.37 toks/s, output: 40.75 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:00, 39.86it/s, est. speed input: 20848.38 toks/s, output: 40.72 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 39.81it/s, est. speed input: 20825.22 toks/s, output: 40.67 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 39.78it/s, est. speed input: 20804.52 toks/s, output: 40.63 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 39.80it/s, est. speed input: 20787.78 toks/s, output: 40.60 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 39.78it/s, est. speed input: 20770.91 toks/s, output: 40.57 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 39.81it/s, est. speed input: 20757.63 toks/s, output: 40.54 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 39.82it/s, est. speed input: 20744.81 toks/s, output: 40.52 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 39.85it/s, est. speed input: 20734.14 toks/s, output: 40.50 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 39.85it/s, est. speed input: 20722.86 toks/s, output: 40.47 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 39.86it/s, est. speed input: 20712.92 toks/s, output: 40.45 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.86it/s, est. speed input: 20701.34 toks/s, output: 40.43 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.43it/s, est. speed input: 20701.34 toks/s, output: 40.43 toks/s]
[rank0]:[W126 16:51:39.200110755 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.5s

测试结果:
  Requests/s:   37.72
  Tokens/s:     19349.99
  Total Reqs:   128
  Elapsed:      3.39s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     19312.27

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:51:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=794964) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=794964) WARNING 01-26 16:52:06 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=794964) WARNING 01-26 16:52:15 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.75 requests/s, 40740.68 total tokens/s, 39.75 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:51:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:51:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:51:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:51:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:51:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:51:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:51:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:51:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:51:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:51:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:51:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:51:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=794964) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=794964) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.05it/s]
(EngineCore_DP0 pid=794964) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.16s/it]
(EngineCore_DP0 pid=794964) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.13s/it]
(EngineCore_DP0 pid=794964) 
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=794964) 2026-01-26 16:52:15,619 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=794964) 2026-01-26 16:52:15,642 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=794964) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.96it/s]
(EngineCore_DP0 pid=794964) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.37it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 274.93it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 318.75it/s]
Adding requests:  75%|███████▌  | 96/128 [00:00<00:00, 316.73it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 180.19it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 212.18it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:00, 224.57it/s, est. speed input: 229969.81 toks/s, output: 224.57 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 64.29it/s, est. speed input: 74445.62 toks/s, output: 72.70 toks/s]   
Processed prompts:  48%|████▊     | 62/128 [00:00<00:01, 54.99it/s, est. speed input: 64574.64 toks/s, output: 63.06 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 50.70it/s, est. speed input: 60322.49 toks/s, output: 58.91 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 48.24it/s, est. speed input: 58004.47 toks/s, output: 56.64 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:00, 46.48it/s, est. speed input: 56419.98 toks/s, output: 55.10 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 45.00it/s, est. speed input: 55103.28 toks/s, output: 53.81 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 43.77it/s, est. speed input: 54093.08 toks/s, output: 52.83 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 42.94it/s, est. speed input: 53273.65 toks/s, output: 52.02 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 42.18it/s, est. speed input: 52526.50 toks/s, output: 51.30 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 41.78it/s, est. speed input: 51912.20 toks/s, output: 50.70 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 41.38it/s, est. speed input: 51338.95 toks/s, output: 50.14 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 41.19it/s, est. speed input: 50848.92 toks/s, output: 49.66 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 40.73it/s, est. speed input: 50339.49 toks/s, output: 49.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.73it/s, est. speed input: 50102.96 toks/s, output: 48.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.93it/s, est. speed input: 50102.96 toks/s, output: 48.93 toks/s]
[rank0]:[W126 16:52:20.850837220 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.3s

测试结果:
  Requests/s:   39.75
  Tokens/s:     40740.68
  Total Reqs:   128
  Elapsed:      3.22s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     40700.93

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:52:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=796089) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=796089) WARNING 01-26 16:52:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=796089) WARNING 01-26 16:52:56 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 76.51 requests/s, 78427.49 total tokens/s, 76.51 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 16:52:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:52:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:52:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:52:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:52:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:52:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:52:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:52:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:52:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:52:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:52:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:52:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:52:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:52:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:52:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:52:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=796089) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=796089) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.04it/s]
(EngineCore_DP0 pid=796089) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.13s/it]
(EngineCore_DP0 pid=796089) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]
(EngineCore_DP0 pid=796089) 
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=796089) 2026-01-26 16:52:56,791 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=796089) 2026-01-26 16:52:56,814 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=796089) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.60it/s]
(EngineCore_DP0 pid=796089) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.71it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.69it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█         | 27/256 [00:00<00:00, 265.37it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 312.77it/s]
Adding requests:  37%|███▋      | 94/256 [00:00<00:01, 136.69it/s]
Adding requests:  49%|████▉     | 126/256 [00:00<00:00, 175.93it/s]
Adding requests:  60%|█████▉    | 153/256 [00:00<00:00, 197.69it/s]
Adding requests:  73%|███████▎  | 188/256 [00:00<00:00, 235.29it/s]
Adding requests:  87%|████████▋ | 223/256 [00:00<00:00, 265.08it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 282.44it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 233.01it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:00<00:00, 858.63it/s, est. speed input: 879286.58 toks/s, output: 858.64 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:00, 127.61it/s, est. speed input: 149801.28 toks/s, output: 146.29 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:01<00:00, 108.19it/s, est. speed input: 128406.52 toks/s, output: 125.40 toks/s]
Processed prompts:  93%|█████████▎| 239/256 [00:02<00:00, 100.97it/s, est. speed input: 121009.77 toks/s, output: 118.17 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 100.97it/s, est. speed input: 116716.52 toks/s, output: 113.98 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 113.98it/s, est. speed input: 116716.52 toks/s, output: 113.98 toks/s]
[rank0]:[W126 16:53:02.254826003 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.5s

测试结果:
  Requests/s:   76.51
  Tokens/s:     78427.49
  Total Reqs:   256
  Elapsed:      3.35s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     78350.97

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:53:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=797223) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=797223) WARNING 01-26 16:53:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=797223) WARNING 01-26 16:53:39 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 105.53 requests/s, 108168.95 total tokens/s, 105.53 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 16:53:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:53:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:53:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:53:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:53:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:53:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:53:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:53:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:53:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:53:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:53:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:53:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:53:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:53:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:53:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:53:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:21] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:21] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:21] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:21] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:21] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=797223) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=797223) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.04it/s]
(EngineCore_DP0 pid=797223) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]
(EngineCore_DP0 pid=797223) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.08s/it]
(EngineCore_DP0 pid=797223) 
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=797223) 2026-01-26 16:53:39,841 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=797223) 2026-01-26 16:53:39,864 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=797223) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.63it/s]
(EngineCore_DP0 pid=797223) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 18.11it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▍         | 25/512 [00:00<00:02, 241.97it/s]
Adding requests:  11%|█▏        | 58/512 [00:00<00:01, 291.24it/s]
Adding requests:  18%|█▊        | 91/512 [00:00<00:01, 304.21it/s]
Adding requests:  24%|██▍       | 125/512 [00:00<00:01, 315.62it/s]
Adding requests:  31%|███       | 157/512 [00:00<00:01, 316.74it/s]
Adding requests:  38%|███▊      | 194/512 [00:00<00:00, 332.37it/s]
Adding requests:  45%|████▍     | 230/512 [00:00<00:00, 339.09it/s]
Adding requests:  52%|█████▏    | 264/512 [00:00<00:00, 336.36it/s]
Adding requests:  59%|█████▊    | 300/512 [00:00<00:00, 342.58it/s]
Adding requests:  66%|██████▌   | 337/512 [00:01<00:00, 350.21it/s]
Adding requests:  73%|███████▎  | 373/512 [00:01<00:00, 353.08it/s]
Adding requests:  80%|████████  | 410/512 [00:01<00:00, 357.61it/s]
Adding requests:  87%|████████▋ | 446/512 [00:01<00:00, 354.61it/s]
Adding requests:  95%|█████████▍| 485/512 [00:01<00:00, 363.83it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 342.29it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:00<00:00, 1515.97it/s, est. speed input: 1552456.70 toks/s, output: 1515.99 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:01<00:01, 180.39it/s, est. speed input: 213890.55 toks/s, output: 208.88 toks/s]   
Processed prompts:  75%|███████▍  | 383/512 [00:02<00:00, 153.89it/s, est. speed input: 184214.66 toks/s, output: 179.90 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:02<00:00, 141.56it/s, est. speed input: 172167.84 toks/s, output: 168.13 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [00:02<00:00, 137.11it/s, est. speed input: 167482.59 toks/s, output: 163.56 toks/s]
Processed prompts:  94%|█████████▍| 480/512 [00:03<00:00, 131.71it/s, est. speed input: 163350.52 toks/s, output: 159.52 toks/s]
Processed prompts:  98%|█████████▊| 500/512 [00:03<00:00, 127.49it/s, est. speed input: 160357.59 toks/s, output: 156.60 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 127.49it/s, est. speed input: 156285.67 toks/s, output: 152.62 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 152.62it/s, est. speed input: 156285.67 toks/s, output: 152.62 toks/s]
[rank0]:[W126 16:53:46.158383990 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.9s

测试结果:
  Requests/s:   105.53
  Tokens/s:     108168.95
  Total Reqs:   512
  Elapsed:      4.85s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     108063.41

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:54:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=798476) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=798476) WARNING 01-26 16:54:18 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=798476) WARNING 01-26 16:54:28 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 114.82 requests/s, 117687.27 total tokens/s, 114.82 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 16:54:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:54:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:54:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:54:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:54:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:54:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:54:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:54:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:54:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:54:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:54:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:54:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:54:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:54:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:54:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:54:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:09] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:09] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:09] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:09] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=798476) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=798476) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.05it/s]
(EngineCore_DP0 pid=798476) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.09s/it]
(EngineCore_DP0 pid=798476) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.07s/it]
(EngineCore_DP0 pid=798476) 
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=798476) 2026-01-26 16:54:28,240 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=798476) 2026-01-26 16:54:28,264 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=798476) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 16.01it/s]
(EngineCore_DP0 pid=798476) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 18.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.44it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 278.84it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:02, 320.52it/s]
Adding requests:   9%|▉         | 96/1024 [00:00<00:02, 317.37it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 322.44it/s]
Adding requests:  16%|█▌        | 166/1024 [00:00<00:02, 332.02it/s]
Adding requests:  20%|█▉        | 203/1024 [00:00<00:02, 343.05it/s]
Adding requests:  23%|██▎       | 239/1024 [00:00<00:02, 347.80it/s]
Adding requests:  27%|██▋       | 274/1024 [00:00<00:02, 347.29it/s]
Adding requests:  30%|███       | 312/1024 [00:00<00:02, 354.93it/s]
Adding requests:  34%|███▍      | 349/1024 [00:01<00:01, 358.63it/s]
Adding requests:  38%|███▊      | 386/1024 [00:01<00:01, 361.91it/s]
Adding requests:  41%|████▏     | 424/1024 [00:01<00:01, 366.55it/s]
Adding requests:  45%|████▌     | 461/1024 [00:01<00:01, 362.92it/s]
Adding requests:  49%|████▉     | 501/1024 [00:01<00:01, 372.08it/s]
Adding requests:  53%|█████▎    | 541/1024 [00:01<00:01, 376.84it/s]
Adding requests:  57%|█████▋    | 579/1024 [00:01<00:01, 372.96it/s]
Adding requests:  60%|██████    | 617/1024 [00:01<00:01, 363.58it/s]
Adding requests:  64%|██████▍   | 654/1024 [00:01<00:01, 357.04it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:01<00:00, 363.37it/s]
Adding requests:  71%|███████   | 729/1024 [00:02<00:00, 354.29it/s]
Adding requests:  75%|███████▍  | 765/1024 [00:02<00:00, 354.79it/s]
Adding requests:  78%|███████▊  | 801/1024 [00:02<00:00, 355.83it/s]
Adding requests:  82%|████████▏ | 839/1024 [00:02<00:00, 361.85it/s]
Adding requests:  86%|████████▌ | 876/1024 [00:02<00:00, 361.88it/s]
Adding requests:  89%|████████▉ | 913/1024 [00:02<00:00, 362.21it/s]
Adding requests:  93%|█████████▎| 950/1024 [00:02<00:00, 357.20it/s]
Adding requests:  96%|█████████▋| 987/1024 [00:02<00:00, 360.08it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 353.76it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 354.86it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:00<00:00, 2257.11it/s, est. speed input: 2311386.97 toks/s, output: 2257.13 toks/s]
Processed prompts:  55%|█████▌    | 564/1024 [00:02<00:02, 226.76it/s, est. speed input: 276996.12 toks/s, output: 270.50 toks/s]   
Processed prompts:  65%|██████▍   | 664/1024 [00:02<00:01, 187.73it/s, est. speed input: 233293.46 toks/s, output: 227.83 toks/s]
Processed prompts:  71%|███████   | 724/1024 [00:03<00:01, 167.10it/s, est. speed input: 213732.15 toks/s, output: 208.72 toks/s]
Processed prompts:  75%|███████▍  | 765/1024 [00:03<00:01, 158.03it/s, est. speed input: 205396.30 toks/s, output: 200.58 toks/s]
Processed prompts:  78%|███████▊  | 796/1024 [00:04<00:01, 150.17it/s, est. speed input: 199404.58 toks/s, output: 194.73 toks/s]
Processed prompts:  80%|████████  | 820/1024 [00:04<00:01, 144.81it/s, est. speed input: 195550.59 toks/s, output: 190.97 toks/s]
Processed prompts:  82%|████████▏ | 840/1024 [00:04<00:01, 144.77it/s, est. speed input: 194064.41 toks/s, output: 189.52 toks/s]
Processed prompts:  84%|████████▍ | 859/1024 [00:04<00:01, 132.97it/s, est. speed input: 189560.92 toks/s, output: 185.12 toks/s]
Processed prompts:  85%|████████▌ | 875/1024 [00:04<00:01, 130.01it/s, est. speed input: 187511.59 toks/s, output: 183.12 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:04<00:01, 125.92it/s, est. speed input: 185401.42 toks/s, output: 181.06 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:05<00:00, 123.76it/s, est. speed input: 183589.46 toks/s, output: 179.29 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:05<00:00, 121.03it/s, est. speed input: 181723.34 toks/s, output: 177.46 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:05<00:00, 119.82it/s, est. speed input: 180105.80 toks/s, output: 175.88 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:05<00:00, 118.31it/s, est. speed input: 178489.15 toks/s, output: 174.31 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:05<00:00, 117.60it/s, est. speed input: 177011.84 toks/s, output: 172.86 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:05<00:00, 117.32it/s, est. speed input: 175635.87 toks/s, output: 171.52 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:05<00:00, 117.01it/s, est. speed input: 174312.96 toks/s, output: 170.23 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:06<00:00, 114.91it/s, est. speed input: 172828.05 toks/s, output: 168.78 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:06<00:00, 114.91it/s, est. speed input: 173843.46 toks/s, output: 169.77 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:06<00:00, 169.77it/s, est. speed input: 173843.46 toks/s, output: 169.77 toks/s]
[rank0]:[W126 16:54:39.522773937 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.4s

测试结果:
  Requests/s:   114.82
  Tokens/s:     117687.27
  Total Reqs:   1024
  Elapsed:      8.92s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     117572.45

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:54:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=799843) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=799843) WARNING 01-26 16:55:16 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=799843) WARNING 01-26 16:55:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 118.44 requests/s, 121403.37 total tokens/s, 118.44 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 16:54:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:54:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:54:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:54:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:54:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:54:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:54:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:54:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:55:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:55:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:55:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:55:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:55:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:55:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:55:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:55:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:55:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:55:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:55:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:55:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:55:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:55:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:07] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:07] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:07] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:07] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=799843) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=799843) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.04it/s]
(EngineCore_DP0 pid=799843) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.09s/it]
(EngineCore_DP0 pid=799843) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.07s/it]
(EngineCore_DP0 pid=799843) 
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=799843) [rank0]:W0126 16:55:21.381000 799843 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=799843) [rank0]:W0126 16:55:21.463000 799843 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=799843) [rank0]:W0126 16:55:22.428000 799843 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=799843) [rank0]:W0126 16:55:22.559000 799843 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=799843) 2026-01-26 16:55:26,023 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=799843) 2026-01-26 16:55:26,049 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=799843) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 12.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 15.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.01it/s]
(EngineCore_DP0 pid=799843) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 17.83it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.00it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 17.99it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 279.87it/s]
Adding requests:   3%|▎         | 58/2048 [00:00<00:06, 288.95it/s]
Adding requests:   4%|▍         | 91/2048 [00:00<00:06, 303.97it/s]
Adding requests:   6%|▌         | 125/2048 [00:00<00:06, 316.83it/s]
Adding requests:   8%|▊         | 160/2048 [00:00<00:05, 326.18it/s]
Adding requests:  10%|▉         | 197/2048 [00:00<00:05, 339.15it/s]
Adding requests:  11%|█▏        | 233/2048 [00:00<00:05, 344.52it/s]
Adding requests:  13%|█▎        | 268/2048 [00:00<00:05, 339.45it/s]
Adding requests:  15%|█▍        | 304/2048 [00:00<00:05, 345.53it/s]
Adding requests:  17%|█▋        | 342/2048 [00:01<00:04, 354.40it/s]
Adding requests:  19%|█▊        | 379/2048 [00:01<00:04, 357.43it/s]
Adding requests:  20%|██        | 418/2048 [00:01<00:04, 366.71it/s]
Adding requests:  22%|██▏       | 455/2048 [00:01<00:04, 362.66it/s]
Adding requests:  24%|██▍       | 495/2048 [00:01<00:04, 373.08it/s]
Adding requests:  26%|██▌       | 534/2048 [00:01<00:04, 377.22it/s]
Adding requests:  28%|██▊       | 572/2048 [00:01<00:03, 375.21it/s]
Adding requests:  30%|██▉       | 610/2048 [00:01<00:03, 361.34it/s]
Adding requests:  32%|███▏      | 647/2048 [00:01<00:03, 353.39it/s]
Adding requests:  33%|███▎      | 683/2048 [00:01<00:03, 351.39it/s]
Adding requests:  35%|███▌      | 719/2048 [00:02<00:03, 352.71it/s]
Adding requests:  37%|███▋      | 755/2048 [00:02<00:03, 350.70it/s]
Adding requests:  39%|███▊      | 792/2048 [00:02<00:03, 352.29it/s]
Adding requests:  41%|████      | 830/2048 [00:02<00:03, 359.46it/s]
Adding requests:  42%|████▏     | 867/2048 [00:02<00:03, 360.42it/s]
Adding requests:  44%|████▍     | 905/2048 [00:02<00:03, 366.06it/s]
Adding requests:  46%|████▌     | 942/2048 [00:02<00:03, 357.92it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:02, 358.49it/s]
Adding requests:  50%|████▉     | 1015/2048 [00:02<00:02, 352.92it/s]
Adding requests:  51%|█████▏    | 1051/2048 [00:02<00:02, 354.62it/s]
Adding requests:  53%|█████▎    | 1087/2048 [00:03<00:02, 354.83it/s]
Adding requests:  55%|█████▍    | 1123/2048 [00:03<00:02, 356.06it/s]
Adding requests:  57%|█████▋    | 1159/2048 [00:03<00:02, 354.77it/s]
Adding requests:  58%|█████▊    | 1195/2048 [00:03<00:02, 356.00it/s]
Adding requests:  60%|██████    | 1234/2048 [00:03<00:02, 363.56it/s]
Adding requests:  62%|██████▏   | 1271/2048 [00:03<00:02, 357.60it/s]
Adding requests:  64%|██████▍   | 1307/2048 [00:03<00:02, 356.63it/s]
Adding requests:  66%|██████▌   | 1343/2048 [00:03<00:01, 357.58it/s]
Adding requests:  67%|██████▋   | 1380/2048 [00:03<00:01, 359.58it/s]
Adding requests:  69%|██████▉   | 1416/2048 [00:04<00:01, 357.36it/s]
Adding requests:  71%|███████   | 1453/2048 [00:04<00:01, 360.13it/s]
Adding requests:  73%|███████▎  | 1490/2048 [00:04<00:01, 362.59it/s]
Adding requests:  75%|███████▍  | 1527/2048 [00:04<00:01, 362.62it/s]
Adding requests:  76%|███████▋  | 1564/2048 [00:04<00:01, 358.75it/s]
Adding requests:  78%|███████▊  | 1600/2048 [00:04<00:01, 353.28it/s]
Adding requests:  80%|███████▉  | 1636/2048 [00:04<00:01, 349.15it/s]
Adding requests:  82%|████████▏ | 1671/2048 [00:04<00:01, 334.56it/s]
Adding requests:  83%|████████▎ | 1708/2048 [00:04<00:00, 344.10it/s]
Adding requests:  85%|████████▌ | 1744/2048 [00:04<00:00, 347.31it/s]
Adding requests:  87%|████████▋ | 1779/2048 [00:05<00:00, 344.73it/s]
Adding requests:  89%|████████▊ | 1814/2048 [00:05<00:00, 345.68it/s]
Adding requests:  90%|█████████ | 1850/2048 [00:05<00:00, 348.98it/s]
Adding requests:  92%|█████████▏| 1887/2048 [00:05<00:00, 354.92it/s]
Adding requests:  94%|█████████▍| 1924/2048 [00:05<00:00, 358.06it/s]
Adding requests:  96%|█████████▌| 1962/2048 [00:05<00:00, 361.13it/s]
Adding requests:  98%|█████████▊| 1999/2048 [00:05<00:00, 356.09it/s]
Adding requests:  99%|█████████▉| 2035/2048 [00:05<00:00, 350.56it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 352.85it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  33%|███▎      | 685/2048 [00:00<00:00, 4652.50it/s, est. speed input: 4764411.35 toks/s, output: 4652.57 toks/s]
Processed prompts:  56%|█████▌    | 1151/2048 [00:04<00:03, 235.55it/s, est. speed input: 290436.82 toks/s, output: 283.63 toks/s]  
Processed prompts:  66%|██████▌   | 1350/2048 [00:05<00:03, 194.83it/s, est. speed input: 243941.77 toks/s, output: 238.22 toks/s]
Processed prompts:  71%|███████▏  | 1464/2048 [00:06<00:03, 177.84it/s, est. speed input: 227027.93 toks/s, output: 221.71 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:07<00:03, 164.95it/s, est. speed input: 216560.87 toks/s, output: 211.49 toks/s]
Processed prompts:  78%|███████▊  | 1590/2048 [00:07<00:02, 159.95it/s, est. speed input: 212170.66 toks/s, output: 207.20 toks/s]
Processed prompts:  80%|███████▉  | 1629/2048 [00:08<00:02, 149.44it/s, est. speed input: 206579.82 toks/s, output: 201.74 toks/s]
Processed prompts:  81%|████████  | 1658/2048 [00:08<00:02, 154.28it/s, est. speed input: 206787.42 toks/s, output: 201.94 toks/s]
Processed prompts:  82%|████████▏ | 1685/2048 [00:08<00:02, 144.63it/s, est. speed input: 203436.46 toks/s, output: 198.67 toks/s]
Processed prompts:  83%|████████▎ | 1707/2048 [00:08<00:02, 146.56it/s, est. speed input: 202840.66 toks/s, output: 198.09 toks/s]
Processed prompts:  84%|████████▍ | 1727/2048 [00:08<00:02, 129.02it/s, est. speed input: 198973.90 toks/s, output: 194.31 toks/s]
Processed prompts:  85%|████████▌ | 1743/2048 [00:09<00:02, 128.28it/s, est. speed input: 197938.49 toks/s, output: 193.30 toks/s]
Processed prompts:  86%|████████▌ | 1758/2048 [00:09<00:02, 126.38it/s, est. speed input: 196841.62 toks/s, output: 192.23 toks/s]
Processed prompts:  87%|████████▋ | 1773/2048 [00:09<00:02, 123.31it/s, est. speed input: 195632.66 toks/s, output: 191.05 toks/s]
Processed prompts:  87%|████████▋ | 1789/2048 [00:09<00:02, 123.19it/s, est. speed input: 194664.83 toks/s, output: 190.10 toks/s]
Processed prompts:  88%|████████▊ | 1805/2048 [00:09<00:01, 122.41it/s, est. speed input: 193660.13 toks/s, output: 189.12 toks/s]
Processed prompts:  89%|████████▉ | 1821/2048 [00:09<00:01, 121.45it/s, est. speed input: 192655.51 toks/s, output: 188.14 toks/s]
Processed prompts:  90%|████████▉ | 1837/2048 [00:09<00:01, 120.68it/s, est. speed input: 191676.94 toks/s, output: 187.18 toks/s]
Processed prompts:  90%|█████████ | 1853/2048 [00:09<00:01, 119.83it/s, est. speed input: 190703.45 toks/s, output: 186.23 toks/s]
Processed prompts:  91%|█████████▏| 1869/2048 [00:10<00:01, 119.35it/s, est. speed input: 189766.82 toks/s, output: 185.32 toks/s]
Processed prompts:  92%|█████████▏| 1885/2048 [00:10<00:01, 120.58it/s, est. speed input: 188968.90 toks/s, output: 184.54 toks/s]
Processed prompts:  93%|█████████▎| 1901/2048 [00:10<00:01, 119.79it/s, est. speed input: 188074.31 toks/s, output: 183.67 toks/s]
Processed prompts:  94%|█████████▎| 1917/2048 [00:10<00:01, 117.84it/s, est. speed input: 187106.50 toks/s, output: 182.72 toks/s]
Processed prompts:  94%|█████████▍| 1933/2048 [00:10<00:00, 119.34it/s, est. speed input: 186358.89 toks/s, output: 181.99 toks/s]
Processed prompts:  95%|█████████▌| 1949/2048 [00:10<00:00, 119.72it/s, est. speed input: 185584.07 toks/s, output: 181.23 toks/s]
Processed prompts:  96%|█████████▌| 1965/2048 [00:10<00:00, 118.18it/s, est. speed input: 184711.80 toks/s, output: 180.38 toks/s]
Processed prompts:  97%|█████████▋| 1981/2048 [00:11<00:00, 119.81it/s, est. speed input: 184032.53 toks/s, output: 179.72 toks/s]
Processed prompts:  98%|█████████▊| 1997/2048 [00:11<00:00, 118.99it/s, est. speed input: 183247.88 toks/s, output: 178.95 toks/s]
Processed prompts:  98%|█████████▊| 2013/2048 [00:11<00:00, 118.55it/s, est. speed input: 182489.69 toks/s, output: 178.21 toks/s]
Processed prompts:  99%|█████████▉| 2029/2048 [00:11<00:00, 118.85it/s, est. speed input: 181786.69 toks/s, output: 177.53 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:11<00:00, 118.85it/s, est. speed input: 182594.14 toks/s, output: 178.31 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:11<00:00, 178.31it/s, est. speed input: 182594.14 toks/s, output: 178.31 toks/s]
[rank0]:[W126 16:55:45.150344684 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 66.7s

测试结果:
  Requests/s:   118.44
  Tokens/s:     121403.37
  Total Reqs:   2048
  Elapsed:      17.29s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     121284.93

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:56:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=801471) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=801471) WARNING 01-26 16:56:35 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=801471) WARNING 01-26 16:56:45 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 121.44 requests/s, 124476.10 total tokens/s, 121.44 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 16:56:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:56:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:56:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:56:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:56:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:56:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:56:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:56:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:56:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:56:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:56:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:56:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:56:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:56:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:56:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:56:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=801471) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=801471) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.03it/s]
(EngineCore_DP0 pid=801471) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]
(EngineCore_DP0 pid=801471) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.08s/it]
(EngineCore_DP0 pid=801471) 
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=801471) [rank0]:W0126 16:56:40.414000 801471 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=801471) [rank0]:W0126 16:56:40.494000 801471 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=801471) [rank0]:W0126 16:56:41.452000 801471 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=801471) [rank0]:W0126 16:56:41.579000 801471 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=801471) 2026-01-26 16:56:45,221 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=801471) 2026-01-26 16:56:45,247 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=801471) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 15.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 16.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 16.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 17.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 16.13it/s]
(EngineCore_DP0 pid=801471) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.79it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.59it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 17.23it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 17.28it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 275.87it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 318.04it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 315.14it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:12, 320.77it/s]
Adding requests:   4%|▍         | 163/4096 [00:00<00:11, 330.79it/s]
Adding requests:   5%|▍         | 197/4096 [00:00<00:11, 327.94it/s]
Adding requests:   6%|▌         | 232/4096 [00:00<00:11, 333.94it/s]
Adding requests:   7%|▋         | 267/4096 [00:00<00:11, 336.18it/s]
Adding requests:   7%|▋         | 303/4096 [00:00<00:11, 341.15it/s]
Adding requests:   8%|▊         | 340/4096 [00:01<00:10, 348.41it/s]
Adding requests:   9%|▉         | 377/4096 [00:01<00:10, 352.63it/s]
Adding requests:  10%|█         | 415/4096 [00:01<00:10, 358.90it/s]
Adding requests:  11%|█         | 451/4096 [00:01<00:10, 356.03it/s]
Adding requests:  12%|█▏        | 491/4096 [00:01<00:09, 368.65it/s]
Adding requests:  13%|█▎        | 530/4096 [00:01<00:09, 374.93it/s]
Adding requests:  14%|█▍        | 568/4096 [00:01<00:09, 372.31it/s]
Adding requests:  15%|█▍        | 606/4096 [00:01<00:09, 358.91it/s]
Adding requests:  16%|█▌        | 643/4096 [00:01<00:09, 355.80it/s]
Adding requests:  17%|█▋        | 679/4096 [00:01<00:09, 354.67it/s]
Adding requests:  17%|█▋        | 716/4096 [00:02<00:09, 356.23it/s]
Adding requests:  18%|█▊        | 752/4096 [00:02<00:09, 346.10it/s]
Adding requests:  19%|█▉        | 788/4096 [00:02<00:09, 348.34it/s]
Adding requests:  20%|██        | 824/4096 [00:02<00:09, 350.21it/s]
Adding requests:  21%|██        | 862/4096 [00:02<00:09, 356.85it/s]
Adding requests:  22%|██▏       | 898/4096 [00:02<00:09, 354.12it/s]
Adding requests:  23%|██▎       | 934/4096 [00:02<00:09, 349.00it/s]
Adding requests:  24%|██▎       | 970/4096 [00:02<00:08, 352.08it/s]
Adding requests:  25%|██▍       | 1006/4096 [00:02<00:08, 348.74it/s]
Adding requests:  25%|██▌       | 1042/4096 [00:02<00:08, 351.16it/s]
Adding requests:  26%|██▋       | 1078/4096 [00:03<00:08, 348.25it/s]
Adding requests:  27%|██▋       | 1114/4096 [00:03<00:08, 349.15it/s]
Adding requests:  28%|██▊       | 1150/4096 [00:03<00:08, 351.39it/s]
Adding requests:  29%|██▉       | 1186/4096 [00:03<00:08, 351.24it/s]
Adding requests:  30%|██▉       | 1224/4096 [00:03<00:08, 358.90it/s]
Adding requests:  31%|███       | 1260/4096 [00:03<00:08, 353.94it/s]
Adding requests:  32%|███▏      | 1296/4096 [00:03<00:08, 349.52it/s]
Adding requests:  33%|███▎      | 1332/4096 [00:03<00:07, 352.21it/s]
Adding requests:  33%|███▎      | 1369/4096 [00:03<00:07, 356.15it/s]
Adding requests:  34%|███▍      | 1405/4096 [00:04<00:07, 354.74it/s]
Adding requests:  35%|███▌      | 1441/4096 [00:04<00:07, 354.51it/s]
Adding requests:  36%|███▌      | 1477/4096 [00:04<00:07, 354.61it/s]
Adding requests:  37%|███▋      | 1515/4096 [00:04<00:07, 359.55it/s]
Adding requests:  38%|███▊      | 1551/4096 [00:04<00:07, 357.03it/s]
Adding requests:  39%|███▊      | 1587/4096 [00:04<00:07, 349.55it/s]
Adding requests:  40%|███▉      | 1622/4096 [00:04<00:07, 345.31it/s]
Adding requests:  40%|████      | 1657/4096 [00:04<00:07, 340.30it/s]
Adding requests:  41%|████▏     | 1693/4096 [00:04<00:07, 342.13it/s]
Adding requests:  42%|████▏     | 1730/4096 [00:04<00:06, 347.47it/s]
Adding requests:  43%|████▎     | 1768/4096 [00:05<00:06, 355.24it/s]
Adding requests:  44%|████▍     | 1804/4096 [00:05<00:06, 352.59it/s]
Adding requests:  45%|████▍     | 1840/4096 [00:05<00:06, 352.97it/s]
Adding requests:  46%|████▌     | 1876/4096 [00:05<00:06, 352.69it/s]
Adding requests:  47%|████▋     | 1913/4096 [00:05<00:06, 357.38it/s]
Adding requests:  48%|████▊     | 1951/4096 [00:05<00:05, 362.93it/s]
Adding requests:  49%|████▊     | 1988/4096 [00:05<00:05, 358.87it/s]
Adding requests:  49%|████▉     | 2024/4096 [00:05<00:06, 341.02it/s]
Adding requests:  50%|█████     | 2059/4096 [00:05<00:05, 341.95it/s]
Adding requests:  51%|█████     | 2094/4096 [00:05<00:05, 341.03it/s]
Adding requests:  52%|█████▏    | 2129/4096 [00:06<00:05, 343.54it/s]
Adding requests:  53%|█████▎    | 2164/4096 [00:06<00:05, 342.12it/s]
Adding requests:  54%|█████▎    | 2199/4096 [00:06<00:05, 338.03it/s]
Adding requests:  55%|█████▍    | 2235/4096 [00:06<00:05, 343.47it/s]
Adding requests:  55%|█████▌    | 2272/4096 [00:06<00:05, 350.43it/s]
Adding requests:  56%|█████▋    | 2308/4096 [00:06<00:05, 352.00it/s]
Adding requests:  57%|█████▋    | 2345/4096 [00:06<00:04, 354.58it/s]
Adding requests:  58%|█████▊    | 2381/4096 [00:06<00:04, 355.23it/s]
Adding requests:  59%|█████▉    | 2419/4096 [00:06<00:04, 361.15it/s]
Adding requests:  60%|█████▉    | 2456/4096 [00:07<00:04, 358.33it/s]
Adding requests:  61%|██████    | 2493/4096 [00:07<00:04, 360.17it/s]
Adding requests:  62%|██████▏   | 2531/4096 [00:07<00:04, 364.25it/s]
Adding requests:  63%|██████▎   | 2571/4096 [00:07<00:04, 372.15it/s]
Adding requests:  64%|██████▎   | 2609/4096 [00:07<00:04, 370.75it/s]
Adding requests:  65%|██████▍   | 2647/4096 [00:07<00:04, 361.24it/s]
Adding requests:  66%|██████▌   | 2684/4096 [00:07<00:03, 355.09it/s]
Adding requests:  66%|██████▋   | 2720/4096 [00:07<00:03, 354.00it/s]
Adding requests:  67%|██████▋   | 2758/4096 [00:07<00:03, 359.63it/s]
Adding requests:  68%|██████▊   | 2796/4096 [00:07<00:03, 364.75it/s]
Adding requests:  69%|██████▉   | 2833/4096 [00:08<00:03, 362.57it/s]
Adding requests:  70%|███████   | 2870/4096 [00:08<00:03, 360.22it/s]
Adding requests:  71%|███████   | 2907/4096 [00:08<00:03, 360.79it/s]
Adding requests:  72%|███████▏  | 2945/4096 [00:08<00:03, 365.19it/s]
Adding requests:  73%|███████▎  | 2982/4096 [00:08<00:03, 361.49it/s]
Adding requests:  74%|███████▎  | 3020/4096 [00:08<00:02, 365.14it/s]
Adding requests:  75%|███████▍  | 3058/4096 [00:08<00:02, 367.89it/s]
Adding requests:  76%|███████▌  | 3095/4096 [00:08<00:02, 357.32it/s]
Adding requests:  76%|███████▋  | 3133/4096 [00:08<00:02, 363.39it/s]
Adding requests:  77%|███████▋  | 3170/4096 [00:08<00:02, 357.66it/s]
Adding requests:  78%|███████▊  | 3206/4096 [00:09<00:02, 354.47it/s]
Adding requests:  79%|███████▉  | 3243/4096 [00:09<00:02, 358.65it/s]
Adding requests:  80%|████████  | 3279/4096 [00:09<00:02, 354.21it/s]
Adding requests:  81%|████████  | 3315/4096 [00:09<00:02, 337.17it/s]
Adding requests:  82%|████████▏ | 3351/4096 [00:09<00:02, 342.28it/s]
Adding requests:  83%|████████▎ | 3388/4096 [00:09<00:02, 349.85it/s]
Adding requests:  84%|████████▎ | 3424/4096 [00:09<00:01, 351.08it/s]
Adding requests:  84%|████████▍ | 3461/4096 [00:09<00:01, 354.41it/s]
Adding requests:  85%|████████▌ | 3497/4096 [00:09<00:01, 352.42it/s]
Adding requests:  86%|████████▋ | 3536/4096 [00:10<00:01, 363.07it/s]
Adding requests:  87%|████████▋ | 3573/4096 [00:10<00:01, 362.39it/s]
Adding requests:  88%|████████▊ | 3610/4096 [00:10<00:01, 362.77it/s]
Adding requests:  89%|████████▉ | 3648/4096 [00:10<00:01, 365.28it/s]
Adding requests:  90%|████████▉ | 3685/4096 [00:10<00:01, 354.31it/s]
Adding requests:  91%|█████████ | 3722/4096 [00:10<00:01, 358.16it/s]
Adding requests:  92%|█████████▏| 3758/4096 [00:10<00:00, 350.41it/s]
Adding requests:  93%|█████████▎| 3794/4096 [00:10<00:00, 339.66it/s]
Adding requests:  93%|█████████▎| 3829/4096 [00:10<00:00, 338.18it/s]
Adding requests:  94%|█████████▍| 3864/4096 [00:10<00:00, 341.45it/s]
Adding requests:  95%|█████████▌| 3899/4096 [00:11<00:00, 341.79it/s]
Adding requests:  96%|█████████▌| 3934/4096 [00:11<00:00, 338.52it/s]
Adding requests:  97%|█████████▋| 3969/4096 [00:11<00:00, 341.37it/s]
Adding requests:  98%|█████████▊| 4004/4096 [00:11<00:00, 342.65it/s]
Adding requests:  99%|█████████▊| 4040/4096 [00:11<00:00, 344.38it/s]
Adding requests:  99%|█████████▉| 4075/4096 [00:11<00:00, 343.28it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 351.76it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  34%|███▍      | 1389/4096 [00:00<00:00, 12328.49it/s, est. speed input: 12625305.96 toks/s, output: 12328.72 toks/s]
Processed prompts:  64%|██████▍   | 2622/4096 [00:10<00:06, 219.86it/s, est. speed input: 266774.30 toks/s, output: 260.52 toks/s]      
Processed prompts:  77%|███████▋  | 3137/4096 [00:14<00:05, 183.32it/s, est. speed input: 225055.05 toks/s, output: 219.78 toks/s]
Processed prompts:  84%|████████▎ | 3423/4096 [00:16<00:03, 168.57it/s, est. speed input: 210419.25 toks/s, output: 205.49 toks/s]
Processed prompts:  88%|████████▊ | 3601/4096 [00:18<00:03, 158.54it/s, est. speed input: 202225.61 toks/s, output: 197.49 toks/s]
Processed prompts:  91%|█████████ | 3719/4096 [00:19<00:02, 157.63it/s, est. speed input: 200241.18 toks/s, output: 195.55 toks/s]
Processed prompts:  93%|█████████▎| 3802/4096 [00:19<00:01, 150.33it/s, est. speed input: 196578.29 toks/s, output: 191.97 toks/s]
Processed prompts:  94%|█████████▍| 3862/4096 [00:20<00:01, 145.98it/s, est. speed input: 194492.57 toks/s, output: 189.93 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [00:20<00:01, 147.64it/s, est. speed input: 194196.22 toks/s, output: 189.64 toks/s]
Processed prompts:  96%|█████████▌| 3942/4096 [00:20<00:01, 147.48it/s, est. speed input: 193667.67 toks/s, output: 189.13 toks/s]
Processed prompts:  97%|█████████▋| 3971/4096 [00:21<00:00, 142.96it/s, est. speed input: 192681.28 toks/s, output: 188.17 toks/s]
Processed prompts:  98%|█████████▊| 3995/4096 [00:21<00:00, 135.48it/s, est. speed input: 191503.97 toks/s, output: 187.02 toks/s]
Processed prompts:  98%|█████████▊| 4015/4096 [00:21<00:00, 124.75it/s, est. speed input: 190112.64 toks/s, output: 185.66 toks/s]
Processed prompts:  99%|█████████▉| 4045/4096 [00:21<00:00, 119.93it/s, est. speed input: 188985.81 toks/s, output: 184.56 toks/s]
Processed prompts: 100%|█████████▉| 4077/4096 [00:22<00:00, 131.22it/s, est. speed input: 189063.58 toks/s, output: 184.63 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:22<00:00, 131.22it/s, est. speed input: 189942.26 toks/s, output: 185.49 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:22<00:00, 185.49it/s, est. speed input: 189942.26 toks/s, output: 185.49 toks/s]
[rank0]:[W126 16:57:22.301887886 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 96.3s

测试结果:
  Requests/s:   121.44
  Tokens/s:     124476.10
  Total Reqs:   4096
  Elapsed:      33.73s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     124354.66

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:58:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=803677) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=803677) WARNING 01-26 16:58:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     def forward(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     raise e
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/tmp/torchinductor_root/g6/cg6eknmfwhlfecxv6oq5xol74qjrpwt77qbf3blpnjezrum6e5qc.py", line 1093, in call
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 10)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 221, in quant_slide_fp8_triton
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered


─── STDERR ───
[2026-01-26 16:58:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:58:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:58:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:58:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:58:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:58:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:58:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:58:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:58:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:58:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:58:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:58:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:58:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:58:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:58:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:58:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:27] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=803677) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=803677) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.04it/s]
(EngineCore_DP0 pid=803677) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.09s/it]
(EngineCore_DP0 pid=803677) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.07s/it]
(EngineCore_DP0 pid=803677) 
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=803677) [rank0]:W0126 16:58:41.065000 803677 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=803677) [rank0]:W0126 16:58:41.145000 803677 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=803677) [rank0]:W0126 16:58:42.203000 803677 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=803677) [rank0]:W0126 16:58:42.325000 803677 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=803677) Process EngineCore_DP0:
(EngineCore_DP0 pid=803677) Traceback (most recent call last):
(EngineCore_DP0 pid=803677)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=803677)     self.run()
(EngineCore_DP0 pid=803677)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=803677)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=803677)     raise e
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=803677)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=803677)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=803677)     super().__init__(
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=803677)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=803677)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=803677)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=803677)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=803677)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=803677)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=803677)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=803677)     return func(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=803677)     return func(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=803677)     self.model_runner.profile_run()
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=803677)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=803677)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=803677)     return func(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=803677)     outputs = self.model(
(EngineCore_DP0 pid=803677)               ^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=803677)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=803677)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=803677)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=803677)     hidden_states = self.model(
(EngineCore_DP0 pid=803677)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=803677)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=803677)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=803677)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=803677)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=803677)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=803677)     def forward(
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=803677)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=803677)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=803677)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=803677)     raise e
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=803677)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=803677)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=803677)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=803677)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=803677)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=803677)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=803677)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=803677)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=803677)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=803677)     return compiled_fn(full_args)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=803677)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=803677)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=803677)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=803677)                             ^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=803677)     outs = compiled_fn(args)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=803677)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=803677)     return self.current_callable(inputs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=803677)     out = model(new_inputs)
(EngineCore_DP0 pid=803677)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/tmp/torchinductor_root/g6/cg6eknmfwhlfecxv6oq5xol74qjrpwt77qbf3blpnjezrum6e5qc.py", line 1093, in call
(EngineCore_DP0 pid=803677)     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 10)
(EngineCore_DP0 pid=803677)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=803677)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=803677)     return fn(input, L)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 221, in quant_slide_fp8_triton
(EngineCore_DP0 pid=803677)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=803677)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=803677)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=803677)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=803677)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=803677)     self._init_handles()
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=803677)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=803677)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 16:58:43.389307136 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,37.7193,19349.9867,3.3935
1024,1024,1,128,128,39.7470,40740.6787,3.2204
2048,1024,2,256,128,76.5146,78427.4885,3.3458
4096,1024,4,512,128,105.5307,108168.9451,4.8517
8192,1024,8,1024,128,114.8168,117687.2657,8.9186
16384,1024,16,2048,128,118.4423,121403.3690,17.2911
32768,1024,32,4096,128,121.4401,124476.1007,33.7286
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 5 失败
============================================================
