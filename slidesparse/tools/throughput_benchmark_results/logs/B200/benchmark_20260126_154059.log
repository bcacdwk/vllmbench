======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-26 15:40:59
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768,65536

命令行参数:
  --model: qwen2.5-7b-int8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: prefill
  --M: 512,1024,2048,4096,8192,16384,32768,65536
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: B200
  Compute Capability: cc100
  VRAM: 178.4 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  Qwen2.5-7B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:41:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=694982) WARNING 01-26 15:41:27 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=694982) WARNING 01-26 15:41:39 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 33.21 requests/s, 17038.05 total tokens/s, 33.21 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 15:41:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:41:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:41:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:41:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:41:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:41:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:41:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:41:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:41:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:41:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:41:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:41:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:41:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:41:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:41:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:41:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=694982) [2026-01-26 15:41:18] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=694982) [2026-01-26 15:41:19] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=694982) [2026-01-26 15:41:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=694982) [2026-01-26 15:41:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=694982) [2026-01-26 15:41:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=694982) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=694982) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.47it/s]
(EngineCore_DP0 pid=694982) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=694982) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.23it/s]
(EngineCore_DP0 pid=694982) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=694982) 2026-01-26 15:41:39,200 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=694982) 2026-01-26 15:41:39,227 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=694982) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.34it/s]
(EngineCore_DP0 pid=694982) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.01it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  39%|███▉      | 50/128 [00:00<00:00, 495.13it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 234.71it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 279.32it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:01, 89.56it/s, est. speed input: 45858.28 toks/s, output: 89.56 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 48.03it/s, est. speed input: 26532.94 toks/s, output: 51.82 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 42.97it/s, est. speed input: 23983.88 toks/s, output: 46.84 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 40.61it/s, est. speed input: 22818.14 toks/s, output: 44.57 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:02, 39.16it/s, est. speed input: 22076.22 toks/s, output: 43.12 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:02, 38.15it/s, est. speed input: 21540.77 toks/s, output: 42.07 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 37.58it/s, est. speed input: 21215.93 toks/s, output: 41.44 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 37.14it/s, est. speed input: 20950.08 toks/s, output: 40.92 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 36.80it/s, est. speed input: 20728.15 toks/s, output: 40.48 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 36.58it/s, est. speed input: 20547.69 toks/s, output: 40.13 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 36.40it/s, est. speed input: 20389.88 toks/s, output: 39.82 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 36.27it/s, est. speed input: 20253.91 toks/s, output: 39.56 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 36.19it/s, est. speed input: 20136.03 toks/s, output: 39.33 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 36.07it/s, est. speed input: 20026.56 toks/s, output: 39.11 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 36.02it/s, est. speed input: 19932.34 toks/s, output: 38.93 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 36.04it/s, est. speed input: 19854.31 toks/s, output: 38.78 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 36.03it/s, est. speed input: 19781.55 toks/s, output: 38.64 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 36.05it/s, est. speed input: 19718.88 toks/s, output: 38.51 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 36.09it/s, est. speed input: 19663.41 toks/s, output: 38.40 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 36.02it/s, est. speed input: 19605.80 toks/s, output: 38.29 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 36.05it/s, est. speed input: 19558.24 toks/s, output: 38.20 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 36.10it/s, est. speed input: 19517.33 toks/s, output: 38.12 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 36.08it/s, est. speed input: 19475.71 toks/s, output: 38.04 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 36.08it/s, est. speed input: 19438.21 toks/s, output: 37.97 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 36.09it/s, est. speed input: 19403.94 toks/s, output: 37.90 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 36.04it/s, est. speed input: 19368.38 toks/s, output: 37.83 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 36.05it/s, est. speed input: 19337.84 toks/s, output: 37.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.11it/s, est. speed input: 19312.36 toks/s, output: 37.72 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.11it/s, est. speed input: 19312.36 toks/s, output: 37.72 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.72it/s, est. speed input: 19312.36 toks/s, output: 37.72 toks/s]
[rank0]:[W126 15:41:45.002144388 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 48.6s

测试结果:
  Requests/s:   33.21
  Tokens/s:     17038.05
  Total Reqs:   128
  Elapsed:      3.85s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     17004.83

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:41:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=696436) WARNING 01-26 15:42:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=696436) WARNING 01-26 15:42:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 34.90 requests/s, 35768.76 total tokens/s, 34.90 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 15:41:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:41:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:41:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:41:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:41:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:41:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:41:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:41:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:41:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:42:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:42:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:42:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:42:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:42:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:42:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:42:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:42:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=696436) [2026-01-26 15:42:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=696436) [2026-01-26 15:42:02] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=696436) [2026-01-26 15:42:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=696436) [2026-01-26 15:42:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=696436) [2026-01-26 15:42:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=696436) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=696436) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.47it/s]
(EngineCore_DP0 pid=696436) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]
(EngineCore_DP0 pid=696436) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.24it/s]
(EngineCore_DP0 pid=696436) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=696436) 2026-01-26 15:42:21,097 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=696436) 2026-01-26 15:42:21,124 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=696436) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.13it/s]
(EngineCore_DP0 pid=696436) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.74it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  21%|██        | 27/128 [00:00<00:00, 268.51it/s]
Adding requests:  48%|████▊     | 61/128 [00:00<00:00, 309.43it/s]
Adding requests:  72%|███████▏  | 92/128 [00:00<00:00, 308.84it/s]
Adding requests:  97%|█████████▋| 124/128 [00:00<00:00, 312.51it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 308.29it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:00, 136.65it/s, est. speed input: 139937.14 toks/s, output: 136.65 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 53.43it/s, est. speed input: 60804.20 toks/s, output: 59.38 toks/s]   
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 46.27it/s, est. speed input: 53424.05 toks/s, output: 52.17 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 42.99it/s, est. speed input: 50239.95 toks/s, output: 49.06 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:01, 40.98it/s, est. speed input: 48364.66 toks/s, output: 47.23 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 39.44it/s, est. speed input: 46932.57 toks/s, output: 45.83 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 38.32it/s, est. speed input: 45812.37 toks/s, output: 44.74 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 37.65it/s, est. speed input: 45083.44 toks/s, output: 44.03 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 37.10it/s, est. speed input: 44457.85 toks/s, output: 43.42 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 36.69it/s, est. speed input: 43919.96 toks/s, output: 42.89 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 36.33it/s, est. speed input: 43435.83 toks/s, output: 42.42 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 36.19it/s, est. speed input: 43041.32 toks/s, output: 42.03 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 36.01it/s, est. speed input: 42671.75 toks/s, output: 41.67 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 35.89it/s, est. speed input: 42342.66 toks/s, output: 41.35 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:01, 35.79it/s, est. speed input: 42044.76 toks/s, output: 41.06 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 35.72it/s, est. speed input: 41774.61 toks/s, output: 40.80 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 35.75it/s, est. speed input: 41544.08 toks/s, output: 40.57 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 35.71it/s, est. speed input: 41322.71 toks/s, output: 40.35 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 35.69it/s, est. speed input: 41121.09 toks/s, output: 40.16 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 35.59it/s, est. speed input: 40922.99 toks/s, output: 39.96 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 35.66it/s, est. speed input: 40760.03 toks/s, output: 39.80 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 35.62it/s, est. speed input: 40597.32 toks/s, output: 39.65 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 35.60it/s, est. speed input: 40447.87 toks/s, output: 39.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.60it/s, est. speed input: 40309.88 toks/s, output: 39.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.60it/s, est. speed input: 40309.88 toks/s, output: 39.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.36it/s, est. speed input: 40309.88 toks/s, output: 39.37 toks/s]
[rank0]:[W126 15:42:26.728698293 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.8s

测试结果:
  Requests/s:   34.90
  Tokens/s:     35768.76
  Total Reqs:   128
  Elapsed:      3.67s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     35733.86

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:42:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=697573) WARNING 01-26 15:42:52 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=697573) WARNING 01-26 15:43:02 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 70.47 requests/s, 72231.10 total tokens/s, 70.47 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 15:42:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:42:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:42:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:42:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:42:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:42:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:42:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:42:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:42:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:42:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:42:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:42:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:42:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:42:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:42:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:42:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:42:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=697573) [2026-01-26 15:42:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=697573) [2026-01-26 15:42:44] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=697573) [2026-01-26 15:42:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=697573) [2026-01-26 15:42:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=697573) [2026-01-26 15:42:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=697573) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=697573) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=697573) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]
(EngineCore_DP0 pid=697573) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.24it/s]
(EngineCore_DP0 pid=697573) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=697573) 2026-01-26 15:43:02,680 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=697573) 2026-01-26 15:43:02,705 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=697573) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 15.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 13.56it/s]
(EngineCore_DP0 pid=697573) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.13it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  10%|▉         | 25/256 [00:00<00:00, 240.73it/s]
Adding requests:  22%|██▏       | 56/256 [00:00<00:00, 279.59it/s]
Adding requests:  33%|███▎      | 85/256 [00:00<00:00, 271.65it/s]
Adding requests:  44%|████▍     | 113/256 [00:00<00:01, 130.73it/s]
Adding requests:  56%|█████▋    | 144/256 [00:00<00:00, 167.62it/s]
Adding requests:  69%|██████▉   | 177/256 [00:00<00:00, 204.30it/s]
Adding requests:  83%|████████▎ | 212/256 [00:01<00:00, 239.28it/s]
Adding requests:  96%|█████████▋| 247/256 [00:01<00:00, 266.37it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 225.24it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:00<00:00, 754.00it/s, est. speed input: 772140.80 toks/s, output: 754.01 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:01<00:00, 119.23it/s, est. speed input: 140505.55 toks/s, output: 137.21 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:01<00:00, 101.26it/s, est. speed input: 120695.76 toks/s, output: 117.87 toks/s]
Processed prompts:  85%|████████▍ | 217/256 [00:01<00:00, 94.92it/s, est. speed input: 114071.73 toks/s, output: 111.40 toks/s] 
Processed prompts:  91%|█████████▏| 234/256 [00:02<00:00, 89.06it/s, est. speed input: 109216.21 toks/s, output: 106.66 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:02<00:00, 85.86it/s, est. speed input: 106465.14 toks/s, output: 103.97 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 85.86it/s, est. speed input: 105080.31 toks/s, output: 102.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.61it/s, est. speed input: 105080.31 toks/s, output: 102.62 toks/s]
[rank0]:[W126 15:43:08.431434193 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.5s

测试结果:
  Requests/s:   70.47
  Tokens/s:     72231.10
  Total Reqs:   256
  Elapsed:      3.63s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     72160.63

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:43:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=698752) WARNING 01-26 15:43:35 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=698752) WARNING 01-26 15:43:45 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 108.77 requests/s, 111488.19 total tokens/s, 108.77 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 15:43:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:43:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:43:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:43:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:43:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:43:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:43:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:43:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:43:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:43:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:43:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:43:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:43:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:43:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:43:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:43:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:43:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:43:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:43:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:43:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:43:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:43:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:43:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:43:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:43:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:43:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:43:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:43:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=698752) [2026-01-26 15:43:27] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=698752) [2026-01-26 15:43:27] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=698752) [2026-01-26 15:43:27] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=698752) [2026-01-26 15:43:27] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=698752) [2026-01-26 15:43:27] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=698752) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=698752) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
(EngineCore_DP0 pid=698752) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.23it/s]
(EngineCore_DP0 pid=698752) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.26it/s]
(EngineCore_DP0 pid=698752) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=698752) 2026-01-26 15:43:45,539 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=698752) 2026-01-26 15:43:45,566 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=698752) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.93it/s]
(EngineCore_DP0 pid=698752) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 14.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 15.31it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 26/512 [00:00<00:01, 254.39it/s]
Adding requests:  12%|█▏        | 60/512 [00:00<00:01, 303.90it/s]
Adding requests:  18%|█▊        | 91/512 [00:00<00:01, 303.11it/s]
Adding requests:  24%|██▍       | 125/512 [00:00<00:01, 316.50it/s]
Adding requests:  31%|███▏      | 160/512 [00:00<00:01, 326.81it/s]
Adding requests:  38%|███▊      | 197/512 [00:00<00:00, 339.56it/s]
Adding requests:  46%|████▌     | 233/512 [00:00<00:00, 345.06it/s]
Adding requests:  52%|█████▏    | 268/512 [00:00<00:00, 344.54it/s]
Adding requests:  59%|█████▉    | 304/512 [00:00<00:00, 348.51it/s]
Adding requests:  67%|██████▋   | 342/512 [00:01<00:00, 355.51it/s]
Adding requests:  74%|███████▍  | 379/512 [00:01<00:00, 358.70it/s]
Adding requests:  82%|████████▏ | 418/512 [00:01<00:00, 367.21it/s]
Adding requests:  89%|████████▉ | 455/512 [00:01<00:00, 362.73it/s]
Adding requests:  97%|█████████▋| 495/512 [00:01<00:00, 372.71it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 348.24it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:00<00:00, 1346.01it/s, est. speed input: 1378391.19 toks/s, output: 1346.03 toks/s]
Processed prompts:  59%|█████▉    | 301/512 [00:01<00:01, 195.81it/s, est. speed input: 233530.84 toks/s, output: 228.06 toks/s]   
Processed prompts:  71%|███████   | 364/512 [00:01<00:00, 162.10it/s, est. speed input: 196669.25 toks/s, output: 192.06 toks/s]
Processed prompts:  79%|███████▊  | 403/512 [00:02<00:00, 147.31it/s, est. speed input: 182311.51 toks/s, output: 178.04 toks/s]
Processed prompts:  84%|████████▍ | 431/512 [00:02<00:00, 139.43it/s, est. speed input: 175157.03 toks/s, output: 171.05 toks/s]
Processed prompts:  88%|████████▊ | 453/512 [00:02<00:00, 137.05it/s, est. speed input: 172106.75 toks/s, output: 168.07 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [00:02<00:00, 130.40it/s, est. speed input: 167932.24 toks/s, output: 164.00 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [00:03<00:00, 126.44it/s, est. speed input: 165212.29 toks/s, output: 161.34 toks/s]
Processed prompts:  98%|█████████▊| 503/512 [00:03<00:00, 121.51it/s, est. speed input: 162462.32 toks/s, output: 158.65 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 121.51it/s, est. speed input: 162033.39 toks/s, output: 158.24 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 158.23it/s, est. speed input: 162033.39 toks/s, output: 158.24 toks/s]
[rank0]:[W126 15:43:52.809326658 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.4s

测试结果:
  Requests/s:   108.77
  Tokens/s:     111488.19
  Total Reqs:   512
  Elapsed:      4.71s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     111379.42

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:44:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=699965) WARNING 01-26 15:44:23 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=699965) WARNING 01-26 15:44:33 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 117.77 requests/s, 120719.03 total tokens/s, 117.77 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 15:44:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:44:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:44:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:44:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:44:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:44:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:44:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:44:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:44:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:44:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:44:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:44:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:44:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:44:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:44:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:44:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:44:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:44:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:44:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:44:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:44:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:44:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:44:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:44:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:44:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:44:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:44:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:44:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=699965) [2026-01-26 15:44:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=699965) [2026-01-26 15:44:15] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=699965) [2026-01-26 15:44:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=699965) [2026-01-26 15:44:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=699965) [2026-01-26 15:44:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=699965) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=699965) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=699965) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.24it/s]
(EngineCore_DP0 pid=699965) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.28it/s]
(EngineCore_DP0 pid=699965) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=699965) 2026-01-26 15:44:33,735 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=699965) 2026-01-26 15:44:33,763 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=699965) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 15.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 15.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 14.53it/s]
(EngineCore_DP0 pid=699965) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 16.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 14.23it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 14.48it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 273.39it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 315.25it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 311.91it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 317.53it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 326.52it/s]
Adding requests:  19%|█▉        | 199/1024 [00:00<00:02, 336.46it/s]
Adding requests:  23%|██▎       | 234/1024 [00:00<00:02, 340.11it/s]
Adding requests:  26%|██▋       | 269/1024 [00:00<00:02, 341.03it/s]
Adding requests:  30%|██▉       | 305/1024 [00:00<00:02, 344.99it/s]
Adding requests:  33%|███▎      | 342/1024 [00:01<00:01, 350.62it/s]
Adding requests:  37%|███▋      | 378/1024 [00:01<00:01, 352.96it/s]
Adding requests:  41%|████      | 415/1024 [00:01<00:01, 356.60it/s]
Adding requests:  44%|████▍     | 451/1024 [00:01<00:01, 353.05it/s]
Adding requests:  48%|████▊     | 491/1024 [00:01<00:01, 365.19it/s]
Adding requests:  52%|█████▏    | 530/1024 [00:01<00:01, 372.08it/s]
Adding requests:  55%|█████▌    | 568/1024 [00:01<00:01, 368.89it/s]
Adding requests:  59%|█████▉    | 605/1024 [00:01<00:01, 354.40it/s]
Adding requests:  63%|██████▎   | 641/1024 [00:01<00:01, 353.85it/s]
Adding requests:  66%|██████▌   | 677/1024 [00:01<00:00, 349.56it/s]
Adding requests:  70%|██████▉   | 713/1024 [00:02<00:00, 350.06it/s]
Adding requests:  73%|███████▎  | 749/1024 [00:02<00:00, 341.27it/s]
Adding requests:  77%|███████▋  | 785/1024 [00:02<00:00, 343.72it/s]
Adding requests:  80%|████████  | 820/1024 [00:02<00:00, 340.12it/s]
Adding requests:  84%|████████▎ | 857/1024 [00:02<00:00, 348.01it/s]
Adding requests:  87%|████████▋ | 892/1024 [00:02<00:00, 348.38it/s]
Adding requests:  91%|█████████ | 927/1024 [00:02<00:00, 341.02it/s]
Adding requests:  94%|█████████▍| 963/1024 [00:02<00:00, 345.10it/s]
Adding requests:  97%|█████████▋| 998/1024 [00:02<00:00, 339.73it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 344.75it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:00<00:00, 2988.10it/s, est. speed input: 3060023.19 toks/s, output: 2988.15 toks/s]
Processed prompts:  64%|██████▍   | 653/1024 [00:02<00:01, 211.28it/s, est. speed input: 254871.99 toks/s, output: 248.90 toks/s]   
Processed prompts:  77%|███████▋  | 784/1024 [00:03<00:01, 177.48it/s, est. speed input: 216582.87 toks/s, output: 211.51 toks/s]
Processed prompts:  84%|████████▍ | 861/1024 [00:04<00:01, 161.98it/s, est. speed input: 201481.08 toks/s, output: 196.76 toks/s]
Processed prompts:  89%|████████▉ | 912/1024 [00:04<00:00, 155.60it/s, est. speed input: 195328.02 toks/s, output: 190.75 toks/s]
Processed prompts:  93%|█████████▎| 949/1024 [00:05<00:00, 147.57it/s, est. speed input: 189829.72 toks/s, output: 185.38 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:05<00:00, 140.70it/s, est. speed input: 185746.44 toks/s, output: 181.39 toks/s]
Processed prompts:  98%|█████████▊| 1001/1024 [00:05<00:00, 143.73it/s, est. speed input: 185437.82 toks/s, output: 181.09 toks/s]
Processed prompts: 100%|█████████▉| 1022/1024 [00:05<00:00, 137.23it/s, est. speed input: 182863.46 toks/s, output: 178.58 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 137.23it/s, est. speed input: 183218.26 toks/s, output: 178.92 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 178.92it/s, est. speed input: 183218.26 toks/s, output: 178.92 toks/s]
[rank0]:[W126 15:44:44.947724567 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.3s

测试结果:
  Requests/s:   117.77
  Tokens/s:     120719.03
  Total Reqs:   1024
  Elapsed:      8.69s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     120601.25

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:45:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=701311) WARNING 01-26 15:45:22 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=701311) WARNING 01-26 15:45:33 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 117.89 requests/s, 120840.96 total tokens/s, 117.89 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 15:45:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:45:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:45:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:45:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:45:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:45:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:45:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:45:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:45:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:45:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:45:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:45:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:45:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:45:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:45:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:45:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:45:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:45:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:45:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:45:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:45:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:45:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:45:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:45:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:45:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:45:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:45:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:45:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=701311) [2026-01-26 15:45:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=701311) [2026-01-26 15:45:13] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=701311) [2026-01-26 15:45:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=701311) [2026-01-26 15:45:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=701311) [2026-01-26 15:45:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=701311) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=701311) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
(EngineCore_DP0 pid=701311) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.17it/s]
(EngineCore_DP0 pid=701311) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]
(EngineCore_DP0 pid=701311) 
(EngineCore_DP0 pid=701311) [rank0]:W0126 15:45:26.898000 701311 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=701311) [rank0]:W0126 15:45:27.606000 701311 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=701311) [rank0]:W0126 15:45:29.168000 701311 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=701311) [rank0]:W0126 15:45:29.299000 701311 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=701311) 2026-01-26 15:45:33,650 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=701311) 2026-01-26 15:45:33,678 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=701311) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 15.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 15.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.90it/s]
(EngineCore_DP0 pid=701311) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 16.17it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.35it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 277.94it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 317.63it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 314.77it/s]
Adding requests:   6%|▌         | 127/2048 [00:00<00:06, 309.38it/s]
Adding requests:   8%|▊         | 161/2048 [00:00<00:05, 318.73it/s]
Adding requests:  10%|▉         | 198/2048 [00:00<00:05, 332.63it/s]
Adding requests:  11%|█▏        | 234/2048 [00:00<00:05, 339.72it/s]
Adding requests:  13%|█▎        | 269/2048 [00:00<00:05, 340.98it/s]
Adding requests:  15%|█▍        | 305/2048 [00:00<00:05, 345.94it/s]
Adding requests:  17%|█▋        | 343/2048 [00:01<00:04, 353.72it/s]
Adding requests:  19%|█▊        | 379/2048 [00:01<00:04, 351.96it/s]
Adding requests:  20%|██        | 418/2048 [00:01<00:04, 361.82it/s]
Adding requests:  22%|██▏       | 455/2048 [00:01<00:04, 358.57it/s]
Adding requests:  24%|██▍       | 495/2048 [00:01<00:04, 369.70it/s]
Adding requests:  26%|██▌       | 534/2048 [00:01<00:04, 371.58it/s]
Adding requests:  28%|██▊       | 572/2048 [00:01<00:03, 370.67it/s]
Adding requests:  30%|██▉       | 610/2048 [00:01<00:04, 357.45it/s]
Adding requests:  32%|███▏      | 646/2048 [00:01<00:03, 354.69it/s]
Adding requests:  33%|███▎      | 682/2048 [00:01<00:03, 356.12it/s]
Adding requests:  35%|███▌      | 718/2048 [00:02<00:03, 357.24it/s]
Adding requests:  37%|███▋      | 754/2048 [00:02<00:03, 352.98it/s]
Adding requests:  39%|███▊      | 790/2048 [00:02<00:03, 349.22it/s]
Adding requests:  40%|████      | 826/2048 [00:02<00:03, 351.60it/s]
Adding requests:  42%|████▏     | 863/2048 [00:02<00:03, 356.42it/s]
Adding requests:  44%|████▍     | 901/2048 [00:02<00:03, 360.01it/s]
Adding requests:  46%|████▌     | 938/2048 [00:02<00:03, 355.25it/s]
Adding requests:  48%|████▊     | 974/2048 [00:02<00:03, 356.58it/s]
Adding requests:  49%|████▉     | 1010/2048 [00:02<00:02, 351.65it/s]
Adding requests:  51%|█████     | 1046/2048 [00:02<00:02, 342.48it/s]
Adding requests:  53%|█████▎    | 1082/2048 [00:03<00:02, 345.00it/s]
Adding requests:  55%|█████▍    | 1117/2048 [00:03<00:02, 343.75it/s]
Adding requests:  56%|█████▋    | 1154/2048 [00:03<00:02, 349.12it/s]
Adding requests:  58%|█████▊    | 1189/2048 [00:03<00:02, 349.17it/s]
Adding requests:  60%|█████▉    | 1227/2048 [00:03<00:02, 357.32it/s]
Adding requests:  62%|██████▏   | 1263/2048 [00:03<00:02, 354.71it/s]
Adding requests:  63%|██████▎   | 1299/2048 [00:03<00:02, 351.41it/s]
Adding requests:  65%|██████▌   | 1335/2048 [00:03<00:02, 352.36it/s]
Adding requests:  67%|██████▋   | 1373/2048 [00:03<00:01, 358.73it/s]
Adding requests:  69%|██████▉   | 1409/2048 [00:04<00:01, 354.08it/s]
Adding requests:  71%|███████   | 1445/2048 [00:04<00:01, 355.16it/s]
Adding requests:  72%|███████▏  | 1482/2048 [00:04<00:01, 357.17it/s]
Adding requests:  74%|███████▍  | 1519/2048 [00:04<00:01, 358.23it/s]
Adding requests:  76%|███████▌  | 1555/2048 [00:04<00:01, 354.64it/s]
Adding requests:  78%|███████▊  | 1591/2048 [00:04<00:01, 351.63it/s]
Adding requests:  79%|███████▉  | 1627/2048 [00:04<00:01, 344.98it/s]
Adding requests:  81%|████████  | 1662/2048 [00:04<00:01, 330.56it/s]
Adding requests:  83%|████████▎ | 1697/2048 [00:04<00:01, 335.59it/s]
Adding requests:  85%|████████▍ | 1734/2048 [00:04<00:00, 344.48it/s]
Adding requests:  87%|████████▋ | 1772/2048 [00:05<00:00, 352.72it/s]
Adding requests:  88%|████████▊ | 1808/2048 [00:05<00:00, 349.26it/s]
Adding requests:  90%|█████████ | 1845/2048 [00:05<00:00, 354.90it/s]
Adding requests:  92%|█████████▏| 1881/2048 [00:05<00:00, 353.70it/s]
Adding requests:  94%|█████████▎| 1919/2048 [00:05<00:00, 358.31it/s]
Adding requests:  96%|█████████▌| 1957/2048 [00:05<00:00, 363.13it/s]
Adding requests:  97%|█████████▋| 1994/2048 [00:05<00:00, 355.20it/s]
Adding requests:  99%|█████████▉| 2030/2048 [00:05<00:00, 347.75it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 350.24it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  33%|███▎      | 684/2048 [00:00<00:00, 5003.25it/s, est. speed input: 5123659.25 toks/s, output: 5003.34 toks/s]
Processed prompts:  58%|█████▊    | 1185/2048 [00:04<00:03, 227.63it/s, est. speed input: 279245.47 toks/s, output: 272.70 toks/s]  
Processed prompts:  68%|██████▊   | 1399/2048 [00:06<00:03, 188.77it/s, est. speed input: 234878.32 toks/s, output: 229.37 toks/s]
Processed prompts:  74%|███████▍  | 1521/2048 [00:07<00:03, 170.34it/s, est. speed input: 217024.35 toks/s, output: 211.94 toks/s]
Processed prompts:  78%|███████▊  | 1599/2048 [00:07<00:02, 160.37it/s, est. speed input: 208452.20 toks/s, output: 203.57 toks/s]
Processed prompts:  81%|████████  | 1653/2048 [00:08<00:02, 157.00it/s, est. speed input: 205031.98 toks/s, output: 200.23 toks/s]
Processed prompts:  83%|████████▎ | 1694/2048 [00:08<00:02, 147.96it/s, est. speed input: 200245.09 toks/s, output: 195.55 toks/s]
Processed prompts:  84%|████████▍ | 1725/2048 [00:08<00:02, 143.37it/s, est. speed input: 197679.85 toks/s, output: 193.05 toks/s]
Processed prompts:  85%|████████▌ | 1750/2048 [00:09<00:02, 148.17it/s, est. speed input: 197836.55 toks/s, output: 193.20 toks/s]
Processed prompts:  87%|████████▋ | 1774/2048 [00:09<00:02, 136.72it/s, est. speed input: 194813.42 toks/s, output: 190.25 toks/s]
Processed prompts:  88%|████████▊ | 1793/2048 [00:09<00:01, 137.63it/s, est. speed input: 194157.02 toks/s, output: 189.61 toks/s]
Processed prompts:  88%|████████▊ | 1811/2048 [00:09<00:01, 136.58it/s, est. speed input: 193292.89 toks/s, output: 188.76 toks/s]
Processed prompts:  89%|████████▉ | 1828/2048 [00:09<00:01, 134.44it/s, est. speed input: 192372.04 toks/s, output: 187.86 toks/s]
Processed prompts:  90%|████████▉ | 1843/2048 [00:09<00:01, 129.78it/s, est. speed input: 191301.53 toks/s, output: 186.82 toks/s]
Processed prompts:  91%|█████████ | 1857/2048 [00:10<00:01, 123.45it/s, est. speed input: 190107.58 toks/s, output: 185.65 toks/s]
Processed prompts:  91%|█████████▏| 1870/2048 [00:10<00:01, 116.49it/s, est. speed input: 188853.61 toks/s, output: 184.43 toks/s]
Processed prompts:  92%|█████████▏| 1884/2048 [00:10<00:01, 114.08it/s, est. speed input: 187840.75 toks/s, output: 183.44 toks/s]
Processed prompts:  93%|█████████▎| 1900/2048 [00:10<00:01, 115.30it/s, est. speed input: 186980.00 toks/s, output: 182.60 toks/s]
Processed prompts:  94%|█████████▎| 1916/2048 [00:10<00:01, 116.50it/s, est. speed input: 186161.80 toks/s, output: 181.80 toks/s]
Processed prompts:  94%|█████████▍| 1932/2048 [00:10<00:00, 120.19it/s, est. speed input: 185556.30 toks/s, output: 181.21 toks/s]
Processed prompts:  95%|█████████▌| 1948/2048 [00:10<00:00, 119.23it/s, est. speed input: 184722.51 toks/s, output: 180.39 toks/s]
Processed prompts:  96%|█████████▌| 1964/2048 [00:10<00:00, 118.57it/s, est. speed input: 183910.95 toks/s, output: 179.60 toks/s]
Processed prompts:  97%|█████████▋| 1980/2048 [00:11<00:00, 119.65it/s, est. speed input: 183217.50 toks/s, output: 178.92 toks/s]
Processed prompts:  97%|█████████▋| 1996/2048 [00:11<00:00, 118.96it/s, est. speed input: 182450.17 toks/s, output: 178.17 toks/s]
Processed prompts:  98%|█████████▊| 2012/2048 [00:11<00:00, 118.49it/s, est. speed input: 181702.23 toks/s, output: 177.44 toks/s]
Processed prompts:  99%|█████████▉| 2028/2048 [00:11<00:00, 118.24it/s, est. speed input: 180976.47 toks/s, output: 176.73 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:11<00:00, 118.24it/s, est. speed input: 182004.63 toks/s, output: 177.74 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:11<00:00, 177.74it/s, est. speed input: 182004.63 toks/s, output: 177.74 toks/s]
[rank0]:[W126 15:45:53.816817591 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 68.8s

测试结果:
  Requests/s:   117.89
  Tokens/s:     120840.96
  Total Reqs:   2048
  Elapsed:      17.37s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     120723.07

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:46:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=703112) WARNING 01-26 15:46:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=703112) WARNING 01-26 15:46:53 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 122.94 requests/s, 126008.46 total tokens/s, 122.94 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 15:46:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:46:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:46:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:46:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:46:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:46:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:46:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:46:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:46:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:46:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:46:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:46:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:46:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:46:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:46:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:46:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:46:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:46:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:46:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:46:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:46:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:46:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:46:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:46:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:46:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:46:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:46:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:46:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=703112) [2026-01-26 15:46:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=703112) [2026-01-26 15:46:34] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=703112) [2026-01-26 15:46:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=703112) [2026-01-26 15:46:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=703112) [2026-01-26 15:46:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=703112) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=703112) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=703112) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.17it/s]
(EngineCore_DP0 pid=703112) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]
(EngineCore_DP0 pid=703112) 
(EngineCore_DP0 pid=703112) [rank0]:W0126 15:46:47.641000 703112 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=703112) [rank0]:W0126 15:46:47.855000 703112 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=703112) [rank0]:W0126 15:46:48.974000 703112 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=703112) [rank0]:W0126 15:46:49.106000 703112 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=703112) 2026-01-26 15:46:53,041 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=703112) 2026-01-26 15:46:53,070 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=703112) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 15.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 15.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 15.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.97it/s]
(EngineCore_DP0 pid=703112) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 13.66it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 14.22it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 15.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 14.94it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 275.66it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 317.36it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 314.43it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:12, 320.49it/s]
Adding requests:   4%|▍         | 163/4096 [00:00<00:11, 330.19it/s]
Adding requests:   5%|▍         | 199/4096 [00:00<00:11, 339.79it/s]
Adding requests:   6%|▌         | 235/4096 [00:00<00:11, 344.95it/s]
Adding requests:   7%|▋         | 270/4096 [00:00<00:11, 343.28it/s]
Adding requests:   7%|▋         | 306/4096 [00:00<00:10, 347.93it/s]
Adding requests:   8%|▊         | 343/4096 [00:01<00:10, 354.05it/s]
Adding requests:   9%|▉         | 380/4096 [00:01<00:10, 356.98it/s]
Adding requests:  10%|█         | 419/4096 [00:01<00:10, 364.94it/s]
Adding requests:  11%|█         | 456/4096 [00:01<00:10, 361.75it/s]
Adding requests:  12%|█▏        | 496/4096 [00:01<00:09, 371.29it/s]
Adding requests:  13%|█▎        | 535/4096 [00:01<00:09, 376.57it/s]
Adding requests:  14%|█▍        | 573/4096 [00:01<00:09, 368.47it/s]
Adding requests:  15%|█▍        | 610/4096 [00:01<00:09, 356.58it/s]
Adding requests:  16%|█▌        | 646/4096 [00:01<00:09, 355.04it/s]
Adding requests:  17%|█▋        | 682/4096 [00:01<00:09, 356.33it/s]
Adding requests:  18%|█▊        | 718/4096 [00:02<00:09, 356.74it/s]
Adding requests:  18%|█▊        | 754/4096 [00:02<00:09, 353.35it/s]
Adding requests:  19%|█▉        | 790/4096 [00:02<00:09, 354.06it/s]
Adding requests:  20%|██        | 827/4096 [00:02<00:09, 357.08it/s]
Adding requests:  21%|██        | 864/4096 [00:02<00:08, 359.82it/s]
Adding requests:  22%|██▏       | 901/4096 [00:02<00:08, 362.36it/s]
Adding requests:  23%|██▎       | 938/4096 [00:02<00:08, 358.29it/s]
Adding requests:  24%|██▍       | 974/4096 [00:02<00:08, 351.53it/s]
Adding requests:  25%|██▍       | 1010/4096 [00:02<00:08, 349.63it/s]
Adding requests:  26%|██▌       | 1046/4096 [00:02<00:08, 350.21it/s]
Adding requests:  26%|██▋       | 1082/4096 [00:03<00:08, 351.07it/s]
Adding requests:  27%|██▋       | 1118/4096 [00:03<00:08, 350.42it/s]
Adding requests:  28%|██▊       | 1154/4096 [00:03<00:08, 351.48it/s]
Adding requests:  29%|██▉       | 1190/4096 [00:03<00:08, 352.45it/s]
Adding requests:  30%|██▉       | 1228/4096 [00:03<00:07, 359.85it/s]
Adding requests:  31%|███       | 1264/4096 [00:03<00:07, 357.11it/s]
Adding requests:  32%|███▏      | 1300/4096 [00:03<00:07, 354.19it/s]
Adding requests:  33%|███▎      | 1336/4096 [00:03<00:08, 343.72it/s]
Adding requests:  34%|███▎      | 1373/4096 [00:03<00:07, 350.93it/s]
Adding requests:  34%|███▍      | 1409/4096 [00:04<00:07, 349.47it/s]
Adding requests:  35%|███▌      | 1445/4096 [00:04<00:07, 350.81it/s]
Adding requests:  36%|███▌      | 1482/4096 [00:04<00:07, 354.99it/s]
Adding requests:  37%|███▋      | 1519/4096 [00:04<00:07, 358.08it/s]
Adding requests:  38%|███▊      | 1555/4096 [00:04<00:07, 355.35it/s]
Adding requests:  39%|███▉      | 1591/4096 [00:04<00:07, 353.41it/s]
Adding requests:  40%|███▉      | 1627/4096 [00:04<00:07, 345.97it/s]
Adding requests:  41%|████      | 1662/4096 [00:04<00:07, 340.38it/s]
Adding requests:  41%|████▏     | 1698/4096 [00:04<00:06, 344.46it/s]
Adding requests:  42%|████▏     | 1735/4096 [00:04<00:06, 350.29it/s]
Adding requests:  43%|████▎     | 1773/4096 [00:05<00:06, 358.01it/s]
Adding requests:  44%|████▍     | 1809/4096 [00:05<00:06, 354.81it/s]
Adding requests:  45%|████▌     | 1846/4096 [00:05<00:06, 357.98it/s]
Adding requests:  46%|████▌     | 1883/4096 [00:05<00:06, 358.47it/s]
Adding requests:  47%|████▋     | 1920/4096 [00:05<00:06, 360.85it/s]
Adding requests:  48%|████▊     | 1959/4096 [00:05<00:05, 367.29it/s]
Adding requests:  49%|████▊     | 1996/4096 [00:05<00:05, 359.22it/s]
Adding requests:  50%|████▉     | 2032/4096 [00:05<00:05, 350.63it/s]
Adding requests:  50%|█████     | 2068/4096 [00:05<00:05, 348.03it/s]
Adding requests:  51%|█████▏    | 2103/4096 [00:05<00:05, 345.66it/s]
Adding requests:  52%|█████▏    | 2138/4096 [00:06<00:05, 340.78it/s]
Adding requests:  53%|█████▎    | 2173/4096 [00:06<00:05, 338.95it/s]
Adding requests:  54%|█████▍    | 2207/4096 [00:06<00:05, 338.88it/s]
Adding requests:  55%|█████▍    | 2243/4096 [00:06<00:05, 344.57it/s]
Adding requests:  56%|█████▌    | 2281/4096 [00:06<00:05, 352.84it/s]
Adding requests:  57%|█████▋    | 2318/4096 [00:06<00:05, 355.12it/s]
Adding requests:  58%|█████▊    | 2356/4096 [00:06<00:04, 360.53it/s]
Adding requests:  58%|█████▊    | 2394/4096 [00:06<00:04, 364.88it/s]
Adding requests:  59%|█████▉    | 2432/4096 [00:06<00:04, 368.55it/s]
Adding requests:  60%|██████    | 2469/4096 [00:06<00:04, 360.67it/s]
Adding requests:  61%|██████    | 2508/4096 [00:07<00:04, 369.14it/s]
Adding requests:  62%|██████▏   | 2546/4096 [00:07<00:04, 372.23it/s]
Adding requests:  63%|██████▎   | 2585/4096 [00:07<00:04, 376.51it/s]
Adding requests:  64%|██████▍   | 2623/4096 [00:07<00:03, 370.23it/s]
Adding requests:  65%|██████▍   | 2661/4096 [00:07<00:03, 362.26it/s]
Adding requests:  66%|██████▌   | 2698/4096 [00:07<00:03, 358.90it/s]
Adding requests:  67%|██████▋   | 2734/4096 [00:07<00:03, 357.09it/s]
Adding requests:  68%|██████▊   | 2773/4096 [00:07<00:03, 364.92it/s]
Adding requests:  69%|██████▊   | 2812/4096 [00:07<00:03, 367.62it/s]
Adding requests:  70%|██████▉   | 2850/4096 [00:08<00:03, 369.95it/s]
Adding requests:  71%|███████   | 2888/4096 [00:08<00:03, 365.38it/s]
Adding requests:  71%|███████▏  | 2925/4096 [00:08<00:03, 364.58it/s]
Adding requests:  72%|███████▏  | 2963/4096 [00:08<00:03, 368.13it/s]
Adding requests:  73%|███████▎  | 3000/4096 [00:08<00:02, 366.04it/s]
Adding requests:  74%|███████▍  | 3038/4096 [00:08<00:02, 370.02it/s]
Adding requests:  75%|███████▌  | 3076/4096 [00:08<00:02, 372.59it/s]
Adding requests:  76%|███████▌  | 3114/4096 [00:08<00:02, 372.44it/s]
Adding requests:  77%|███████▋  | 3152/4096 [00:08<00:02, 371.21it/s]
Adding requests:  78%|███████▊  | 3190/4096 [00:08<00:02, 367.66it/s]
Adding requests:  79%|███████▉  | 3227/4096 [00:09<00:02, 365.62it/s]
Adding requests:  80%|███████▉  | 3264/4096 [00:09<00:02, 363.78it/s]
Adding requests:  81%|████████  | 3301/4096 [00:09<00:02, 354.62it/s]
Adding requests:  81%|████████▏ | 3337/4096 [00:09<00:02, 353.39it/s]
Adding requests:  82%|████████▏ | 3375/4096 [00:09<00:02, 360.07it/s]
Adding requests:  83%|████████▎ | 3412/4096 [00:09<00:01, 352.34it/s]
Adding requests:  84%|████████▍ | 3449/4096 [00:09<00:01, 356.13it/s]
Adding requests:  85%|████████▌ | 3485/4096 [00:09<00:01, 354.59it/s]
Adding requests:  86%|████████▌ | 3523/4096 [00:09<00:01, 359.66it/s]
Adding requests:  87%|████████▋ | 3563/4096 [00:09<00:01, 370.00it/s]
Adding requests:  88%|████████▊ | 3601/4096 [00:10<00:01, 364.62it/s]
Adding requests:  89%|████████▉ | 3638/4096 [00:10<00:01, 365.60it/s]
Adding requests:  90%|████████▉ | 3675/4096 [00:10<00:01, 359.91it/s]
Adding requests:  91%|█████████ | 3712/4096 [00:10<00:01, 358.00it/s]
Adding requests:  92%|█████████▏| 3748/4096 [00:10<00:00, 356.80it/s]
Adding requests:  92%|█████████▏| 3784/4096 [00:10<00:00, 347.47it/s]
Adding requests:  93%|█████████▎| 3819/4096 [00:10<00:00, 341.28it/s]
Adding requests:  94%|█████████▍| 3855/4096 [00:10<00:00, 344.88it/s]
Adding requests:  95%|█████████▍| 3891/4096 [00:10<00:00, 346.73it/s]
Adding requests:  96%|█████████▌| 3926/4096 [00:11<00:00, 344.11it/s]
Adding requests:  97%|█████████▋| 3962/4096 [00:11<00:00, 346.11it/s]
Adding requests:  98%|█████████▊| 3997/4096 [00:11<00:00, 344.73it/s]
Adding requests:  98%|█████████▊| 4033/4096 [00:11<00:00, 348.37it/s]
Adding requests:  99%|█████████▉| 4068/4096 [00:11<00:00, 347.18it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 355.26it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▍      | 1414/4096 [00:00<00:00, 4986.50it/s, est. speed input: 5106332.47 toks/s, output: 4986.54 toks/s]
Processed prompts:  47%|████▋     | 1913/4096 [00:04<00:06, 363.76it/s, est. speed input: 468883.94 toks/s, output: 457.89 toks/s]   
Processed prompts:  52%|█████▏    | 2126/4096 [00:05<00:07, 268.46it/s, est. speed input: 363543.42 toks/s, output: 355.02 toks/s]
Processed prompts:  55%|█████▍    | 2248/4096 [00:07<00:07, 231.86it/s, est. speed input: 327391.07 toks/s, output: 319.72 toks/s]
Processed prompts:  57%|█████▋    | 2326/4096 [00:07<00:08, 219.73it/s, est. speed input: 315351.28 toks/s, output: 307.96 toks/s]
Processed prompts:  58%|█████▊    | 2382/4096 [00:08<00:08, 201.17it/s, est. speed input: 302626.25 toks/s, output: 295.53 toks/s]
Processed prompts:  59%|█████▉    | 2423/4096 [00:08<00:08, 196.39it/s, est. speed input: 298227.57 toks/s, output: 291.24 toks/s]
Processed prompts:  60%|█████▉    | 2456/4096 [00:08<00:08, 186.92it/s, est. speed input: 293130.41 toks/s, output: 286.26 toks/s]
Processed prompts:  61%|██████    | 2482/4096 [00:08<00:09, 173.36it/s, est. speed input: 287706.26 toks/s, output: 280.96 toks/s]
Processed prompts:  61%|██████    | 2504/4096 [00:09<00:10, 156.49it/s, est. speed input: 281943.51 toks/s, output: 275.34 toks/s]
Processed prompts:  62%|██████▏   | 2534/4096 [00:09<00:10, 147.51it/s, est. speed input: 277338.23 toks/s, output: 270.84 toks/s]
Processed prompts:  63%|██████▎   | 2566/4096 [00:09<00:10, 141.82it/s, est. speed input: 273247.42 toks/s, output: 266.84 toks/s]
Processed prompts:  63%|██████▎   | 2598/4096 [00:09<00:10, 137.11it/s, est. speed input: 269365.87 toks/s, output: 263.05 toks/s]
Processed prompts:  64%|██████▍   | 2630/4096 [00:10<00:11, 132.64it/s, est. speed input: 265536.62 toks/s, output: 259.31 toks/s]
Processed prompts:  65%|██████▍   | 2662/4096 [00:10<00:11, 130.16it/s, est. speed input: 262071.55 toks/s, output: 255.93 toks/s]
Processed prompts:  66%|██████▌   | 2694/4096 [00:10<00:10, 128.33it/s, est. speed input: 258780.30 toks/s, output: 252.71 toks/s]
Processed prompts:  67%|██████▋   | 2726/4096 [00:10<00:10, 126.54it/s, est. speed input: 255571.09 toks/s, output: 249.58 toks/s]
Processed prompts:  67%|██████▋   | 2758/4096 [00:11<00:10, 124.73it/s, est. speed input: 252428.31 toks/s, output: 246.51 toks/s]
Processed prompts:  68%|██████▊   | 2790/4096 [00:11<00:10, 124.51it/s, est. speed input: 249598.35 toks/s, output: 243.75 toks/s]
Processed prompts:  69%|██████▉   | 2822/4096 [00:11<00:10, 123.50it/s, est. speed input: 246765.57 toks/s, output: 240.98 toks/s]
Processed prompts:  70%|██████▉   | 2854/4096 [00:11<00:10, 123.07it/s, est. speed input: 244098.12 toks/s, output: 238.38 toks/s]
Processed prompts:  70%|███████   | 2886/4096 [00:12<00:09, 125.22it/s, est. speed input: 241886.27 toks/s, output: 236.22 toks/s]
Processed prompts:  71%|███████   | 2918/4096 [00:12<00:09, 124.45it/s, est. speed input: 239455.79 toks/s, output: 233.84 toks/s]
Processed prompts:  72%|███████▏  | 2950/4096 [00:12<00:09, 124.37it/s, est. speed input: 237183.73 toks/s, output: 231.62 toks/s]
Processed prompts:  73%|███████▎  | 2982/4096 [00:12<00:08, 124.06it/s, est. speed input: 234969.88 toks/s, output: 229.46 toks/s]
Processed prompts:  74%|███████▎  | 3014/4096 [00:13<00:08, 123.40it/s, est. speed input: 232788.13 toks/s, output: 227.33 toks/s]
Processed prompts:  74%|███████▍  | 3046/4096 [00:13<00:08, 123.08it/s, est. speed input: 230708.39 toks/s, output: 225.30 toks/s]
Processed prompts:  75%|███████▌  | 3078/4096 [00:13<00:08, 122.93it/s, est. speed input: 228715.69 toks/s, output: 223.36 toks/s]
Processed prompts:  76%|███████▌  | 3110/4096 [00:14<00:08, 122.97it/s, est. speed input: 226813.58 toks/s, output: 221.50 toks/s]
Processed prompts:  77%|███████▋  | 3142/4096 [00:14<00:07, 122.80it/s, est. speed input: 224959.00 toks/s, output: 219.69 toks/s]
Processed prompts:  77%|███████▋  | 3174/4096 [00:14<00:07, 123.28it/s, est. speed input: 223235.26 toks/s, output: 218.00 toks/s]
Processed prompts:  78%|███████▊  | 3206/4096 [00:14<00:07, 122.74it/s, est. speed input: 221479.27 toks/s, output: 216.29 toks/s]
Processed prompts:  79%|███████▉  | 3238/4096 [00:15<00:06, 123.00it/s, est. speed input: 219850.87 toks/s, output: 214.70 toks/s]
Processed prompts:  80%|███████▉  | 3270/4096 [00:15<00:06, 122.77it/s, est. speed input: 218235.21 toks/s, output: 213.12 toks/s]
Processed prompts:  81%|████████  | 3302/4096 [00:15<00:06, 122.45it/s, est. speed input: 216657.85 toks/s, output: 211.58 toks/s]
Processed prompts:  81%|████████▏ | 3334/4096 [00:15<00:06, 122.50it/s, est. speed input: 215159.96 toks/s, output: 210.12 toks/s]
Processed prompts:  82%|████████▏ | 3366/4096 [00:16<00:05, 122.41it/s, est. speed input: 213697.77 toks/s, output: 208.69 toks/s]
Processed prompts:  83%|████████▎ | 3398/4096 [00:16<00:05, 122.75it/s, est. speed input: 212320.12 toks/s, output: 207.34 toks/s]
Processed prompts:  84%|████████▎ | 3430/4096 [00:16<00:05, 122.05it/s, est. speed input: 210900.90 toks/s, output: 205.96 toks/s]
Processed prompts:  85%|████████▍ | 3462/4096 [00:16<00:05, 122.45it/s, est. speed input: 209604.25 toks/s, output: 204.69 toks/s]
Processed prompts:  85%|████████▌ | 3494/4096 [00:17<00:04, 122.07it/s, est. speed input: 208290.40 toks/s, output: 203.41 toks/s]
Processed prompts:  86%|████████▌ | 3526/4096 [00:17<00:04, 121.72it/s, est. speed input: 207008.57 toks/s, output: 202.16 toks/s]
Processed prompts:  87%|████████▋ | 3558/4096 [00:17<00:04, 123.56it/s, est. speed input: 205937.31 toks/s, output: 201.11 toks/s]
Processed prompts:  88%|████████▊ | 3590/4096 [00:17<00:04, 123.06it/s, est. speed input: 204751.30 toks/s, output: 199.95 toks/s]
Processed prompts:  88%|████████▊ | 3622/4096 [00:18<00:03, 122.84it/s, est. speed input: 203610.00 toks/s, output: 198.84 toks/s]
Processed prompts:  89%|████████▉ | 3654/4096 [00:18<00:03, 122.43it/s, est. speed input: 202480.87 toks/s, output: 197.74 toks/s]
Processed prompts:  90%|████████▉ | 3686/4096 [00:18<00:03, 123.34it/s, est. speed input: 201474.29 toks/s, output: 196.75 toks/s]
Processed prompts:  91%|█████████ | 3718/4096 [00:18<00:03, 123.27it/s, est. speed input: 200442.03 toks/s, output: 195.74 toks/s]
Processed prompts:  92%|█████████▏| 3750/4096 [00:19<00:02, 122.79it/s, est. speed input: 199406.83 toks/s, output: 194.73 toks/s]
Processed prompts:  92%|█████████▏| 3782/4096 [00:19<00:02, 122.36it/s, est. speed input: 198392.19 toks/s, output: 193.74 toks/s]
Processed prompts:  93%|█████████▎| 3814/4096 [00:19<00:02, 122.55it/s, est. speed input: 197439.14 toks/s, output: 192.81 toks/s]
Processed prompts:  94%|█████████▍| 3846/4096 [00:20<00:02, 122.80it/s, est. speed input: 196518.92 toks/s, output: 191.91 toks/s]
Processed prompts:  95%|█████████▍| 3878/4096 [00:20<00:01, 122.64it/s, est. speed input: 195599.80 toks/s, output: 191.02 toks/s]
Processed prompts:  95%|█████████▌| 3910/4096 [00:20<00:01, 123.74it/s, est. speed input: 194784.91 toks/s, output: 190.22 toks/s]
Processed prompts:  96%|█████████▌| 3942/4096 [00:20<00:01, 124.52it/s, est. speed input: 193989.32 toks/s, output: 189.44 toks/s]
Processed prompts:  97%|█████████▋| 3974/4096 [00:21<00:00, 123.92it/s, est. speed input: 193139.98 toks/s, output: 188.61 toks/s]
Processed prompts:  98%|█████████▊| 4006/4096 [00:21<00:00, 123.18it/s, est. speed input: 192291.30 toks/s, output: 187.78 toks/s]
Processed prompts:  99%|█████████▊| 4038/4096 [00:21<00:00, 125.41it/s, est. speed input: 191631.82 toks/s, output: 187.14 toks/s]
Processed prompts:  99%|█████████▉| 4070/4096 [00:21<00:00, 132.64it/s, est. speed input: 191299.24 toks/s, output: 186.82 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:21<00:00, 132.64it/s, est. speed input: 192518.08 toks/s, output: 188.01 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:21<00:00, 188.01it/s, est. speed input: 192518.08 toks/s, output: 188.01 toks/s]
[rank0]:[W126 15:47:29.877938916 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 96.2s

测试结果:
  Requests/s:   122.94
  Tokens/s:     126008.46
  Total Reqs:   4096
  Elapsed:      33.32s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     125885.52

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:48:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=705293) WARNING 01-26 15:48:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     def forward(
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     raise e
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/tmp/torchinductor_root/3h/c3hb6ft3ydjsx5wj4xik4ljautqwlqc2wdoeiif6e7io5zuxpdxl.py", line 1090, in call
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     triton_poi_fused_mul_quant_only_int8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_only_int8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     torch.cuda.synchronize()
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866] Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=705293) ERROR 01-26 15:48:49 [core.py:866] 


─── STDERR ───
[2026-01-26 15:48:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:48:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:48:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:48:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:48:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:48:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:48:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:48:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:48:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:48:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:48:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:48:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:48:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:48:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:48:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:48:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=705293) [2026-01-26 15:48:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=705293) [2026-01-26 15:48:34] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=705293) [2026-01-26 15:48:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=705293) [2026-01-26 15:48:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=705293) [2026-01-26 15:48:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=705293) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=705293) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.50it/s]
(EngineCore_DP0 pid=705293) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.22it/s]
(EngineCore_DP0 pid=705293) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.25it/s]
(EngineCore_DP0 pid=705293) 
(EngineCore_DP0 pid=705293) [rank0]:W0126 15:48:47.665000 705293 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=705293) [rank0]:W0126 15:48:47.907000 705293 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=705293) [2026-01-26 15:48:48] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=4608, K=3584), falling back to default heuristic
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=705293) [rank0]:W0126 15:48:49.069000 705293 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=705293) [rank0]:W0126 15:48:49.191000 705293 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=705293) [2026-01-26 15:48:49] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3584, K=3584), falling back to default heuristic
(EngineCore_DP0 pid=705293) [2026-01-26 15:48:49] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=37888, K=3584), falling back to default heuristic
(EngineCore_DP0 pid=705293) Process EngineCore_DP0:
(EngineCore_DP0 pid=705293) Traceback (most recent call last):
(EngineCore_DP0 pid=705293)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=705293)     self.run()
(EngineCore_DP0 pid=705293)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=705293)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=705293)     raise e
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=705293)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=705293)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=705293)     super().__init__(
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=705293)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=705293)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=705293)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=705293)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=705293)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=705293)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=705293)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=705293)     return func(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=705293)     return func(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=705293)     self.model_runner.profile_run()
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=705293)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=705293)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=705293)     return func(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=705293)     outputs = self.model(
(EngineCore_DP0 pid=705293)               ^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=705293)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=705293)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=705293)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=705293)     hidden_states = self.model(
(EngineCore_DP0 pid=705293)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=705293)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=705293)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=705293)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=705293)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=705293)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=705293)     def forward(
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=705293)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=705293)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=705293)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=705293)     raise e
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=705293)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=705293)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=705293)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=705293)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=705293)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=705293)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=705293)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=705293)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=705293)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=705293)     return compiled_fn(full_args)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=705293)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=705293)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=705293)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=705293)                             ^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=705293)     outs = compiled_fn(args)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=705293)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=705293)     return self.current_callable(inputs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=705293)     out = model(new_inputs)
(EngineCore_DP0 pid=705293)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/tmp/torchinductor_root/3h/c3hb6ft3ydjsx5wj4xik4ljautqwlqc2wdoeiif6e7io5zuxpdxl.py", line 1090, in call
(EngineCore_DP0 pid=705293)     triton_poi_fused_mul_quant_only_int8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_only_int8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=705293)     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=705293)     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=705293)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=705293)     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=705293)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=705293)     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=705293)     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=705293)     torch.cuda.synchronize()
(EngineCore_DP0 pid=705293)   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=705293)     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=705293)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=705293) torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=705293) Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=705293) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=705293) For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=705293) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=705293) 
[rank0]:[W126 15:48:50.603512668 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,33.2126,17038.0459,3.8540
1024,1024,1,128,128,34.8963,35768.7575,3.6680
2048,1024,2,256,128,70.4694,72231.1029,3.6328
4096,1024,4,512,128,108.7690,111488.1853,4.7072
8192,1024,8,1024,128,117.7747,120719.0257,8.6946
16384,1024,16,2048,128,117.8936,120840.9612,17.3716
32768,1024,32,4096,128,122.9351,126008.4555,33.3184
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:48:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=706317) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=706317) WARNING 01-26 15:49:15 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=706317) WARNING 01-26 15:49:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.39 requests/s, 19180.61 total tokens/s, 37.39 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 15:48:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:48:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:48:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:48:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:48:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:48:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:48:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:48:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:49:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:49:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:49:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:49:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:49:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:49:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:49:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:49:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:07] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=706317) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=706317) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=706317) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=706317) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]
(EngineCore_DP0 pid=706317) 
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=706317) 2026-01-26 15:49:26,114 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=706317) 2026-01-26 15:49:26,137 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=706317) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=706317) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.81it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  40%|███▉      | 51/128 [00:00<00:00, 507.21it/s]
Adding requests:  87%|████████▋ | 111/128 [00:00<00:00, 548.19it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 549.17it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 54.65it/s, est. speed input: 27981.46 toks/s, output: 54.65 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:02, 44.52it/s, est. speed input: 23446.53 toks/s, output: 45.79 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 42.28it/s, est. speed input: 22386.95 toks/s, output: 43.72 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 41.18it/s, est. speed input: 21847.36 toks/s, output: 42.67 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 40.54it/s, est. speed input: 21514.61 toks/s, output: 42.02 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 40.26it/s, est. speed input: 21319.31 toks/s, output: 41.64 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 40.07it/s, est. speed input: 21177.67 toks/s, output: 41.36 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 39.93it/s, est. speed input: 21068.92 toks/s, output: 41.15 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 39.82it/s, est. speed input: 20992.92 toks/s, output: 41.00 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:01, 39.69it/s, est. speed input: 20922.83 toks/s, output: 40.86 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 39.68it/s, est. speed input: 20875.23 toks/s, output: 40.77 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 39.61it/s, est. speed input: 20827.38 toks/s, output: 40.68 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 39.53it/s, est. speed input: 20781.25 toks/s, output: 40.59 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 39.55it/s, est. speed input: 20749.67 toks/s, output: 40.53 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 39.52it/s, est. speed input: 20717.85 toks/s, output: 40.46 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 39.55it/s, est. speed input: 20693.85 toks/s, output: 40.42 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 39.57it/s, est. speed input: 20672.41 toks/s, output: 40.38 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 39.58it/s, est. speed input: 20653.13 toks/s, output: 40.34 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 39.59it/s, est. speed input: 20635.37 toks/s, output: 40.30 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:00, 39.64it/s, est. speed input: 20622.63 toks/s, output: 40.28 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 39.66it/s, est. speed input: 20609.73 toks/s, output: 40.25 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 39.67it/s, est. speed input: 20598.45 toks/s, output: 40.23 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 39.73it/s, est. speed input: 20590.57 toks/s, output: 40.22 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 39.73it/s, est. speed input: 20581.23 toks/s, output: 40.20 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 39.72it/s, est. speed input: 20572.12 toks/s, output: 40.18 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 39.79it/s, est. speed input: 20567.90 toks/s, output: 40.17 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 39.80it/s, est. speed input: 20561.62 toks/s, output: 40.16 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 39.83it/s, est. speed input: 20557.40 toks/s, output: 40.15 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 39.84it/s, est. speed input: 20552.53 toks/s, output: 40.14 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.84it/s, est. speed input: 20548.96 toks/s, output: 40.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.13it/s, est. speed input: 20548.96 toks/s, output: 40.13 toks/s]
[rank0]:[W126 15:49:32.594181079 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.1s

测试结果:
  Requests/s:   37.39
  Tokens/s:     19180.61
  Total Reqs:   128
  Elapsed:      3.42s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     19143.22

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:49:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=707578) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=707578) WARNING 01-26 15:49:58 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=707578) WARNING 01-26 15:50:07 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 38.76 requests/s, 39726.12 total tokens/s, 38.76 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 15:49:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:49:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:49:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:49:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:49:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:49:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:49:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:49:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:49:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:49:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:49:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:49:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:49:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:49:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:49:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:49:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:49] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:49] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:49] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:49] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:49] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=707578) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=707578) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.41it/s]
(EngineCore_DP0 pid=707578) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.27it/s]
(EngineCore_DP0 pid=707578) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]
(EngineCore_DP0 pid=707578) 
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=707578) 2026-01-26 15:50:07,914 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=707578) 2026-01-26 15:50:07,937 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=707578) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.70it/s]
(EngineCore_DP0 pid=707578) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.06it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 277.61it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 319.42it/s]
Adding requests:  75%|███████▌  | 96/128 [00:00<00:00, 316.11it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 316.17it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:00, 160.69it/s, est. speed input: 164560.94 toks/s, output: 160.70 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:01, 58.19it/s, est. speed input: 66088.10 toks/s, output: 64.54 toks/s]   
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 51.02it/s, est. speed input: 58608.36 toks/s, output: 57.23 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:01, 47.52it/s, est. speed input: 55201.56 toks/s, output: 53.91 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 45.38it/s, est. speed input: 53179.66 toks/s, output: 51.93 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 43.75it/s, est. speed input: 51633.11 toks/s, output: 50.42 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 42.73it/s, est. speed input: 50623.37 toks/s, output: 49.44 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 41.92it/s, est. speed input: 49779.85 toks/s, output: 48.61 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 41.22it/s, est. speed input: 49041.06 toks/s, output: 47.89 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:01, 40.72it/s, est. speed input: 48416.34 toks/s, output: 47.28 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 40.36it/s, est. speed input: 47876.73 toks/s, output: 46.75 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 40.09it/s, est. speed input: 47404.43 toks/s, output: 46.29 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 39.91it/s, est. speed input: 46988.74 toks/s, output: 45.89 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 39.82it/s, est. speed input: 46695.62 toks/s, output: 45.60 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 39.67it/s, est. speed input: 46412.23 toks/s, output: 45.32 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 39.54it/s, est. speed input: 46150.21 toks/s, output: 45.07 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 39.50it/s, est. speed input: 45917.75 toks/s, output: 44.84 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 39.45it/s, est. speed input: 45701.22 toks/s, output: 44.63 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 39.47it/s, est. speed input: 45507.67 toks/s, output: 44.44 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 39.47it/s, est. speed input: 45327.26 toks/s, output: 44.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 39.47it/s, est. speed input: 45249.71 toks/s, output: 44.19 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.19it/s, est. speed input: 45249.71 toks/s, output: 44.19 toks/s]
[rank0]:[W126 15:50:12.151080401 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.4s

测试结果:
  Requests/s:   38.76
  Tokens/s:     39726.12
  Total Reqs:   128
  Elapsed:      3.30s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     39687.37

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:50:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=708708) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=708708) WARNING 01-26 15:50:38 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=708708) WARNING 01-26 15:50:48 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 72.45 requests/s, 74257.96 total tokens/s, 72.45 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 15:50:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:50:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:50:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:50:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:50:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:50:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:50:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:50:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:50:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:50:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:50:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:50:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:50:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:50:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:50:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:50:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:30] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=708708) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=708708) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.56it/s]
(EngineCore_DP0 pid=708708) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]
(EngineCore_DP0 pid=708708) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.36it/s]
(EngineCore_DP0 pid=708708) 
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=708708) 2026-01-26 15:50:48,244 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=708708) 2026-01-26 15:50:48,267 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=708708) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 12.65it/s]
(EngineCore_DP0 pid=708708) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.72it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.71it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<01:01,  4.13it/s]
Adding requests:  11%|█▏        | 29/256 [00:00<00:02, 106.09it/s]
Adding requests:  23%|██▎       | 60/256 [00:00<00:01, 176.49it/s]
Adding requests:  35%|███▌      | 90/256 [00:00<00:00, 217.01it/s]
Adding requests:  48%|████▊     | 123/256 [00:00<00:00, 252.25it/s]
Adding requests:  61%|██████▏   | 157/256 [00:00<00:00, 278.11it/s]
Adding requests:  76%|███████▌  | 194/256 [00:00<00:00, 303.22it/s]
Adding requests:  90%|████████▉ | 230/256 [00:00<00:00, 319.19it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 248.61it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:00<00:00, 516.28it/s, est. speed input: 528694.73 toks/s, output: 516.28 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:00<00:01, 132.97it/s, est. speed input: 155243.96 toks/s, output: 151.60 toks/s]
Processed prompts:  56%|█████▌    | 143/256 [00:01<00:00, 114.18it/s, est. speed input: 134896.60 toks/s, output: 131.73 toks/s]
Processed prompts:  63%|██████▎   | 161/256 [00:01<00:00, 105.03it/s, est. speed input: 126091.00 toks/s, output: 123.14 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:01<00:00, 97.48it/s, est. speed input: 119759.98 toks/s, output: 116.95 toks/s] 
Processed prompts:  73%|███████▎  | 188/256 [00:01<00:00, 93.82it/s, est. speed input: 116425.10 toks/s, output: 113.70 toks/s]
Processed prompts:  78%|███████▊  | 199/256 [00:01<00:00, 92.85it/s, est. speed input: 114675.27 toks/s, output: 111.99 toks/s]
Processed prompts:  82%|████████▏ | 209/256 [00:01<00:00, 88.66it/s, est. speed input: 112072.23 toks/s, output: 109.45 toks/s]
Processed prompts:  86%|████████▌ | 219/256 [00:02<00:00, 86.80it/s, est. speed input: 110300.53 toks/s, output: 107.72 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 83.30it/s, est. speed input: 108280.99 toks/s, output: 105.74 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:02<00:00, 82.58it/s, est. speed input: 106881.63 toks/s, output: 104.38 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:02<00:00, 82.06it/s, est. speed input: 105633.68 toks/s, output: 103.16 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 82.06it/s, est. speed input: 104747.69 toks/s, output: 102.29 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.29it/s, est. speed input: 104747.69 toks/s, output: 102.29 toks/s]
[rank0]:[W126 15:50:53.906697463 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.4s

测试结果:
  Requests/s:   72.45
  Tokens/s:     74257.96
  Total Reqs:   256
  Elapsed:      3.53s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     74185.51

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:51:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=709857) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=709857) WARNING 01-26 15:51:20 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=709857) WARNING 01-26 15:51:30 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 138.43 requests/s, 141893.64 total tokens/s, 138.43 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 15:51:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:51:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:51:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:51:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:51:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:51:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:51:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:51:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:51:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:51:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:51:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:51:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:12] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=709857) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=709857) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.73it/s]
(EngineCore_DP0 pid=709857) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=709857) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.42it/s]
(EngineCore_DP0 pid=709857) 
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=709857) 2026-01-26 15:51:30,897 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=709857) 2026-01-26 15:51:30,920 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=709857) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 13.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.80it/s]
(EngineCore_DP0 pid=709857) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 18.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 18.25it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 277.62it/s]
Adding requests:  12%|█▏        | 63/512 [00:00<00:01, 318.95it/s]
Adding requests:  19%|█▊        | 95/512 [00:00<00:01, 310.17it/s]
Adding requests:  25%|██▌       | 128/512 [00:00<00:01, 317.68it/s]
Adding requests:  32%|███▏      | 163/512 [00:00<00:01, 327.19it/s]
Adding requests:  39%|███▉      | 199/512 [00:00<00:00, 337.85it/s]
Adding requests:  46%|████▌     | 235/512 [00:00<00:00, 344.14it/s]
Adding requests:  53%|█████▎    | 270/512 [00:00<00:00, 342.35it/s]
Adding requests:  60%|█████▉    | 306/512 [00:00<00:00, 347.27it/s]
Adding requests:  67%|██████▋   | 343/512 [00:01<00:00, 353.79it/s]
Adding requests:  74%|███████▍  | 380/512 [00:01<00:00, 357.30it/s]
Adding requests:  82%|████████▏ | 419/512 [00:01<00:00, 365.73it/s]
Adding requests:  89%|████████▉ | 456/512 [00:01<00:00, 361.51it/s]
Adding requests:  97%|█████████▋| 496/512 [00:01<00:00, 371.23it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 348.51it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  41%|████      | 210/512 [00:00<00:00, 1810.26it/s, est. speed input: 1853820.78 toks/s, output: 1810.29 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:01<00:00, 244.67it/s, est. speed input: 290987.55 toks/s, output: 284.17 toks/s]   
Processed prompts:  93%|█████████▎| 476/512 [00:01<00:00, 205.52it/s, est. speed input: 247735.76 toks/s, output: 241.93 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 205.52it/s, est. speed input: 235291.44 toks/s, output: 229.78 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 229.77it/s, est. speed input: 235291.44 toks/s, output: 229.78 toks/s]
[rank0]:[W126 15:51:36.136067208 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.3s

测试结果:
  Requests/s:   138.43
  Tokens/s:     141893.64
  Total Reqs:   512
  Elapsed:      3.70s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     141755.21

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:51:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=711057) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=711057) WARNING 01-26 15:52:07 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=711057) WARNING 01-26 15:52:17 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 153.89 requests/s, 157735.61 total tokens/s, 153.89 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 15:51:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:51:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:51:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:51:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:51:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:51:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:51:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:51:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:51:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:51:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:51:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:51:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=711057) [2026-01-26 15:51:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=711057) [2026-01-26 15:51:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=711057) [2026-01-26 15:51:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=711057) [2026-01-26 15:51:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=711057) [2026-01-26 15:51:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=711057) [2026-01-26 15:51:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=711057) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=711057) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.74it/s]
(EngineCore_DP0 pid=711057) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=711057) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.43it/s]
(EngineCore_DP0 pid=711057) 
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=711057) 2026-01-26 15:52:17,609 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=711057) 2026-01-26 15:52:17,633 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=711057) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 16.05it/s]
(EngineCore_DP0 pid=711057) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 18.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.45it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 277.66it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 319.03it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 315.94it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 321.07it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 329.80it/s]
Adding requests:  19%|█▉        | 199/1024 [00:00<00:02, 339.71it/s]
Adding requests:  23%|██▎       | 235/1024 [00:00<00:02, 345.39it/s]
Adding requests:  26%|██▋       | 270/1024 [00:00<00:02, 343.97it/s]
Adding requests:  30%|██▉       | 307/1024 [00:00<00:02, 348.72it/s]
Adding requests:  34%|███▎      | 345/1024 [00:01<00:01, 356.87it/s]
Adding requests:  37%|███▋      | 382/1024 [00:01<00:01, 358.28it/s]
Adding requests:  41%|████      | 420/1024 [00:01<00:01, 364.46it/s]
Adding requests:  45%|████▍     | 457/1024 [00:01<00:01, 361.11it/s]
Adding requests:  49%|████▊     | 497/1024 [00:01<00:01, 371.65it/s]
Adding requests:  52%|█████▏    | 536/1024 [00:01<00:01, 376.28it/s]
Adding requests:  56%|█████▌    | 574/1024 [00:01<00:01, 373.43it/s]
Adding requests:  60%|█████▉    | 612/1024 [00:01<00:01, 360.64it/s]
Adding requests:  63%|██████▎   | 649/1024 [00:01<00:01, 356.00it/s]
Adding requests:  67%|██████▋   | 686/1024 [00:01<00:00, 357.90it/s]
Adding requests:  71%|███████   | 722/1024 [00:02<00:00, 356.83it/s]
Adding requests:  74%|███████▍  | 758/1024 [00:02<00:00, 354.85it/s]
Adding requests:  78%|███████▊  | 794/1024 [00:02<00:00, 352.97it/s]
Adding requests:  81%|████████▏ | 832/1024 [00:02<00:00, 359.60it/s]
Adding requests:  85%|████████▍ | 868/1024 [00:02<00:00, 358.26it/s]
Adding requests:  89%|████████▊ | 907/1024 [00:02<00:00, 363.36it/s]
Adding requests:  92%|█████████▏| 944/1024 [00:02<00:00, 356.39it/s]
Adding requests:  96%|█████████▌| 980/1024 [00:02<00:00, 356.47it/s]
Adding requests:  99%|█████████▉| 1016/1024 [00:02<00:00, 353.42it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 353.23it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:00<00:00, 3512.10it/s, est. speed input: 3596598.89 toks/s, output: 3512.14 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:02<00:00, 287.30it/s, est. speed input: 348471.86 toks/s, output: 340.30 toks/s]   
Processed prompts:  94%|█████████▍| 964/1024 [00:03<00:00, 239.11it/s, est. speed input: 294269.74 toks/s, output: 287.37 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:03<00:00, 239.11it/s, est. speed input: 279320.41 toks/s, output: 272.77 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:03<00:00, 272.77it/s, est. speed input: 279320.41 toks/s, output: 272.77 toks/s]
[rank0]:[W126 15:52:26.684856732 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.6s

测试结果:
  Requests/s:   153.89
  Tokens/s:     157735.61
  Total Reqs:   1024
  Elapsed:      6.65s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     157581.72

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:52:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=712381) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=712381) WARNING 01-26 15:53:03 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=712381) WARNING 01-26 15:53:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 162.52 requests/s, 166584.17 total tokens/s, 162.52 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 15:52:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:52:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:52:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:52:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:52:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:52:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:52:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:52:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:52:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:52:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:52:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:52:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:52:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:52:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:52:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:52:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:55] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:55] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:55] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:55] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=712381) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=712381) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.75it/s]
(EngineCore_DP0 pid=712381) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=712381) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.43it/s]
(EngineCore_DP0 pid=712381) 
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=712381) [rank0]:W0126 15:53:08.220000 712381 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=712381) [rank0]:W0126 15:53:08.303000 712381 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=712381) [rank0]:W0126 15:53:09.264000 712381 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=712381) [rank0]:W0126 15:53:09.393000 712381 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=712381) 2026-01-26 15:53:13,173 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=712381) 2026-01-26 15:53:13,200 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=712381) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 14.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 14.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 13.88it/s]
(EngineCore_DP0 pid=712381) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.01it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.21it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 26/2048 [00:00<00:08, 251.65it/s]
Adding requests:   3%|▎         | 60/2048 [00:00<00:06, 301.85it/s]
Adding requests:   4%|▍         | 92/2048 [00:00<00:06, 307.74it/s]
Adding requests:   6%|▌         | 126/2048 [00:00<00:06, 319.91it/s]
Adding requests:   8%|▊         | 161/2048 [00:00<00:05, 328.90it/s]
Adding requests:  10%|▉         | 198/2048 [00:00<00:05, 340.82it/s]
Adding requests:  11%|█▏        | 233/2048 [00:00<00:05, 341.73it/s]
Adding requests:  13%|█▎        | 268/2048 [00:00<00:05, 342.68it/s]
Adding requests:  15%|█▍        | 304/2048 [00:00<00:05, 347.88it/s]
Adding requests:  17%|█▋        | 342/2048 [00:01<00:04, 354.98it/s]
Adding requests:  19%|█▊        | 379/2048 [00:01<00:04, 358.30it/s]
Adding requests:  20%|██        | 418/2048 [00:01<00:04, 366.79it/s]
Adding requests:  22%|██▏       | 455/2048 [00:01<00:04, 362.25it/s]
Adding requests:  24%|██▍       | 495/2048 [00:01<00:04, 372.75it/s]
Adding requests:  26%|██▌       | 534/2048 [00:01<00:04, 376.99it/s]
Adding requests:  28%|██▊       | 572/2048 [00:01<00:03, 374.81it/s]
Adding requests:  30%|██▉       | 610/2048 [00:01<00:03, 361.12it/s]
Adding requests:  32%|███▏      | 647/2048 [00:01<00:04, 349.20it/s]
Adding requests:  33%|███▎      | 684/2048 [00:01<00:03, 352.64it/s]
Adding requests:  35%|███▌      | 720/2048 [00:02<00:03, 352.33it/s]
Adding requests:  37%|███▋      | 756/2048 [00:02<00:03, 351.48it/s]
Adding requests:  39%|███▊      | 792/2048 [00:02<00:03, 352.63it/s]
Adding requests:  41%|████      | 830/2048 [00:02<00:03, 359.45it/s]
Adding requests:  42%|████▏     | 867/2048 [00:02<00:03, 360.12it/s]
Adding requests:  44%|████▍     | 905/2048 [00:02<00:03, 364.60it/s]
Adding requests:  46%|████▌     | 942/2048 [00:02<00:03, 357.18it/s]
Adding requests:  48%|████▊     | 978/2048 [00:02<00:02, 357.60it/s]
Adding requests:  50%|████▉     | 1014/2048 [00:02<00:02, 352.15it/s]
Adding requests:  51%|█████▏    | 1050/2048 [00:02<00:02, 353.01it/s]
Adding requests:  53%|█████▎    | 1086/2048 [00:03<00:02, 352.60it/s]
Adding requests:  55%|█████▍    | 1122/2048 [00:03<00:02, 353.33it/s]
Adding requests:  57%|█████▋    | 1158/2048 [00:03<00:02, 353.57it/s]
Adding requests:  58%|█████▊    | 1194/2048 [00:03<00:02, 354.73it/s]
Adding requests:  60%|██████    | 1232/2048 [00:03<00:02, 360.06it/s]
Adding requests:  62%|██████▏   | 1269/2048 [00:03<00:02, 356.47it/s]
Adding requests:  64%|██████▎   | 1305/2048 [00:03<00:02, 355.64it/s]
Adding requests:  65%|██████▌   | 1341/2048 [00:03<00:01, 355.45it/s]
Adding requests:  67%|██████▋   | 1379/2048 [00:03<00:01, 360.06it/s]
Adding requests:  69%|██████▉   | 1416/2048 [00:04<00:01, 348.63it/s]
Adding requests:  71%|███████   | 1452/2048 [00:04<00:01, 350.19it/s]
Adding requests:  73%|███████▎  | 1490/2048 [00:04<00:01, 356.47it/s]
Adding requests:  75%|███████▍  | 1527/2048 [00:04<00:01, 357.31it/s]
Adding requests:  76%|███████▋  | 1563/2048 [00:04<00:01, 353.03it/s]
Adding requests:  78%|███████▊  | 1599/2048 [00:04<00:01, 349.03it/s]
Adding requests:  80%|███████▉  | 1634/2048 [00:04<00:01, 345.25it/s]
Adding requests:  81%|████████▏ | 1669/2048 [00:04<00:01, 339.41it/s]
Adding requests:  83%|████████▎ | 1706/2048 [00:04<00:00, 346.49it/s]
Adding requests:  85%|████████▌ | 1741/2048 [00:04<00:00, 340.87it/s]
Adding requests:  87%|████████▋ | 1779/2048 [00:05<00:00, 350.20it/s]
Adding requests:  89%|████████▊ | 1815/2048 [00:05<00:00, 348.41it/s]
Adding requests:  90%|█████████ | 1852/2048 [00:05<00:00, 351.74it/s]
Adding requests:  92%|█████████▏| 1889/2048 [00:05<00:00, 356.04it/s]
Adding requests:  94%|█████████▍| 1926/2048 [00:05<00:00, 359.19it/s]
Adding requests:  96%|█████████▌| 1964/2048 [00:05<00:00, 362.50it/s]
Adding requests:  98%|█████████▊| 2001/2048 [00:05<00:00, 356.73it/s]
Adding requests:  99%|█████████▉| 2037/2048 [00:05<00:00, 349.50it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 352.12it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  46%|████▌     | 940/2048 [00:00<00:00, 8821.07it/s, est. speed input: 9033781.58 toks/s, output: 8821.34 toks/s]
Processed prompts:  89%|████████▉ | 1823/2048 [00:05<00:00, 283.41it/s, est. speed input: 341319.01 toks/s, output: 333.32 toks/s]  
Processed prompts: 100%|██████████| 2048/2048 [00:06<00:00, 283.41it/s, est. speed input: 309149.01 toks/s, output: 301.90 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:06<00:00, 301.90it/s, est. speed input: 309149.01 toks/s, output: 301.90 toks/s]
[rank0]:[W126 15:53:28.625790482 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 62.0s

测试结果:
  Requests/s:   162.52
  Tokens/s:     166584.17
  Total Reqs:   2048
  Elapsed:      12.60s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     166421.65

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:54:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=713954) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=713954) WARNING 01-26 15:54:17 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=713954) WARNING 01-26 15:54:27 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 166.22 requests/s, 170373.20 total tokens/s, 166.22 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 15:54:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:54:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:54:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:54:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:54:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:54:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:54:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:54:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:54:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:54:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:54:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:54:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:54:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:54:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:54:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:54:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=713954) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=713954) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.74it/s]
(EngineCore_DP0 pid=713954) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=713954) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.43it/s]
(EngineCore_DP0 pid=713954) 
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=713954) [rank0]:W0126 15:54:22.014000 713954 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=713954) [rank0]:W0126 15:54:22.097000 713954 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=713954) [rank0]:W0126 15:54:23.065000 713954 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=713954) [rank0]:W0126 15:54:23.196000 713954 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=713954) 2026-01-26 15:54:27,093 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=713954) 2026-01-26 15:54:27,119 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=713954) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 16.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 15.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 16.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 15.11it/s]
(EngineCore_DP0 pid=713954) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.90it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.10it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 18.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.25it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 274.94it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 317.08it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 314.67it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:12, 320.21it/s]
Adding requests:   4%|▍         | 163/4096 [00:00<00:11, 329.91it/s]
Adding requests:   5%|▍         | 199/4096 [00:00<00:11, 337.46it/s]
Adding requests:   6%|▌         | 235/4096 [00:00<00:11, 343.33it/s]
Adding requests:   7%|▋         | 270/4096 [00:00<00:11, 341.92it/s]
Adding requests:   7%|▋         | 306/4096 [00:00<00:10, 345.81it/s]
Adding requests:   8%|▊         | 344/4096 [00:01<00:10, 354.03it/s]
Adding requests:   9%|▉         | 381/4096 [00:01<00:10, 356.46it/s]
Adding requests:  10%|█         | 420/4096 [00:01<00:10, 364.28it/s]
Adding requests:  11%|█         | 457/4096 [00:01<00:10, 359.98it/s]
Adding requests:  12%|█▏        | 497/4096 [00:01<00:09, 369.65it/s]
Adding requests:  13%|█▎        | 536/4096 [00:01<00:09, 375.27it/s]
Adding requests:  14%|█▍        | 574/4096 [00:01<00:09, 373.41it/s]
Adding requests:  15%|█▍        | 612/4096 [00:01<00:09, 360.42it/s]
Adding requests:  16%|█▌        | 649/4096 [00:01<00:09, 355.89it/s]
Adding requests:  17%|█▋        | 686/4096 [00:01<00:09, 357.52it/s]
Adding requests:  18%|█▊        | 722/4096 [00:02<00:09, 356.67it/s]
Adding requests:  19%|█▊        | 758/4096 [00:02<00:09, 354.45it/s]
Adding requests:  19%|█▉        | 794/4096 [00:02<00:09, 352.97it/s]
Adding requests:  20%|██        | 832/4096 [00:02<00:09, 359.53it/s]
Adding requests:  21%|██        | 868/4096 [00:02<00:09, 353.89it/s]
Adding requests:  22%|██▏       | 907/4096 [00:02<00:08, 360.15it/s]
Adding requests:  23%|██▎       | 944/4096 [00:02<00:08, 353.96it/s]
Adding requests:  24%|██▍       | 980/4096 [00:02<00:08, 354.16it/s]
Adding requests:  25%|██▍       | 1016/4096 [00:02<00:08, 351.42it/s]
Adding requests:  26%|██▌       | 1052/4096 [00:02<00:08, 350.99it/s]
Adding requests:  27%|██▋       | 1088/4096 [00:03<00:08, 349.53it/s]
Adding requests:  27%|██▋       | 1125/4096 [00:03<00:08, 353.96it/s]
Adding requests:  28%|██▊       | 1161/4096 [00:03<00:08, 350.39it/s]
Adding requests:  29%|██▉       | 1198/4096 [00:03<00:08, 353.58it/s]
Adding requests:  30%|███       | 1236/4096 [00:03<00:07, 359.60it/s]
Adding requests:  31%|███       | 1272/4096 [00:03<00:07, 355.81it/s]
Adding requests:  32%|███▏      | 1308/4096 [00:03<00:07, 355.31it/s]
Adding requests:  33%|███▎      | 1344/4096 [00:03<00:07, 356.24it/s]
Adding requests:  34%|███▎      | 1380/4096 [00:03<00:07, 353.82it/s]
Adding requests:  35%|███▍      | 1416/4096 [00:04<00:07, 351.18it/s]
Adding requests:  35%|███▌      | 1453/4096 [00:04<00:07, 355.00it/s]
Adding requests:  36%|███▋      | 1490/4096 [00:04<00:07, 358.75it/s]
Adding requests:  37%|███▋      | 1527/4096 [00:04<00:07, 359.16it/s]
Adding requests:  38%|███▊      | 1563/4096 [00:04<00:07, 354.99it/s]
Adding requests:  39%|███▉      | 1599/4096 [00:04<00:07, 349.24it/s]
Adding requests:  40%|███▉      | 1634/4096 [00:04<00:07, 345.70it/s]
Adding requests:  41%|████      | 1669/4096 [00:04<00:07, 340.35it/s]
Adding requests:  42%|████▏     | 1705/4096 [00:04<00:06, 345.28it/s]
Adding requests:  43%|████▎     | 1742/4096 [00:04<00:06, 349.68it/s]
Adding requests:  43%|████▎     | 1780/4096 [00:05<00:06, 356.40it/s]
Adding requests:  44%|████▍     | 1816/4096 [00:05<00:06, 352.37it/s]
Adding requests:  45%|████▌     | 1853/4096 [00:05<00:06, 355.74it/s]
Adding requests:  46%|████▌     | 1890/4096 [00:05<00:06, 357.56it/s]
Adding requests:  47%|████▋     | 1928/4096 [00:05<00:05, 362.99it/s]
Adding requests:  48%|████▊     | 1965/4096 [00:05<00:05, 363.90it/s]
Adding requests:  49%|████▉     | 2002/4096 [00:05<00:05, 351.38it/s]
Adding requests:  50%|████▉     | 2038/4096 [00:05<00:05, 347.54it/s]
Adding requests:  51%|█████     | 2073/4096 [00:05<00:05, 341.74it/s]
Adding requests:  51%|█████▏    | 2108/4096 [00:05<00:05, 336.58it/s]
Adding requests:  52%|█████▏    | 2143/4096 [00:06<00:05, 340.04it/s]
Adding requests:  53%|█████▎    | 2178/4096 [00:06<00:05, 334.72it/s]
Adding requests:  54%|█████▍    | 2213/4096 [00:06<00:05, 336.87it/s]
Adding requests:  55%|█████▍    | 2249/4096 [00:06<00:05, 343.57it/s]
Adding requests:  56%|█████▌    | 2286/4096 [00:06<00:05, 349.81it/s]
Adding requests:  57%|█████▋    | 2324/4096 [00:06<00:04, 356.33it/s]
Adding requests:  58%|█████▊    | 2361/4096 [00:06<00:04, 358.25it/s]
Adding requests:  59%|█████▊    | 2399/4096 [00:06<00:04, 364.26it/s]
Adding requests:  59%|█████▉    | 2436/4096 [00:06<00:04, 363.12it/s]
Adding requests:  60%|██████    | 2474/4096 [00:07<00:04, 364.77it/s]
Adding requests:  61%|██████▏   | 2511/4096 [00:07<00:04, 364.89it/s]
Adding requests:  62%|██████▏   | 2551/4096 [00:07<00:04, 372.30it/s]
Adding requests:  63%|██████▎   | 2590/4096 [00:07<00:04, 375.55it/s]
Adding requests:  64%|██████▍   | 2628/4096 [00:07<00:03, 367.93it/s]
Adding requests:  65%|██████▌   | 2665/4096 [00:07<00:03, 360.65it/s]
Adding requests:  66%|██████▌   | 2702/4096 [00:07<00:03, 356.97it/s]
Adding requests:  67%|██████▋   | 2738/4096 [00:07<00:03, 356.32it/s]
Adding requests:  68%|██████▊   | 2776/4096 [00:07<00:03, 361.91it/s]
Adding requests:  69%|██████▊   | 2815/4096 [00:07<00:03, 369.04it/s]
Adding requests:  70%|██████▉   | 2852/4096 [00:08<00:03, 362.68it/s]
Adding requests:  71%|███████   | 2889/4096 [00:08<00:03, 362.26it/s]
Adding requests:  71%|███████▏  | 2926/4096 [00:08<00:03, 362.20it/s]
Adding requests:  72%|███████▏  | 2964/4096 [00:08<00:03, 365.03it/s]
Adding requests:  73%|███████▎  | 3001/4096 [00:08<00:03, 364.89it/s]
Adding requests:  74%|███████▍  | 3039/4096 [00:08<00:02, 367.79it/s]
Adding requests:  75%|███████▌  | 3077/4096 [00:08<00:02, 371.28it/s]
Adding requests:  76%|███████▌  | 3115/4096 [00:08<00:02, 371.72it/s]
Adding requests:  77%|███████▋  | 3153/4096 [00:08<00:02, 369.03it/s]
Adding requests:  78%|███████▊  | 3190/4096 [00:08<00:02, 366.11it/s]
Adding requests:  79%|███████▉  | 3227/4096 [00:09<00:02, 365.20it/s]
Adding requests:  80%|███████▉  | 3264/4096 [00:09<00:02, 353.02it/s]
Adding requests:  81%|████████  | 3300/4096 [00:09<00:02, 347.34it/s]
Adding requests:  81%|████████▏ | 3336/4096 [00:09<00:02, 347.91it/s]
Adding requests:  82%|████████▏ | 3374/4096 [00:09<00:02, 355.16it/s]
Adding requests:  83%|████████▎ | 3412/4096 [00:09<00:01, 359.39it/s]
Adding requests:  84%|████████▍ | 3449/4096 [00:09<00:01, 361.34it/s]
Adding requests:  85%|████████▌ | 3486/4096 [00:09<00:01, 355.71it/s]
Adding requests:  86%|████████▌ | 3524/4096 [00:09<00:01, 362.23it/s]
Adding requests:  87%|████████▋ | 3564/4096 [00:10<00:01, 371.11it/s]
Adding requests:  88%|████████▊ | 3602/4096 [00:10<00:01, 365.47it/s]
Adding requests:  89%|████████▉ | 3639/4096 [00:10<00:01, 366.30it/s]
Adding requests:  90%|████████▉ | 3676/4096 [00:10<00:01, 359.60it/s]
Adding requests:  91%|█████████ | 3713/4096 [00:10<00:01, 357.33it/s]
Adding requests:  92%|█████████▏| 3749/4096 [00:10<00:00, 356.61it/s]
Adding requests:  92%|█████████▏| 3785/4096 [00:10<00:00, 346.92it/s]
Adding requests:  93%|█████████▎| 3820/4096 [00:10<00:00, 340.90it/s]
Adding requests:  94%|█████████▍| 3856/4096 [00:10<00:00, 345.58it/s]
Adding requests:  95%|█████████▍| 3891/4096 [00:10<00:00, 345.23it/s]
Adding requests:  96%|█████████▌| 3926/4096 [00:11<00:00, 343.08it/s]
Adding requests:  97%|█████████▋| 3962/4096 [00:11<00:00, 345.91it/s]
Adding requests:  98%|█████████▊| 3997/4096 [00:11<00:00, 343.48it/s]
Adding requests:  98%|█████████▊| 4033/4096 [00:11<00:00, 347.25it/s]
Adding requests:  99%|█████████▉| 4068/4096 [00:11<00:00, 346.93it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 354.44it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  46%|████▌     | 1893/4096 [00:00<00:00, 18435.20it/s, est. speed input: 18879130.56 toks/s, output: 18435.58 toks/s]
Processed prompts:  91%|█████████ | 3737/4096 [00:11<00:01, 288.50it/s, est. speed input: 347384.35 toks/s, output: 339.24 toks/s]      
Processed prompts:  92%|█████████▏| 3754/4096 [00:11<00:01, 283.49it/s, est. speed input: 342869.56 toks/s, output: 334.83 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:13<00:00, 283.49it/s, est. speed input: 320573.87 toks/s, output: 313.06 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:13<00:00, 313.06it/s, est. speed input: 320573.87 toks/s, output: 313.06 toks/s]
[rank0]:[W126 15:54:54.084219949 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 86.5s

测试结果:
  Requests/s:   166.22
  Tokens/s:     170373.20
  Total Reqs:   4096
  Elapsed:      24.64s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     170206.98

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:55:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=716025) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=716025) WARNING 01-26 15:56:07 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     def forward(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     raise e
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/tmp/torchinductor_root/b4/cb4kg3jf6kelto57mv2kddkonp6x2kscwufazyb6dsg7i5kkzser.py", line 1090, in call
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     triton_poi_fused_mul_quant_slide_int8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_slide_int8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     torch.cuda.synchronize()
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] 


─── STDERR ───
[2026-01-26 15:55:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:55:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:55:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:55:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:55:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:55:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:55:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:55:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:55:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:55:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:55:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:55:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:55:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:55:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:55:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:55:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=716025) [2026-01-26 15:55:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=716025) [2026-01-26 15:55:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=716025) [2026-01-26 15:55:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=716025) [2026-01-26 15:55:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=716025) [2026-01-26 15:55:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=716025) [2026-01-26 15:55:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=716025) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=716025) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.74it/s]
(EngineCore_DP0 pid=716025) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=716025) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.43it/s]
(EngineCore_DP0 pid=716025) 
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=716025) [rank0]:W0126 15:56:12.602000 716025 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=716025) [rank0]:W0126 15:56:12.684000 716025 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=716025) [rank0]:W0126 15:56:13.790000 716025 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=716025) [rank0]:W0126 15:56:13.916000 716025 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=716025) Process EngineCore_DP0:
(EngineCore_DP0 pid=716025) Traceback (most recent call last):
(EngineCore_DP0 pid=716025)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=716025)     self.run()
(EngineCore_DP0 pid=716025)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=716025)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=716025)     raise e
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=716025)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=716025)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=716025)     super().__init__(
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=716025)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=716025)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=716025)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=716025)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=716025)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=716025)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=716025)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=716025)     return func(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=716025)     return func(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=716025)     self.model_runner.profile_run()
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=716025)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=716025)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=716025)     return func(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=716025)     outputs = self.model(
(EngineCore_DP0 pid=716025)               ^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=716025)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=716025)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=716025)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=716025)     hidden_states = self.model(
(EngineCore_DP0 pid=716025)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=716025)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=716025)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=716025)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=716025)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=716025)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=716025)     def forward(
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=716025)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=716025)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=716025)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=716025)     raise e
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=716025)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=716025)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=716025)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=716025)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=716025)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=716025)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=716025)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=716025)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=716025)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=716025)     return compiled_fn(full_args)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=716025)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=716025)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=716025)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=716025)                             ^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=716025)     outs = compiled_fn(args)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=716025)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=716025)     return self.current_callable(inputs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=716025)     out = model(new_inputs)
(EngineCore_DP0 pid=716025)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/tmp/torchinductor_root/b4/cb4kg3jf6kelto57mv2kddkonp6x2kscwufazyb6dsg7i5kkzser.py", line 1090, in call
(EngineCore_DP0 pid=716025)     triton_poi_fused_mul_quant_slide_int8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_slide_int8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=716025)     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=716025)     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=716025)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=716025)     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=716025)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=716025)     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=716025)     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=716025)     torch.cuda.synchronize()
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=716025)     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=716025) Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=716025) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=716025) For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=716025) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=716025) 
[rank0]:[W126 15:56:15.270880011 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,37.3891,19180.6068,3.4235
1024,1024,1,128,128,38.7572,39726.1239,3.3026
2048,1024,2,256,128,72.4468,74257.9615,3.5336
4096,1024,4,512,128,138.4328,141893.6408,3.6985
8192,1024,8,1024,128,153.8884,157735.6074,6.6542
16384,1024,16,2048,128,162.5211,166584.1700,12.6014
32768,1024,32,4096,128,166.2178,170373.1986,24.6424
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:56:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=717046) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=717046) WARNING 01-26 15:56:40 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=717046) WARNING 01-26 15:56:50 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.42 requests/s, 19195.68 total tokens/s, 37.42 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 15:56:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:56:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:56:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:56:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:56:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:56:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:56:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:56:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:56:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:56:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:56:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:56:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:56:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:56:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:56:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:56:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=717046) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=717046) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.14it/s]
(EngineCore_DP0 pid=717046) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]
(EngineCore_DP0 pid=717046) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.03s/it]
(EngineCore_DP0 pid=717046) 
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=717046) 2026-01-26 15:56:50,418 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=717046) 2026-01-26 15:56:50,442 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=717046) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.69it/s]
(EngineCore_DP0 pid=717046) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.22it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 510.56it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 255.00it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 303.29it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:00, 114.05it/s, est. speed input: 58395.66 toks/s, output: 114.05 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:01, 56.05it/s, est. speed input: 31268.47 toks/s, output: 61.07 toks/s]  
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 49.08it/s, est. speed input: 27745.47 toks/s, output: 54.19 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 46.11it/s, est. speed input: 26294.93 toks/s, output: 51.36 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 44.39it/s, est. speed input: 25468.60 toks/s, output: 49.74 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:01, 43.04it/s, est. speed input: 24830.87 toks/s, output: 48.50 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 42.06it/s, est. speed input: 24334.04 toks/s, output: 47.53 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 41.35it/s, est. speed input: 23935.77 toks/s, output: 46.75 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 40.82it/s, est. speed input: 23604.99 toks/s, output: 46.10 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 40.45it/s, est. speed input: 23329.81 toks/s, output: 45.57 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 40.20it/s, est. speed input: 23097.16 toks/s, output: 45.11 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 40.02it/s, est. speed input: 22897.70 toks/s, output: 44.72 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:01, 39.88it/s, est. speed input: 22723.84 toks/s, output: 44.38 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:00, 39.78it/s, est. speed input: 22597.05 toks/s, output: 44.13 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 39.73it/s, est. speed input: 22486.73 toks/s, output: 43.92 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 39.73it/s, est. speed input: 22389.35 toks/s, output: 43.73 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 39.72it/s, est. speed input: 22299.55 toks/s, output: 43.55 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 39.73it/s, est. speed input: 22218.29 toks/s, output: 43.39 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 39.72it/s, est. speed input: 22142.80 toks/s, output: 43.25 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 39.72it/s, est. speed input: 22073.41 toks/s, output: 43.11 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 39.75it/s, est. speed input: 22011.37 toks/s, output: 42.99 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 39.75it/s, est. speed input: 21952.38 toks/s, output: 42.88 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 39.82it/s, est. speed input: 21901.28 toks/s, output: 42.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 39.82it/s, est. speed input: 21862.13 toks/s, output: 42.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 42.70it/s, est. speed input: 21862.13 toks/s, output: 42.70 toks/s]
[rank0]:[W126 15:56:56.557202459 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.0s

测试结果:
  Requests/s:   37.42
  Tokens/s:     19195.68
  Total Reqs:   128
  Elapsed:      3.42s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     19158.27

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:57:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=718247) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=718247) WARNING 01-26 15:57:22 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=718247) WARNING 01-26 15:57:31 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 38.90 requests/s, 39877.29 total tokens/s, 38.90 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 15:57:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:57:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:57:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:57:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:57:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:57:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:57:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:57:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:57:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:57:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:57:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:57:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=718247) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=718247) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.08it/s]
(EngineCore_DP0 pid=718247) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.07s/it]
(EngineCore_DP0 pid=718247) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.05s/it]
(EngineCore_DP0 pid=718247) 
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=718247) 2026-01-26 15:57:31,461 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=718247) 2026-01-26 15:57:31,484 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=718247) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 13.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.99it/s]
(EngineCore_DP0 pid=718247) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.03it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  21%|██        | 27/128 [00:00<00:00, 265.42it/s]
Adding requests:  48%|████▊     | 62/128 [00:00<00:00, 311.51it/s]
Adding requests:  73%|███████▎  | 94/128 [00:00<00:00, 311.92it/s]
Adding requests:  99%|█████████▉| 127/128 [00:00<00:00, 314.76it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 309.89it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:00, 177.81it/s, est. speed input: 182093.02 toks/s, output: 177.82 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 58.27it/s, est. speed input: 66361.55 toks/s, output: 64.81 toks/s]   
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 50.75it/s, est. speed input: 58370.19 toks/s, output: 57.00 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 47.24it/s, est. speed input: 54829.34 toks/s, output: 53.54 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 45.32it/s, est. speed input: 52978.11 toks/s, output: 51.74 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 43.81it/s, est. speed input: 51546.44 toks/s, output: 50.34 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 42.82it/s, est. speed input: 50592.45 toks/s, output: 49.41 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 42.01it/s, est. speed input: 49785.39 toks/s, output: 48.62 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 41.41it/s, est. speed input: 49109.92 toks/s, output: 47.96 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:01, 40.94it/s, est. speed input: 48524.55 toks/s, output: 47.39 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 40.53it/s, est. speed input: 47996.88 toks/s, output: 46.87 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 40.27it/s, est. speed input: 47544.68 toks/s, output: 46.43 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 40.08it/s, est. speed input: 47144.43 toks/s, output: 46.04 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 39.92it/s, est. speed input: 46780.67 toks/s, output: 45.68 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 39.79it/s, est. speed input: 46452.22 toks/s, output: 45.36 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 39.80it/s, est. speed input: 46227.88 toks/s, output: 45.14 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 39.76it/s, est. speed input: 46014.03 toks/s, output: 44.94 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 39.66it/s, est. speed input: 45804.96 toks/s, output: 44.73 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 39.65it/s, est. speed input: 45620.18 toks/s, output: 44.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 39.65it/s, est. speed input: 45577.04 toks/s, output: 44.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.51it/s, est. speed input: 45577.04 toks/s, output: 44.51 toks/s]
[rank0]:[W126 15:57:36.651712956 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.2s

测试结果:
  Requests/s:   38.90
  Tokens/s:     39877.29
  Total Reqs:   128
  Elapsed:      3.29s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     39838.39

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:57:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=719363) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=719363) WARNING 01-26 15:58:03 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=719363) WARNING 01-26 15:58:12 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 77.95 requests/s, 79896.56 total tokens/s, 77.95 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 15:57:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:57:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:57:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:57:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:57:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:57:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:57:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:57:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:57:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:57:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:57:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:57:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=719363) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=719363) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.13it/s]
(EngineCore_DP0 pid=719363) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.13s/it]
(EngineCore_DP0 pid=719363) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]
(EngineCore_DP0 pid=719363) 
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=719363) 2026-01-26 15:58:12,855 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=719363) 2026-01-26 15:58:12,878 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=719363) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 11.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 11.14it/s]
(EngineCore_DP0 pid=719363) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.86it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█         | 27/256 [00:00<00:00, 266.18it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 311.27it/s]
Adding requests:  37%|███▋      | 94/256 [00:00<00:01, 139.14it/s]
Adding requests:  49%|████▉     | 125/256 [00:00<00:00, 176.75it/s]
Adding requests:  62%|██████▏   | 158/256 [00:00<00:00, 212.89it/s]
Adding requests:  76%|███████▌  | 194/256 [00:00<00:00, 248.53it/s]
Adding requests:  89%|████████▉ | 229/256 [00:00<00:00, 274.45it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 238.68it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:00<00:00, 770.03it/s, est. speed input: 788563.72 toks/s, output: 770.05 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:00, 133.35it/s, est. speed input: 156966.47 toks/s, output: 153.29 toks/s]
Processed prompts:  79%|███████▊  | 201/256 [00:01<00:00, 114.63it/s, est. speed input: 136201.42 toks/s, output: 133.01 toks/s]
Processed prompts:  88%|████████▊ | 225/256 [00:01<00:00, 104.77it/s, est. speed input: 126797.39 toks/s, output: 123.83 toks/s]
Processed prompts:  95%|█████████▍| 243/256 [00:02<00:00, 99.57it/s, est. speed input: 122041.20 toks/s, output: 119.18 toks/s] 
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 99.57it/s, est. speed input: 118599.88 toks/s, output: 115.82 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 115.81it/s, est. speed input: 118599.88 toks/s, output: 115.82 toks/s]
[rank0]:[W126 15:58:18.298867977 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.5s

测试结果:
  Requests/s:   77.95
  Tokens/s:     79896.56
  Total Reqs:   256
  Elapsed:      3.28s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     79818.61

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:58:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=720517) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=720517) WARNING 01-26 15:58:46 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=720517) WARNING 01-26 15:58:55 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 119.54 requests/s, 122528.16 total tokens/s, 119.54 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 15:58:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:58:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:58:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:58:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:58:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:58:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:58:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:58:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:58:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:58:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:58:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:58:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:58:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:58:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:58:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:58:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=720517) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=720517) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.14it/s]
(EngineCore_DP0 pid=720517) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.05s/it]
(EngineCore_DP0 pid=720517) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.03s/it]
(EngineCore_DP0 pid=720517) 
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=720517) 2026-01-26 15:58:55,883 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=720517) 2026-01-26 15:58:55,907 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=720517) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.54it/s]
(EngineCore_DP0 pid=720517) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 18.02it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 27/512 [00:00<00:01, 266.62it/s]
Adding requests:  12%|█▏        | 62/512 [00:00<00:01, 312.05it/s]
Adding requests:  18%|█▊        | 94/512 [00:00<00:01, 311.66it/s]
Adding requests:  25%|██▍       | 126/512 [00:00<00:01, 309.78it/s]
Adding requests:  31%|███▏      | 160/512 [00:00<00:01, 317.54it/s]
Adding requests:  38%|███▊      | 196/512 [00:00<00:00, 331.68it/s]
Adding requests:  45%|████▌     | 232/512 [00:00<00:00, 338.21it/s]
Adding requests:  52%|█████▏    | 267/512 [00:00<00:00, 338.90it/s]
Adding requests:  59%|█████▉    | 301/512 [00:00<00:00, 339.04it/s]
Adding requests:  66%|██████▌   | 338/512 [00:01<00:00, 348.19it/s]
Adding requests:  73%|███████▎  | 375/512 [00:01<00:00, 351.70it/s]
Adding requests:  81%|████████  | 413/512 [00:01<00:00, 357.42it/s]
Adding requests:  88%|████████▊ | 449/512 [00:01<00:00, 354.72it/s]
Adding requests:  96%|█████████▌| 489/512 [00:01<00:00, 367.12it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 343.42it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:00<00:00, 1455.13it/s, est. speed input: 1490145.30 toks/s, output: 1455.15 toks/s]
Processed prompts:  65%|██████▍   | 332/512 [00:01<00:00, 216.92it/s, est. speed input: 259200.18 toks/s, output: 253.12 toks/s]   
Processed prompts:  78%|███████▊  | 400/512 [00:01<00:00, 180.22it/s, est. speed input: 218988.84 toks/s, output: 213.86 toks/s]
Processed prompts:  87%|████████▋ | 443/512 [00:02<00:00, 164.67it/s, est. speed input: 203656.64 toks/s, output: 198.88 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:02<00:00, 154.46it/s, est. speed input: 194805.41 toks/s, output: 190.24 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:02<00:00, 148.13it/s, est. speed input: 189557.62 toks/s, output: 185.11 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 148.13it/s, est. speed input: 187856.38 toks/s, output: 183.45 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 183.45it/s, est. speed input: 187856.38 toks/s, output: 183.45 toks/s]
[rank0]:[W126 15:59:02.698490215 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.5s

测试结果:
  Requests/s:   119.54
  Tokens/s:     122528.16
  Total Reqs:   512
  Elapsed:      4.28s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     122408.62

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:59:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=721743) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=721743) WARNING 01-26 15:59:34 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=721743) WARNING 01-26 15:59:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 128.48 requests/s, 131688.49 total tokens/s, 128.48 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 15:59:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:59:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:59:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:59:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:59:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:59:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:59:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:59:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:59:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:59:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:59:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:59:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:59:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:59:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:59:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:59:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=721743) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=721743) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=721743) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
(EngineCore_DP0 pid=721743) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.07it/s]
(EngineCore_DP0 pid=721743) 
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=721743) 2026-01-26 15:59:43,887 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=721743) 2026-01-26 15:59:43,912 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=721743) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 15.82it/s]
(EngineCore_DP0 pid=721743) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 17.70it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.96it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 277.22it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 319.52it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 316.89it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 322.02it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 331.27it/s]
Adding requests:  20%|█▉        | 200/1024 [00:00<00:02, 341.77it/s]
Adding requests:  23%|██▎       | 236/1024 [00:00<00:02, 345.98it/s]
Adding requests:  26%|██▋       | 271/1024 [00:00<00:02, 344.35it/s]
Adding requests:  30%|███       | 308/1024 [00:00<00:02, 350.71it/s]
Adding requests:  34%|███▍      | 346/1024 [00:01<00:01, 356.31it/s]
Adding requests:  37%|███▋      | 383/1024 [00:01<00:01, 360.06it/s]
Adding requests:  41%|████      | 421/1024 [00:01<00:01, 363.09it/s]
Adding requests:  45%|████▍     | 458/1024 [00:01<00:01, 361.27it/s]
Adding requests:  49%|████▊     | 498/1024 [00:01<00:01, 369.94it/s]
Adding requests:  52%|█████▏    | 537/1024 [00:01<00:01, 374.20it/s]
Adding requests:  56%|█████▌    | 575/1024 [00:01<00:01, 372.11it/s]
Adding requests:  60%|█████▉    | 613/1024 [00:01<00:01, 361.81it/s]
Adding requests:  63%|██████▎   | 650/1024 [00:01<00:01, 355.94it/s]
Adding requests:  67%|██████▋   | 687/1024 [00:01<00:00, 359.42it/s]
Adding requests:  71%|███████   | 723/1024 [00:02<00:00, 355.73it/s]
Adding requests:  74%|███████▍  | 759/1024 [00:02<00:00, 354.28it/s]
Adding requests:  78%|███████▊  | 795/1024 [00:02<00:00, 354.16it/s]
Adding requests:  81%|████████▏ | 833/1024 [00:02<00:00, 358.87it/s]
Adding requests:  85%|████████▍ | 870/1024 [00:02<00:00, 360.83it/s]
Adding requests:  89%|████████▊ | 907/1024 [00:02<00:00, 362.57it/s]
Adding requests:  92%|█████████▏| 944/1024 [00:02<00:00, 351.44it/s]
Adding requests:  96%|█████████▌| 980/1024 [00:02<00:00, 352.44it/s]
Adding requests:  99%|█████████▉| 1016/1024 [00:02<00:00, 350.64it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 352.76it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:00<00:00, 2478.35it/s, est. speed input: 2537956.28 toks/s, output: 2478.38 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:02<00:01, 255.62it/s, est. speed input: 312534.85 toks/s, output: 305.21 toks/s]   
Processed prompts:  72%|███████▏  | 736/1024 [00:02<00:01, 212.59it/s, est. speed input: 264201.48 toks/s, output: 258.01 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:03<00:01, 188.48it/s, est. speed input: 241524.37 toks/s, output: 235.86 toks/s]
Processed prompts:  83%|████████▎ | 847/1024 [00:03<00:00, 180.94it/s, est. speed input: 233785.34 toks/s, output: 228.31 toks/s]
Processed prompts:  86%|████████▌ | 881/1024 [00:03<00:00, 173.82it/s, est. speed input: 228020.86 toks/s, output: 222.68 toks/s]
Processed prompts:  89%|████████▊ | 908/1024 [00:04<00:00, 161.50it/s, est. speed input: 221220.14 toks/s, output: 216.04 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:04<00:00, 154.33it/s, est. speed input: 217053.31 toks/s, output: 211.97 toks/s]
Processed prompts:  93%|█████████▎| 949/1024 [00:04<00:00, 154.36it/s, est. speed input: 215451.09 toks/s, output: 210.40 toks/s]
Processed prompts:  94%|█████████▍| 967/1024 [00:04<00:00, 153.07it/s, est. speed input: 213718.30 toks/s, output: 208.71 toks/s]
Processed prompts:  96%|█████████▌| 984/1024 [00:04<00:00, 150.19it/s, est. speed input: 211844.79 toks/s, output: 206.88 toks/s]
Processed prompts:  98%|█████████▊| 1000/1024 [00:04<00:00, 145.87it/s, est. speed input: 209862.71 toks/s, output: 204.94 toks/s]
Processed prompts:  99%|█████████▉| 1015/1024 [00:05<00:00, 140.10it/s, est. speed input: 207750.13 toks/s, output: 202.88 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 140.10it/s, est. speed input: 206978.23 toks/s, output: 202.13 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 202.12it/s, est. speed input: 206978.23 toks/s, output: 202.13 toks/s]
[rank0]:[W126 15:59:54.370040787 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.7s

测试结果:
  Requests/s:   128.48
  Tokens/s:     131688.49
  Total Reqs:   1024
  Elapsed:      7.97s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     131560.02

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:00:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=723104) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=723104) WARNING 01-26 16:00:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=723104) WARNING 01-26 16:00:41 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 134.50 requests/s, 137866.07 total tokens/s, 134.50 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 16:00:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:00:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:00:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:00:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:00:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:00:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:00:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:00:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:00:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:00:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:00:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:00:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:00:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:00:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:00:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:00:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=723104) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=723104) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=723104) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
(EngineCore_DP0 pid=723104) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.07it/s]
(EngineCore_DP0 pid=723104) 
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=723104) [rank0]:W0126 16:00:36.450000 723104 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=723104) [rank0]:W0126 16:00:36.533000 723104 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=723104) [rank0]:W0126 16:00:37.535000 723104 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=723104) [rank0]:W0126 16:00:37.665000 723104 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=723104) 2026-01-26 16:00:41,145 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=723104) 2026-01-26 16:00:41,171 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=723104) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.89it/s]
(EngineCore_DP0 pid=723104) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 17.98it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 15.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 15.45it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 278.32it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 318.28it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 315.35it/s]
Adding requests:   6%|▌         | 127/2048 [00:00<00:06, 308.86it/s]
Adding requests:   8%|▊         | 162/2048 [00:00<00:05, 320.72it/s]
Adding requests:  10%|▉         | 199/2048 [00:00<00:05, 333.71it/s]
Adding requests:  12%|█▏        | 236/2048 [00:00<00:05, 342.10it/s]
Adding requests:  13%|█▎        | 271/2048 [00:00<00:05, 341.00it/s]
Adding requests:  15%|█▌        | 308/2048 [00:00<00:04, 348.50it/s]
Adding requests:  17%|█▋        | 344/2048 [00:01<00:04, 350.59it/s]
Adding requests:  19%|█▊        | 380/2048 [00:01<00:04, 353.39it/s]
Adding requests:  20%|██        | 419/2048 [00:01<00:04, 363.48it/s]
Adding requests:  22%|██▏       | 456/2048 [00:01<00:04, 360.52it/s]
Adding requests:  24%|██▍       | 496/2048 [00:01<00:04, 371.78it/s]
Adding requests:  26%|██▌       | 535/2048 [00:01<00:04, 373.54it/s]
Adding requests:  28%|██▊       | 573/2048 [00:01<00:03, 373.05it/s]
Adding requests:  30%|██▉       | 611/2048 [00:01<00:04, 359.18it/s]
Adding requests:  32%|███▏      | 648/2048 [00:01<00:03, 355.60it/s]
Adding requests:  33%|███▎      | 685/2048 [00:01<00:03, 357.85it/s]
Adding requests:  35%|███▌      | 721/2048 [00:02<00:03, 356.79it/s]
Adding requests:  37%|███▋      | 757/2048 [00:02<00:03, 351.10it/s]
Adding requests:  39%|███▊      | 793/2048 [00:02<00:03, 349.95it/s]
Adding requests:  41%|████      | 831/2048 [00:02<00:03, 356.99it/s]
Adding requests:  42%|████▏     | 868/2048 [00:02<00:03, 357.62it/s]
Adding requests:  44%|████▍     | 906/2048 [00:02<00:03, 363.98it/s]
Adding requests:  46%|████▌     | 943/2048 [00:02<00:03, 355.52it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:02, 356.56it/s]
Adding requests:  50%|████▉     | 1015/2048 [00:02<00:02, 350.72it/s]
Adding requests:  51%|█████▏    | 1051/2048 [00:02<00:02, 352.33it/s]
Adding requests:  53%|█████▎    | 1087/2048 [00:03<00:02, 352.75it/s]
Adding requests:  55%|█████▍    | 1123/2048 [00:03<00:02, 354.18it/s]
Adding requests:  57%|█████▋    | 1159/2048 [00:03<00:02, 353.10it/s]
Adding requests:  58%|█████▊    | 1195/2048 [00:03<00:02, 354.46it/s]
Adding requests:  60%|██████    | 1233/2048 [00:03<00:02, 361.90it/s]
Adding requests:  62%|██████▏   | 1270/2048 [00:03<00:02, 357.04it/s]
Adding requests:  64%|██████▍   | 1306/2048 [00:03<00:02, 355.91it/s]
Adding requests:  66%|██████▌   | 1343/2048 [00:03<00:01, 356.92it/s]
Adding requests:  67%|██████▋   | 1380/2048 [00:03<00:01, 359.03it/s]
Adding requests:  69%|██████▉   | 1416/2048 [00:04<00:01, 356.83it/s]
Adding requests:  71%|███████   | 1453/2048 [00:04<00:01, 359.76it/s]
Adding requests:  73%|███████▎  | 1490/2048 [00:04<00:01, 362.38it/s]
Adding requests:  75%|███████▍  | 1527/2048 [00:04<00:01, 353.07it/s]
Adding requests:  76%|███████▋  | 1563/2048 [00:04<00:01, 349.82it/s]
Adding requests:  78%|███████▊  | 1599/2048 [00:04<00:01, 346.45it/s]
Adding requests:  80%|███████▉  | 1634/2048 [00:04<00:01, 344.19it/s]
Adding requests:  81%|████████▏ | 1669/2048 [00:04<00:01, 330.37it/s]
Adding requests:  83%|████████▎ | 1706/2048 [00:04<00:01, 339.72it/s]
Adding requests:  85%|████████▌ | 1742/2048 [00:04<00:00, 345.43it/s]
Adding requests:  87%|████████▋ | 1780/2048 [00:05<00:00, 353.29it/s]
Adding requests:  89%|████████▊ | 1816/2048 [00:05<00:00, 350.60it/s]
Adding requests:  90%|█████████ | 1852/2048 [00:05<00:00, 352.92it/s]
Adding requests:  92%|█████████▏| 1889/2048 [00:05<00:00, 357.02it/s]
Adding requests:  94%|█████████▍| 1927/2048 [00:05<00:00, 362.78it/s]
Adding requests:  96%|█████████▌| 1964/2048 [00:05<00:00, 362.90it/s]
Adding requests:  98%|█████████▊| 2001/2048 [00:05<00:00, 355.12it/s]
Adding requests:  99%|█████████▉| 2037/2048 [00:05<00:00, 348.61it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 351.95it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  38%|███▊      | 781/2048 [00:00<00:00, 5562.65it/s, est. speed input: 5696494.61 toks/s, output: 5562.76 toks/s]
Processed prompts:  65%|██████▌   | 1338/2048 [00:04<00:02, 266.81it/s, est. speed input: 327871.15 toks/s, output: 320.19 toks/s]  
Processed prompts:  77%|███████▋  | 1576/2048 [00:05<00:02, 216.91it/s, est. speed input: 271592.30 toks/s, output: 265.23 toks/s]
Processed prompts:  84%|████████▎ | 1712/2048 [00:07<00:01, 194.93it/s, est. speed input: 250171.91 toks/s, output: 244.31 toks/s]
Processed prompts:  88%|████████▊ | 1799/2048 [00:07<00:01, 188.10it/s, est. speed input: 243030.29 toks/s, output: 237.33 toks/s]
Processed prompts:  91%|█████████ | 1860/2048 [00:08<00:01, 178.35it/s, est. speed input: 236313.53 toks/s, output: 230.77 toks/s]
Processed prompts:  93%|█████████▎| 1905/2048 [00:08<00:00, 171.12it/s, est. speed input: 231908.62 toks/s, output: 226.47 toks/s]
Processed prompts:  95%|█████████▍| 1940/2048 [00:08<00:00, 167.88it/s, est. speed input: 229576.92 toks/s, output: 224.20 toks/s]
Processed prompts:  96%|█████████▌| 1968/2048 [00:08<00:00, 160.71it/s, est. speed input: 226722.52 toks/s, output: 221.41 toks/s]
Processed prompts:  97%|█████████▋| 1991/2048 [00:09<00:00, 164.51it/s, est. speed input: 226471.44 toks/s, output: 221.16 toks/s]
Processed prompts:  98%|█████████▊| 2014/2048 [00:09<00:00, 149.57it/s, est. speed input: 223130.66 toks/s, output: 217.90 toks/s]
Processed prompts:  99%|█████████▉| 2033/2048 [00:09<00:00, 151.29it/s, est. speed input: 222425.05 toks/s, output: 217.21 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:09<00:00, 151.29it/s, est. speed input: 222966.48 toks/s, output: 217.74 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:09<00:00, 217.74it/s, est. speed input: 222966.48 toks/s, output: 217.74 toks/s]
[rank0]:[W126 16:00:58.206826161 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 64.9s

测试结果:
  Requests/s:   134.50
  Tokens/s:     137866.07
  Total Reqs:   2048
  Elapsed:      15.23s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     137731.57

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:01:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=724688) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=724688) WARNING 01-26 16:01:48 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=724688) WARNING 01-26 16:01:58 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 139.82 requests/s, 143315.09 total tokens/s, 139.82 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 16:01:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:01:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:01:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:01:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:01:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:01:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:01:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:01:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:01:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:01:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:01:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:01:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:01:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:01:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:01:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:01:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:39] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:39] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:39] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:39] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:39] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=724688) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=724688) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=724688) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
(EngineCore_DP0 pid=724688) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.07it/s]
(EngineCore_DP0 pid=724688) 
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=724688) [rank0]:W0126 16:01:53.394000 724688 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=724688) [rank0]:W0126 16:01:53.478000 724688 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=724688) [rank0]:W0126 16:01:54.447000 724688 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=724688) [rank0]:W0126 16:01:54.578000 724688 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=724688) 2026-01-26 16:01:58,329 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=724688) 2026-01-26 16:01:58,355 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=724688) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 15.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 14.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.51it/s]
(EngineCore_DP0 pid=724688) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.67it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.80it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 18.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 17.98it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 277.88it/s]
Adding requests:   2%|▏         | 64/4096 [00:00<00:12, 319.60it/s]
Adding requests:   2%|▏         | 96/4096 [00:00<00:12, 316.95it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:12, 308.44it/s]
Adding requests:   4%|▍         | 163/4096 [00:00<00:12, 321.54it/s]
Adding requests:   5%|▍         | 200/4096 [00:00<00:11, 335.18it/s]
Adding requests:   6%|▌         | 236/4096 [00:00<00:11, 341.62it/s]
Adding requests:   7%|▋         | 271/4096 [00:00<00:11, 341.34it/s]
Adding requests:   7%|▋         | 307/4096 [00:00<00:10, 346.43it/s]
Adding requests:   8%|▊         | 345/4096 [00:01<00:10, 356.02it/s]
Adding requests:   9%|▉         | 382/4096 [00:01<00:10, 357.59it/s]
Adding requests:  10%|█         | 421/4096 [00:01<00:10, 365.42it/s]
Adding requests:  11%|█         | 458/4096 [00:01<00:10, 363.32it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:09, 371.46it/s]
Adding requests:  13%|█▎        | 537/4096 [00:01<00:09, 376.86it/s]
Adding requests:  14%|█▍        | 575/4096 [00:01<00:09, 375.94it/s]
Adding requests:  15%|█▍        | 613/4096 [00:01<00:09, 360.20it/s]
Adding requests:  16%|█▌        | 650/4096 [00:01<00:09, 355.22it/s]
Adding requests:  17%|█▋        | 687/4096 [00:01<00:09, 358.90it/s]
Adding requests:  18%|█▊        | 723/4096 [00:02<00:09, 356.68it/s]
Adding requests:  19%|█▊        | 759/4096 [00:02<00:09, 355.46it/s]
Adding requests:  19%|█▉        | 795/4096 [00:02<00:09, 355.73it/s]
Adding requests:  20%|██        | 833/4096 [00:02<00:09, 360.66it/s]
Adding requests:  21%|██        | 870/4096 [00:02<00:09, 357.73it/s]
Adding requests:  22%|██▏       | 908/4096 [00:02<00:08, 361.26it/s]
Adding requests:  23%|██▎       | 945/4096 [00:02<00:08, 355.45it/s]
Adding requests:  24%|██▍       | 982/4096 [00:02<00:08, 356.63it/s]
Adding requests:  25%|██▍       | 1018/4096 [00:02<00:08, 354.02it/s]
Adding requests:  26%|██▌       | 1054/4096 [00:02<00:08, 352.78it/s]
Adding requests:  27%|██▋       | 1090/4096 [00:03<00:08, 351.83it/s]
Adding requests:  28%|██▊       | 1128/4096 [00:03<00:08, 358.04it/s]
Adding requests:  28%|██▊       | 1164/4096 [00:03<00:08, 356.04it/s]
Adding requests:  29%|██▉       | 1200/4096 [00:03<00:08, 354.43it/s]
Adding requests:  30%|███       | 1238/4096 [00:03<00:07, 361.48it/s]
Adding requests:  31%|███       | 1275/4096 [00:03<00:07, 356.12it/s]
Adding requests:  32%|███▏      | 1311/4096 [00:03<00:07, 356.97it/s]
Adding requests:  33%|███▎      | 1347/4096 [00:03<00:07, 357.74it/s]
Adding requests:  34%|███▍      | 1384/4096 [00:03<00:07, 360.11it/s]
Adding requests:  35%|███▍      | 1421/4096 [00:04<00:07, 358.10it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:04<00:07, 358.53it/s]
Adding requests:  36%|███▋      | 1495/4096 [00:04<00:07, 364.50it/s]
Adding requests:  37%|███▋      | 1532/4096 [00:04<00:07, 363.15it/s]
Adding requests:  38%|███▊      | 1569/4096 [00:04<00:07, 357.14it/s]
Adding requests:  39%|███▉      | 1605/4096 [00:04<00:07, 355.07it/s]
Adding requests:  40%|████      | 1641/4096 [00:04<00:07, 347.10it/s]
Adding requests:  41%|████      | 1676/4096 [00:04<00:07, 343.31it/s]
Adding requests:  42%|████▏     | 1713/4096 [00:04<00:06, 350.38it/s]
Adding requests:  43%|████▎     | 1750/4096 [00:04<00:06, 355.55it/s]
Adding requests:  44%|████▎     | 1786/4096 [00:05<00:06, 356.35it/s]
Adding requests:  44%|████▍     | 1822/4096 [00:05<00:06, 354.23it/s]
Adding requests:  45%|████▌     | 1859/4096 [00:05<00:06, 357.33it/s]
Adding requests:  46%|████▋     | 1895/4096 [00:05<00:06, 351.98it/s]
Adding requests:  47%|████▋     | 1933/4096 [00:05<00:06, 359.05it/s]
Adding requests:  48%|████▊     | 1970/4096 [00:05<00:05, 359.94it/s]
Adding requests:  49%|████▉     | 2007/4096 [00:05<00:05, 349.45it/s]
Adding requests:  50%|████▉     | 2043/4096 [00:05<00:05, 347.16it/s]
Adding requests:  51%|█████     | 2078/4096 [00:05<00:05, 341.42it/s]
Adding requests:  52%|█████▏    | 2115/4096 [00:05<00:05, 347.49it/s]
Adding requests:  53%|█████▎    | 2151/4096 [00:06<00:05, 348.56it/s]
Adding requests:  53%|█████▎    | 2186/4096 [00:06<00:05, 342.51it/s]
Adding requests:  54%|█████▍    | 2222/4096 [00:06<00:05, 344.78it/s]
Adding requests:  55%|█████▌    | 2259/4096 [00:06<00:05, 349.49it/s]
Adding requests:  56%|█████▌    | 2297/4096 [00:06<00:05, 356.05it/s]
Adding requests:  57%|█████▋    | 2334/4096 [00:06<00:04, 358.07it/s]
Adding requests:  58%|█████▊    | 2371/4096 [00:06<00:04, 360.47it/s]
Adding requests:  59%|█████▉    | 2410/4096 [00:06<00:04, 366.88it/s]
Adding requests:  60%|█████▉    | 2447/4096 [00:06<00:04, 357.29it/s]
Adding requests:  61%|██████    | 2483/4096 [00:07<00:04, 357.42it/s]
Adding requests:  62%|██████▏   | 2521/4096 [00:07<00:04, 361.97it/s]
Adding requests:  62%|██████▎   | 2560/4096 [00:07<00:04, 369.52it/s]
Adding requests:  63%|██████▎   | 2600/4096 [00:07<00:03, 375.38it/s]
Adding requests:  64%|██████▍   | 2638/4096 [00:07<00:04, 361.72it/s]
Adding requests:  65%|██████▌   | 2675/4096 [00:07<00:03, 361.17it/s]
Adding requests:  66%|██████▌   | 2712/4096 [00:07<00:03, 355.93it/s]
Adding requests:  67%|██████▋   | 2749/4096 [00:07<00:03, 359.59it/s]
Adding requests:  68%|██████▊   | 2788/4096 [00:07<00:03, 365.29it/s]
Adding requests:  69%|██████▉   | 2826/4096 [00:07<00:03, 367.84it/s]
Adding requests:  70%|██████▉   | 2863/4096 [00:08<00:03, 366.75it/s]
Adding requests:  71%|███████   | 2900/4096 [00:08<00:03, 365.06it/s]
Adding requests:  72%|███████▏  | 2939/4096 [00:08<00:03, 368.89it/s]
Adding requests:  73%|███████▎  | 2976/4096 [00:08<00:03, 365.41it/s]
Adding requests:  74%|███████▎  | 3015/4096 [00:08<00:02, 369.16it/s]
Adding requests:  75%|███████▍  | 3053/4096 [00:08<00:02, 371.72it/s]
Adding requests:  75%|███████▌  | 3091/4096 [00:08<00:02, 369.46it/s]
Adding requests:  76%|███████▋  | 3130/4096 [00:08<00:02, 373.20it/s]
Adding requests:  77%|███████▋  | 3168/4096 [00:08<00:02, 368.96it/s]
Adding requests:  78%|███████▊  | 3205/4096 [00:08<00:02, 363.97it/s]
Adding requests:  79%|███████▉  | 3243/4096 [00:09<00:02, 365.94it/s]
Adding requests:  80%|████████  | 3280/4096 [00:09<00:02, 360.03it/s]
Adding requests:  81%|████████  | 3317/4096 [00:09<00:02, 343.59it/s]
Adding requests:  82%|████████▏ | 3354/4096 [00:09<00:02, 348.52it/s]
Adding requests:  83%|████████▎ | 3391/4096 [00:09<00:01, 353.60it/s]
Adding requests:  84%|████████▎ | 3428/4096 [00:09<00:01, 356.64it/s]
Adding requests:  85%|████████▍ | 3466/4096 [00:09<00:01, 361.38it/s]
Adding requests:  86%|████████▌ | 3503/4096 [00:09<00:01, 357.05it/s]
Adding requests:  86%|████████▋ | 3543/4096 [00:09<00:01, 367.41it/s]
Adding requests:  87%|████████▋ | 3580/4096 [00:10<00:01, 366.37it/s]
Adding requests:  88%|████████▊ | 3617/4096 [00:10<00:01, 365.56it/s]
Adding requests:  89%|████████▉ | 3654/4096 [00:10<00:01, 364.48it/s]
Adding requests:  90%|█████████ | 3691/4096 [00:10<00:01, 357.49it/s]
Adding requests:  91%|█████████ | 3728/4096 [00:10<00:01, 358.10it/s]
Adding requests:  92%|█████████▏| 3764/4096 [00:10<00:00, 351.86it/s]
Adding requests:  93%|█████████▎| 3800/4096 [00:10<00:00, 342.32it/s]
Adding requests:  94%|█████████▎| 3835/4096 [00:10<00:00, 342.96it/s]
Adding requests:  95%|█████████▍| 3871/4096 [00:10<00:00, 347.49it/s]
Adding requests:  95%|█████████▌| 3906/4096 [00:10<00:00, 343.36it/s]
Adding requests:  96%|█████████▌| 3941/4096 [00:11<00:00, 344.42it/s]
Adding requests:  97%|█████████▋| 3976/4096 [00:11<00:00, 344.99it/s]
Adding requests:  98%|█████████▊| 4012/4096 [00:11<00:00, 348.10it/s]
Adding requests:  99%|█████████▉| 4047/4096 [00:11<00:00, 344.54it/s]
Adding requests: 100%|█████████▉| 4083/4096 [00:11<00:00, 346.22it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 355.58it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  39%|███▊      | 1578/4096 [00:00<00:00, 12760.20it/s, est. speed input: 13067272.32 toks/s, output: 12760.37 toks/s]
Processed prompts:  70%|██████▉   | 2855/4096 [00:08<00:04, 266.08it/s, est. speed input: 325271.82 toks/s, output: 317.65 toks/s]      
Processed prompts:  83%|████████▎ | 3389/4096 [00:12<00:03, 217.24it/s, est. speed input: 269971.62 toks/s, output: 263.64 toks/s]
Processed prompts:  90%|████████▉ | 3686/4096 [00:14<00:02, 200.81it/s, est. speed input: 253083.01 toks/s, output: 247.15 toks/s]
Processed prompts:  95%|█████████▍| 3872/4096 [00:16<00:01, 189.53it/s, est. speed input: 243596.58 toks/s, output: 237.89 toks/s]
Processed prompts:  98%|█████████▊| 3996/4096 [00:17<00:00, 182.13it/s, est. speed input: 238200.90 toks/s, output: 232.62 toks/s]
Processed prompts: 100%|█████████▉| 4083/4096 [00:17<00:00, 177.89it/s, est. speed input: 235241.11 toks/s, output: 229.73 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:17<00:00, 177.89it/s, est. speed input: 235987.39 toks/s, output: 230.46 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:17<00:00, 230.46it/s, est. speed input: 235987.39 toks/s, output: 230.46 toks/s]
[rank0]:[W126 16:02:30.070290381 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 91.9s

测试结果:
  Requests/s:   139.82
  Tokens/s:     143315.09
  Total Reqs:   4096
  Elapsed:      29.29s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     143175.28

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:03:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=726849) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=726849) WARNING 01-26 16:03:44 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     def forward(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     raise e
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/tmp/torchinductor_root/2w/c2wdcwqri7mchk5gopfhtl34ozq3lcknjbygpd6cgdwtsk2mgubx.py", line 1093, in call
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 6)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 327, in quant_slide_int8_triton
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered


─── STDERR ───
[2026-01-26 16:03:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:03:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:03:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:03:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:03:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:03:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:03:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:03:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:03:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:03:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:03:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:03:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:03:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:03:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:03:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:03:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:35] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=726849) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=726849) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=726849) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
(EngineCore_DP0 pid=726849) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.07it/s]
(EngineCore_DP0 pid=726849) 
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=726849) [rank0]:W0126 16:03:49.584000 726849 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=726849) [rank0]:W0126 16:03:49.665000 726849 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=726849) [rank0]:W0126 16:03:50.769000 726849 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=726849) [rank0]:W0126 16:03:50.892000 726849 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=726849) Process EngineCore_DP0:
(EngineCore_DP0 pid=726849) Traceback (most recent call last):
(EngineCore_DP0 pid=726849)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=726849)     self.run()
(EngineCore_DP0 pid=726849)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=726849)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=726849)     raise e
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=726849)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=726849)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=726849)     super().__init__(
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=726849)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=726849)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=726849)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=726849)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=726849)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=726849)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=726849)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=726849)     return func(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=726849)     return func(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=726849)     self.model_runner.profile_run()
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=726849)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=726849)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=726849)     return func(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=726849)     outputs = self.model(
(EngineCore_DP0 pid=726849)               ^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=726849)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=726849)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=726849)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=726849)     hidden_states = self.model(
(EngineCore_DP0 pid=726849)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=726849)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=726849)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=726849)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=726849)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=726849)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=726849)     def forward(
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=726849)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=726849)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=726849)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=726849)     raise e
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=726849)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=726849)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=726849)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=726849)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=726849)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=726849)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=726849)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=726849)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=726849)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=726849)     return compiled_fn(full_args)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=726849)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=726849)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=726849)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=726849)                             ^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=726849)     outs = compiled_fn(args)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=726849)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=726849)     return self.current_callable(inputs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=726849)     out = model(new_inputs)
(EngineCore_DP0 pid=726849)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/tmp/torchinductor_root/2w/c2wdcwqri7mchk5gopfhtl34ozq3lcknjbygpd6cgdwtsk2mgubx.py", line 1093, in call
(EngineCore_DP0 pid=726849)     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 6)
(EngineCore_DP0 pid=726849)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=726849)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=726849)     return fn(input, L)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 327, in quant_slide_int8_triton
(EngineCore_DP0 pid=726849)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=726849)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=726849)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=726849)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=726849)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=726849)     self._init_handles()
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=726849)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=726849)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 16:03:51.964286872 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,37.4185,19195.6843,3.4208
1024,1024,1,128,128,38.9047,39877.2897,3.2901
2048,1024,2,256,128,77.9479,79896.5551,3.2842
4096,1024,4,512,128,119.5397,122528.1633,4.2831
8192,1024,8,1024,128,128.4766,131688.4932,7.9703
16384,1024,16,2048,128,134.5035,137866.0714,15.2264
32768,1024,32,4096,128,139.8196,143315.0948,29.2949
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:04:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=727850) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=727850) WARNING 01-26 16:04:17 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=727850) WARNING 01-26 16:04:27 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.30 requests/s, 19135.65 total tokens/s, 37.30 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:04:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:04:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:04:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:04:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:04:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:04:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:04:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:04:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:04:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:04:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:04:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:04:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=727850) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=727850) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.06it/s]
(EngineCore_DP0 pid=727850) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.16s/it]
(EngineCore_DP0 pid=727850) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.13s/it]
(EngineCore_DP0 pid=727850) 
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=727850) 2026-01-26 16:04:27,520 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=727850) 2026-01-26 16:04:27,543 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=727850) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.69it/s]
(EngineCore_DP0 pid=727850) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.35it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  40%|███▉      | 51/128 [00:00<00:00, 508.72it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 252.08it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 297.96it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:00, 117.61it/s, est. speed input: 60221.80 toks/s, output: 117.62 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:01, 56.28it/s, est. speed input: 31467.60 toks/s, output: 61.46 toks/s]  
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 49.10it/s, est. speed input: 27824.97 toks/s, output: 54.35 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 46.14it/s, est. speed input: 26358.77 toks/s, output: 51.48 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 44.11it/s, est. speed input: 25364.08 toks/s, output: 49.54 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 42.85it/s, est. speed input: 24742.56 toks/s, output: 48.33 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 41.94it/s, est. speed input: 24263.37 toks/s, output: 47.39 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 41.27it/s, est. speed input: 23877.52 toks/s, output: 46.64 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 40.79it/s, est. speed input: 23561.14 toks/s, output: 46.02 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 40.42it/s, est. speed input: 23291.96 toks/s, output: 45.49 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 40.11it/s, est. speed input: 23056.48 toks/s, output: 45.03 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 39.98it/s, est. speed input: 22865.00 toks/s, output: 44.66 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:01, 39.88it/s, est. speed input: 22698.12 toks/s, output: 44.33 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:00, 39.79it/s, est. speed input: 22576.63 toks/s, output: 44.09 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 39.72it/s, est. speed input: 22465.64 toks/s, output: 43.88 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 39.71it/s, est. speed input: 22369.57 toks/s, output: 43.69 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 39.71it/s, est. speed input: 22281.50 toks/s, output: 43.52 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 39.71it/s, est. speed input: 22201.48 toks/s, output: 43.36 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 39.73it/s, est. speed input: 22129.17 toks/s, output: 43.22 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 39.61it/s, est. speed input: 22052.32 toks/s, output: 43.07 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 39.56it/s, est. speed input: 21984.05 toks/s, output: 42.94 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 39.55it/s, est. speed input: 21922.07 toks/s, output: 42.82 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 39.56it/s, est. speed input: 21864.92 toks/s, output: 42.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.56it/s, est. speed input: 21839.08 toks/s, output: 42.65 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 42.65it/s, est. speed input: 21839.08 toks/s, output: 42.65 toks/s]
[rank0]:[W126 16:04:33.750322262 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.4s

测试结果:
  Requests/s:   37.30
  Tokens/s:     19135.65
  Total Reqs:   128
  Elapsed:      3.43s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     19098.35

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:04:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=729044) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=729044) WARNING 01-26 16:04:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=729044) WARNING 01-26 16:05:08 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.69 requests/s, 40677.24 total tokens/s, 39.69 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:04:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:04:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:04:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:04:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:04:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:04:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:04:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:04:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:04:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:04:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:04:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:04:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=729044) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=729044) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.05it/s]
(EngineCore_DP0 pid=729044) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.07s/it]
(EngineCore_DP0 pid=729044) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.05s/it]
(EngineCore_DP0 pid=729044) 
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=729044) 2026-01-26 16:05:08,861 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=729044) 2026-01-26 16:05:08,884 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=729044) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.79it/s]
(EngineCore_DP0 pid=729044) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.88it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 275.42it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 315.74it/s]
Adding requests:  74%|███████▍  | 95/128 [00:00<00:00, 313.53it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 317.67it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 313.49it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:00, 178.45it/s, est. speed input: 182746.85 toks/s, output: 178.46 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 59.64it/s, est. speed input: 67844.67 toks/s, output: 66.25 toks/s]   
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 51.86it/s, est. speed input: 59625.84 toks/s, output: 58.23 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:00<00:01, 48.20it/s, est. speed input: 55954.34 toks/s, output: 54.64 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 46.28it/s, est. speed input: 54089.45 toks/s, output: 52.82 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 44.75it/s, est. speed input: 52637.30 toks/s, output: 51.40 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 43.80it/s, est. speed input: 51690.49 toks/s, output: 50.48 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 42.94it/s, est. speed input: 50860.17 toks/s, output: 49.67 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 42.28it/s, est. speed input: 50156.67 toks/s, output: 48.98 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:01, 41.76it/s, est. speed input: 49546.46 toks/s, output: 48.39 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 41.40it/s, est. speed input: 49021.04 toks/s, output: 47.87 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 41.17it/s, est. speed input: 48568.22 toks/s, output: 47.43 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 40.99it/s, est. speed input: 48164.00 toks/s, output: 47.04 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.77it/s, est. speed input: 47784.05 toks/s, output: 46.66 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.66it/s, est. speed input: 47453.04 toks/s, output: 46.34 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 40.58it/s, est. speed input: 47153.51 toks/s, output: 46.05 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 40.55it/s, est. speed input: 46887.43 toks/s, output: 45.79 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 40.55it/s, est. speed input: 46647.28 toks/s, output: 45.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.55it/s, est. speed input: 46546.20 toks/s, output: 45.46 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.45it/s, est. speed input: 46546.20 toks/s, output: 45.46 toks/s]
[rank0]:[W126 16:05:13.982880668 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.3s

测试结果:
  Requests/s:   39.69
  Tokens/s:     40677.24
  Total Reqs:   128
  Elapsed:      3.23s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     40637.55

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:05:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=730166) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=730166) WARNING 01-26 16:05:40 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=730166) WARNING 01-26 16:05:50 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 77.30 requests/s, 79237.15 total tokens/s, 77.30 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 16:05:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:05:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:05:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:05:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:05:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:05:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:05:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:05:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:05:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:05:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:05:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:05:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:05:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:05:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:05:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:05:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=730166) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=730166) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.03it/s]
(EngineCore_DP0 pid=730166) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.23s/it]
(EngineCore_DP0 pid=730166) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.19s/it]
(EngineCore_DP0 pid=730166) 
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=730166) 2026-01-26 16:05:50,414 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=730166) 2026-01-26 16:05:50,436 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=730166) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.73it/s]
(EngineCore_DP0 pid=730166) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.95it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   9%|▉         | 23/256 [00:00<00:01, 228.72it/s]
Adding requests:  21%|██        | 53/256 [00:00<00:00, 267.45it/s]
Adding requests:  32%|███▏      | 81/256 [00:00<00:00, 271.97it/s]
Adding requests:  43%|████▎     | 109/256 [00:00<00:01, 135.13it/s]
Adding requests:  51%|█████     | 131/256 [00:00<00:00, 151.71it/s]
Adding requests:  62%|██████▎   | 160/256 [00:00<00:00, 183.92it/s]
Adding requests:  76%|███████▌  | 194/256 [00:00<00:00, 222.42it/s]
Adding requests:  89%|████████▉ | 229/256 [00:01<00:00, 254.33it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 220.80it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:00<00:00, 849.60it/s, est. speed input: 870048.88 toks/s, output: 849.61 toks/s]
Processed prompts:  68%|██████▊   | 175/256 [00:01<00:00, 134.13it/s, est. speed input: 157857.51 toks/s, output: 154.16 toks/s]
Processed prompts:  84%|████████▍ | 215/256 [00:01<00:00, 112.72it/s, est. speed input: 134513.60 toks/s, output: 131.36 toks/s]
Processed prompts:  94%|█████████▍| 241/256 [00:01<00:00, 104.56it/s, est. speed input: 126233.97 toks/s, output: 123.28 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 104.56it/s, est. speed input: 121878.40 toks/s, output: 119.02 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 119.02it/s, est. speed input: 121878.40 toks/s, output: 119.02 toks/s]
[rank0]:[W126 16:05:55.817859212 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.7s

测试结果:
  Requests/s:   77.30
  Tokens/s:     79237.15
  Total Reqs:   256
  Elapsed:      3.31s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     79159.85

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:06:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=731312) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=731312) WARNING 01-26 16:06:23 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=731312) WARNING 01-26 16:06:33 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 114.94 requests/s, 117816.03 total tokens/s, 114.94 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 16:06:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:06:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:06:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:06:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:06:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:06:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:06:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:06:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:06:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:06:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:06:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:06:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:06:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:06:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:06:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:06:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=731312) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=731312) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.16it/s]
(EngineCore_DP0 pid=731312) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.09s/it]
(EngineCore_DP0 pid=731312) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]
(EngineCore_DP0 pid=731312) 
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=731312) 2026-01-26 16:06:33,210 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=731312) 2026-01-26 16:06:33,233 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=731312) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 10.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 11.23it/s]
(EngineCore_DP0 pid=731312) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 15.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.25it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 27/512 [00:00<00:01, 265.96it/s]
Adding requests:  12%|█▏        | 61/512 [00:00<00:01, 309.06it/s]
Adding requests:  18%|█▊        | 93/512 [00:00<00:01, 310.16it/s]
Adding requests:  25%|██▍       | 127/512 [00:00<00:01, 318.73it/s]
Adding requests:  31%|███       | 159/512 [00:00<00:01, 313.93it/s]
Adding requests:  38%|███▊      | 195/512 [00:00<00:00, 326.20it/s]
Adding requests:  45%|████▌     | 231/512 [00:00<00:00, 335.43it/s]
Adding requests:  52%|█████▏    | 265/512 [00:00<00:00, 334.87it/s]
Adding requests:  59%|█████▉    | 301/512 [00:00<00:00, 340.87it/s]
Adding requests:  66%|██████▌   | 337/512 [00:01<00:00, 346.22it/s]
Adding requests:  73%|███████▎  | 373/512 [00:01<00:00, 350.35it/s]
Adding requests:  80%|████████  | 410/512 [00:01<00:00, 355.58it/s]
Adding requests:  87%|████████▋ | 446/512 [00:01<00:00, 352.78it/s]
Adding requests:  95%|█████████▍| 485/512 [00:01<00:00, 361.85it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 341.63it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:00<00:00, 1406.07it/s, est. speed input: 1439899.59 toks/s, output: 1406.09 toks/s]
Processed prompts:  63%|██████▎   | 323/512 [00:01<00:00, 209.76it/s, est. speed input: 250878.43 toks/s, output: 245.00 toks/s]   
Processed prompts:  76%|███████▌  | 388/512 [00:01<00:00, 175.47it/s, est. speed input: 213166.86 toks/s, output: 208.17 toks/s]
Processed prompts:  84%|████████▍ | 429/512 [00:02<00:00, 161.50it/s, est. speed input: 199230.97 toks/s, output: 194.56 toks/s]
Processed prompts:  90%|████████▉ | 459/512 [00:02<00:00, 150.62it/s, est. speed input: 190101.11 toks/s, output: 185.65 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:02<00:00, 143.14it/s, est. speed input: 184344.24 toks/s, output: 180.02 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [00:02<00:00, 142.65it/s, est. speed input: 182363.44 toks/s, output: 178.09 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 142.65it/s, est. speed input: 177461.67 toks/s, output: 173.30 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 173.30it/s, est. speed input: 177461.67 toks/s, output: 173.30 toks/s]
[rank0]:[W126 16:06:40.263730177 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.5s

测试结果:
  Requests/s:   114.94
  Tokens/s:     117816.03
  Total Reqs:   512
  Elapsed:      4.45s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     117701.09

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:06:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=732538) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=732538) WARNING 01-26 16:07:10 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=732538) WARNING 01-26 16:07:20 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 123.88 requests/s, 126981.26 total tokens/s, 123.88 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 16:06:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:06:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:06:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:06:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:06:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:06:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:06:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:06:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:07:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:07:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:07:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:07:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:07:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:07:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:07:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:07:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=732538) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=732538) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.30it/s]
(EngineCore_DP0 pid=732538) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
(EngineCore_DP0 pid=732538) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.06it/s]
(EngineCore_DP0 pid=732538) 
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=732538) 2026-01-26 16:07:20,457 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=732538) 2026-01-26 16:07:20,481 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=732538) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 15.75it/s]
(EngineCore_DP0 pid=732538) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 17.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.08it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 278.35it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 318.11it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 315.38it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 320.72it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 330.18it/s]
Adding requests:  19%|█▉        | 199/1024 [00:00<00:02, 339.99it/s]
Adding requests:  23%|██▎       | 235/1024 [00:00<00:02, 346.24it/s]
Adding requests:  26%|██▋       | 270/1024 [00:00<00:02, 344.41it/s]
Adding requests:  30%|██▉       | 306/1024 [00:00<00:02, 348.56it/s]
Adding requests:  33%|███▎      | 343/1024 [00:01<00:01, 352.14it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:01, 356.57it/s]
Adding requests:  41%|████      | 419/1024 [00:01<00:01, 364.16it/s]
Adding requests:  45%|████▍     | 456/1024 [00:01<00:01, 361.78it/s]
Adding requests:  48%|████▊     | 496/1024 [00:01<00:01, 371.09it/s]
Adding requests:  52%|█████▏    | 535/1024 [00:01<00:01, 375.90it/s]
Adding requests:  56%|█████▌    | 573/1024 [00:01<00:01, 373.30it/s]
Adding requests:  60%|█████▉    | 611/1024 [00:01<00:01, 356.69it/s]
Adding requests:  63%|██████▎   | 647/1024 [00:01<00:01, 350.16it/s]
Adding requests:  67%|██████▋   | 683/1024 [00:01<00:00, 350.11it/s]
Adding requests:  70%|███████   | 719/1024 [00:02<00:00, 348.77it/s]
Adding requests:  74%|███████▎  | 754/1024 [00:02<00:00, 343.96it/s]
Adding requests:  77%|███████▋  | 790/1024 [00:02<00:00, 347.57it/s]
Adding requests:  81%|████████  | 827/1024 [00:02<00:00, 352.13it/s]
Adding requests:  84%|████████▍ | 864/1024 [00:02<00:00, 355.66it/s]
Adding requests:  88%|████████▊ | 901/1024 [00:02<00:00, 358.90it/s]
Adding requests:  92%|█████████▏| 937/1024 [00:02<00:00, 355.38it/s]
Adding requests:  95%|█████████▌| 974/1024 [00:02<00:00, 356.66it/s]
Adding requests:  99%|█████████▊| 1010/1024 [00:02<00:00, 352.95it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 350.86it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:00<00:00, 3228.86it/s, est. speed input: 3306575.31 toks/s, output: 3228.92 toks/s]
Processed prompts:  67%|██████▋   | 685/1024 [00:02<00:01, 218.96it/s, est. speed input: 263103.29 toks/s, output: 256.94 toks/s]   
Processed prompts:  81%|████████  | 826/1024 [00:03<00:01, 182.25it/s, est. speed input: 221953.91 toks/s, output: 216.75 toks/s]
Processed prompts:  89%|████████▊ | 908/1024 [00:04<00:00, 169.63it/s, est. speed input: 208945.64 toks/s, output: 204.05 toks/s]
Processed prompts:  94%|█████████▍| 963/1024 [00:04<00:00, 160.89it/s, est. speed input: 201428.68 toks/s, output: 196.71 toks/s]
Processed prompts:  98%|█████████▊| 1003/1024 [00:05<00:00, 155.09it/s, est. speed input: 196937.25 toks/s, output: 192.32 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 155.09it/s, est. speed input: 196144.03 toks/s, output: 191.55 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 191.54it/s, est. speed input: 196144.03 toks/s, output: 191.55 toks/s]
[rank0]:[W126 16:07:31.476315184 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.3s

测试结果:
  Requests/s:   123.88
  Tokens/s:     126981.26
  Total Reqs:   1024
  Elapsed:      8.27s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     126857.37

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:07:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=733881) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=733881) WARNING 01-26 16:08:09 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=733881) WARNING 01-26 16:08:36 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 127.42 requests/s, 130600.86 total tokens/s, 127.42 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 16:07:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:07:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:07:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:07:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:07:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:07:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:07:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:07:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:07:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:07:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:07:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:07:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:07:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:07:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:07:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:07:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=733881) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=733881) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.07it/s]
(EngineCore_DP0 pid=733881) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.19s/it]
(EngineCore_DP0 pid=733881) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.15s/it]
(EngineCore_DP0 pid=733881) 
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=733881) [rank0]:W0126 16:08:14.393000 733881 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=733881) [rank0]:W0126 16:08:14.476000 733881 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=733881) [rank0]:W0126 16:08:15.447000 733881 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=733881) [rank0]:W0126 16:08:15.577000 733881 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=733881) 2026-01-26 16:08:36,009 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=733881) 2026-01-26 16:08:36,099 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=733881) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  4.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:01,  4.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  4.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  4.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:01<00:00,  5.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  5.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  4.18it/s]
(EngineCore_DP0 pid=733881) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  5.03it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  5.31it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  5.11it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  5.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  5.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  5.07it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 11/2048 [00:00<00:19, 105.80it/s]
Adding requests:   1%|          | 22/2048 [00:00<00:20, 96.80it/s] 
Adding requests:   2%|▏         | 35/2048 [00:00<00:18, 108.07it/s]
Adding requests:   2%|▏         | 48/2048 [00:00<00:17, 114.09it/s]
Adding requests:   3%|▎         | 69/2048 [00:00<00:13, 145.75it/s]
Adding requests:   4%|▍         | 91/2048 [00:00<00:11, 169.85it/s]
Adding requests:   6%|▌         | 114/2048 [00:00<00:10, 186.13it/s]
Adding requests:   7%|▋         | 140/2048 [00:00<00:09, 207.42it/s]
Adding requests:   8%|▊         | 170/2048 [00:00<00:08, 234.52it/s]
Adding requests:  10%|▉         | 201/2048 [00:01<00:07, 257.00it/s]
Adding requests:  11%|█         | 227/2048 [00:01<00:07, 236.40it/s]
Adding requests:  12%|█▏        | 252/2048 [00:01<00:07, 229.01it/s]
Adding requests:  14%|█▎        | 280/2048 [00:01<00:07, 242.55it/s]
Adding requests:  15%|█▌        | 312/2048 [00:01<00:06, 262.88it/s]
Adding requests:  17%|█▋        | 345/2048 [00:01<00:06, 279.90it/s]
Adding requests:  18%|█▊        | 377/2048 [00:01<00:05, 288.73it/s]
Adding requests:  20%|█▉        | 408/2048 [00:01<00:05, 294.54it/s]
Adding requests:  21%|██▏       | 440/2048 [00:01<00:05, 301.01it/s]
Adding requests:  23%|██▎       | 473/2048 [00:02<00:05, 303.55it/s]
Adding requests:  25%|██▍       | 507/2048 [00:02<00:04, 313.12it/s]
Adding requests:  26%|██▋       | 542/2048 [00:02<00:04, 321.47it/s]
Adding requests:  28%|██▊       | 575/2048 [00:02<00:04, 322.91it/s]
Adding requests:  30%|██▉       | 608/2048 [00:02<00:04, 310.52it/s]
Adding requests:  31%|███▏      | 640/2048 [00:02<00:04, 301.40it/s]
Adding requests:  33%|███▎      | 671/2048 [00:02<00:04, 297.70it/s]
Adding requests:  34%|███▍      | 704/2048 [00:02<00:04, 305.54it/s]
Adding requests:  36%|███▌      | 735/2048 [00:02<00:04, 300.40it/s]
Adding requests:  37%|███▋      | 766/2048 [00:02<00:04, 303.11it/s]
Adding requests:  39%|███▉      | 797/2048 [00:03<00:04, 302.81it/s]
Adding requests:  40%|████      | 829/2048 [00:03<00:03, 307.41it/s]
Adding requests:  42%|████▏     | 861/2048 [00:03<00:03, 310.12it/s]
Adding requests:  44%|████▎     | 893/2048 [00:03<00:03, 308.40it/s]
Adding requests:  45%|████▌     | 924/2048 [00:03<00:03, 303.68it/s]
Adding requests:  47%|████▋     | 957/2048 [00:03<00:03, 309.89it/s]
Adding requests:  48%|████▊     | 989/2048 [00:03<00:03, 307.03it/s]
Adding requests:  50%|████▉     | 1020/2048 [00:03<00:03, 303.21it/s]
Adding requests:  51%|█████▏    | 1051/2048 [00:03<00:03, 292.83it/s]
Adding requests:  53%|█████▎    | 1081/2048 [00:04<00:03, 282.12it/s]
Adding requests:  54%|█████▍    | 1111/2048 [00:04<00:03, 285.26it/s]
Adding requests:  56%|█████▌    | 1143/2048 [00:04<00:03, 294.46it/s]
Adding requests:  57%|█████▋    | 1173/2048 [00:04<00:03, 285.44it/s]
Adding requests:  59%|█████▊    | 1203/2048 [00:04<00:02, 287.72it/s]
Adding requests:  60%|██████    | 1235/2048 [00:04<00:02, 296.49it/s]
Adding requests:  62%|██████▏   | 1265/2048 [00:04<00:02, 296.77it/s]
Adding requests:  63%|██████▎   | 1295/2048 [00:04<00:02, 293.13it/s]
Adding requests:  65%|██████▍   | 1326/2048 [00:04<00:02, 297.47it/s]
Adding requests:  66%|██████▋   | 1357/2048 [00:04<00:02, 298.29it/s]
Adding requests:  68%|██████▊   | 1387/2048 [00:05<00:02, 286.42it/s]
Adding requests:  69%|██████▉   | 1416/2048 [00:05<00:02, 285.80it/s]
Adding requests:  71%|███████   | 1445/2048 [00:05<00:02, 283.87it/s]
Adding requests:  72%|███████▏  | 1474/2048 [00:05<00:02, 282.78it/s]
Adding requests:  73%|███████▎  | 1504/2048 [00:05<00:01, 287.00it/s]
Adding requests:  75%|███████▍  | 1533/2048 [00:05<00:01, 282.13it/s]
Adding requests:  76%|███████▋  | 1562/2048 [00:05<00:01, 281.54it/s]
Adding requests:  78%|███████▊  | 1591/2048 [00:05<00:01, 270.67it/s]
Adding requests:  79%|███████▉  | 1619/2048 [00:05<00:01, 235.75it/s]
Adding requests:  80%|████████  | 1644/2048 [00:06<00:01, 238.19it/s]
Adding requests:  82%|████████▏ | 1670/2048 [00:06<00:01, 242.81it/s]
Adding requests:  83%|████████▎ | 1695/2048 [00:06<00:01, 234.71it/s]
Adding requests:  84%|████████▍ | 1721/2048 [00:06<00:01, 238.56it/s]
Adding requests:  85%|████████▌ | 1749/2048 [00:06<00:01, 248.66it/s]
Adding requests:  87%|████████▋ | 1778/2048 [00:06<00:01, 260.18it/s]
Adding requests:  88%|████████▊ | 1807/2048 [00:06<00:00, 267.83it/s]
Adding requests:  90%|████████▉ | 1838/2048 [00:06<00:00, 277.77it/s]
Adding requests:  91%|█████████ | 1868/2048 [00:06<00:00, 283.24it/s]
Adding requests:  93%|█████████▎| 1897/2048 [00:07<00:00, 276.94it/s]
Adding requests:  94%|█████████▍| 1927/2048 [00:07<00:00, 282.98it/s]
Adding requests:  96%|█████████▌| 1957/2048 [00:07<00:00, 287.64it/s]
Adding requests:  97%|█████████▋| 1986/2048 [00:07<00:00, 278.71it/s]
Adding requests:  98%|█████████▊| 2014/2048 [00:07<00:00, 223.27it/s]
Adding requests: 100%|█████████▉| 2042/2048 [00:07<00:00, 236.30it/s]
Adding requests: 100%|██████████| 2048/2048 [00:07<00:00, 268.64it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  46%|████▌     | 945/2048 [00:00<00:00, 6444.56it/s, est. speed input: 6600539.09 toks/s, output: 6445.02 toks/s]
Processed prompts:  78%|███████▊  | 1590/2048 [00:05<00:01, 262.89it/s, est. speed input: 324740.79 toks/s, output: 317.13 toks/s]  
Processed prompts:  91%|█████████ | 1864/2048 [00:07<00:00, 214.50it/s, est. speed input: 269751.22 toks/s, output: 263.43 toks/s]
Processed prompts:  99%|█████████▊| 2019/2048 [00:08<00:00, 192.52it/s, est. speed input: 248374.06 toks/s, output: 242.55 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:08<00:00, 192.52it/s, est. speed input: 248365.15 toks/s, output: 242.54 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:08<00:00, 242.54it/s, est. speed input: 248365.15 toks/s, output: 242.54 toks/s]
[rank0]:[W126 16:09:03.482897780 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 94.6s

测试结果:
  Requests/s:   127.42
  Tokens/s:     130600.86
  Total Reqs:   2048
  Elapsed:      16.07s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     130473.44

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:09:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=735933) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=735933) WARNING 01-26 16:10:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=735933) WARNING 01-26 16:10:25 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 131.41 requests/s, 134690.99 total tokens/s, 131.41 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 16:09:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:09:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:09:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:09:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:09:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:09:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:09:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:09:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:09:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:09:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:09:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:09:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:09:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:09:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:09:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:09:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=735933) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=735933) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.23s/it]
(EngineCore_DP0 pid=735933) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.33s/it]
(EngineCore_DP0 pid=735933) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.31s/it]
(EngineCore_DP0 pid=735933) 
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=735933) [rank0]:W0126 16:10:17.805000 735933 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=735933) [rank0]:W0126 16:10:17.900000 735933 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=735933) [rank0]:W0126 16:10:19.001000 735933 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=735933) [rank0]:W0126 16:10:19.148000 735933 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=735933) 2026-01-26 16:10:24,788 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=735933) 2026-01-26 16:10:24,858 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=735933) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 13.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 15.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.30it/s]
(EngineCore_DP0 pid=735933) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 14.54it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 12.31it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 10.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 11.88it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   0%|          | 9/4096 [00:00<00:45, 89.41it/s]
Adding requests:   1%|          | 23/4096 [00:00<00:35, 115.44it/s]
Adding requests:   1%|          | 38/4096 [00:00<00:31, 129.42it/s]
Adding requests:   1%|▏         | 52/4096 [00:00<00:30, 131.48it/s]
Adding requests:   2%|▏         | 80/4096 [00:00<00:22, 182.36it/s]
Adding requests:   3%|▎         | 104/4096 [00:00<00:20, 199.43it/s]
Adding requests:   3%|▎         | 130/4096 [00:00<00:18, 218.36it/s]
Adding requests:   4%|▍         | 159/4096 [00:00<00:16, 238.62it/s]
Adding requests:   5%|▍         | 187/4096 [00:00<00:15, 250.69it/s]
Adding requests:   5%|▌         | 213/4096 [00:01<00:15, 251.00it/s]
Adding requests:   6%|▌         | 239/4096 [00:01<00:15, 251.13it/s]
Adding requests:   6%|▋         | 265/4096 [00:01<00:15, 252.38it/s]
Adding requests:   7%|▋         | 293/4096 [00:01<00:14, 259.49it/s]
Adding requests:   8%|▊         | 323/4096 [00:01<00:13, 271.46it/s]
Adding requests:   9%|▊         | 354/4096 [00:01<00:13, 282.09it/s]
Adding requests:   9%|▉         | 384/4096 [00:01<00:13, 284.32it/s]
Adding requests:  10%|█         | 415/4096 [00:01<00:12, 289.02it/s]
Adding requests:  11%|█         | 444/4096 [00:01<00:12, 285.69it/s]
Adding requests:  12%|█▏        | 473/4096 [00:01<00:12, 279.53it/s]
Adding requests:  12%|█▏        | 501/4096 [00:02<00:13, 271.76it/s]
Adding requests:  13%|█▎        | 529/4096 [00:02<00:15, 231.53it/s]
Adding requests:  14%|█▎        | 554/4096 [00:02<00:18, 189.65it/s]
Adding requests:  14%|█▍        | 575/4096 [00:02<00:21, 166.35it/s]
Adding requests:  15%|█▍        | 594/4096 [00:02<00:21, 161.79it/s]
Adding requests:  15%|█▍        | 612/4096 [00:02<00:22, 155.96it/s]
Adding requests:  15%|█▌        | 629/4096 [00:02<00:22, 156.13it/s]
Adding requests:  16%|█▌        | 646/4096 [00:03<00:22, 152.23it/s]
Adding requests:  16%|█▌        | 662/4096 [00:03<00:23, 149.05it/s]
Adding requests:  17%|█▋        | 678/4096 [00:03<00:23, 147.08it/s]
Adding requests:  17%|█▋        | 706/4096 [00:03<00:18, 180.99it/s]
Adding requests:  18%|█▊        | 733/4096 [00:03<00:16, 204.11it/s]
Adding requests:  18%|█▊        | 754/4096 [00:03<00:16, 200.11it/s]
Adding requests:  19%|█▉        | 775/4096 [00:03<00:18, 176.21it/s]
Adding requests:  19%|█▉        | 794/4096 [00:03<00:18, 177.70it/s]
Adding requests:  20%|█▉        | 819/4096 [00:03<00:16, 196.14it/s]
Adding requests:  21%|██        | 846/4096 [00:04<00:16, 199.94it/s]
Adding requests:  21%|██        | 867/4096 [00:04<00:17, 183.52it/s]
Adding requests:  22%|██▏       | 886/4096 [00:04<00:19, 162.12it/s]
Adding requests:  22%|██▏       | 908/4096 [00:04<00:18, 175.62it/s]
Adding requests:  23%|██▎       | 935/4096 [00:04<00:15, 198.04it/s]
Adding requests:  24%|██▎       | 966/4096 [00:04<00:13, 226.50it/s]
Adding requests:  24%|██▍       | 994/4096 [00:04<00:12, 240.52it/s]
Adding requests:  25%|██▍       | 1022/4096 [00:04<00:12, 250.50it/s]
Adding requests:  26%|██▌       | 1048/4096 [00:05<00:12, 252.76it/s]
Adding requests:  26%|██▌       | 1075/4096 [00:05<00:11, 257.60it/s]
Adding requests:  27%|██▋       | 1102/4096 [00:05<00:11, 252.13it/s]
Adding requests:  28%|██▊       | 1128/4096 [00:05<00:11, 249.61it/s]
Adding requests:  28%|██▊       | 1155/4096 [00:05<00:11, 255.03it/s]
Adding requests:  29%|██▉       | 1182/4096 [00:05<00:11, 258.80it/s]
Adding requests:  30%|██▉       | 1213/4096 [00:05<00:10, 271.33it/s]
Adding requests:  30%|███       | 1243/4096 [00:05<00:10, 277.60it/s]
Adding requests:  31%|███       | 1271/4096 [00:05<00:11, 240.13it/s]
Adding requests:  32%|███▏      | 1296/4096 [00:06<00:11, 235.87it/s]
Adding requests:  32%|███▏      | 1322/4096 [00:06<00:11, 241.65it/s]
Adding requests:  33%|███▎      | 1347/4096 [00:06<00:12, 217.85it/s]
Adding requests:  33%|███▎      | 1370/4096 [00:06<00:13, 197.85it/s]
Adding requests:  34%|███▍      | 1391/4096 [00:06<00:14, 192.00it/s]
Adding requests:  34%|███▍      | 1411/4096 [00:06<00:15, 175.85it/s]
Adding requests:  35%|███▌      | 1438/4096 [00:06<00:13, 198.91it/s]
Adding requests:  36%|███▌      | 1467/4096 [00:06<00:11, 220.90it/s]
Adding requests:  36%|███▋      | 1495/4096 [00:06<00:11, 234.81it/s]
Adding requests:  37%|███▋      | 1520/4096 [00:07<00:10, 236.35it/s]
Adding requests:  38%|███▊      | 1546/4096 [00:07<00:10, 242.83it/s]
Adding requests:  38%|███▊      | 1571/4096 [00:07<00:10, 241.04it/s]
Adding requests:  39%|███▉      | 1600/4096 [00:07<00:09, 253.75it/s]
Adding requests:  40%|███▉      | 1628/4096 [00:07<00:09, 259.26it/s]
Adding requests:  40%|████      | 1655/4096 [00:07<00:09, 257.23it/s]
Adding requests:  41%|████      | 1681/4096 [00:07<00:09, 250.06it/s]
Adding requests:  42%|████▏     | 1708/4096 [00:07<00:09, 255.43it/s]
Adding requests:  42%|████▏     | 1736/4096 [00:07<00:08, 262.28it/s]
Adding requests:  43%|████▎     | 1768/4096 [00:07<00:08, 277.17it/s]
Adding requests:  44%|████▍     | 1796/4096 [00:08<00:08, 273.18it/s]
Adding requests:  45%|████▍     | 1824/4096 [00:08<00:08, 272.08it/s]
Adding requests:  45%|████▌     | 1852/4096 [00:08<00:09, 245.14it/s]
Adding requests:  46%|████▌     | 1878/4096 [00:08<00:11, 192.67it/s]
Adding requests:  46%|████▋     | 1900/4096 [00:08<00:12, 174.98it/s]
Adding requests:  47%|████▋     | 1920/4096 [00:08<00:12, 177.97it/s]
Adding requests:  47%|████▋     | 1944/4096 [00:08<00:11, 192.45it/s]
Adding requests:  48%|████▊     | 1974/4096 [00:09<00:09, 217.74it/s]
Adding requests:  49%|████▉     | 2003/4096 [00:09<00:08, 234.89it/s]
Adding requests:  50%|████▉     | 2031/4096 [00:09<00:08, 245.59it/s]
Adding requests:  50%|█████     | 2058/4096 [00:09<00:08, 249.95it/s]
Adding requests:  51%|█████     | 2084/4096 [00:09<00:08, 248.16it/s]
Adding requests:  52%|█████▏    | 2115/4096 [00:09<00:07, 265.02it/s]
Adding requests:  52%|█████▏    | 2144/4096 [00:09<00:07, 269.85it/s]
Adding requests:  53%|█████▎    | 2172/4096 [00:09<00:07, 258.74it/s]
Adding requests:  54%|█████▎    | 2199/4096 [00:09<00:07, 254.69it/s]
Adding requests:  54%|█████▍    | 2226/4096 [00:09<00:07, 256.88it/s]
Adding requests:  55%|█████▌    | 2254/4096 [00:10<00:07, 262.92it/s]
Adding requests:  56%|█████▌    | 2282/4096 [00:10<00:06, 267.46it/s]
Adding requests:  56%|█████▋    | 2309/4096 [00:10<00:06, 267.21it/s]
Adding requests:  57%|█████▋    | 2341/4096 [00:10<00:06, 279.82it/s]
Adding requests:  58%|█████▊    | 2370/4096 [00:10<00:06, 252.18it/s]
Adding requests:  58%|█████▊    | 2396/4096 [00:10<00:07, 238.00it/s]
Adding requests:  59%|█████▉    | 2424/4096 [00:10<00:06, 246.96it/s]
Adding requests:  60%|█████▉    | 2452/4096 [00:10<00:06, 254.27it/s]
Adding requests:  60%|██████    | 2478/4096 [00:10<00:06, 250.46it/s]
Adding requests:  61%|██████    | 2504/4096 [00:11<00:06, 238.73it/s]
Adding requests:  62%|██████▏   | 2533/4096 [00:11<00:06, 250.95it/s]
Adding requests:  63%|██████▎   | 2564/4096 [00:11<00:05, 265.21it/s]
Adding requests:  63%|██████▎   | 2592/4096 [00:11<00:05, 267.17it/s]
Adding requests:  64%|██████▍   | 2619/4096 [00:11<00:05, 267.52it/s]
Adding requests:  65%|██████▍   | 2646/4096 [00:11<00:05, 263.39it/s]
Adding requests:  65%|██████▌   | 2673/4096 [00:11<00:05, 263.15it/s]
Adding requests:  66%|██████▌   | 2700/4096 [00:11<00:05, 263.82it/s]
Adding requests:  67%|██████▋   | 2727/4096 [00:11<00:05, 262.84it/s]
Adding requests:  67%|██████▋   | 2759/4096 [00:12<00:04, 277.29it/s]
Adding requests:  68%|██████▊   | 2789/4096 [00:12<00:04, 283.68it/s]
Adding requests:  69%|██████▉   | 2821/4096 [00:12<00:04, 292.56it/s]
Adding requests:  70%|██████▉   | 2852/4096 [00:12<00:04, 294.11it/s]
Adding requests:  70%|███████   | 2882/4096 [00:12<00:04, 291.69it/s]
Adding requests:  71%|███████   | 2913/4096 [00:12<00:04, 295.68it/s]
Adding requests:  72%|███████▏  | 2944/4096 [00:12<00:03, 298.12it/s]
Adding requests:  73%|███████▎  | 2974/4096 [00:12<00:05, 213.48it/s]
Adding requests:  73%|███████▎  | 2999/4096 [00:12<00:04, 220.86it/s]
Adding requests:  74%|███████▍  | 3027/4096 [00:13<00:04, 234.03it/s]
Adding requests:  75%|███████▍  | 3057/4096 [00:13<00:04, 250.77it/s]
Adding requests:  75%|███████▌  | 3088/4096 [00:13<00:03, 266.35it/s]
Adding requests:  76%|███████▌  | 3119/4096 [00:13<00:03, 278.14it/s]
Adding requests:  77%|███████▋  | 3149/4096 [00:13<00:03, 282.81it/s]
Adding requests:  78%|███████▊  | 3179/4096 [00:13<00:03, 282.70it/s]
Adding requests:  78%|███████▊  | 3208/4096 [00:13<00:03, 275.64it/s]
Adding requests:  79%|███████▉  | 3240/4096 [00:13<00:02, 285.51it/s]
Adding requests:  80%|███████▉  | 3269/4096 [00:13<00:02, 284.98it/s]
Adding requests:  81%|████████  | 3298/4096 [00:14<00:02, 271.37it/s]
Adding requests:  81%|████████  | 3326/4096 [00:14<00:02, 258.52it/s]
Adding requests:  82%|████████▏ | 3356/4096 [00:14<00:02, 268.04it/s]
Adding requests:  83%|████████▎ | 3385/4096 [00:14<00:02, 273.66it/s]
Adding requests:  83%|████████▎ | 3413/4096 [00:14<00:02, 233.56it/s]
Adding requests:  84%|████████▍ | 3438/4096 [00:14<00:03, 204.54it/s]
Adding requests:  84%|████████▍ | 3460/4096 [00:14<00:03, 208.03it/s]
Adding requests:  85%|████████▌ | 3482/4096 [00:14<00:03, 204.39it/s]
Adding requests:  86%|████████▌ | 3504/4096 [00:15<00:03, 183.46it/s]
Adding requests:  86%|████████▋ | 3533/4096 [00:15<00:02, 207.88it/s]
Adding requests:  87%|████████▋ | 3564/4096 [00:15<00:02, 232.89it/s]
Adding requests:  88%|████████▊ | 3593/4096 [00:15<00:02, 247.15it/s]
Adding requests:  88%|████████▊ | 3622/4096 [00:15<00:01, 258.41it/s]
Adding requests:  89%|████████▉ | 3653/4096 [00:15<00:01, 269.78it/s]
Adding requests:  90%|████████▉ | 3681/4096 [00:15<00:01, 266.22it/s]
Adding requests:  91%|█████████ | 3708/4096 [00:15<00:01, 266.24it/s]
Adding requests:  91%|█████████ | 3735/4096 [00:15<00:01, 220.50it/s]
Adding requests:  92%|█████████▏| 3759/4096 [00:16<00:01, 171.50it/s]
Adding requests:  92%|█████████▏| 3779/4096 [00:16<00:01, 162.84it/s]
Adding requests:  93%|█████████▎| 3797/4096 [00:16<00:01, 157.70it/s]
Adding requests:  93%|█████████▎| 3814/4096 [00:16<00:01, 153.28it/s]
Adding requests:  94%|█████████▎| 3831/4096 [00:16<00:01, 152.19it/s]
Adding requests:  94%|█████████▍| 3849/4096 [00:16<00:01, 158.22it/s]
Adding requests:  95%|█████████▍| 3879/4096 [00:16<00:01, 195.05it/s]
Adding requests:  95%|█████████▌| 3906/4096 [00:16<00:00, 213.81it/s]
Adding requests:  96%|█████████▌| 3932/4096 [00:17<00:00, 224.78it/s]
Adding requests:  97%|█████████▋| 3961/4096 [00:17<00:00, 243.15it/s]
Adding requests:  97%|█████████▋| 3990/4096 [00:17<00:00, 254.67it/s]
Adding requests:  98%|█████████▊| 4020/4096 [00:17<00:00, 264.96it/s]
Adding requests:  99%|█████████▉| 4048/4096 [00:17<00:00, 268.87it/s]
Adding requests: 100%|█████████▉| 4077/4096 [00:17<00:00, 274.37it/s]
Adding requests: 100%|██████████| 4096/4096 [00:17<00:00, 232.20it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  56%|█████▌    | 2291/4096 [00:00<00:00, 7133.19it/s, est. speed input: 7305016.88 toks/s, output: 7133.41 toks/s]
Processed prompts:  73%|███████▎  | 3005/4096 [00:05<00:02, 426.44it/s, est. speed input: 556308.65 toks/s, output: 543.27 toks/s]   
Processed prompts:  81%|████████  | 3307/4096 [00:07<00:02, 322.56it/s, est. speed input: 440195.20 toks/s, output: 429.88 toks/s]
Processed prompts:  85%|████████▍ | 3478/4096 [00:09<00:02, 269.72it/s, est. speed input: 389445.45 toks/s, output: 380.32 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:09<00:02, 251.68it/s, est. speed input: 372550.23 toks/s, output: 363.82 toks/s]
Processed prompts:  89%|████████▉ | 3661/4096 [00:10<00:01, 239.53it/s, est. speed input: 362740.03 toks/s, output: 354.24 toks/s]
Processed prompts:  91%|█████████ | 3716/4096 [00:10<00:01, 220.87it/s, est. speed input: 352178.18 toks/s, output: 343.92 toks/s]
Processed prompts:  92%|█████████▏| 3757/4096 [00:11<00:01, 216.15it/s, est. speed input: 348429.37 toks/s, output: 340.26 toks/s]
Processed prompts:  93%|█████████▎| 3791/4096 [00:11<00:01, 206.71it/s, est. speed input: 344070.31 toks/s, output: 336.01 toks/s]
Processed prompts:  93%|█████████▎| 3819/4096 [00:11<00:01, 193.16it/s, est. speed input: 339417.57 toks/s, output: 331.46 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [00:11<00:01, 175.23it/s, est. speed input: 334365.72 toks/s, output: 326.53 toks/s]
Processed prompts:  94%|█████████▍| 3861/4096 [00:12<00:01, 155.23it/s, est. speed input: 329183.45 toks/s, output: 321.47 toks/s]
Processed prompts:  95%|█████████▍| 3891/4096 [00:12<00:01, 147.97it/s, est. speed input: 325145.67 toks/s, output: 317.52 toks/s]
Processed prompts:  96%|█████████▌| 3923/4096 [00:12<00:01, 147.37it/s, est. speed input: 322033.59 toks/s, output: 314.49 toks/s]
Processed prompts:  97%|█████████▋| 3955/4096 [00:12<00:00, 143.64it/s, est. speed input: 318519.82 toks/s, output: 311.05 toks/s]
Processed prompts:  97%|█████████▋| 3987/4096 [00:12<00:00, 141.03it/s, est. speed input: 315175.78 toks/s, output: 307.79 toks/s]
Processed prompts:  98%|█████████▊| 4019/4096 [00:13<00:00, 138.34it/s, est. speed input: 311847.74 toks/s, output: 304.54 toks/s]
Processed prompts:  99%|█████████▉| 4051/4096 [00:13<00:00, 140.94it/s, est. speed input: 309270.53 toks/s, output: 302.02 toks/s]
Processed prompts: 100%|█████████▉| 4083/4096 [00:13<00:00, 163.98it/s, est. speed input: 309074.56 toks/s, output: 301.83 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:13<00:00, 163.98it/s, est. speed input: 310049.77 toks/s, output: 302.78 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:13<00:00, 302.78it/s, est. speed input: 310049.77 toks/s, output: 302.78 toks/s]
[rank0]:[W126 16:11:00.500836382 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 116.0s

测试结果:
  Requests/s:   131.41
  Tokens/s:     134690.99
  Total Reqs:   4096
  Elapsed:      31.17s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     134559.58

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:12:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=738393) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=738393) WARNING 01-26 16:12:32 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     def forward(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     raise e
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/tmp/torchinductor_root/77/c77zs2iegpb4fphxop7pddy5hebnskano2xyzjd5gklmhiigfqg3.py", line 1093, in call
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 8)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 327, in quant_slide_int8_triton
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered


─── STDERR ───
[2026-01-26 16:12:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:12:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:12:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:12:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:12:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:12:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:12:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:12:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:12:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:12:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:12:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:12:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:12:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:12:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:12:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:12:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=738393) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=738393) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=738393) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01s/it]
(EngineCore_DP0 pid=738393) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.02it/s]
(EngineCore_DP0 pid=738393) 
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=738393) [rank0]:W0126 16:12:38.159000 738393 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=738393) [rank0]:W0126 16:12:38.246000 738393 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=738393) [rank0]:W0126 16:12:39.428000 738393 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=738393) [rank0]:W0126 16:12:39.561000 738393 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=738393) Process EngineCore_DP0:
(EngineCore_DP0 pid=738393) Traceback (most recent call last):
(EngineCore_DP0 pid=738393)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=738393)     self.run()
(EngineCore_DP0 pid=738393)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=738393)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=738393)     raise e
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=738393)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=738393)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=738393)     super().__init__(
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=738393)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=738393)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=738393)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=738393)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=738393)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=738393)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=738393)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=738393)     return func(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=738393)     return func(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=738393)     self.model_runner.profile_run()
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=738393)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=738393)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=738393)     return func(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=738393)     outputs = self.model(
(EngineCore_DP0 pid=738393)               ^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=738393)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=738393)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=738393)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=738393)     hidden_states = self.model(
(EngineCore_DP0 pid=738393)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=738393)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=738393)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=738393)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=738393)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=738393)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=738393)     def forward(
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=738393)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=738393)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=738393)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=738393)     raise e
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=738393)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=738393)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=738393)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=738393)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=738393)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=738393)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=738393)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=738393)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=738393)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=738393)     return compiled_fn(full_args)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=738393)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=738393)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=738393)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=738393)                             ^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=738393)     outs = compiled_fn(args)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=738393)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=738393)     return self.current_callable(inputs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=738393)     out = model(new_inputs)
(EngineCore_DP0 pid=738393)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/tmp/torchinductor_root/77/c77zs2iegpb4fphxop7pddy5hebnskano2xyzjd5gklmhiigfqg3.py", line 1093, in call
(EngineCore_DP0 pid=738393)     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 8)
(EngineCore_DP0 pid=738393)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=738393)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=738393)     return fn(input, L)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 327, in quant_slide_int8_triton
(EngineCore_DP0 pid=738393)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=738393)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=738393)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=738393)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=738393)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=738393)     self._init_handles()
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=738393)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=738393)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 16:12:40.046627870 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,37.3015,19135.6467,3.4315
1024,1024,1,128,128,39.6851,40677.2369,3.2254
2048,1024,2,256,128,77.3045,79237.1501,3.3116
4096,1024,4,512,128,114.9425,117816.0282,4.4544
8192,1024,8,1024,128,123.8842,126981.2572,8.2658
16384,1024,16,2048,128,127.4155,130600.8551,16.0734
32768,1024,32,4096,128,131.4058,134690.9873,31.1706
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:12:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=739510) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=739510) WARNING 01-26 16:13:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=739510) WARNING 01-26 16:13:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 38.30 requests/s, 19646.34 total tokens/s, 38.30 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:12:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:12:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:12:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:12:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:12:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:12:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:12:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:12:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:12:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:13:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:13:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:13:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:13:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:13:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:13:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:13:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=739510) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=739510) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.01it/s]
(EngineCore_DP0 pid=739510) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.27s/it]
(EngineCore_DP0 pid=739510) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.23s/it]
(EngineCore_DP0 pid=739510) 
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=739510) 2026-01-26 16:13:21,052 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=739510) 2026-01-26 16:13:21,075 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=739510) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.54it/s]
(EngineCore_DP0 pid=739510) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.19it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 517.40it/s]
Adding requests:  88%|████████▊ | 112/128 [00:00<00:00, 561.55it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 559.69it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 52.95it/s, est. speed input: 27111.67 toks/s, output: 52.95 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:02, 44.98it/s, est. speed input: 23562.95 toks/s, output: 46.02 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 43.21it/s, est. speed input: 22726.36 toks/s, output: 44.39 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 42.37it/s, est. speed input: 22309.79 toks/s, output: 43.57 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 41.78it/s, est. speed input: 22022.42 toks/s, output: 43.01 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 41.42it/s, est. speed input: 21829.13 toks/s, output: 42.63 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 41.18it/s, est. speed input: 21688.85 toks/s, output: 42.36 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:02, 40.98it/s, est. speed input: 21573.38 toks/s, output: 42.14 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:01, 40.81it/s, est. speed input: 21478.72 toks/s, output: 41.95 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 40.84it/s, est. speed input: 21426.64 toks/s, output: 41.85 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 40.86it/s, est. speed input: 21382.27 toks/s, output: 41.76 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 40.87it/s, est. speed input: 21345.43 toks/s, output: 41.69 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 40.86it/s, est. speed input: 21312.01 toks/s, output: 41.62 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 40.77it/s, est. speed input: 21274.17 toks/s, output: 41.55 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 40.77it/s, est. speed input: 21246.95 toks/s, output: 41.50 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 40.75it/s, est. speed input: 21221.83 toks/s, output: 41.45 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 40.71it/s, est. speed input: 21197.37 toks/s, output: 41.40 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 40.73it/s, est. speed input: 21179.45 toks/s, output: 41.37 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 40.68it/s, est. speed input: 21158.24 toks/s, output: 41.32 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 40.62it/s, est. speed input: 21136.20 toks/s, output: 41.28 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 40.65it/s, est. speed input: 21122.77 toks/s, output: 41.26 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 40.54it/s, est. speed input: 21100.02 toks/s, output: 41.21 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 40.59it/s, est. speed input: 21089.06 toks/s, output: 41.19 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 40.56it/s, est. speed input: 21073.88 toks/s, output: 41.16 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 40.51it/s, est. speed input: 21058.08 toks/s, output: 41.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.51it/s, est. speed input: 21056.66 toks/s, output: 41.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.12it/s, est. speed input: 21056.66 toks/s, output: 41.13 toks/s]
[rank0]:[W126 16:13:27.415505137 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.5s

测试结果:
  Requests/s:   38.30
  Tokens/s:     19646.34
  Total Reqs:   128
  Elapsed:      3.34s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     19608.04

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:13:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=740752) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=740752) WARNING 01-26 16:13:55 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=740752) WARNING 01-26 16:14:05 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.46 requests/s, 40448.83 total tokens/s, 39.46 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 16:13:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:13:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:13:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:13:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:13:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:13:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:13:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:13:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:13:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:13:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:13:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:13:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:13:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:13:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:13:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:13:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=740752) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=740752) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.02it/s]
(EngineCore_DP0 pid=740752) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]
(EngineCore_DP0 pid=740752) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.21s/it]
(EngineCore_DP0 pid=740752) 
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=740752) 2026-01-26 16:14:05,492 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=740752) 2026-01-26 16:14:05,515 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=740752) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.88it/s]
(EngineCore_DP0 pid=740752) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.14it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  20%|██        | 26/128 [00:00<00:00, 254.56it/s]
Adding requests:  45%|████▌     | 58/128 [00:00<00:00, 289.48it/s]
Adding requests:  68%|██████▊   | 87/128 [00:00<00:00, 256.58it/s]
Adding requests:  91%|█████████▏| 117/128 [00:00<00:00, 272.10it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 273.02it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:00, 183.76it/s, est. speed input: 188180.43 toks/s, output: 183.76 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 60.93it/s, est. speed input: 69731.39 toks/s, output: 68.10 toks/s]   
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 52.91it/s, est. speed input: 61314.82 toks/s, output: 59.88 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 49.02it/s, est. speed input: 57468.45 toks/s, output: 56.12 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 46.81it/s, est. speed input: 55402.57 toks/s, output: 54.10 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 45.10it/s, est. speed input: 53820.50 toks/s, output: 52.56 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 43.87it/s, est. speed input: 52709.58 toks/s, output: 51.47 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 42.96it/s, est. speed input: 51809.40 toks/s, output: 50.59 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:01, 42.24it/s, est. speed input: 51038.81 toks/s, output: 49.84 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 41.65it/s, est. speed input: 50360.01 toks/s, output: 49.18 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 41.32it/s, est. speed input: 49795.38 toks/s, output: 48.63 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 41.02it/s, est. speed input: 49284.09 toks/s, output: 48.13 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 40.89it/s, est. speed input: 48851.35 toks/s, output: 47.71 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 40.58it/s, est. speed input: 48417.89 toks/s, output: 47.28 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 40.46it/s, est. speed input: 48046.50 toks/s, output: 46.92 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 40.40it/s, est. speed input: 47718.65 toks/s, output: 46.60 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 40.34it/s, est. speed input: 47416.00 toks/s, output: 46.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.34it/s, est. speed input: 47263.83 toks/s, output: 46.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 46.15it/s, est. speed input: 47263.83 toks/s, output: 46.16 toks/s]
[rank0]:[W126 16:14:10.006281229 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.4s

测试结果:
  Requests/s:   39.46
  Tokens/s:     40448.83
  Total Reqs:   128
  Elapsed:      3.24s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     40409.37

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:14:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=741907) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=741907) WARNING 01-26 16:14:39 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=741907) WARNING 01-26 16:14:49 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 76.88 requests/s, 78802.15 total tokens/s, 76.88 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 16:14:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:14:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:14:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:14:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:14:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:14:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:14:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:14:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:14:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:14:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:14:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:14:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:14:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:14:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:14:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:14:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:29] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=741907) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=741907) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.02s/it]
(EngineCore_DP0 pid=741907) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.29s/it]
(EngineCore_DP0 pid=741907) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]
(EngineCore_DP0 pid=741907) 
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=741907) 2026-01-26 16:14:49,218 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=741907) 2026-01-26 16:14:49,241 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=741907) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 13.26it/s]
(EngineCore_DP0 pid=741907) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.51it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.49it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 20/256 [00:00<00:01, 196.92it/s]
Adding requests:  20%|█▉        | 51/256 [00:00<00:00, 262.27it/s]
Adding requests:  33%|███▎      | 84/256 [00:00<00:00, 288.74it/s]
Adding requests:  44%|████▍     | 113/256 [00:00<00:01, 124.91it/s]
Adding requests:  56%|█████▋    | 144/256 [00:00<00:00, 160.58it/s]
Adding requests:  69%|██████▉   | 177/256 [00:00<00:00, 196.90it/s]
Adding requests:  83%|████████▎ | 212/256 [00:01<00:00, 232.74it/s]
Adding requests:  96%|█████████▌| 246/256 [00:01<00:00, 259.00it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 217.20it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:00<00:00, 918.72it/s, est. speed input: 940840.44 toks/s, output: 918.74 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:01<00:00, 128.92it/s, est. speed input: 151555.00 toks/s, output: 148.00 toks/s]
Processed prompts:  89%|████████▊ | 227/256 [00:01<00:00, 110.33it/s, est. speed input: 130882.79 toks/s, output: 127.81 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:02<00:00, 101.62it/s, est. speed input: 122416.27 toks/s, output: 119.55 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 101.62it/s, est. speed input: 121964.97 toks/s, output: 119.11 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 119.10it/s, est. speed input: 121964.97 toks/s, output: 119.11 toks/s]
[rank0]:[W126 16:14:54.881211953 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.6s

测试结果:
  Requests/s:   76.88
  Tokens/s:     78802.15
  Total Reqs:   256
  Elapsed:      3.33s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     78725.27

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:15:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=743076) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=743076) WARNING 01-26 16:15:23 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=743076) WARNING 01-26 16:15:33 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 111.13 requests/s, 113905.16 total tokens/s, 111.13 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 16:15:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:15:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:15:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:15:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:15:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:15:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:15:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:15:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:15:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:15:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:15:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:15:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:15:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:15:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:15:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:15:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=743076) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=743076) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.10it/s]
(EngineCore_DP0 pid=743076) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.16s/it]
(EngineCore_DP0 pid=743076) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.12s/it]
(EngineCore_DP0 pid=743076) 
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=743076) 2026-01-26 16:15:33,554 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=743076) 2026-01-26 16:15:33,577 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=743076) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.76it/s]
(EngineCore_DP0 pid=743076) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.30it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 17.49it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 23/512 [00:00<00:02, 226.72it/s]
Adding requests:  11%|█         | 54/512 [00:00<00:01, 274.73it/s]
Adding requests:  17%|█▋        | 87/512 [00:00<00:01, 298.60it/s]
Adding requests:  23%|██▎       | 120/512 [00:00<00:01, 308.87it/s]
Adding requests:  30%|██▉       | 153/512 [00:00<00:01, 314.44it/s]
Adding requests:  37%|███▋      | 188/512 [00:00<00:00, 326.08it/s]
Adding requests:  44%|████▎     | 223/512 [00:00<00:00, 333.81it/s]
Adding requests:  50%|█████     | 257/512 [00:00<00:00, 331.60it/s]
Adding requests:  57%|█████▋    | 292/512 [00:00<00:00, 335.84it/s]
Adding requests:  64%|██████▍   | 327/512 [00:01<00:00, 338.61it/s]
Adding requests:  71%|███████▏  | 365/512 [00:01<00:00, 347.56it/s]
Adding requests:  79%|███████▊  | 402/512 [00:01<00:00, 353.05it/s]
Adding requests:  86%|████████▌ | 438/512 [00:01<00:00, 354.24it/s]
Adding requests:  93%|█████████▎| 474/512 [00:01<00:00, 352.04it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 359.98it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 336.14it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:00<00:00, 1330.62it/s, est. speed input: 1362638.39 toks/s, output: 1330.64 toks/s]
Processed prompts:  61%|██████    | 312/512 [00:01<00:00, 204.54it/s, est. speed input: 244931.64 toks/s, output: 239.19 toks/s]   
Processed prompts:  73%|███████▎  | 374/512 [00:01<00:00, 167.35it/s, est. speed input: 204627.67 toks/s, output: 199.83 toks/s]
Processed prompts:  81%|████████  | 413/512 [00:02<00:00, 156.75it/s, est. speed input: 193185.54 toks/s, output: 188.66 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:02<00:00, 144.42it/s, est. speed input: 183268.80 toks/s, output: 178.97 toks/s]
Processed prompts:  91%|█████████ | 464/512 [00:02<00:00, 140.59it/s, est. speed input: 179342.66 toks/s, output: 175.14 toks/s]
Processed prompts:  94%|█████████▍| 483/512 [00:02<00:00, 133.66it/s, est. speed input: 174838.98 toks/s, output: 170.74 toks/s]
Processed prompts:  97%|█████████▋| 499/512 [00:02<00:00, 129.19it/s, est. speed input: 171785.60 toks/s, output: 167.76 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 129.19it/s, est. speed input: 170073.81 toks/s, output: 166.09 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 166.08it/s, est. speed input: 170073.81 toks/s, output: 166.09 toks/s]
[rank0]:[W126 16:15:40.604741239 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.7s

测试结果:
  Requests/s:   111.13
  Tokens/s:     113905.16
  Total Reqs:   512
  Elapsed:      4.61s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     113794.03

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:15:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=744321) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=744321) WARNING 01-26 16:16:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=744321) WARNING 01-26 16:16:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 118.74 requests/s, 121707.27 total tokens/s, 118.74 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 16:15:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:15:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:15:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:15:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:15:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:15:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:15:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:15:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:16:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:16:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:16:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:16:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:16:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:16:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:16:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:16:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=744321) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=744321) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.25it/s]
(EngineCore_DP0 pid=744321) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03s/it]
(EngineCore_DP0 pid=744321) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=744321) 
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=744321) 2026-01-26 16:16:21,877 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=744321) 2026-01-26 16:16:21,902 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=744321) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 14.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 12.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 12.28it/s]
(EngineCore_DP0 pid=744321) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 17.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.11it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 276.92it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 317.11it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 314.16it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 318.71it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 328.69it/s]
Adding requests:  20%|█▉        | 200/1024 [00:00<00:02, 339.89it/s]
Adding requests:  23%|██▎       | 236/1024 [00:00<00:02, 344.48it/s]
Adding requests:  26%|██▋       | 271/1024 [00:00<00:02, 342.74it/s]
Adding requests:  30%|██▉       | 307/1024 [00:00<00:02, 346.89it/s]
Adding requests:  34%|███▎      | 344/1024 [00:01<00:01, 353.14it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:01, 355.14it/s]
Adding requests:  41%|████      | 419/1024 [00:01<00:01, 363.60it/s]
Adding requests:  45%|████▍     | 456/1024 [00:01<00:01, 360.79it/s]
Adding requests:  48%|████▊     | 496/1024 [00:01<00:01, 370.10it/s]
Adding requests:  52%|█████▏    | 535/1024 [00:01<00:01, 374.04it/s]
Adding requests:  56%|█████▌    | 573/1024 [00:01<00:01, 371.35it/s]
Adding requests:  60%|█████▉    | 611/1024 [00:01<00:01, 357.35it/s]
Adding requests:  63%|██████▎   | 647/1024 [00:01<00:01, 354.26it/s]
Adding requests:  67%|██████▋   | 684/1024 [00:01<00:00, 356.71it/s]
Adding requests:  70%|███████   | 720/1024 [00:02<00:00, 354.90it/s]
Adding requests:  74%|███████▍  | 756/1024 [00:02<00:00, 351.58it/s]
Adding requests:  77%|███████▋  | 792/1024 [00:02<00:00, 351.39it/s]
Adding requests:  81%|████████  | 830/1024 [00:02<00:00, 358.84it/s]
Adding requests:  85%|████████▍ | 866/1024 [00:02<00:00, 358.31it/s]
Adding requests:  88%|████████▊ | 904/1024 [00:02<00:00, 363.06it/s]
Adding requests:  92%|█████████▏| 941/1024 [00:02<00:00, 355.91it/s]
Adding requests:  95%|█████████▌| 977/1024 [00:02<00:00, 354.36it/s]
Adding requests:  99%|█████████▉| 1013/1024 [00:02<00:00, 350.11it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 351.47it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:00<00:00, 2296.95it/s, est. speed input: 2352194.32 toks/s, output: 2296.99 toks/s]
Processed prompts:  57%|█████▋    | 584/1024 [00:02<00:01, 241.20it/s, est. speed input: 295007.40 toks/s, output: 288.09 toks/s]   
Processed prompts:  67%|██████▋   | 686/1024 [00:02<00:01, 193.10it/s, est. speed input: 242164.57 toks/s, output: 236.49 toks/s]
Processed prompts:  73%|███████▎  | 747/1024 [00:03<00:01, 173.04it/s, est. speed input: 222578.53 toks/s, output: 217.36 toks/s]
Processed prompts:  77%|███████▋  | 789/1024 [00:03<00:01, 164.61it/s, est. speed input: 214430.70 toks/s, output: 209.40 toks/s]
Processed prompts:  80%|████████  | 820/1024 [00:04<00:01, 156.28it/s, est. speed input: 208159.36 toks/s, output: 203.28 toks/s]
Processed prompts:  83%|████████▎ | 845/1024 [00:04<00:01, 151.33it/s, est. speed input: 204355.01 toks/s, output: 199.57 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:04<00:01, 142.89it/s, est. speed input: 200001.62 toks/s, output: 195.31 toks/s]
Processed prompts:  86%|████████▋ | 884/1024 [00:04<00:00, 141.46it/s, est. speed input: 198128.02 toks/s, output: 193.48 toks/s]
Processed prompts:  88%|████████▊ | 901/1024 [00:04<00:00, 138.79it/s, est. speed input: 196158.73 toks/s, output: 191.56 toks/s]
Processed prompts:  90%|████████▉ | 917/1024 [00:04<00:00, 134.94it/s, est. speed input: 194118.76 toks/s, output: 189.57 toks/s]
Processed prompts:  91%|█████████ | 932/1024 [00:04<00:00, 129.84it/s, est. speed input: 191978.20 toks/s, output: 187.48 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:05<00:00, 123.93it/s, est. speed input: 189782.51 toks/s, output: 185.33 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:05<00:00, 122.89it/s, est. speed input: 188074.37 toks/s, output: 183.67 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:05<00:00, 121.64it/s, est. speed input: 186387.78 toks/s, output: 182.02 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:05<00:00, 121.10it/s, est. speed input: 184837.85 toks/s, output: 180.51 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:05<00:00, 120.71it/s, est. speed input: 183362.74 toks/s, output: 179.06 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 120.71it/s, est. speed input: 183660.64 toks/s, output: 179.36 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 179.35it/s, est. speed input: 183660.64 toks/s, output: 179.36 toks/s]
[rank0]:[W126 16:16:33.251868985 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.5s

测试结果:
  Requests/s:   118.74
  Tokens/s:     121707.27
  Total Reqs:   1024
  Elapsed:      8.62s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     121588.54

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:16:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=745670) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=745670) WARNING 01-26 16:17:10 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=745670) WARNING 01-26 16:17:20 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 126.50 requests/s, 129661.82 total tokens/s, 126.50 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 16:16:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:16:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:16:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:16:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:16:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:16:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:16:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:16:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:17:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:17:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:17:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:17:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:17:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:17:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:17:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:17:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:17:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:17:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:17:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:17:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:17:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:17:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=745670) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=745670) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=745670) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03s/it]
(EngineCore_DP0 pid=745670) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.00it/s]
(EngineCore_DP0 pid=745670) 
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=745670) [rank0]:W0126 16:17:15.361000 745670 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=745670) [rank0]:W0126 16:17:15.443000 745670 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=745670) [rank0]:W0126 16:17:16.416000 745670 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=745670) [rank0]:W0126 16:17:16.546000 745670 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=745670) 2026-01-26 16:17:19,951 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=745670) 2026-01-26 16:17:19,977 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=745670) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 15.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.56it/s]
(EngineCore_DP0 pid=745670) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 16.13it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 17.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 17.19it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 278.53it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 318.87it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 316.45it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:05, 323.39it/s]
Adding requests:   8%|▊         | 164/2048 [00:00<00:05, 332.81it/s]
Adding requests:  10%|▉         | 200/2048 [00:00<00:05, 341.35it/s]
Adding requests:  11%|█▏        | 235/2048 [00:00<00:05, 339.58it/s]
Adding requests:  13%|█▎        | 269/2048 [00:00<00:05, 333.66it/s]
Adding requests:  15%|█▍        | 304/2048 [00:00<00:05, 335.06it/s]
Adding requests:  17%|█▋        | 340/2048 [00:01<00:05, 340.10it/s]
Adding requests:  18%|█▊        | 375/2048 [00:01<00:04, 340.47it/s]
Adding requests:  20%|██        | 411/2048 [00:01<00:04, 344.68it/s]
Adding requests:  22%|██▏       | 446/2048 [00:01<00:04, 341.64it/s]
Adding requests:  24%|██▎       | 483/2048 [00:01<00:04, 347.54it/s]
Adding requests:  25%|██▌       | 522/2048 [00:01<00:04, 359.01it/s]
Adding requests:  27%|██▋       | 558/2048 [00:01<00:04, 354.31it/s]
Adding requests:  29%|██▉       | 594/2048 [00:01<00:04, 343.32it/s]
Adding requests:  31%|███       | 629/2048 [00:01<00:04, 341.71it/s]
Adding requests:  32%|███▏      | 664/2048 [00:01<00:04, 339.92it/s]
Adding requests:  34%|███▍      | 701/2048 [00:02<00:03, 346.82it/s]
Adding requests:  36%|███▌      | 736/2048 [00:02<00:03, 342.36it/s]
Adding requests:  38%|███▊      | 772/2048 [00:02<00:03, 346.44it/s]
Adding requests:  39%|███▉      | 808/2048 [00:02<00:03, 348.30it/s]
Adding requests:  41%|████      | 844/2048 [00:02<00:03, 349.39it/s]
Adding requests:  43%|████▎     | 881/2048 [00:02<00:03, 354.42it/s]
Adding requests:  45%|████▍     | 917/2048 [00:02<00:03, 351.55it/s]
Adding requests:  47%|████▋     | 953/2048 [00:02<00:03, 352.49it/s]
Adding requests:  48%|████▊     | 989/2048 [00:02<00:02, 353.19it/s]
Adding requests:  50%|█████     | 1025/2048 [00:02<00:03, 337.05it/s]
Adding requests:  52%|█████▏    | 1060/2048 [00:03<00:02, 339.21it/s]
Adding requests:  53%|█████▎    | 1095/2048 [00:03<00:02, 339.12it/s]
Adding requests:  55%|█████▌    | 1133/2048 [00:03<00:02, 350.10it/s]
Adding requests:  57%|█████▋    | 1169/2048 [00:03<00:02, 347.05it/s]
Adding requests:  59%|█████▉    | 1206/2048 [00:03<00:02, 351.90it/s]
Adding requests:  61%|██████    | 1243/2048 [00:03<00:02, 355.08it/s]
Adding requests:  62%|██████▏   | 1279/2048 [00:03<00:02, 350.03it/s]
Adding requests:  64%|██████▍   | 1315/2048 [00:03<00:02, 350.19it/s]
Adding requests:  66%|██████▌   | 1352/2048 [00:03<00:01, 355.74it/s]
Adding requests:  68%|██████▊   | 1388/2048 [00:04<00:01, 354.43it/s]
Adding requests:  70%|██████▉   | 1424/2048 [00:04<00:01, 351.55it/s]
Adding requests:  71%|███████▏  | 1460/2048 [00:04<00:01, 353.50it/s]
Adding requests:  73%|███████▎  | 1498/2048 [00:04<00:01, 358.46it/s]
Adding requests:  75%|███████▍  | 1534/2048 [00:04<00:01, 356.65it/s]
Adding requests:  77%|███████▋  | 1570/2048 [00:04<00:01, 350.18it/s]
Adding requests:  78%|███████▊  | 1606/2048 [00:04<00:01, 349.07it/s]
Adding requests:  80%|████████  | 1641/2048 [00:04<00:01, 342.72it/s]
Adding requests:  82%|████████▏ | 1676/2048 [00:04<00:01, 338.39it/s]
Adding requests:  84%|████████▎ | 1713/2048 [00:04<00:00, 346.50it/s]
Adding requests:  85%|████████▌ | 1748/2048 [00:05<00:00, 340.80it/s]
Adding requests:  87%|████████▋ | 1784/2048 [00:05<00:00, 344.88it/s]
Adding requests:  89%|████████▉ | 1819/2048 [00:05<00:00, 343.05it/s]
Adding requests:  91%|█████████ | 1857/2048 [00:05<00:00, 352.06it/s]
Adding requests:  92%|█████████▏| 1893/2048 [00:05<00:00, 351.73it/s]
Adding requests:  94%|█████████▍| 1931/2048 [00:05<00:00, 357.19it/s]
Adding requests:  96%|█████████▌| 1968/2048 [00:05<00:00, 357.93it/s]
Adding requests:  98%|█████████▊| 2004/2048 [00:05<00:00, 351.81it/s]
Adding requests: 100%|█████████▉| 2040/2048 [00:05<00:00, 348.36it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 346.11it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  37%|███▋      | 749/2048 [00:00<00:00, 4074.57it/s, est. speed input: 4172542.99 toks/s, output: 4074.62 toks/s]
Processed prompts:  56%|█████▋    | 1157/2048 [00:03<00:03, 284.39it/s, est. speed input: 355430.26 toks/s, output: 347.10 toks/s]  
Processed prompts:  65%|██████▌   | 1333/2048 [00:04<00:03, 225.74it/s, est. speed input: 289856.65 toks/s, output: 283.06 toks/s]
Processed prompts:  70%|███████   | 1435/2048 [00:05<00:02, 205.14it/s, est. speed input: 268910.93 toks/s, output: 262.61 toks/s]
Processed prompts:  73%|███████▎  | 1502/2048 [00:06<00:02, 184.62it/s, est. speed input: 252719.11 toks/s, output: 246.80 toks/s]
Processed prompts:  76%|███████▌  | 1549/2048 [00:06<00:02, 175.47it/s, est. speed input: 245480.90 toks/s, output: 239.73 toks/s]
Processed prompts:  77%|███████▋  | 1584/2048 [00:06<00:02, 171.02it/s, est. speed input: 241709.53 toks/s, output: 236.04 toks/s]
Processed prompts:  79%|███████▉  | 1613/2048 [00:06<00:02, 162.64it/s, est. speed input: 237331.75 toks/s, output: 231.77 toks/s]
Processed prompts:  80%|███████▉  | 1636/2048 [00:07<00:02, 164.47it/s, est. speed input: 236395.42 toks/s, output: 230.85 toks/s]
Processed prompts:  81%|████████  | 1658/2048 [00:07<00:02, 165.83it/s, est. speed input: 235407.53 toks/s, output: 229.89 toks/s]
Processed prompts:  82%|████████▏ | 1679/2048 [00:07<00:02, 143.87it/s, est. speed input: 230289.68 toks/s, output: 224.89 toks/s]
Processed prompts:  83%|████████▎ | 1696/2048 [00:07<00:02, 141.94it/s, est. speed input: 228673.79 toks/s, output: 223.31 toks/s]
Processed prompts:  84%|████████▎ | 1712/2048 [00:07<00:02, 139.05it/s, est. speed input: 227043.27 toks/s, output: 221.72 toks/s]
Processed prompts:  84%|████████▍ | 1727/2048 [00:07<00:02, 134.70it/s, est. speed input: 225329.44 toks/s, output: 220.05 toks/s]
Processed prompts:  85%|████████▌ | 1741/2048 [00:07<00:02, 130.31it/s, est. speed input: 223685.27 toks/s, output: 218.44 toks/s]
Processed prompts:  86%|████████▌ | 1757/2048 [00:08<00:02, 130.62it/s, est. speed input: 222348.81 toks/s, output: 217.14 toks/s]
Processed prompts:  87%|████████▋ | 1773/2048 [00:08<00:02, 129.76it/s, est. speed input: 220943.74 toks/s, output: 215.76 toks/s]
Processed prompts:  87%|████████▋ | 1789/2048 [00:08<00:01, 129.97it/s, est. speed input: 219660.68 toks/s, output: 214.51 toks/s]
Processed prompts:  88%|████████▊ | 1805/2048 [00:08<00:01, 129.29it/s, est. speed input: 218341.20 toks/s, output: 213.22 toks/s]
Processed prompts:  89%|████████▉ | 1821/2048 [00:08<00:01, 128.71it/s, est. speed input: 217053.29 toks/s, output: 211.97 toks/s]
Processed prompts:  90%|████████▉ | 1837/2048 [00:08<00:01, 128.32it/s, est. speed input: 215805.02 toks/s, output: 210.75 toks/s]
Processed prompts:  90%|█████████ | 1853/2048 [00:08<00:01, 127.06it/s, est. speed input: 214511.52 toks/s, output: 209.48 toks/s]
Processed prompts:  91%|█████████▏| 1869/2048 [00:08<00:01, 127.13it/s, est. speed input: 213332.54 toks/s, output: 208.33 toks/s]
Processed prompts:  92%|█████████▏| 1885/2048 [00:09<00:01, 128.46it/s, est. speed input: 212285.20 toks/s, output: 207.31 toks/s]
Processed prompts:  93%|█████████▎| 1901/2048 [00:09<00:01, 128.28it/s, est. speed input: 211180.08 toks/s, output: 206.23 toks/s]
Processed prompts:  94%|█████████▎| 1917/2048 [00:09<00:01, 126.82it/s, est. speed input: 210006.42 toks/s, output: 205.08 toks/s]
Processed prompts:  94%|█████████▍| 1933/2048 [00:09<00:00, 128.41it/s, est. speed input: 209054.57 toks/s, output: 204.15 toks/s]
Processed prompts:  95%|█████████▌| 1949/2048 [00:09<00:00, 127.99it/s, est. speed input: 208017.18 toks/s, output: 203.14 toks/s]
Processed prompts:  96%|█████████▌| 1965/2048 [00:09<00:00, 127.67it/s, est. speed input: 207005.17 toks/s, output: 202.15 toks/s]
Processed prompts:  97%|█████████▋| 1981/2048 [00:09<00:00, 128.74it/s, est. speed input: 206107.52 toks/s, output: 201.28 toks/s]
Processed prompts:  98%|█████████▊| 1997/2048 [00:09<00:00, 127.75it/s, est. speed input: 205114.71 toks/s, output: 200.31 toks/s]
Processed prompts:  98%|█████████▊| 2013/2048 [00:10<00:00, 127.41it/s, est. speed input: 204170.60 toks/s, output: 199.39 toks/s]
Processed prompts:  99%|█████████▉| 2029/2048 [00:10<00:00, 127.68it/s, est. speed input: 203282.51 toks/s, output: 198.52 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:10<00:00, 127.68it/s, est. speed input: 204182.27 toks/s, output: 199.40 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:10<00:00, 199.39it/s, est. speed input: 204182.27 toks/s, output: 199.40 toks/s]
[rank0]:[W126 16:17:38.930659967 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 65.8s

测试结果:
  Requests/s:   126.50
  Tokens/s:     129661.82
  Total Reqs:   2048
  Elapsed:      16.19s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     129535.32

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:18:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=747285) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=747285) WARNING 01-26 16:18:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=747285) WARNING 01-26 16:18:38 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 129.17 requests/s, 132396.13 total tokens/s, 129.17 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 16:18:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:18:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:18:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:18:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:18:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:18:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:18:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:18:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:18:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:18:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:18:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:18:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:18:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:18:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:18:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:18:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=747285) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=747285) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=747285) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03s/it]
(EngineCore_DP0 pid=747285) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=747285) 
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=747285) [rank0]:W0126 16:18:33.101000 747285 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=747285) [rank0]:W0126 16:18:33.184000 747285 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=747285) [rank0]:W0126 16:18:34.153000 747285 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=747285) [rank0]:W0126 16:18:34.286000 747285 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=747285) 2026-01-26 16:18:37,951 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=747285) 2026-01-26 16:18:37,978 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=747285) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 15.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 15.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 16.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 17.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 15.69it/s]
(EngineCore_DP0 pid=747285) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.72it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.88it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 17.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 16.47it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 277.35it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 319.39it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 316.61it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:12, 322.02it/s]
Adding requests:   4%|▍         | 165/4096 [00:00<00:11, 332.08it/s]
Adding requests:   5%|▍         | 202/4096 [00:00<00:11, 342.38it/s]
Adding requests:   6%|▌         | 238/4096 [00:00<00:11, 347.07it/s]
Adding requests:   7%|▋         | 273/4096 [00:00<00:11, 344.43it/s]
Adding requests:   8%|▊         | 310/4096 [00:00<00:10, 349.91it/s]
Adding requests:   8%|▊         | 346/4096 [00:01<00:10, 351.02it/s]
Adding requests:   9%|▉         | 383/4096 [00:01<00:10, 355.82it/s]
Adding requests:  10%|█         | 421/4096 [00:01<00:10, 361.99it/s]
Adding requests:  11%|█         | 458/4096 [00:01<00:10, 360.17it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:09, 368.33it/s]
Adding requests:  13%|█▎        | 537/4096 [00:01<00:09, 373.97it/s]
Adding requests:  14%|█▍        | 575/4096 [00:01<00:09, 373.09it/s]
Adding requests:  15%|█▍        | 613/4096 [00:01<00:09, 363.10it/s]
Adding requests:  16%|█▌        | 650/4096 [00:01<00:09, 356.75it/s]
Adding requests:  17%|█▋        | 687/4096 [00:01<00:09, 359.07it/s]
Adding requests:  18%|█▊        | 723/4096 [00:02<00:09, 356.09it/s]
Adding requests:  19%|█▊        | 759/4096 [00:02<00:09, 354.35it/s]
Adding requests:  19%|█▉        | 795/4096 [00:02<00:09, 354.07it/s]
Adding requests:  20%|██        | 833/4096 [00:02<00:09, 358.76it/s]
Adding requests:  21%|██        | 870/4096 [00:02<00:08, 361.97it/s]
Adding requests:  22%|██▏       | 907/4096 [00:02<00:08, 357.71it/s]
Adding requests:  23%|██▎       | 943/4096 [00:02<00:08, 352.46it/s]
Adding requests:  24%|██▍       | 979/4096 [00:02<00:08, 353.42it/s]
Adding requests:  25%|██▍       | 1015/4096 [00:02<00:08, 348.90it/s]
Adding requests:  26%|██▌       | 1051/4096 [00:02<00:08, 350.84it/s]
Adding requests:  27%|██▋       | 1087/4096 [00:03<00:08, 350.65it/s]
Adding requests:  27%|██▋       | 1123/4096 [00:03<00:08, 353.13it/s]
Adding requests:  28%|██▊       | 1159/4096 [00:03<00:08, 349.88it/s]
Adding requests:  29%|██▉       | 1196/4096 [00:03<00:08, 352.63it/s]
Adding requests:  30%|███       | 1234/4096 [00:03<00:07, 360.37it/s]
Adding requests:  31%|███       | 1271/4096 [00:03<00:07, 354.28it/s]
Adding requests:  32%|███▏      | 1307/4096 [00:03<00:08, 344.41it/s]
Adding requests:  33%|███▎      | 1342/4096 [00:03<00:07, 344.93it/s]
Adding requests:  34%|███▎      | 1379/4096 [00:03<00:07, 350.53it/s]
Adding requests:  35%|███▍      | 1415/4096 [00:04<00:07, 349.70it/s]
Adding requests:  35%|███▌      | 1451/4096 [00:04<00:07, 351.35it/s]
Adding requests:  36%|███▋      | 1488/4096 [00:04<00:07, 356.53it/s]
Adding requests:  37%|███▋      | 1525/4096 [00:04<00:07, 359.11it/s]
Adding requests:  38%|███▊      | 1561/4096 [00:04<00:07, 354.20it/s]
Adding requests:  39%|███▉      | 1597/4096 [00:04<00:07, 350.69it/s]
Adding requests:  40%|███▉      | 1633/4096 [00:04<00:07, 345.59it/s]
Adding requests:  41%|████      | 1668/4096 [00:04<00:07, 339.52it/s]
Adding requests:  42%|████▏     | 1705/4096 [00:04<00:06, 346.57it/s]
Adding requests:  43%|████▎     | 1741/4096 [00:04<00:06, 349.72it/s]
Adding requests:  43%|████▎     | 1779/4096 [00:05<00:06, 356.05it/s]
Adding requests:  44%|████▍     | 1815/4096 [00:05<00:06, 352.47it/s]
Adding requests:  45%|████▌     | 1851/4096 [00:05<00:06, 354.45it/s]
Adding requests:  46%|████▌     | 1888/4096 [00:05<00:06, 358.15it/s]
Adding requests:  47%|████▋     | 1925/4096 [00:05<00:06, 359.13it/s]
Adding requests:  48%|████▊     | 1963/4096 [00:05<00:05, 362.95it/s]
Adding requests:  49%|████▉     | 2000/4096 [00:05<00:06, 348.06it/s]
Adding requests:  50%|████▉     | 2035/4096 [00:05<00:05, 344.53it/s]
Adding requests:  51%|█████     | 2070/4096 [00:05<00:05, 340.64it/s]
Adding requests:  51%|█████▏    | 2106/4096 [00:05<00:05, 345.73it/s]
Adding requests:  52%|█████▏    | 2142/4096 [00:06<00:05, 347.59it/s]
Adding requests:  53%|█████▎    | 2177/4096 [00:06<00:05, 342.15it/s]
Adding requests:  54%|█████▍    | 2212/4096 [00:06<00:05, 341.48it/s]
Adding requests:  55%|█████▍    | 2248/4096 [00:06<00:05, 346.47it/s]
Adding requests:  56%|█████▌    | 2284/4096 [00:06<00:05, 348.56it/s]
Adding requests:  57%|█████▋    | 2320/4096 [00:06<00:05, 351.69it/s]
Adding requests:  58%|█████▊    | 2358/4096 [00:06<00:04, 357.47it/s]
Adding requests:  58%|█████▊    | 2396/4096 [00:06<00:04, 363.62it/s]
Adding requests:  59%|█████▉    | 2433/4096 [00:06<00:04, 365.39it/s]
Adding requests:  60%|██████    | 2470/4096 [00:07<00:04, 361.14it/s]
Adding requests:  61%|██████▏   | 2509/4096 [00:07<00:04, 367.03it/s]
Adding requests:  62%|██████▏   | 2548/4096 [00:07<00:04, 371.64it/s]
Adding requests:  63%|██████▎   | 2587/4096 [00:07<00:04, 374.98it/s]
Adding requests:  64%|██████▍   | 2625/4096 [00:07<00:03, 368.74it/s]
Adding requests:  65%|██████▍   | 2662/4096 [00:07<00:03, 360.41it/s]
Adding requests:  66%|██████▌   | 2699/4096 [00:07<00:03, 356.92it/s]
Adding requests:  67%|██████▋   | 2735/4096 [00:07<00:03, 356.45it/s]
Adding requests:  68%|██████▊   | 2773/4096 [00:07<00:03, 363.12it/s]
Adding requests:  69%|██████▊   | 2811/4096 [00:07<00:03, 368.06it/s]
Adding requests:  70%|██████▉   | 2849/4096 [00:08<00:03, 368.59it/s]
Adding requests:  70%|███████   | 2886/4096 [00:08<00:03, 362.13it/s]
Adding requests:  71%|███████▏  | 2923/4096 [00:08<00:03, 362.69it/s]
Adding requests:  72%|███████▏  | 2962/4096 [00:08<00:03, 368.47it/s]
Adding requests:  73%|███████▎  | 2999/4096 [00:08<00:02, 366.39it/s]
Adding requests:  74%|███████▍  | 3037/4096 [00:08<00:02, 369.66it/s]
Adding requests:  75%|███████▌  | 3075/4096 [00:08<00:02, 372.49it/s]
Adding requests:  76%|███████▌  | 3113/4096 [00:08<00:02, 369.74it/s]
Adding requests:  77%|███████▋  | 3151/4096 [00:08<00:02, 370.30it/s]
Adding requests:  78%|███████▊  | 3189/4096 [00:08<00:02, 365.93it/s]
Adding requests:  79%|███████▉  | 3226/4096 [00:09<00:02, 363.32it/s]
Adding requests:  80%|███████▉  | 3263/4096 [00:09<00:02, 363.32it/s]
Adding requests:  81%|████████  | 3300/4096 [00:09<00:02, 344.41it/s]
Adding requests:  81%|████████▏ | 3335/4096 [00:09<00:02, 345.89it/s]
Adding requests:  82%|████████▏ | 3373/4096 [00:09<00:02, 354.32it/s]
Adding requests:  83%|████████▎ | 3410/4096 [00:09<00:01, 356.80it/s]
Adding requests:  84%|████████▍ | 3447/4096 [00:09<00:01, 359.06it/s]
Adding requests:  85%|████████▌ | 3483/4096 [00:09<00:01, 358.84it/s]
Adding requests:  86%|████████▌ | 3521/4096 [00:09<00:01, 362.56it/s]
Adding requests:  87%|████████▋ | 3561/4096 [00:10<00:01, 372.09it/s]
Adding requests:  88%|████████▊ | 3599/4096 [00:10<00:01, 364.64it/s]
Adding requests:  89%|████████▉ | 3636/4096 [00:10<00:01, 366.11it/s]
Adding requests:  90%|████████▉ | 3673/4096 [00:10<00:01, 358.55it/s]
Adding requests:  91%|█████████ | 3709/4096 [00:10<00:01, 356.85it/s]
Adding requests:  91%|█████████▏| 3745/4096 [00:10<00:00, 357.10it/s]
Adding requests:  92%|█████████▏| 3781/4096 [00:10<00:00, 346.94it/s]
Adding requests:  93%|█████████▎| 3816/4096 [00:10<00:00, 338.51it/s]
Adding requests:  94%|█████████▍| 3852/4096 [00:10<00:00, 342.49it/s]
Adding requests:  95%|█████████▍| 3888/4096 [00:10<00:00, 346.66it/s]
Adding requests:  96%|█████████▌| 3923/4096 [00:11<00:00, 342.52it/s]
Adding requests:  97%|█████████▋| 3959/4096 [00:11<00:00, 346.06it/s]
Adding requests:  98%|█████████▊| 3994/4096 [00:11<00:00, 345.55it/s]
Adding requests:  98%|█████████▊| 4029/4096 [00:11<00:00, 345.54it/s]
Adding requests:  99%|█████████▉| 4064/4096 [00:11<00:00, 344.86it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 354.45it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▋      | 1489/4096 [00:00<00:00, 4696.85it/s, est. speed input: 4809702.35 toks/s, output: 4696.89 toks/s]
Processed prompts:  48%|████▊     | 1959/4096 [00:03<00:05, 412.31it/s, est. speed input: 533088.71 toks/s, output: 520.59 toks/s]   
Processed prompts:  53%|█████▎    | 2161/4096 [00:05<00:06, 294.19it/s, est. speed input: 403768.90 toks/s, output: 394.30 toks/s]
Processed prompts:  56%|█████▌    | 2276/4096 [00:06<00:06, 266.56it/s, est. speed input: 374526.26 toks/s, output: 365.75 toks/s]
Processed prompts:  57%|█████▋    | 2352/4096 [00:06<00:07, 248.90it/s, est. speed input: 358717.94 toks/s, output: 350.31 toks/s]
Processed prompts:  59%|█████▊    | 2406/4096 [00:07<00:07, 224.03it/s, est. speed input: 342340.12 toks/s, output: 334.32 toks/s]
Processed prompts:  60%|█████▉    | 2446/4096 [00:07<00:07, 216.64it/s, est. speed input: 336430.58 toks/s, output: 328.55 toks/s]
Processed prompts:  60%|██████    | 2478/4096 [00:07<00:07, 203.67it/s, est. speed input: 329708.26 toks/s, output: 321.98 toks/s]
Processed prompts:  61%|██████    | 2504/4096 [00:07<00:08, 188.07it/s, est. speed input: 323149.36 toks/s, output: 315.58 toks/s]
Processed prompts:  62%|██████▏   | 2526/4096 [00:08<00:09, 168.30it/s, est. speed input: 316038.96 toks/s, output: 308.63 toks/s]
Processed prompts:  62%|██████▏   | 2545/4096 [00:08<00:10, 148.19it/s, est. speed input: 309041.32 toks/s, output: 301.80 toks/s]
Processed prompts:  63%|██████▎   | 2577/4096 [00:08<00:10, 144.48it/s, est. speed input: 304203.91 toks/s, output: 297.07 toks/s]
Processed prompts:  64%|██████▎   | 2609/4096 [00:08<00:10, 139.75it/s, est. speed input: 299236.69 toks/s, output: 292.22 toks/s]
Processed prompts:  64%|██████▍   | 2641/4096 [00:09<00:10, 137.50it/s, est. speed input: 294851.12 toks/s, output: 287.94 toks/s]
Processed prompts:  65%|██████▌   | 2673/4096 [00:09<00:10, 134.97it/s, est. speed input: 290527.09 toks/s, output: 283.72 toks/s]
Processed prompts:  66%|██████▌   | 2705/4096 [00:09<00:10, 132.65it/s, est. speed input: 286339.48 toks/s, output: 279.63 toks/s]
Processed prompts:  67%|██████▋   | 2737/4096 [00:09<00:10, 132.09it/s, est. speed input: 282573.19 toks/s, output: 275.95 toks/s]
Processed prompts:  68%|██████▊   | 2769/4096 [00:10<00:10, 130.86it/s, est. speed input: 278842.86 toks/s, output: 272.31 toks/s]
Processed prompts:  68%|██████▊   | 2801/4096 [00:10<00:09, 129.84it/s, est. speed input: 275267.33 toks/s, output: 268.82 toks/s]
Processed prompts:  69%|██████▉   | 2833/4096 [00:10<00:09, 129.11it/s, est. speed input: 271857.56 toks/s, output: 265.49 toks/s]
Processed prompts:  70%|██████▉   | 2865/4096 [00:10<00:09, 130.01it/s, est. speed input: 268831.44 toks/s, output: 262.53 toks/s]
Processed prompts:  71%|███████   | 2897/4096 [00:11<00:09, 131.50it/s, est. speed input: 266062.09 toks/s, output: 259.83 toks/s]
Processed prompts:  72%|███████▏  | 2929/4096 [00:11<00:08, 130.76it/s, est. speed input: 263148.72 toks/s, output: 256.98 toks/s]
Processed prompts:  72%|███████▏  | 2961/4096 [00:11<00:08, 131.07it/s, est. speed input: 260474.86 toks/s, output: 254.37 toks/s]
Processed prompts:  73%|███████▎  | 2993/4096 [00:11<00:08, 131.32it/s, est. speed input: 257914.78 toks/s, output: 251.87 toks/s]
Processed prompts:  74%|███████▍  | 3025/4096 [00:12<00:08, 130.58it/s, est. speed input: 255337.56 toks/s, output: 249.35 toks/s]
Processed prompts:  75%|███████▍  | 3057/4096 [00:12<00:07, 129.90it/s, est. speed input: 252841.38 toks/s, output: 246.92 toks/s]
Processed prompts:  75%|███████▌  | 3089/4096 [00:12<00:07, 129.79it/s, est. speed input: 250490.22 toks/s, output: 244.62 toks/s]
Processed prompts:  76%|███████▌  | 3121/4096 [00:12<00:07, 129.32it/s, est. speed input: 248180.07 toks/s, output: 242.36 toks/s]
Processed prompts:  77%|███████▋  | 3153/4096 [00:13<00:07, 130.16it/s, est. speed input: 246098.04 toks/s, output: 240.33 toks/s]
Processed prompts:  78%|███████▊  | 3185/4096 [00:13<00:07, 129.55it/s, est. speed input: 243952.48 toks/s, output: 238.23 toks/s]
Processed prompts:  79%|███████▊  | 3217/4096 [00:13<00:06, 129.26it/s, est. speed input: 241900.78 toks/s, output: 236.23 toks/s]
Processed prompts:  79%|███████▉  | 3249/4096 [00:13<00:06, 128.80it/s, est. speed input: 239894.19 toks/s, output: 234.27 toks/s]
Processed prompts:  80%|████████  | 3281/4096 [00:14<00:06, 128.05it/s, est. speed input: 237911.95 toks/s, output: 232.34 toks/s]
Processed prompts:  81%|████████  | 3313/4096 [00:14<00:06, 128.47it/s, est. speed input: 236098.94 toks/s, output: 230.57 toks/s]
Processed prompts:  82%|████████▏ | 3345/4096 [00:14<00:05, 128.37it/s, est. speed input: 234307.08 toks/s, output: 228.82 toks/s]
Processed prompts:  82%|████████▏ | 3377/4096 [00:14<00:05, 128.14it/s, est. speed input: 232559.07 toks/s, output: 227.11 toks/s]
Processed prompts:  83%|████████▎ | 3409/4096 [00:15<00:05, 129.17it/s, est. speed input: 230987.30 toks/s, output: 225.57 toks/s]
Processed prompts:  84%|████████▍ | 3441/4096 [00:15<00:05, 128.33it/s, est. speed input: 229314.44 toks/s, output: 223.94 toks/s]
Processed prompts:  85%|████████▍ | 3473/4096 [00:15<00:04, 128.47it/s, est. speed input: 227763.70 toks/s, output: 222.43 toks/s]
Processed prompts:  86%|████████▌ | 3505/4096 [00:15<00:04, 127.97it/s, est. speed input: 226206.97 toks/s, output: 220.91 toks/s]
Processed prompts:  86%|████████▋ | 3537/4096 [00:16<00:04, 128.89it/s, est. speed input: 224813.44 toks/s, output: 219.54 toks/s]
Processed prompts:  87%|████████▋ | 3569/4096 [00:16<00:04, 129.54it/s, est. speed input: 223460.82 toks/s, output: 218.22 toks/s]
Processed prompts:  88%|████████▊ | 3601/4096 [00:16<00:03, 128.98it/s, est. speed input: 222062.41 toks/s, output: 216.86 toks/s]
Processed prompts:  89%|████████▊ | 3633/4096 [00:16<00:03, 129.94it/s, est. speed input: 220817.59 toks/s, output: 215.64 toks/s]
Processed prompts:  89%|████████▉ | 3665/4096 [00:17<00:03, 130.85it/s, est. speed input: 219626.51 toks/s, output: 214.48 toks/s]
Processed prompts:  90%|█████████ | 3697/4096 [00:17<00:03, 129.85it/s, est. speed input: 218339.17 toks/s, output: 213.22 toks/s]
Processed prompts:  91%|█████████ | 3729/4096 [00:17<00:02, 130.06it/s, est. speed input: 217159.09 toks/s, output: 212.07 toks/s]
Processed prompts:  92%|█████████▏| 3761/4096 [00:17<00:02, 129.86it/s, est. speed input: 215984.69 toks/s, output: 210.92 toks/s]
Processed prompts:  93%|█████████▎| 3793/4096 [00:18<00:02, 129.39it/s, est. speed input: 214818.11 toks/s, output: 209.78 toks/s]
Processed prompts:  93%|█████████▎| 3825/4096 [00:18<00:02, 129.53it/s, est. speed input: 213717.17 toks/s, output: 208.71 toks/s]
Processed prompts:  94%|█████████▍| 3857/4096 [00:18<00:01, 128.93it/s, est. speed input: 212594.83 toks/s, output: 207.61 toks/s]
Processed prompts:  95%|█████████▍| 3889/4096 [00:18<00:01, 128.45it/s, est. speed input: 211497.96 toks/s, output: 206.54 toks/s]
Processed prompts:  96%|█████████▌| 3921/4096 [00:19<00:01, 131.36it/s, est. speed input: 210657.19 toks/s, output: 205.72 toks/s]
Processed prompts:  97%|█████████▋| 3953/4096 [00:19<00:01, 130.61it/s, est. speed input: 209645.16 toks/s, output: 204.73 toks/s]
Processed prompts:  97%|█████████▋| 3985/4096 [00:19<00:00, 130.50it/s, est. speed input: 208687.20 toks/s, output: 203.80 toks/s]
Processed prompts:  98%|█████████▊| 4017/4096 [00:19<00:00, 129.72it/s, est. speed input: 207705.74 toks/s, output: 202.84 toks/s]
Processed prompts:  99%|█████████▉| 4049/4096 [00:20<00:00, 133.81it/s, est. speed input: 207044.12 toks/s, output: 202.19 toks/s]
Processed prompts: 100%|█████████▉| 4081/4096 [00:20<00:00, 155.70it/s, est. speed input: 207364.74 toks/s, output: 202.50 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:20<00:00, 155.70it/s, est. speed input: 208124.52 toks/s, output: 203.25 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:20<00:00, 203.25it/s, est. speed input: 208124.52 toks/s, output: 203.25 toks/s]
[rank0]:[W126 16:19:12.054547988 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 94.2s

测试结果:
  Requests/s:   129.17
  Tokens/s:     132396.13
  Total Reqs:   4096
  Elapsed:      31.71s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     132266.97

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:20:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=749465) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=749465) WARNING 01-26 16:20:27 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     def forward(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     raise e
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/tmp/torchinductor_root/pd/cpdfnyfxaigvavknvje6smbtj4tkg266uncqqyt44bgx5tueerkj.py", line 1093, in call
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 10)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 327, in quant_slide_int8_triton
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered


─── STDERR ───
[2026-01-26 16:20:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:20:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:20:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:20:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:20:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:20:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:20:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:20:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:20:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:20:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:20:16] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:20:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:20:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:20:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:20:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:20:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:18] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:18] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:18] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:18] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:18] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=749465) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=749465) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=749465) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03s/it]
(EngineCore_DP0 pid=749465) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.00it/s]
(EngineCore_DP0 pid=749465) 
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=749465) [rank0]:W0126 16:20:32.038000 749465 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=749465) [rank0]:W0126 16:20:32.121000 749465 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=749465) [rank0]:W0126 16:20:33.229000 749465 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=749465) [rank0]:W0126 16:20:33.354000 749465 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=749465) Process EngineCore_DP0:
(EngineCore_DP0 pid=749465) Traceback (most recent call last):
(EngineCore_DP0 pid=749465)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=749465)     self.run()
(EngineCore_DP0 pid=749465)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=749465)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=749465)     raise e
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=749465)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=749465)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=749465)     super().__init__(
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=749465)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=749465)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=749465)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=749465)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=749465)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=749465)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=749465)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=749465)     return func(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=749465)     return func(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=749465)     self.model_runner.profile_run()
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=749465)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=749465)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=749465)     return func(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=749465)     outputs = self.model(
(EngineCore_DP0 pid=749465)               ^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=749465)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=749465)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=749465)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=749465)     hidden_states = self.model(
(EngineCore_DP0 pid=749465)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=749465)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=749465)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=749465)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=749465)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=749465)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=749465)     def forward(
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=749465)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=749465)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=749465)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=749465)     raise e
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=749465)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=749465)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=749465)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=749465)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=749465)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=749465)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=749465)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=749465)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=749465)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=749465)     return compiled_fn(full_args)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=749465)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=749465)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=749465)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=749465)                             ^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=749465)     outs = compiled_fn(args)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=749465)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=749465)     return self.current_callable(inputs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=749465)     out = model(new_inputs)
(EngineCore_DP0 pid=749465)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/tmp/torchinductor_root/pd/cpdfnyfxaigvavknvje6smbtj4tkg266uncqqyt44bgx5tueerkj.py", line 1093, in call
(EngineCore_DP0 pid=749465)     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 10)
(EngineCore_DP0 pid=749465)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=749465)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=749465)     return fn(input, L)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 327, in quant_slide_int8_triton
(EngineCore_DP0 pid=749465)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=749465)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=749465)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=749465)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=749465)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=749465)     self._init_handles()
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=749465)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=749465)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 16:20:34.387107116 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,38.2970,19646.3364,3.3423
1024,1024,1,128,128,39.4623,40448.8294,3.2436
2048,1024,2,256,128,76.8801,78802.1527,3.3299
4096,1024,4,512,128,111.1270,113905.1560,4.6073
8192,1024,8,1024,128,118.7388,121707.2747,8.6240
16384,1024,16,2048,128,126.4993,129661.8218,16.1898
32768,1024,32,4096,128,129.1670,132396.1321,31.7109
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 5 失败
============================================================
