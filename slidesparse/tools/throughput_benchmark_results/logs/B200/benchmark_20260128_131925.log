======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-28 13:19:25
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-int8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768

命令行参数:
  --model: bitnet1.58-2b-int8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: prefill
  --M: 512,1024,2048,4096,8192,16384,32768
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: B200
  Compute Capability: cc100
  VRAM: 178.4 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  BitNet-2B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:19:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3205667) WARNING 01-28 13:19:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3205667) WARNING 01-28 13:20:00 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 30.03 requests/s, 15406.17 total tokens/s, 30.03 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:19:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:19:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:19:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:19:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:19:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:19:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:19:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:19:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:19:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:19:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:19:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:19:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:19:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:19:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:19:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:19:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:19:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:19:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:19:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:19:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:19:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:19:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:19:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:19:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:19:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:19:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:19:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:19:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3205667) [2026-01-28 13:19:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3205667) [2026-01-28 13:19:40] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3205667) [2026-01-28 13:19:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3205667) [2026-01-28 13:19:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3205667) [2026-01-28 13:19:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3205667) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3205667) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=3205667) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=3205667) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3205667) 2026-01-28 13:20:00,611 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3205667) 2026-01-28 13:20:00,640 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3205667) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.83it/s]
(EngineCore_DP0 pid=3205667) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.09it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  58%|█████▊    | 74/128 [00:00<00:00, 738.70it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 751.27it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:25,  4.93it/s, est. speed input: 2524.51 toks/s, output: 4.93 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:06, 17.62it/s, est. speed input: 7814.28 toks/s, output: 15.26 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 23.57it/s, est. speed input: 10235.88 toks/s, output: 19.99 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 26.59it/s, est. speed input: 11556.27 toks/s, output: 22.57 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 28.63it/s, est. speed input: 12459.76 toks/s, output: 24.33 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 29.93it/s, est. speed input: 13092.45 toks/s, output: 25.57 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 30.80it/s, est. speed input: 13563.11 toks/s, output: 26.49 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:03, 31.36it/s, est. speed input: 13924.01 toks/s, output: 27.20 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 31.75it/s, est. speed input: 14210.51 toks/s, output: 27.75 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 32.03it/s, est. speed input: 14444.96 toks/s, output: 28.21 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 32.24it/s, est. speed input: 14641.37 toks/s, output: 28.60 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 32.35it/s, est. speed input: 14802.89 toks/s, output: 28.91 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 32.44it/s, est. speed input: 14942.36 toks/s, output: 29.18 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 32.56it/s, est. speed input: 15069.44 toks/s, output: 29.43 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 32.57it/s, est. speed input: 15172.35 toks/s, output: 29.63 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:02, 32.60it/s, est. speed input: 15265.26 toks/s, output: 29.81 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 32.71it/s, est. speed input: 15355.84 toks/s, output: 29.99 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 32.73it/s, est. speed input: 15431.52 toks/s, output: 30.14 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 32.73it/s, est. speed input: 15498.88 toks/s, output: 30.27 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 32.77it/s, est. speed input: 15562.32 toks/s, output: 30.40 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 32.78it/s, est. speed input: 15618.99 toks/s, output: 30.51 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 32.74it/s, est. speed input: 15667.68 toks/s, output: 30.60 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 32.78it/s, est. speed input: 15716.17 toks/s, output: 30.70 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 32.76it/s, est. speed input: 15758.23 toks/s, output: 30.78 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:00, 32.80it/s, est. speed input: 15800.16 toks/s, output: 30.86 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 32.80it/s, est. speed input: 15836.95 toks/s, output: 30.93 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 32.80it/s, est. speed input: 15871.53 toks/s, output: 31.00 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 32.82it/s, est. speed input: 15904.65 toks/s, output: 31.06 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 32.75it/s, est. speed input: 15931.26 toks/s, output: 31.12 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 32.70it/s, est. speed input: 15955.78 toks/s, output: 31.16 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 32.71it/s, est. speed input: 15980.87 toks/s, output: 31.21 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 32.69it/s, est. speed input: 16003.36 toks/s, output: 31.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 32.69it/s, est. speed input: 16020.47 toks/s, output: 31.29 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 31.29it/s, est. speed input: 16020.47 toks/s, output: 31.29 toks/s]
[rank0]:[W128 13:20:07.775969982 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.4s

测试结果:
  Requests/s:   30.03
  Tokens/s:     15406.17
  Total Reqs:   128
  Elapsed:      4.26s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     15376.14

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:20:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3207224) WARNING 01-28 13:20:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3207224) WARNING 01-28 13:20:41 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 31.21 requests/s, 31992.38 total tokens/s, 31.21 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:20:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:20:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:20:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:20:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:20:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:20:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:20:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:20:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:20:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:20:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:20:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:20:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:20:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:20:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:20:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:20:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:20:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:20:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3207224) [2026-01-28 13:20:24] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3207224) [2026-01-28 13:20:24] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3207224) [2026-01-28 13:20:24] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3207224) [2026-01-28 13:20:24] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3207224) [2026-01-28 13:20:24] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3207224) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3207224) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]
(EngineCore_DP0 pid=3207224) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]
(EngineCore_DP0 pid=3207224) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3207224) 2026-01-28 13:20:41,817 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3207224) 2026-01-28 13:20:41,847 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3207224) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 13.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 13.73it/s]
(EngineCore_DP0 pid=3207224) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 13.62it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 399.56it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 427.88it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 429.04it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 71.22it/s, est. speed input: 72929.14 toks/s, output: 71.22 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 42.46it/s, est. speed input: 46456.48 toks/s, output: 45.37 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 38.12it/s, est. speed input: 42287.22 toks/s, output: 41.30 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 36.06it/s, est. speed input: 40236.72 toks/s, output: 39.29 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 34.98it/s, est. speed input: 39153.97 toks/s, output: 38.24 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:02, 34.25it/s, est. speed input: 38375.91 toks/s, output: 37.48 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:02, 33.71it/s, est. speed input: 37770.55 toks/s, output: 36.89 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 33.35it/s, est. speed input: 37302.15 toks/s, output: 36.43 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 33.10it/s, est. speed input: 36923.69 toks/s, output: 36.06 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 32.92it/s, est. speed input: 36608.95 toks/s, output: 35.75 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:02, 32.81it/s, est. speed input: 36349.04 toks/s, output: 35.50 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:02, 32.69it/s, est. speed input: 36116.12 toks/s, output: 35.27 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 32.65it/s, est. speed input: 35926.12 toks/s, output: 35.08 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 32.55it/s, est. speed input: 35744.21 toks/s, output: 34.91 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:01, 32.54it/s, est. speed input: 35596.54 toks/s, output: 34.76 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:01, 32.56it/s, est. speed input: 35471.99 toks/s, output: 34.64 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 32.52it/s, est. speed input: 35349.20 toks/s, output: 34.52 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 32.56it/s, est. speed input: 35252.34 toks/s, output: 34.43 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 32.52it/s, est. speed input: 35152.92 toks/s, output: 34.33 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:01, 32.45it/s, est. speed input: 35055.38 toks/s, output: 34.23 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:01, 32.44it/s, est. speed input: 34972.88 toks/s, output: 34.15 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 32.44it/s, est. speed input: 34898.10 toks/s, output: 34.08 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:03<00:00, 32.38it/s, est. speed input: 34821.31 toks/s, output: 34.01 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:03<00:00, 32.41it/s, est. speed input: 34760.60 toks/s, output: 33.95 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:03<00:00, 32.39it/s, est. speed input: 34697.82 toks/s, output: 33.88 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:03<00:00, 32.36it/s, est. speed input: 34638.56 toks/s, output: 33.83 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 32.37it/s, est. speed input: 34587.54 toks/s, output: 33.78 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 32.40it/s, est. speed input: 34542.32 toks/s, output: 33.73 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 32.33it/s, est. speed input: 34488.85 toks/s, output: 33.68 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 32.33it/s, est. speed input: 34478.16 toks/s, output: 33.67 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.67it/s, est. speed input: 34478.16 toks/s, output: 33.67 toks/s]
[rank0]:[W128 13:20:47.928302468 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.1s

测试结果:
  Requests/s:   31.21
  Tokens/s:     31992.38
  Total Reqs:   128
  Elapsed:      4.10s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     31961.16

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:20:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3208352) WARNING 01-28 13:21:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3208352) WARNING 01-28 13:21:22 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 62.53 requests/s, 64089.92 total tokens/s, 62.53 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 13:20:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:20:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:20:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:20:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:20:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:20:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:20:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:20:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:20:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:20:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:21:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:21:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:21:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:21:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:21:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:21:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:21:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:21:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:21:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3208352) [2026-01-28 13:21:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3208352) [2026-01-28 13:21:04] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3208352) [2026-01-28 13:21:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3208352) [2026-01-28 13:21:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3208352) [2026-01-28 13:21:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3208352) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3208352) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]
(EngineCore_DP0 pid=3208352) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]
(EngineCore_DP0 pid=3208352) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3208352) 2026-01-28 13:21:22,525 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3208352) 2026-01-28 13:21:22,556 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3208352) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 10.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 10.68it/s]
(EngineCore_DP0 pid=3208352) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 14.58it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 14.56it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  16%|█▌        | 40/256 [00:00<00:00, 393.08it/s]
Adding requests:  33%|███▎      | 84/256 [00:00<00:00, 417.90it/s]
Adding requests:  50%|█████     | 128/256 [00:00<00:00, 426.65it/s]
Adding requests:  67%|██████▋   | 171/256 [00:00<00:00, 427.96it/s]
Adding requests:  84%|████████▍ | 216/256 [00:00<00:00, 432.88it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 433.39it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:00<00:00, 315.46it/s, est. speed input: 323044.93 toks/s, output: 315.46 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:00<00:01, 100.80it/s, est. speed input: 116081.45 toks/s, output: 113.36 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:00<00:01, 88.54it/s, est. speed input: 102889.81 toks/s, output: 100.48 toks/s] 
Processed prompts:  39%|███▉      | 100/256 [00:01<00:01, 79.38it/s, est. speed input: 94589.97 toks/s, output: 92.37 toks/s] 
Processed prompts:  43%|████▎     | 110/256 [00:01<00:01, 75.66it/s, est. speed input: 90987.34 toks/s, output: 88.85 toks/s]
Processed prompts:  46%|████▋     | 119/256 [00:01<00:01, 74.89it/s, est. speed input: 89425.07 toks/s, output: 87.33 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:01<00:01, 70.18it/s, est. speed input: 86377.95 toks/s, output: 84.35 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:01<00:01, 68.74it/s, est. speed input: 84822.08 toks/s, output: 82.83 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:01<00:01, 67.70it/s, est. speed input: 83521.88 toks/s, output: 81.56 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:01<00:01, 66.79it/s, est. speed input: 82360.52 toks/s, output: 80.43 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:02<00:01, 66.15it/s, est. speed input: 81354.44 toks/s, output: 79.45 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:02<00:01, 65.76it/s, est. speed input: 80486.85 toks/s, output: 78.60 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:02<00:01, 65.38it/s, est. speed input: 79691.22 toks/s, output: 77.82 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:02<00:01, 65.17it/s, est. speed input: 78990.81 toks/s, output: 77.14 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:02<00:00, 64.98it/s, est. speed input: 78353.24 toks/s, output: 76.52 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:02<00:00, 64.89it/s, est. speed input: 77784.42 toks/s, output: 75.96 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:02<00:00, 64.73it/s, est. speed input: 77247.57 toks/s, output: 75.44 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:02<00:00, 64.52it/s, est. speed input: 76741.10 toks/s, output: 74.94 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:03<00:00, 64.48it/s, est. speed input: 76294.94 toks/s, output: 74.51 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:03<00:00, 64.48it/s, est. speed input: 75888.40 toks/s, output: 74.11 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:03<00:00, 64.55it/s, est. speed input: 75522.74 toks/s, output: 73.75 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:03<00:00, 64.52it/s, est. speed input: 75173.09 toks/s, output: 73.41 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 64.49it/s, est. speed input: 74846.47 toks/s, output: 73.09 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 64.49it/s, est. speed input: 74846.47 toks/s, output: 73.09 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 73.09it/s, est. speed input: 74846.47 toks/s, output: 73.09 toks/s]
[rank0]:[W128 13:21:28.055148412 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.0s

测试结果:
  Requests/s:   62.53
  Tokens/s:     64089.92
  Total Reqs:   256
  Elapsed:      4.09s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     64027.39

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:21:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3209503) WARNING 01-28 13:21:54 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3209503) WARNING 01-28 13:22:04 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 122.17 requests/s, 125221.60 total tokens/s, 122.17 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 13:21:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:21:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:21:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:21:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:21:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:21:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:21:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:21:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:21:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:21:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:21:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:21:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:21:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:21:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:21:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:21:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:21:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:21:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:21:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3209503) [2026-01-28 13:21:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3209503) [2026-01-28 13:21:47] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3209503) [2026-01-28 13:21:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3209503) [2026-01-28 13:21:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3209503) [2026-01-28 13:21:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3209503) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3209503) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=3209503) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=3209503) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3209503) 2026-01-28 13:22:04,197 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3209503) 2026-01-28 13:22:04,227 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3209503) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 14.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.04it/s]
(EngineCore_DP0 pid=3209503) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 14.74it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.86it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 39/512 [00:00<00:01, 387.03it/s]
Adding requests:  16%|█▌        | 83/512 [00:00<00:01, 418.01it/s]
Adding requests:  25%|██▌       | 128/512 [00:00<00:00, 431.15it/s]
Adding requests:  34%|███▎      | 172/512 [00:00<00:00, 434.02it/s]
Adding requests:  43%|████▎     | 218/512 [00:00<00:00, 442.31it/s]
Adding requests:  52%|█████▏    | 266/512 [00:00<00:00, 453.99it/s]
Adding requests:  61%|██████    | 312/512 [00:00<00:00, 440.37it/s]
Adding requests:  70%|██████▉   | 357/512 [00:00<00:00, 442.74it/s]
Adding requests:  79%|███████▉  | 405/512 [00:00<00:00, 451.66it/s]
Adding requests:  88%|████████▊ | 452/512 [00:01<00:00, 456.02it/s]
Adding requests:  97%|█████████▋| 499/512 [00:01<00:00, 459.85it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 446.75it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:00<00:00, 1218.09it/s, est. speed input: 1247403.15 toks/s, output: 1218.11 toks/s]
Processed prompts:  50%|█████     | 256/512 [00:01<00:01, 213.02it/s, est. speed input: 250608.42 toks/s, output: 244.73 toks/s]   
Processed prompts:  61%|██████▏   | 314/512 [00:01<00:01, 177.73it/s, est. speed input: 212486.90 toks/s, output: 207.51 toks/s]
Processed prompts:  69%|██████▉   | 352/512 [00:01<00:00, 166.28it/s, est. speed input: 200424.66 toks/s, output: 195.73 toks/s]
Processed prompts:  74%|███████▍  | 380/512 [00:02<00:00, 157.84it/s, est. speed input: 192893.00 toks/s, output: 188.37 toks/s]
Processed prompts:  79%|███████▊  | 403/512 [00:02<00:00, 150.44it/s, est. speed input: 187199.54 toks/s, output: 182.81 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:02<00:00, 144.73it/s, est. speed input: 183093.99 toks/s, output: 178.80 toks/s]
Processed prompts:  86%|████████▌ | 439/512 [00:02<00:00, 143.26it/s, est. speed input: 180945.83 toks/s, output: 176.70 toks/s]
Processed prompts:  89%|████████▉ | 455/512 [00:02<00:00, 140.09it/s, est. speed input: 178572.89 toks/s, output: 174.39 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:02<00:00, 135.60it/s, est. speed input: 176070.40 toks/s, output: 171.94 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:02<00:00, 133.93it/s, est. speed input: 174154.23 toks/s, output: 170.07 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:02<00:00, 132.63it/s, est. speed input: 172402.72 toks/s, output: 168.36 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 132.63it/s, est. speed input: 172254.90 toks/s, output: 168.22 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 168.21it/s, est. speed input: 172254.90 toks/s, output: 168.22 toks/s]
[rank0]:[W128 13:22:10.892859706 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.9s

测试结果:
  Requests/s:   122.17
  Tokens/s:     125221.60
  Total Reqs:   512
  Elapsed:      4.19s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     125099.43

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:22:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3210660) WARNING 01-28 13:22:38 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3210660) WARNING 01-28 13:22:48 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 224.06 requests/s, 229657.54 total tokens/s, 224.06 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 13:22:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:22:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:22:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:22:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:22:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:22:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:22:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:22:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:22:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:22:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:22:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:22:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:22:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:22:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:22:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:22:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:22:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:22:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:22:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:22:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:22:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:22:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:22:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:22:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:22:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:22:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:22:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:22:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3210660) [2026-01-28 13:22:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3210660) [2026-01-28 13:22:31] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3210660) [2026-01-28 13:22:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3210660) [2026-01-28 13:22:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3210660) [2026-01-28 13:22:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3210660) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3210660) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3210660) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3210660) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3210660) 2026-01-28 13:22:48,750 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3210660) 2026-01-28 13:22:48,782 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3210660) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 14.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 14.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 13.37it/s]
(EngineCore_DP0 pid=3210660) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 13.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 14.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 14.00it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 397.08it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 422.32it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 431.88it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 433.63it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 440.04it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 452.46it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 448.16it/s]
Adding requests:  35%|███▌      | 361/1024 [00:00<00:01, 452.49it/s]
Adding requests:  40%|███▉      | 408/1024 [00:00<00:01, 457.25it/s]
Adding requests:  44%|████▍     | 455/1024 [00:01<00:01, 460.31it/s]
Adding requests:  49%|████▉     | 502/1024 [00:01<00:01, 459.61it/s]
Adding requests:  54%|█████▎    | 548/1024 [00:01<00:01, 454.48it/s]
Adding requests:  58%|█████▊    | 596/1024 [00:01<00:00, 460.92it/s]
Adding requests:  63%|██████▎   | 645/1024 [00:01<00:00, 467.22it/s]
Adding requests:  68%|██████▊   | 694/1024 [00:01<00:00, 473.71it/s]
Adding requests:  72%|███████▏  | 742/1024 [00:01<00:00, 472.41it/s]
Adding requests:  77%|███████▋  | 790/1024 [00:01<00:00, 469.82it/s]
Adding requests:  82%|████████▏ | 837/1024 [00:01<00:00, 460.34it/s]
Adding requests:  87%|████████▋ | 886/1024 [00:01<00:00, 466.17it/s]
Adding requests:  91%|█████████▏| 935/1024 [00:02<00:00, 469.22it/s]
Adding requests:  96%|█████████▌| 983/1024 [00:02<00:00, 470.47it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 459.53it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:00<00:00, 4014.35it/s, est. speed input: 4110956.59 toks/s, output: 4014.41 toks/s]
Processed prompts:  88%|████████▊ | 900/1024 [00:01<00:00, 417.17it/s, est. speed input: 501827.46 toks/s, output: 490.06 toks/s]   
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 417.17it/s, est. speed input: 447972.53 toks/s, output: 437.47 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 437.45it/s, est. speed input: 447972.53 toks/s, output: 437.47 toks/s]
[rank0]:[W128 13:22:55.092806129 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.2s

测试结果:
  Requests/s:   224.06
  Tokens/s:     229657.54
  Total Reqs:   1024
  Elapsed:      4.57s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     229433.48

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:23:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3211904) WARNING 01-28 13:23:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3211904) WARNING 01-28 13:23:38 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 262.47 requests/s, 269035.96 total tokens/s, 262.47 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 13:23:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:23:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:23:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:23:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:23:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:23:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:23:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:23:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:23:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:23:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:23:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:23:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:23:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:23:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:23:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:23:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:23:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:23:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:23:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:23:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:23:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:23:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:23:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:23:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:23:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:23:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:23:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:23:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3211904) [2026-01-28 13:23:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3211904) [2026-01-28 13:23:21] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3211904) [2026-01-28 13:23:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3211904) [2026-01-28 13:23:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3211904) [2026-01-28 13:23:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3211904) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3211904) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]
(EngineCore_DP0 pid=3211904) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]
(EngineCore_DP0 pid=3211904) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3211904) 2026-01-28 13:23:38,891 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3211904) 2026-01-28 13:23:38,922 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3211904) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 13.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 12.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 12.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 12.51it/s]
(EngineCore_DP0 pid=3211904) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 14.82it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 15.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 14.99it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 41/2048 [00:00<00:04, 404.54it/s]
Adding requests:   4%|▍         | 86/2048 [00:00<00:04, 429.65it/s]
Adding requests:   6%|▋         | 131/2048 [00:00<00:04, 435.85it/s]
Adding requests:   9%|▊         | 175/2048 [00:00<00:04, 435.99it/s]
Adding requests:  11%|█         | 222/2048 [00:00<00:04, 445.44it/s]
Adding requests:  13%|█▎        | 270/2048 [00:00<00:03, 455.03it/s]
Adding requests:  15%|█▌        | 316/2048 [00:00<00:03, 452.25it/s]
Adding requests:  18%|█▊        | 363/2048 [00:00<00:03, 457.26it/s]
Adding requests:  20%|██        | 410/2048 [00:00<00:03, 460.70it/s]
Adding requests:  22%|██▏       | 457/2048 [00:01<00:03, 463.10it/s]
Adding requests:  25%|██▍       | 504/2048 [00:01<00:03, 463.37it/s]
Adding requests:  27%|██▋       | 551/2048 [00:01<00:03, 458.30it/s]
Adding requests:  29%|██▉       | 600/2048 [00:01<00:03, 463.25it/s]
Adding requests:  32%|███▏      | 648/2048 [00:01<00:03, 466.05it/s]
Adding requests:  34%|███▍      | 698/2048 [00:01<00:02, 473.95it/s]
Adding requests:  36%|███▋      | 746/2048 [00:01<00:02, 474.45it/s]
Adding requests:  39%|███▉      | 794/2048 [00:01<00:02, 473.12it/s]
Adding requests:  41%|████      | 842/2048 [00:01<00:02, 464.29it/s]
Adding requests:  44%|████▎     | 892/2048 [00:01<00:02, 473.04it/s]
Adding requests:  46%|████▌     | 940/2048 [00:02<00:02, 474.62it/s]
Adding requests:  48%|████▊     | 989/2048 [00:02<00:02, 476.48it/s]
Adding requests:  51%|█████     | 1038/2048 [00:02<00:02, 479.08it/s]
Adding requests:  53%|█████▎    | 1086/2048 [00:02<00:02, 474.79it/s]
Adding requests:  55%|█████▌    | 1134/2048 [00:02<00:01, 471.87it/s]
Adding requests:  58%|█████▊    | 1185/2048 [00:02<00:01, 481.56it/s]
Adding requests:  60%|██████    | 1235/2048 [00:02<00:01, 484.95it/s]
Adding requests:  63%|██████▎   | 1284/2048 [00:02<00:01, 480.75it/s]
Adding requests:  65%|██████▌   | 1334/2048 [00:02<00:01, 484.33it/s]
Adding requests:  68%|██████▊   | 1383/2048 [00:02<00:01, 484.75it/s]
Adding requests:  70%|██████▉   | 1432/2048 [00:03<00:01, 485.02it/s]
Adding requests:  72%|███████▏  | 1481/2048 [00:03<00:01, 486.17it/s]
Adding requests:  75%|███████▍  | 1531/2048 [00:03<00:01, 488.99it/s]
Adding requests:  77%|███████▋  | 1580/2048 [00:03<00:00, 488.61it/s]
Adding requests:  80%|███████▉  | 1631/2048 [00:03<00:00, 492.17it/s]
Adding requests:  82%|████████▏ | 1681/2048 [00:03<00:00, 485.93it/s]
Adding requests:  84%|████████▍ | 1730/2048 [00:03<00:00, 476.00it/s]
Adding requests:  87%|████████▋ | 1778/2048 [00:03<00:00, 472.64it/s]
Adding requests:  89%|████████▉ | 1827/2048 [00:03<00:00, 474.57it/s]
Adding requests:  92%|█████████▏| 1876/2048 [00:03<00:00, 477.28it/s]
Adding requests:  94%|█████████▍| 1925/2048 [00:04<00:00, 478.04it/s]
Adding requests:  96%|█████████▋| 1973/2048 [00:04<00:00, 478.00it/s]
Adding requests:  99%|█████████▉| 2023/2048 [00:04<00:00, 482.35it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 472.36it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:00<00:00, 11305.83it/s, est. speed input: 11577992.72 toks/s, output: 11306.04 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 11305.83it/s, est. speed input: 605137.34 toks/s, output: 590.95 toks/s]    
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 590.93it/s, est. speed input: 605137.34 toks/s, output: 590.95 toks/s]  
[rank0]:[W128 13:23:49.488356512 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.5s

测试结果:
  Requests/s:   262.47
  Tokens/s:     269035.96
  Total Reqs:   2048
  Elapsed:      7.80s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     268773.48

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:24:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3213290) WARNING 01-28 13:24:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3213290) WARNING 01-28 13:24:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 275.39 requests/s, 282279.74 total tokens/s, 275.39 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 13:24:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:24:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:24:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:24:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:24:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:24:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:24:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:24:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:24:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:24:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:24:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:24:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:24:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:24:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:24:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:24:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:24:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:24:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:24:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:24:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:24:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:24:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:24:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:24:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:24:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:24:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:24:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:24:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3213290) [2026-01-28 13:24:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3213290) [2026-01-28 13:24:23] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3213290) [2026-01-28 13:24:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3213290) [2026-01-28 13:24:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3213290) [2026-01-28 13:24:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3213290) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3213290) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]
(EngineCore_DP0 pid=3213290) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]
(EngineCore_DP0 pid=3213290) 
(EngineCore_DP0 pid=3213290) [rank0]:W0128 13:24:36.178000 3213290 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3213290) [rank0]:W0128 13:24:36.918000 3213290 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3213290) [rank0]:W0128 13:24:38.512000 3213290 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3213290) [rank0]:W0128 13:24:38.643000 3213290 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3213290) 2026-01-28 13:24:43,214 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3213290) 2026-01-28 13:24:43,245 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3213290) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 13.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 13.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 13.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 13.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 12.92it/s]
(EngineCore_DP0 pid=3213290) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 14.61it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 13.74it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 14.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 14.32it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/4096 [00:00<00:10, 401.84it/s]
Adding requests:   2%|▏         | 85/4096 [00:00<00:09, 422.80it/s]
Adding requests:   3%|▎         | 130/4096 [00:00<00:09, 433.65it/s]
Adding requests:   4%|▍         | 174/4096 [00:00<00:09, 425.02it/s]
Adding requests:   5%|▌         | 217/4096 [00:00<00:09, 421.94it/s]
Adding requests:   6%|▋         | 262/4096 [00:00<00:08, 428.28it/s]
Adding requests:   8%|▊         | 308/4096 [00:00<00:08, 435.57it/s]
Adding requests:   9%|▊         | 356/4096 [00:00<00:08, 446.06it/s]
Adding requests:  10%|▉         | 403/4096 [00:00<00:08, 452.02it/s]
Adding requests:  11%|█         | 450/4096 [00:01<00:07, 456.22it/s]
Adding requests:  12%|█▏        | 497/4096 [00:01<00:07, 458.71it/s]
Adding requests:  13%|█▎        | 543/4096 [00:01<00:07, 453.55it/s]
Adding requests:  14%|█▍        | 593/4096 [00:01<00:07, 464.23it/s]
Adding requests:  16%|█▌        | 641/4096 [00:01<00:07, 468.30it/s]
Adding requests:  17%|█▋        | 690/4096 [00:01<00:07, 473.78it/s]
Adding requests:  18%|█▊        | 740/4096 [00:01<00:07, 476.83it/s]
Adding requests:  19%|█▉        | 788/4096 [00:01<00:06, 474.45it/s]
Adding requests:  20%|██        | 836/4096 [00:01<00:07, 462.77it/s]
Adding requests:  22%|██▏       | 885/4096 [00:01<00:06, 470.33it/s]
Adding requests:  23%|██▎       | 933/4096 [00:02<00:06, 467.81it/s]
Adding requests:  24%|██▍       | 981/4096 [00:02<00:06, 470.21it/s]
Adding requests:  25%|██▌       | 1030/4096 [00:02<00:06, 474.16it/s]
Adding requests:  26%|██▋       | 1078/4096 [00:02<00:06, 470.07it/s]
Adding requests:  27%|██▋       | 1126/4096 [00:02<00:06, 458.72it/s]
Adding requests:  29%|██▊       | 1175/4096 [00:02<00:06, 466.87it/s]
Adding requests:  30%|██▉       | 1225/4096 [00:02<00:06, 474.40it/s]
Adding requests:  31%|███       | 1273/4096 [00:02<00:05, 471.14it/s]
Adding requests:  32%|███▏      | 1322/4096 [00:02<00:05, 474.24it/s]
Adding requests:  33%|███▎      | 1371/4096 [00:02<00:05, 477.88it/s]
Adding requests:  35%|███▍      | 1420/4096 [00:03<00:05, 479.45it/s]
Adding requests:  36%|███▌      | 1469/4096 [00:03<00:05, 481.56it/s]
Adding requests:  37%|███▋      | 1519/4096 [00:03<00:05, 484.33it/s]
Adding requests:  38%|███▊      | 1568/4096 [00:03<00:05, 483.64it/s]
Adding requests:  40%|███▉      | 1618/4096 [00:03<00:05, 487.35it/s]
Adding requests:  41%|████      | 1667/4096 [00:03<00:05, 484.14it/s]
Adding requests:  42%|████▏     | 1717/4096 [00:03<00:04, 486.03it/s]
Adding requests:  43%|████▎     | 1766/4096 [00:03<00:04, 481.87it/s]
Adding requests:  44%|████▍     | 1816/4096 [00:03<00:04, 484.64it/s]
Adding requests:  46%|████▌     | 1865/4096 [00:03<00:04, 480.93it/s]
Adding requests:  47%|████▋     | 1914/4096 [00:04<00:04, 482.52it/s]
Adding requests:  48%|████▊     | 1963/4096 [00:04<00:04, 482.38it/s]
Adding requests:  49%|████▉     | 2013/4096 [00:04<00:04, 485.95it/s]
Adding requests:  50%|█████     | 2062/4096 [00:04<00:04, 474.98it/s]
Adding requests:  52%|█████▏    | 2110/4096 [00:04<00:04, 464.15it/s]
Adding requests:  53%|█████▎    | 2157/4096 [00:04<00:04, 458.10it/s]
Adding requests:  54%|█████▍    | 2204/4096 [00:04<00:04, 460.60it/s]
Adding requests:  55%|█████▌    | 2254/4096 [00:04<00:03, 470.24it/s]
Adding requests:  56%|█████▌    | 2302/4096 [00:04<00:03, 471.94it/s]
Adding requests:  57%|█████▋    | 2350/4096 [00:05<00:03, 472.83it/s]
Adding requests:  59%|█████▊    | 2398/4096 [00:05<00:03, 474.00it/s]
Adding requests:  60%|█████▉    | 2446/4096 [00:05<00:03, 473.36it/s]
Adding requests:  61%|██████    | 2495/4096 [00:05<00:03, 477.32it/s]
Adding requests:  62%|██████▏   | 2543/4096 [00:05<00:03, 477.14it/s]
Adding requests:  63%|██████▎   | 2593/4096 [00:05<00:03, 480.89it/s]
Adding requests:  65%|██████▍   | 2643/4096 [00:05<00:03, 483.76it/s]
Adding requests:  66%|██████▌   | 2692/4096 [00:05<00:02, 480.64it/s]
Adding requests:  67%|██████▋   | 2741/4096 [00:05<00:02, 479.10it/s]
Adding requests:  68%|██████▊   | 2789/4096 [00:05<00:02, 477.66it/s]
Adding requests:  69%|██████▉   | 2837/4096 [00:06<00:02, 474.41it/s]
Adding requests:  70%|███████   | 2886/4096 [00:06<00:02, 478.60it/s]
Adding requests:  72%|███████▏  | 2934/4096 [00:06<00:02, 475.41it/s]
Adding requests:  73%|███████▎  | 2982/4096 [00:06<00:02, 476.17it/s]
Adding requests:  74%|███████▍  | 3031/4096 [00:06<00:02, 477.80it/s]
Adding requests:  75%|███████▌  | 3079/4096 [00:06<00:02, 474.12it/s]
Adding requests:  76%|███████▋  | 3127/4096 [00:06<00:02, 475.44it/s]
Adding requests:  78%|███████▊  | 3175/4096 [00:06<00:01, 475.20it/s]
Adding requests:  79%|███████▊  | 3223/4096 [00:06<00:01, 474.56it/s]
Adding requests:  80%|███████▉  | 3272/4096 [00:06<00:01, 479.07it/s]
Adding requests:  81%|████████  | 3320/4096 [00:07<00:01, 467.58it/s]
Adding requests:  82%|████████▏ | 3369/4096 [00:07<00:01, 472.97it/s]
Adding requests:  83%|████████▎ | 3418/4096 [00:07<00:01, 475.86it/s]
Adding requests:  85%|████████▍ | 3466/4096 [00:07<00:01, 471.23it/s]
Adding requests:  86%|████████▌ | 3514/4096 [00:07<00:01, 473.33it/s]
Adding requests:  87%|████████▋ | 3562/4096 [00:07<00:01, 472.21it/s]
Adding requests:  88%|████████▊ | 3610/4096 [00:07<00:01, 474.07it/s]
Adding requests:  89%|████████▉ | 3658/4096 [00:07<00:00, 472.58it/s]
Adding requests:  91%|█████████ | 3707/4096 [00:07<00:00, 477.44it/s]
Adding requests:  92%|█████████▏| 3755/4096 [00:07<00:00, 477.94it/s]
Adding requests:  93%|█████████▎| 3805/4096 [00:08<00:00, 482.85it/s]
Adding requests:  94%|█████████▍| 3854/4096 [00:08<00:00, 484.06it/s]
Adding requests:  95%|█████████▌| 3903/4096 [00:08<00:00, 483.35it/s]
Adding requests:  96%|█████████▋| 3952/4096 [00:08<00:00, 483.20it/s]
Adding requests:  98%|█████████▊| 4001/4096 [00:08<00:00, 481.77it/s]
Adding requests:  99%|█████████▉| 4050/4096 [00:08<00:00, 479.87it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 471.80it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  58%|█████▊    | 2384/4096 [00:00<00:00, 16359.45it/s, est. speed input: 16753059.69 toks/s, output: 16359.69 toks/s]
Processed prompts:  98%|█████████▊| 4020/4096 [00:06<00:00, 553.20it/s, est. speed input: 684063.95 toks/s, output: 668.03 toks/s]      
Processed prompts: 100%|██████████| 4096/4096 [00:06<00:00, 553.20it/s, est. speed input: 677635.24 toks/s, output: 661.75 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:06<00:00, 661.74it/s, est. speed input: 677635.24 toks/s, output: 661.75 toks/s]
[rank0]:[W128 13:25:01.578785866 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 72.2s

测试结果:
  Requests/s:   275.39
  Tokens/s:     282279.74
  Total Reqs:   4096
  Elapsed:      14.87s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     282004.34


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cublaslt/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,30.0315,15406.1749,4.2622
1024,1024,1,128,128,31.2121,31992.3754,4.1010
2048,1024,2,256,128,62.5267,64089.9154,4.0942
4096,1024,4,512,128,122.1674,125221.5997,4.1910
8192,1024,8,1024,128,224.0561,229657.5389,4.5703
16384,1024,16,2048,128,262.4741,269035.9589,7.8027
32768,1024,32,4096,128,275.3949,282279.7355,14.8732

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:25:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3214747) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3214747) WARNING 01-28 13:25:25 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3214747) WARNING 01-28 13:25:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.42 requests/s, 16631.08 total tokens/s, 32.42 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:25:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:25:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:25:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:25:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:25:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:25:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:25:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:25:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:25:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:25:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:25:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:25:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:25:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:25:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:25:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:25:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:25:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:25:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:17] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3214747) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3214747) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3214747) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3214747) 
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3214747) 2026-01-28 13:25:35,870 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3214747) 2026-01-28 13:25:35,895 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3214747) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=3214747) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.57it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  59%|█████▊    | 75/128 [00:00<00:00, 743.88it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 755.73it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:34,  3.73it/s, est. speed input: 1908.62 toks/s, output: 3.73 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:07, 15.67it/s, est. speed input: 6730.52 toks/s, output: 13.15 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 22.81it/s, est. speed input: 9403.69 toks/s, output: 18.37 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 27.24it/s, est. speed input: 11083.39 toks/s, output: 21.65 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 30.12it/s, est. speed input: 12243.53 toks/s, output: 23.91 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 32.11it/s, est. speed input: 13102.80 toks/s, output: 25.59 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 33.45it/s, est. speed input: 13758.16 toks/s, output: 26.87 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:02, 34.32it/s, est. speed input: 14269.16 toks/s, output: 27.87 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 34.97it/s, est. speed input: 14688.32 toks/s, output: 28.69 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 35.37it/s, est. speed input: 15029.13 toks/s, output: 29.35 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 35.69it/s, est. speed input: 15319.04 toks/s, output: 29.92 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 35.87it/s, est. speed input: 15561.39 toks/s, output: 30.39 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 36.09it/s, est. speed input: 15779.71 toks/s, output: 30.82 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 36.19it/s, est. speed input: 15965.10 toks/s, output: 31.18 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 36.25it/s, est. speed input: 16127.38 toks/s, output: 31.50 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 35.87it/s, est. speed input: 16234.67 toks/s, output: 31.71 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 36.08it/s, est. speed input: 16368.72 toks/s, output: 31.97 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 36.15it/s, est. speed input: 16482.99 toks/s, output: 32.19 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 36.24it/s, est. speed input: 16588.80 toks/s, output: 32.40 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 36.31it/s, est. speed input: 16686.14 toks/s, output: 32.59 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 36.04it/s, est. speed input: 16751.79 toks/s, output: 32.72 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 36.17it/s, est. speed input: 16833.37 toks/s, output: 32.88 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 36.23it/s, est. speed input: 16906.28 toks/s, output: 33.02 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 36.37it/s, est. speed input: 16979.64 toks/s, output: 33.16 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 36.34it/s, est. speed input: 17039.98 toks/s, output: 33.28 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 36.38it/s, est. speed input: 17099.39 toks/s, output: 33.40 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 35.69it/s, est. speed input: 17113.97 toks/s, output: 33.43 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 35.86it/s, est. speed input: 17163.38 toks/s, output: 33.52 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 36.03it/s, est. speed input: 17212.17 toks/s, output: 33.62 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 36.07it/s, est. speed input: 17253.78 toks/s, output: 33.70 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 36.10it/s, est. speed input: 17292.63 toks/s, output: 33.77 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.88it/s, est. speed input: 17317.98 toks/s, output: 33.82 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.88it/s, est. speed input: 17347.39 toks/s, output: 33.88 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.88it/s, est. speed input: 17347.39 toks/s, output: 33.88 toks/s]
[rank0]:[W128 13:25:42.913259009 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.3s

测试结果:
  Requests/s:   32.42
  Tokens/s:     16631.08
  Total Reqs:   128
  Elapsed:      3.95s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16598.66

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:25:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3216009) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3216009) WARNING 01-28 13:26:06 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3216009) WARNING 01-28 13:26:16 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 31.94 requests/s, 32733.51 total tokens/s, 31.94 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:25:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:25:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:25:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:25:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:25:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:25:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:25:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:25:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:25:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:25:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:25:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:25:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:25:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:25:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:25:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:25:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:25:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:25:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3216009) [2026-01-28 13:25:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3216009) [2026-01-28 13:25:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3216009) [2026-01-28 13:25:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3216009) [2026-01-28 13:25:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3216009) [2026-01-28 13:25:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3216009) [2026-01-28 13:25:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3216009) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3216009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3216009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3216009) 
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3216009) 2026-01-28 13:26:16,411 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3216009) 2026-01-28 13:26:16,436 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3216009) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.98it/s]
(EngineCore_DP0 pid=3216009) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.97it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 393.77it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 420.99it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 216.70it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 247.64it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:01, 64.58it/s, est. speed input: 66130.05 toks/s, output: 64.58 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:02, 43.94it/s, est. speed input: 47259.54 toks/s, output: 46.15 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 40.48it/s, est. speed input: 43909.30 toks/s, output: 42.88 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 38.69it/s, est. speed input: 42159.96 toks/s, output: 41.17 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 37.85it/s, est. speed input: 41291.82 toks/s, output: 40.32 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 37.23it/s, est. speed input: 40647.15 toks/s, output: 39.69 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 36.79it/s, est. speed input: 40155.24 toks/s, output: 39.21 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:02, 36.42it/s, est. speed input: 39749.47 toks/s, output: 38.82 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 36.21it/s, est. speed input: 39436.33 toks/s, output: 38.51 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 36.06it/s, est. speed input: 39182.00 toks/s, output: 38.26 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 36.01it/s, est. speed input: 38983.57 toks/s, output: 38.07 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 36.01it/s, est. speed input: 38824.41 toks/s, output: 37.91 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 35.99it/s, est. speed input: 38684.51 toks/s, output: 37.78 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 35.96it/s, est. speed input: 38557.79 toks/s, output: 37.65 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 35.92it/s, est. speed input: 38441.34 toks/s, output: 37.54 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 35.91it/s, est. speed input: 38343.99 toks/s, output: 37.45 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 35.95it/s, est. speed input: 38264.67 toks/s, output: 37.37 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 35.92it/s, est. speed input: 38183.39 toks/s, output: 37.29 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 35.93it/s, est. speed input: 38116.69 toks/s, output: 37.22 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 35.94it/s, est. speed input: 38056.30 toks/s, output: 37.16 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:01, 35.90it/s, est. speed input: 37992.82 toks/s, output: 37.10 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 35.90it/s, est. speed input: 37940.03 toks/s, output: 37.05 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 35.79it/s, est. speed input: 37875.05 toks/s, output: 36.99 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 35.85it/s, est. speed input: 37834.62 toks/s, output: 36.95 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 35.80it/s, est. speed input: 37786.11 toks/s, output: 36.90 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:03<00:00, 35.84it/s, est. speed input: 37749.42 toks/s, output: 36.86 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 35.84it/s, est. speed input: 37712.23 toks/s, output: 36.83 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 35.36it/s, est. speed input: 37619.15 toks/s, output: 36.74 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 35.46it/s, est. speed input: 37583.55 toks/s, output: 36.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.56it/s, est. speed input: 37554.08 toks/s, output: 36.67 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.56it/s, est. speed input: 37554.08 toks/s, output: 36.67 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.67it/s, est. speed input: 37554.08 toks/s, output: 36.67 toks/s]
[rank0]:[W128 13:26:22.367173620 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.3s

测试结果:
  Requests/s:   31.94
  Tokens/s:     32733.51
  Total Reqs:   128
  Elapsed:      4.01s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     32701.58

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:26:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3217205) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3217205) WARNING 01-28 13:26:46 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3217205) WARNING 01-28 13:26:56 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 65.61 requests/s, 67248.78 total tokens/s, 65.61 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 13:26:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:26:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:26:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:26:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:26:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:26:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:26:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:26:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:26:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:26:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:26:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:26:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:26:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:26:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:26:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:26:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:26:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:26:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3217205) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3217205) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3217205) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3217205) 
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3217205) 2026-01-28 13:26:56,698 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3217205) 2026-01-28 13:26:56,724 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3217205) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 12.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 11.71it/s]
(EngineCore_DP0 pid=3217205) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.07it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:51,  4.96it/s]
Adding requests:  16%|█▌        | 41/256 [00:00<00:01, 167.95it/s]
Adding requests:  33%|███▎      | 85/256 [00:00<00:00, 267.60it/s]
Adding requests:  50%|█████     | 129/256 [00:00<00:00, 325.36it/s]
Adding requests:  68%|██████▊   | 173/256 [00:00<00:00, 361.33it/s]
Adding requests:  85%|████████▌ | 218/256 [00:00<00:00, 388.15it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 324.42it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:00<00:00, 354.74it/s, est. speed input: 363275.13 toks/s, output: 354.75 toks/s]
Processed prompts:  30%|███       | 78/256 [00:00<00:01, 111.34it/s, est. speed input: 128221.62 toks/s, output: 125.22 toks/s]
Processed prompts:  38%|███▊      | 97/256 [00:00<00:01, 97.68it/s, est. speed input: 113524.24 toks/s, output: 110.86 toks/s] 
Processed prompts:  43%|████▎     | 111/256 [00:01<00:01, 89.95it/s, est. speed input: 106178.22 toks/s, output: 103.69 toks/s]
Processed prompts:  48%|████▊     | 123/256 [00:01<00:01, 85.03it/s, est. speed input: 101709.50 toks/s, output: 99.33 toks/s] 
Processed prompts:  52%|█████▏    | 133/256 [00:01<00:01, 81.77it/s, est. speed input: 98820.22 toks/s, output: 96.50 toks/s] 
Processed prompts:  55%|█████▌    | 142/256 [00:01<00:01, 77.38it/s, est. speed input: 95818.22 toks/s, output: 93.57 toks/s]
Processed prompts:  59%|█████▉    | 151/256 [00:01<00:01, 78.13it/s, est. speed input: 94916.38 toks/s, output: 92.69 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:01<00:01, 74.10it/s, est. speed input: 92599.80 toks/s, output: 90.43 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:01<00:01, 73.45it/s, est. speed input: 91446.17 toks/s, output: 89.30 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:01<00:01, 72.92it/s, est. speed input: 90417.56 toks/s, output: 88.30 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:02<00:00, 72.55it/s, est. speed input: 89506.54 toks/s, output: 87.41 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:02<00:00, 72.36it/s, est. speed input: 88707.40 toks/s, output: 86.63 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:02<00:00, 72.09it/s, est. speed input: 87958.55 toks/s, output: 85.90 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:02<00:00, 71.96it/s, est. speed input: 87289.47 toks/s, output: 85.24 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:02<00:00, 71.73it/s, est. speed input: 86656.04 toks/s, output: 84.62 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:02<00:00, 71.55it/s, est. speed input: 86071.60 toks/s, output: 84.05 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:02<00:00, 71.52it/s, est. speed input: 85552.09 toks/s, output: 83.55 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:02<00:00, 71.61it/s, est. speed input: 85088.94 toks/s, output: 83.09 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:02<00:00, 71.65it/s, est. speed input: 84657.46 toks/s, output: 82.67 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 71.60it/s, est. speed input: 84244.18 toks/s, output: 82.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 71.60it/s, est. speed input: 84244.18 toks/s, output: 82.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 82.27it/s, est. speed input: 84244.18 toks/s, output: 82.27 toks/s]
[rank0]:[W128 13:27:02.791273297 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.5s

测试结果:
  Requests/s:   65.61
  Tokens/s:     67248.78
  Total Reqs:   256
  Elapsed:      3.90s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     67183.17

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:27:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3218356) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3218356) WARNING 01-28 13:27:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3218356) WARNING 01-28 13:27:38 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 133.11 requests/s, 136437.10 total tokens/s, 133.11 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 13:27:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:27:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:27:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:27:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:27:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:27:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:27:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:27:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:27:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:27:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:27:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:27:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:27:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:27:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:27:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:27:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:27:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:27:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3218356) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3218356) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3218356) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3218356) 
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3218356) 2026-01-28 13:27:38,384 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3218356) 2026-01-28 13:27:38,410 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3218356) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.09it/s]
(EngineCore_DP0 pid=3218356) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 15.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.02it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 41/512 [00:00<00:01, 400.47it/s]
Adding requests:  17%|█▋        | 86/512 [00:00<00:01, 424.90it/s]
Adding requests:  26%|██▌       | 131/512 [00:00<00:00, 435.06it/s]
Adding requests:  34%|███▍      | 175/512 [00:00<00:00, 434.98it/s]
Adding requests:  43%|████▎     | 221/512 [00:00<00:00, 443.54it/s]
Adding requests:  53%|█████▎    | 269/512 [00:00<00:00, 454.80it/s]
Adding requests:  62%|██████▏   | 315/512 [00:00<00:00, 452.34it/s]
Adding requests:  71%|███████   | 362/512 [00:00<00:00, 457.80it/s]
Adding requests:  80%|████████  | 410/512 [00:00<00:00, 462.30it/s]
Adding requests:  89%|████████▉ | 458/512 [00:01<00:00, 464.56it/s]
Adding requests:  99%|█████████▊| 505/512 [00:01<00:00, 464.39it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 453.11it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:00<00:00, 1306.59it/s, est. speed input: 1338038.87 toks/s, output: 1306.61 toks/s]
Processed prompts:  54%|█████▍    | 277/512 [00:01<00:00, 236.51it/s, est. speed input: 278212.50 toks/s, output: 271.69 toks/s]   
Processed prompts:  66%|██████▋   | 340/512 [00:01<00:00, 197.94it/s, est. speed input: 236473.34 toks/s, output: 230.93 toks/s]
Processed prompts:  74%|███████▍  | 381/512 [00:01<00:00, 181.95it/s, est. speed input: 220678.65 toks/s, output: 215.51 toks/s]
Processed prompts:  80%|████████  | 411/512 [00:01<00:00, 170.78it/s, est. speed input: 211087.71 toks/s, output: 206.14 toks/s]
Processed prompts:  85%|████████▍ | 435/512 [00:02<00:00, 165.11it/s, est. speed input: 205936.33 toks/s, output: 201.11 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [00:02<00:00, 160.39it/s, est. speed input: 201987.58 toks/s, output: 197.25 toks/s]
Processed prompts:  93%|█████████▎| 475/512 [00:02<00:00, 155.00it/s, est. speed input: 198314.35 toks/s, output: 193.67 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [00:02<00:00, 154.09it/s, est. speed input: 196359.94 toks/s, output: 191.76 toks/s]
Processed prompts:  99%|█████████▉| 509/512 [00:02<00:00, 153.33it/s, est. speed input: 194582.26 toks/s, output: 190.02 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 153.33it/s, est. speed input: 193079.02 toks/s, output: 188.55 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 188.55it/s, est. speed input: 193079.02 toks/s, output: 188.55 toks/s]
[rank0]:[W128 13:27:44.522079812 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.8s

测试结果:
  Requests/s:   133.11
  Tokens/s:     136437.10
  Total Reqs:   512
  Elapsed:      3.85s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     136303.99

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:27:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3219494) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3219494) WARNING 01-28 13:28:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3219494) WARNING 01-28 13:28:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 253.18 requests/s, 259511.08 total tokens/s, 253.18 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 13:27:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:27:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:27:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:27:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:27:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:27:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:27:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:27:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:27:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:28:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:28:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:28:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:28:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:28:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:28:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:28:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:28:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:28:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3219494) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3219494) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3219494) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3219494) 
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3219494) 2026-01-28 13:28:21,954 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3219494) 2026-01-28 13:28:21,980 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3219494) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 15.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 16.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 13.59it/s]
(EngineCore_DP0 pid=3219494) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 15.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.31it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 397.12it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 423.92it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 433.02it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 434.89it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 441.63it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 452.94it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 450.77it/s]
Adding requests:  35%|███▌      | 360/1024 [00:00<00:01, 453.42it/s]
Adding requests:  40%|███▉      | 407/1024 [00:00<00:01, 455.58it/s]
Adding requests:  44%|████▍     | 454/1024 [00:01<00:01, 458.56it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 458.01it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 454.39it/s]
Adding requests:  58%|█████▊    | 595/1024 [00:01<00:00, 462.93it/s]
Adding requests:  63%|██████▎   | 643/1024 [00:01<00:00, 465.09it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:01<00:00, 471.85it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:01<00:00, 472.89it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:01<00:00, 470.43it/s]
Adding requests:  82%|████████▏ | 836/1024 [00:01<00:00, 459.72it/s]
Adding requests:  86%|████████▋ | 884/1024 [00:01<00:00, 465.25it/s]
Adding requests:  91%|█████████ | 933/1024 [00:02<00:00, 471.39it/s]
Adding requests:  96%|█████████▌| 981/1024 [00:02<00:00, 471.22it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 459.98it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:00<00:00, 5068.29it/s, est. speed input: 5190292.80 toks/s, output: 5068.38 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 5068.29it/s, est. speed input: 577062.26 toks/s, output: 563.54 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 563.50it/s, est. speed input: 577062.26 toks/s, output: 563.54 toks/s] 
[rank0]:[W128 13:28:28.571943084 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.1s

测试结果:
  Requests/s:   253.18
  Tokens/s:     259511.08
  Total Reqs:   1024
  Elapsed:      4.04s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     259257.90

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:28:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3220715) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3220715) WARNING 01-28 13:29:01 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3220715) WARNING 01-28 13:29:11 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 315.34 requests/s, 323218.91 total tokens/s, 315.34 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 13:28:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:28:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:28:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:28:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:28:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:28:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:28:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:28:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:28:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:28:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:28:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:28:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:28:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:28:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:28:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:28:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:28:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:28:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:53] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3220715) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3220715) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3220715) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3220715) 
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3220715) 2026-01-28 13:29:11,034 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3220715) 2026-01-28 13:29:11,060 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3220715) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 13.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 12.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 11.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 12.06it/s]
(EngineCore_DP0 pid=3220715) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 16.35it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.66it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 40/2048 [00:00<00:05, 398.16it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 422.28it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:04, 426.84it/s]
Adding requests:   8%|▊         | 171/2048 [00:00<00:04, 426.09it/s]
Adding requests:  10%|█         | 215/2048 [00:00<00:04, 429.23it/s]
Adding requests:  13%|█▎        | 262/2048 [00:00<00:04, 441.62it/s]
Adding requests:  15%|█▍        | 307/2048 [00:00<00:03, 444.21it/s]
Adding requests:  17%|█▋        | 354/2048 [00:00<00:03, 450.40it/s]
Adding requests:  20%|█▉        | 401/2048 [00:00<00:03, 455.74it/s]
Adding requests:  22%|██▏       | 448/2048 [00:01<00:03, 458.92it/s]
Adding requests:  24%|██▍       | 495/2048 [00:01<00:03, 461.36it/s]
Adding requests:  26%|██▋       | 542/2048 [00:01<00:03, 452.08it/s]
Adding requests:  29%|██▉       | 592/2048 [00:01<00:03, 464.00it/s]
Adding requests:  31%|███       | 639/2048 [00:01<00:03, 461.58it/s]
Adding requests:  34%|███▎      | 688/2048 [00:01<00:02, 468.22it/s]
Adding requests:  36%|███▌      | 737/2048 [00:01<00:02, 472.39it/s]
Adding requests:  38%|███▊      | 785/2048 [00:01<00:02, 467.42it/s]
Adding requests:  41%|████      | 832/2048 [00:01<00:02, 458.71it/s]
Adding requests:  43%|████▎     | 879/2048 [00:01<00:02, 461.44it/s]
Adding requests:  45%|████▌     | 928/2048 [00:02<00:02, 467.71it/s]
Adding requests:  48%|████▊     | 976/2048 [00:02<00:02, 470.20it/s]
Adding requests:  50%|█████     | 1025/2048 [00:02<00:02, 473.13it/s]
Adding requests:  52%|█████▏    | 1073/2048 [00:02<00:02, 468.17it/s]
Adding requests:  55%|█████▍    | 1120/2048 [00:02<00:01, 466.79it/s]
Adding requests:  57%|█████▋    | 1169/2048 [00:02<00:01, 472.96it/s]
Adding requests:  60%|█████▉    | 1220/2048 [00:02<00:01, 481.48it/s]
Adding requests:  62%|██████▏   | 1269/2048 [00:02<00:01, 472.75it/s]
Adding requests:  64%|██████▍   | 1317/2048 [00:02<00:01, 473.54it/s]
Adding requests:  67%|██████▋   | 1365/2048 [00:02<00:01, 475.15it/s]
Adding requests:  69%|██████▉   | 1414/2048 [00:03<00:01, 477.76it/s]
Adding requests:  71%|███████▏  | 1462/2048 [00:03<00:01, 478.04it/s]
Adding requests:  74%|███████▍  | 1511/2048 [00:03<00:01, 479.55it/s]
Adding requests:  76%|███████▌  | 1559/2048 [00:03<00:01, 478.61it/s]
Adding requests:  79%|███████▊  | 1609/2048 [00:03<00:00, 483.82it/s]
Adding requests:  81%|████████  | 1658/2048 [00:03<00:00, 480.35it/s]
Adding requests:  83%|████████▎ | 1707/2048 [00:03<00:00, 466.73it/s]
Adding requests:  86%|████████▌ | 1755/2048 [00:03<00:00, 469.55it/s]
Adding requests:  88%|████████▊ | 1803/2048 [00:03<00:00, 467.83it/s]
Adding requests:  90%|█████████ | 1851/2048 [00:03<00:00, 469.50it/s]
Adding requests:  93%|█████████▎| 1899/2048 [00:04<00:00, 471.33it/s]
Adding requests:  95%|█████████▌| 1947/2048 [00:04<00:00, 471.17it/s]
Adding requests:  97%|█████████▋| 1995/2048 [00:04<00:00, 472.27it/s]
Adding requests: 100%|█████████▉| 2044/2048 [00:04<00:00, 474.73it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 465.42it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:00<00:00, 12824.37it/s, est. speed input: 13132987.22 toks/s, output: 12824.54 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:02<00:00, 12824.37it/s, est. speed input: 1002039.49 toks/s, output: 978.55 toks/s]   
Processed prompts: 100%|██████████| 2048/2048 [00:02<00:00, 978.50it/s, est. speed input: 1002039.49 toks/s, output: 978.55 toks/s]  
[rank0]:[W128 13:29:20.247219098 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.7s

测试结果:
  Requests/s:   315.34
  Tokens/s:     323218.91
  Total Reqs:   2048
  Elapsed:      6.49s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     322903.57

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:29:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3222101) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3222101) WARNING 01-28 13:30:01 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3222101) WARNING 01-28 13:30:12 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 336.08 requests/s, 344482.47 total tokens/s, 336.08 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 13:29:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:29:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:29:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:29:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:29:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:29:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:29:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:29:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:29:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:29:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:29:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:29:53] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:29:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:53] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:53] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:29:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:29:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:29:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:29:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:29:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3222101) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3222101) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=3222101) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=3222101) 
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3222101) [rank0]:W0128 13:30:06.735000 3222101 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3222101) [rank0]:W0128 13:30:06.818000 3222101 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3222101) [rank0]:W0128 13:30:07.934000 3222101 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3222101) [rank0]:W0128 13:30:08.064000 3222101 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3222101) 2026-01-28 13:30:12,003 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3222101) 2026-01-28 13:30:12,031 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3222101) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 14.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 15.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.81it/s]
(EngineCore_DP0 pid=3222101) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 15.40it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 15.30it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 15.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 15.18it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/4096 [00:00<00:10, 403.11it/s]
Adding requests:   2%|▏         | 85/4096 [00:00<00:09, 422.93it/s]
Adding requests:   3%|▎         | 130/4096 [00:00<00:09, 433.99it/s]
Adding requests:   4%|▍         | 175/4096 [00:00<00:08, 436.30it/s]
Adding requests:   5%|▌         | 221/4096 [00:00<00:08, 444.23it/s]
Adding requests:   7%|▋         | 270/4096 [00:00<00:08, 456.28it/s]
Adding requests:   8%|▊         | 316/4096 [00:00<00:08, 452.46it/s]
Adding requests:   9%|▉         | 364/4096 [00:00<00:08, 458.87it/s]
Adding requests:  10%|█         | 411/4096 [00:00<00:07, 460.87it/s]
Adding requests:  11%|█         | 459/4096 [00:01<00:07, 464.96it/s]
Adding requests:  12%|█▏        | 506/4096 [00:01<00:07, 464.60it/s]
Adding requests:  14%|█▎        | 553/4096 [00:01<00:07, 458.81it/s]
Adding requests:  15%|█▍        | 601/4096 [00:01<00:07, 464.08it/s]
Adding requests:  16%|█▌        | 651/4096 [00:01<00:07, 471.77it/s]
Adding requests:  17%|█▋        | 702/4096 [00:01<00:07, 480.44it/s]
Adding requests:  18%|█▊        | 751/4096 [00:01<00:06, 479.34it/s]
Adding requests:  20%|█▉        | 799/4096 [00:01<00:06, 476.74it/s]
Adding requests:  21%|██        | 847/4096 [00:01<00:06, 467.77it/s]
Adding requests:  22%|██▏       | 897/4096 [00:01<00:06, 476.98it/s]
Adding requests:  23%|██▎       | 945/4096 [00:02<00:06, 469.86it/s]
Adding requests:  24%|██▍       | 994/4096 [00:02<00:06, 474.07it/s]
Adding requests:  25%|██▌       | 1043/4096 [00:02<00:06, 476.17it/s]
Adding requests:  27%|██▋       | 1091/4096 [00:02<00:06, 474.45it/s]
Adding requests:  28%|██▊       | 1139/4096 [00:02<00:06, 469.36it/s]
Adding requests:  29%|██▉       | 1191/4096 [00:02<00:06, 482.22it/s]
Adding requests:  30%|███       | 1240/4096 [00:02<00:05, 482.74it/s]
Adding requests:  31%|███▏      | 1289/4096 [00:02<00:05, 478.12it/s]
Adding requests:  33%|███▎      | 1339/4096 [00:02<00:05, 483.01it/s]
Adding requests:  34%|███▍      | 1388/4096 [00:02<00:05, 483.06it/s]
Adding requests:  35%|███▌      | 1437/4096 [00:03<00:05, 481.16it/s]
Adding requests:  36%|███▋      | 1486/4096 [00:03<00:05, 483.33it/s]
Adding requests:  37%|███▋      | 1535/4096 [00:03<00:05, 484.71it/s]
Adding requests:  39%|███▊      | 1584/4096 [00:03<00:05, 485.69it/s]
Adding requests:  40%|███▉      | 1634/4096 [00:03<00:05, 489.76it/s]
Adding requests:  41%|████      | 1683/4096 [00:03<00:05, 474.92it/s]
Adding requests:  42%|████▏     | 1732/4096 [00:03<00:04, 477.43it/s]
Adding requests:  43%|████▎     | 1780/4096 [00:03<00:04, 474.58it/s]
Adding requests:  45%|████▍     | 1829/4096 [00:03<00:04, 477.77it/s]
Adding requests:  46%|████▌     | 1878/4096 [00:03<00:04, 479.37it/s]
Adding requests:  47%|████▋     | 1926/4096 [00:04<00:04, 479.02it/s]
Adding requests:  48%|████▊     | 1974/4096 [00:04<00:04, 479.26it/s]
Adding requests:  49%|████▉     | 2022/4096 [00:04<00:04, 472.71it/s]
Adding requests:  51%|█████     | 2072/4096 [00:04<00:04, 479.29it/s]
Adding requests:  52%|█████▏    | 2120/4096 [00:04<00:04, 476.63it/s]
Adding requests:  53%|█████▎    | 2168/4096 [00:04<00:04, 472.63it/s]
Adding requests:  54%|█████▍    | 2216/4096 [00:04<00:03, 471.57it/s]
Adding requests:  55%|█████▌    | 2265/4096 [00:04<00:03, 473.99it/s]
Adding requests:  57%|█████▋    | 2315/4096 [00:04<00:03, 479.51it/s]
Adding requests:  58%|█████▊    | 2363/4096 [00:05<00:03, 477.41it/s]
Adding requests:  59%|█████▉    | 2411/4096 [00:05<00:03, 476.25it/s]
Adding requests:  60%|██████    | 2461/4096 [00:05<00:03, 480.85it/s]
Adding requests:  61%|██████▏   | 2510/4096 [00:05<00:03, 478.53it/s]
Adding requests:  62%|██████▎   | 2560/4096 [00:05<00:03, 483.90it/s]
Adding requests:  64%|██████▎   | 2609/4096 [00:05<00:03, 482.41it/s]
Adding requests:  65%|██████▍   | 2659/4096 [00:05<00:02, 485.97it/s]
Adding requests:  66%|██████▌   | 2708/4096 [00:05<00:02, 479.94it/s]
Adding requests:  67%|██████▋   | 2757/4096 [00:05<00:02, 480.59it/s]
Adding requests:  69%|██████▊   | 2806/4096 [00:05<00:02, 477.15it/s]
Adding requests:  70%|██████▉   | 2855/4096 [00:06<00:02, 479.07it/s]
Adding requests:  71%|███████   | 2905/4096 [00:06<00:02, 482.75it/s]
Adding requests:  72%|███████▏  | 2954/4096 [00:06<00:02, 478.12it/s]
Adding requests:  73%|███████▎  | 3003/4096 [00:06<00:02, 479.77it/s]
Adding requests:  74%|███████▍  | 3051/4096 [00:06<00:02, 478.69it/s]
Adding requests:  76%|███████▌  | 3099/4096 [00:06<00:02, 476.09it/s]
Adding requests:  77%|███████▋  | 3147/4096 [00:06<00:02, 473.80it/s]
Adding requests:  78%|███████▊  | 3196/4096 [00:06<00:01, 475.41it/s]
Adding requests:  79%|███████▉  | 3244/4096 [00:06<00:01, 463.35it/s]
Adding requests:  80%|████████  | 3292/4096 [00:06<00:01, 467.60it/s]
Adding requests:  82%|████████▏ | 3339/4096 [00:07<00:01, 451.15it/s]
Adding requests:  83%|████████▎ | 3388/4096 [00:07<00:01, 459.37it/s]
Adding requests:  84%|████████▍ | 3437/4096 [00:07<00:01, 466.42it/s]
Adding requests:  85%|████████▌ | 3484/4096 [00:07<00:01, 459.57it/s]
Adding requests:  86%|████████▌ | 3532/4096 [00:07<00:01, 463.27it/s]
Adding requests:  87%|████████▋ | 3580/4096 [00:07<00:01, 466.17it/s]
Adding requests:  89%|████████▊ | 3627/4096 [00:07<00:01, 465.31it/s]
Adding requests:  90%|████████▉ | 3676/4096 [00:07<00:00, 470.12it/s]
Adding requests:  91%|█████████ | 3724/4096 [00:07<00:00, 471.50it/s]
Adding requests:  92%|█████████▏| 3773/4096 [00:07<00:00, 476.78it/s]
Adding requests:  93%|█████████▎| 3822/4096 [00:08<00:00, 478.83it/s]
Adding requests:  95%|█████████▍| 3873/4096 [00:08<00:00, 485.15it/s]
Adding requests:  96%|█████████▌| 3922/4096 [00:08<00:00, 483.63it/s]
Adding requests:  97%|█████████▋| 3971/4096 [00:08<00:00, 481.79it/s]
Adding requests:  98%|█████████▊| 4020/4096 [00:08<00:00, 482.30it/s]
Adding requests:  99%|█████████▉| 4069/4096 [00:08<00:00, 475.56it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 473.25it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  71%|███████   | 2903/4096 [00:00<00:00, 22552.36it/s, est. speed input: 23094985.94 toks/s, output: 22552.69 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:03<00:00, 22552.36it/s, est. speed input: 1187979.82 toks/s, output: 1160.13 toks/s]  
Processed prompts: 100%|██████████| 4096/4096 [00:03<00:00, 1160.09it/s, est. speed input: 1187979.82 toks/s, output: 1160.13 toks/s] 
[rank0]:[W128 13:30:27.416342380 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 67.3s

测试结果:
  Requests/s:   336.08
  Tokens/s:     344482.47
  Total Reqs:   4096
  Elapsed:      12.19s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     344146.39


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,32.4193,16631.0762,3.9483
1024,1024,1,128,128,31.9351,32733.5128,4.0081
2048,1024,2,256,128,65.6086,67248.7803,3.9019
4096,1024,4,512,128,133.1094,136437.1014,3.8465
8192,1024,8,1024,128,253.1815,259511.0775,4.0445
16384,1024,16,2048,128,315.3355,323218.9052,6.4947
32768,1024,32,4096,128,336.0805,344482.4664,12.1876

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:30:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3223341) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3223341) WARNING 01-28 13:30:51 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3223341) WARNING 01-28 13:31:01 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.90 requests/s, 16876.07 total tokens/s, 32.90 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:30:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:30:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:30:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:30:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:30:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:30:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:30:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:30:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:30:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:30:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:30:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:30:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:30:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:30:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:30:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:30:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:30:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:30:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:30:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:30:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:30:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:30:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:30:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:30:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:30:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:30:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:30:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:30:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3223341) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3223341) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.81it/s]
(EngineCore_DP0 pid=3223341) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.80it/s]
(EngineCore_DP0 pid=3223341) 
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3223341) [2026-01-28 13:30:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3223341) 2026-01-28 13:31:01,193 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3223341) 2026-01-28 13:31:01,218 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3223341) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=3223341) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.15it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  59%|█████▊    | 75/128 [00:00<00:00, 744.01it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 756.76it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:24,  5.25it/s, est. speed input: 2688.35 toks/s, output: 5.25 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:06, 19.12it/s, est. speed input: 8451.25 toks/s, output: 16.51 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 25.72it/s, est. speed input: 11121.79 toks/s, output: 21.72 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 29.44it/s, est. speed input: 12675.84 toks/s, output: 24.76 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 31.70it/s, est. speed input: 13688.01 toks/s, output: 26.73 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 33.17it/s, est. speed input: 14406.09 toks/s, output: 28.14 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 34.16it/s, est. speed input: 14943.64 toks/s, output: 29.19 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 34.81it/s, est. speed input: 15356.96 toks/s, output: 29.99 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 35.19it/s, est. speed input: 15675.50 toks/s, output: 30.62 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 35.44it/s, est. speed input: 15933.66 toks/s, output: 31.12 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 35.69it/s, est. speed input: 16157.18 toks/s, output: 31.56 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 35.81it/s, est. speed input: 16339.45 toks/s, output: 31.91 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 35.87it/s, est. speed input: 16492.17 toks/s, output: 32.21 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 35.93it/s, est. speed input: 16627.04 toks/s, output: 32.47 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 35.64it/s, est. speed input: 16710.57 toks/s, output: 32.64 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 35.78it/s, est. speed input: 16816.97 toks/s, output: 32.85 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 35.88it/s, est. speed input: 16910.91 toks/s, output: 33.03 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 35.97it/s, est. speed input: 16996.51 toks/s, output: 33.20 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 36.00it/s, est. speed input: 17071.16 toks/s, output: 33.34 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 36.06it/s, est. speed input: 17141.45 toks/s, output: 33.48 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 35.74it/s, est. speed input: 17178.51 toks/s, output: 33.55 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 35.80it/s, est. speed input: 17232.70 toks/s, output: 33.66 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 35.85it/s, est. speed input: 17282.25 toks/s, output: 33.75 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 35.93it/s, est. speed input: 17330.86 toks/s, output: 33.85 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 35.98it/s, est. speed input: 17375.74 toks/s, output: 33.94 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 35.82it/s, est. speed input: 17405.35 toks/s, output: 33.99 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 35.92it/s, est. speed input: 17444.66 toks/s, output: 34.07 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 35.99it/s, est. speed input: 17481.95 toks/s, output: 34.14 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 35.98it/s, est. speed input: 17512.94 toks/s, output: 34.20 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 35.96it/s, est. speed input: 17541.56 toks/s, output: 34.26 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 35.85it/s, est. speed input: 17563.59 toks/s, output: 34.30 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.94it/s, est. speed input: 17591.99 toks/s, output: 34.36 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.94it/s, est. speed input: 17613.37 toks/s, output: 34.40 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.40it/s, est. speed input: 17613.37 toks/s, output: 34.40 toks/s]
[rank0]:[W128 13:31:07.993787484 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.5s

测试结果:
  Requests/s:   32.90
  Tokens/s:     16876.07
  Total Reqs:   128
  Elapsed:      3.89s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16843.17

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:31:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3224543) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3224543) WARNING 01-28 13:31:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3224543) WARNING 01-28 13:31:41 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.97 requests/s, 33791.90 total tokens/s, 32.97 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:31:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:31:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:31:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:31:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:31:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:31:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:31:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:31:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:31:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:31:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:31:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:31:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:31:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:31:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:31:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:31:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:31:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:31:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:24] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:24] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:24] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:24] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:24] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3224543) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3224543) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.80it/s]
(EngineCore_DP0 pid=3224543) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.80it/s]
(EngineCore_DP0 pid=3224543) 
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3224543) [2026-01-28 13:31:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3224543) 2026-01-28 13:31:41,719 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3224543) 2026-01-28 13:31:41,745 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3224543) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.89it/s]
(EngineCore_DP0 pid=3224543) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.99it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 399.94it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 425.17it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 425.92it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 15.86it/s, est. speed input: 16237.63 toks/s, output: 15.86 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:04, 26.96it/s, est. speed input: 25797.61 toks/s, output: 25.19 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 31.04it/s, est. speed input: 29379.51 toks/s, output: 28.69 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:03, 33.06it/s, est. speed input: 31242.55 toks/s, output: 30.51 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:03, 34.14it/s, est. speed input: 32354.55 toks/s, output: 31.60 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:03, 34.84it/s, est. speed input: 33122.35 toks/s, output: 32.35 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 35.31it/s, est. speed input: 33687.51 toks/s, output: 32.90 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 35.57it/s, est. speed input: 34092.75 toks/s, output: 33.29 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:02, 35.76it/s, est. speed input: 34415.74 toks/s, output: 33.61 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 35.89it/s, est. speed input: 34675.81 toks/s, output: 33.86 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 36.15it/s, est. speed input: 34936.81 toks/s, output: 34.12 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 36.31it/s, est. speed input: 35150.93 toks/s, output: 34.33 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 36.42it/s, est. speed input: 35330.46 toks/s, output: 34.50 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:02, 36.46it/s, est. speed input: 35477.67 toks/s, output: 34.65 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 36.55it/s, est. speed input: 35619.02 toks/s, output: 34.78 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 36.65it/s, est. speed input: 35750.73 toks/s, output: 34.91 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 36.66it/s, est. speed input: 35855.53 toks/s, output: 35.02 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 36.59it/s, est. speed input: 35935.89 toks/s, output: 35.09 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:01, 36.54it/s, est. speed input: 36007.05 toks/s, output: 35.16 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 36.57it/s, est. speed input: 36081.80 toks/s, output: 35.24 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 36.58it/s, est. speed input: 36146.67 toks/s, output: 35.30 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 36.59it/s, est. speed input: 36207.02 toks/s, output: 35.36 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 36.60it/s, est. speed input: 36262.95 toks/s, output: 35.41 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 36.58it/s, est. speed input: 36311.04 toks/s, output: 35.46 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 36.57it/s, est. speed input: 36355.18 toks/s, output: 35.50 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 36.55it/s, est. speed input: 36393.60 toks/s, output: 35.54 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 36.55it/s, est. speed input: 36431.56 toks/s, output: 35.58 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:03<00:00, 36.57it/s, est. speed input: 36469.32 toks/s, output: 35.61 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:03<00:00, 36.54it/s, est. speed input: 36499.32 toks/s, output: 35.64 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 36.56it/s, est. speed input: 36531.53 toks/s, output: 35.68 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 36.49it/s, est. speed input: 36553.44 toks/s, output: 35.70 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 36.55it/s, est. speed input: 36585.13 toks/s, output: 35.73 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.55it/s, est. speed input: 36601.95 toks/s, output: 35.74 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.74it/s, est. speed input: 36601.95 toks/s, output: 35.74 toks/s]
[rank0]:[W128 13:31:47.522240394 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.5s

测试结果:
  Requests/s:   32.97
  Tokens/s:     33791.90
  Total Reqs:   128
  Elapsed:      3.88s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33758.93

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:31:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3225680) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3225680) WARNING 01-28 13:32:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3225680) WARNING 01-28 13:32:22 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 66.02 requests/s, 67672.78 total tokens/s, 66.02 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 13:31:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:31:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:31:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:31:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:31:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:31:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:31:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:31:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:31:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:31:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:32:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:32:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:32:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:32:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:32:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:32:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:32:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:32:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:32:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3225680) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3225680) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.79it/s]
(EngineCore_DP0 pid=3225680) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.79it/s]
(EngineCore_DP0 pid=3225680) 
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3225680) [2026-01-28 13:32:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3225680) 2026-01-28 13:32:22,139 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3225680) 2026-01-28 13:32:22,165 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3225680) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 15.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 13.56it/s]
(EngineCore_DP0 pid=3225680) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.96it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:50,  5.02it/s]
Adding requests:  16%|█▋        | 42/256 [00:00<00:01, 174.02it/s]
Adding requests:  34%|███▍      | 87/256 [00:00<00:00, 275.57it/s]
Adding requests:  52%|█████▏    | 132/256 [00:00<00:00, 335.91it/s]
Adding requests:  69%|██████▉   | 176/256 [00:00<00:00, 369.85it/s]
Adding requests:  87%|████████▋ | 223/256 [00:00<00:00, 399.77it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 331.44it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:00<00:00, 394.41it/s, est. speed input: 403906.24 toks/s, output: 394.42 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:00<00:01, 108.88it/s, est. speed input: 125076.81 toks/s, output: 122.14 toks/s]
Processed prompts:  39%|███▉      | 101/256 [00:00<00:01, 95.96it/s, est. speed input: 110888.53 toks/s, output: 108.29 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:01<00:01, 87.29it/s, est. speed input: 102911.87 toks/s, output: 100.50 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:01<00:01, 82.88it/s, est. speed input: 98858.28 toks/s, output: 96.54 toks/s]  
Processed prompts:  54%|█████▍    | 138/256 [00:01<00:01, 80.37it/s, est. speed input: 96449.81 toks/s, output: 94.19 toks/s]
Processed prompts:  57%|█████▋    | 147/256 [00:01<00:01, 80.56it/s, est. speed input: 95524.17 toks/s, output: 93.29 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:01<00:01, 76.43it/s, est. speed input: 93198.09 toks/s, output: 91.01 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:01, 75.35it/s, est. speed input: 92002.51 toks/s, output: 89.85 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:01, 74.04it/s, est. speed input: 90822.43 toks/s, output: 88.69 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:02<00:01, 73.53it/s, est. speed input: 89903.76 toks/s, output: 87.80 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:00, 73.04it/s, est. speed input: 89054.03 toks/s, output: 86.97 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:02<00:00, 72.88it/s, est. speed input: 88332.83 toks/s, output: 86.26 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:02<00:00, 72.79it/s, est. speed input: 87683.35 toks/s, output: 85.63 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 72.57it/s, est. speed input: 87062.98 toks/s, output: 85.02 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:02<00:00, 71.94it/s, est. speed input: 86413.59 toks/s, output: 84.39 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 72.03it/s, est. speed input: 85907.72 toks/s, output: 83.89 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 72.22it/s, est. speed input: 85459.53 toks/s, output: 83.46 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 72.21it/s, est. speed input: 85024.61 toks/s, output: 83.03 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:03<00:00, 72.26it/s, est. speed input: 84628.63 toks/s, output: 82.64 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 72.26it/s, est. speed input: 84451.74 toks/s, output: 82.47 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 82.47it/s, est. speed input: 84451.74 toks/s, output: 82.47 toks/s]
[rank0]:[W128 13:32:27.190230513 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.6s

测试结果:
  Requests/s:   66.02
  Tokens/s:     67672.78
  Total Reqs:   256
  Elapsed:      3.88s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     67606.76

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:32:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3226900) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3226900) WARNING 01-28 13:32:54 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3226900) WARNING 01-28 13:33:03 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 133.02 requests/s, 136344.64 total tokens/s, 133.02 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 13:32:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:32:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:32:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:32:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:32:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:32:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:32:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:32:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:32:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:32:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:32:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:32:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:32:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:32:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:32:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:32:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:32:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:32:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:32:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3226900) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3226900) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.79it/s]
(EngineCore_DP0 pid=3226900) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.79it/s]
(EngineCore_DP0 pid=3226900) 
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3226900) [2026-01-28 13:32:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3226900) 2026-01-28 13:33:03,800 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3226900) 2026-01-28 13:33:03,827 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3226900) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.12it/s]
(EngineCore_DP0 pid=3226900) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 14.63it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 13.46it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 41/512 [00:00<00:01, 402.24it/s]
Adding requests:  17%|█▋        | 86/512 [00:00<00:00, 427.35it/s]
Adding requests:  26%|██▌       | 131/512 [00:00<00:00, 437.60it/s]
Adding requests:  34%|███▍      | 175/512 [00:00<00:00, 436.83it/s]
Adding requests:  43%|████▎     | 222/512 [00:00<00:00, 446.29it/s]
Adding requests:  53%|█████▎    | 270/512 [00:00<00:00, 455.97it/s]
Adding requests:  62%|██████▏   | 316/512 [00:00<00:00, 453.35it/s]
Adding requests:  71%|███████   | 363/512 [00:00<00:00, 458.57it/s]
Adding requests:  80%|████████  | 410/512 [00:00<00:00, 462.09it/s]
Adding requests:  89%|████████▉ | 457/512 [00:01<00:00, 464.43it/s]
Adding requests:  98%|█████████▊| 504/512 [00:01<00:00, 465.15it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 454.15it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:00<00:00, 1257.76it/s, est. speed input: 1288028.79 toks/s, output: 1257.79 toks/s]
Processed prompts:  53%|█████▎    | 272/512 [00:00<00:01, 237.54it/s, est. speed input: 279792.51 toks/s, output: 273.23 toks/s]   
Processed prompts:  65%|██████▌   | 333/512 [00:01<00:00, 201.23it/s, est. speed input: 240137.66 toks/s, output: 234.51 toks/s]
Processed prompts:  73%|███████▎  | 373/512 [00:01<00:00, 182.84it/s, est. speed input: 222582.77 toks/s, output: 217.37 toks/s]
Processed prompts:  79%|███████▊  | 403/512 [00:01<00:00, 171.20it/s, est. speed input: 212515.48 toks/s, output: 207.53 toks/s]
Processed prompts:  83%|████████▎ | 427/512 [00:02<00:00, 165.34it/s, est. speed input: 207119.57 toks/s, output: 202.26 toks/s]
Processed prompts:  88%|████████▊ | 448/512 [00:02<00:00, 162.09it/s, est. speed input: 203659.45 toks/s, output: 198.89 toks/s]
Processed prompts:  91%|█████████ | 467/512 [00:02<00:00, 154.28it/s, est. speed input: 199105.42 toks/s, output: 194.44 toks/s]
Processed prompts:  95%|█████████▍| 484/512 [00:02<00:00, 153.58it/s, est. speed input: 197097.66 toks/s, output: 192.48 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [00:02<00:00, 152.80it/s, est. speed input: 195222.03 toks/s, output: 190.65 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 152.80it/s, est. speed input: 192711.69 toks/s, output: 188.19 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 188.19it/s, est. speed input: 192711.69 toks/s, output: 188.19 toks/s]
[rank0]:[W128 13:33:09.194410091 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.9s

测试结果:
  Requests/s:   133.02
  Tokens/s:     136344.64
  Total Reqs:   512
  Elapsed:      3.85s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     136211.62

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:33:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3228166) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3228166) WARNING 01-28 13:33:37 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3228166) WARNING 01-28 13:33:47 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 240.69 requests/s, 246702.52 total tokens/s, 240.69 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 13:33:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:33:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:33:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:33:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:33:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:33:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:33:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:33:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:33:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:33:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:33:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:33:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:33:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:33:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:33:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:33:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:33:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:33:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:33:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:33:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:33:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:33:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:33:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:33:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:33:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:33:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:33:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:33:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:30] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3228166) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3228166) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.81it/s]
(EngineCore_DP0 pid=3228166) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.81it/s]
(EngineCore_DP0 pid=3228166) 
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3228166) [2026-01-28 13:33:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3228166) 2026-01-28 13:33:47,597 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3228166) 2026-01-28 13:33:47,623 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3228166) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 15.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 13.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 13.03it/s]
(EngineCore_DP0 pid=3228166) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 15.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.30it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.22it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 399.11it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 425.71it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 434.47it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 436.01it/s]
Adding requests:  21%|██▏       | 219/1024 [00:00<00:01, 440.88it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 454.65it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 449.13it/s]
Adding requests:  35%|███▌      | 361/1024 [00:00<00:01, 453.48it/s]
Adding requests:  40%|███▉      | 409/1024 [00:00<00:01, 459.28it/s]
Adding requests:  45%|████▍     | 457/1024 [00:01<00:01, 463.31it/s]
Adding requests:  49%|████▉     | 504/1024 [00:01<00:01, 462.68it/s]
Adding requests:  54%|█████▍    | 551/1024 [00:01<00:01, 458.82it/s]
Adding requests:  58%|█████▊    | 599/1024 [00:01<00:00, 464.53it/s]
Adding requests:  63%|██████▎   | 648/1024 [00:01<00:00, 469.99it/s]
Adding requests:  68%|██████▊   | 697/1024 [00:01<00:00, 474.99it/s]
Adding requests:  73%|███████▎  | 745/1024 [00:01<00:00, 475.13it/s]
Adding requests:  77%|███████▋  | 793/1024 [00:01<00:00, 473.25it/s]
Adding requests:  82%|████████▏ | 841/1024 [00:01<00:00, 463.88it/s]
Adding requests:  87%|████████▋ | 890/1024 [00:01<00:00, 470.94it/s]
Adding requests:  92%|█████████▏| 938/1024 [00:02<00:00, 473.57it/s]
Adding requests:  96%|█████████▋| 986/1024 [00:02<00:00, 473.95it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 462.38it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:00<00:00, 4292.21it/s, est. speed input: 4395491.70 toks/s, output: 4292.28 toks/s]
Processed prompts:  95%|█████████▍| 968/1024 [00:01<00:00, 454.25it/s, est. speed input: 546658.38 toks/s, output: 533.84 toks/s]   
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 454.25it/s, est. speed input: 514350.98 toks/s, output: 502.29 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 502.27it/s, est. speed input: 514350.98 toks/s, output: 502.29 toks/s]
[rank0]:[W128 13:33:54.558910549 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.4s

测试结果:
  Requests/s:   240.69
  Tokens/s:     246702.52
  Total Reqs:   1024
  Elapsed:      4.25s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     246461.83

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:34:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3229389) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3229389) WARNING 01-28 13:34:27 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3229389) WARNING 01-28 13:34:37 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 272.41 requests/s, 279225.06 total tokens/s, 272.41 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 13:34:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:34:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:34:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:34:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:34:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:34:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:34:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:34:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:34:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:34:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:34:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:34:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:34:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:34:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:34:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:34:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:34:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:34:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:34:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:34:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:34:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:34:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:34:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:34:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:34:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:34:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:34:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:34:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3229389) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3229389) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.79it/s]
(EngineCore_DP0 pid=3229389) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.79it/s]
(EngineCore_DP0 pid=3229389) 
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3229389) [2026-01-28 13:34:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3229389) 2026-01-28 13:34:37,128 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3229389) 2026-01-28 13:34:37,156 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3229389) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 15.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 15.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.71it/s]
(EngineCore_DP0 pid=3229389) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 16.13it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.45it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 40/2048 [00:00<00:05, 397.86it/s]
Adding requests:   4%|▍         | 85/2048 [00:00<00:04, 423.67it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 429.67it/s]
Adding requests:   8%|▊         | 173/2048 [00:00<00:04, 432.55it/s]
Adding requests:  11%|█         | 219/2048 [00:00<00:04, 440.91it/s]
Adding requests:  13%|█▎        | 267/2048 [00:00<00:03, 451.62it/s]
Adding requests:  15%|█▌        | 313/2048 [00:00<00:03, 451.07it/s]
Adding requests:  18%|█▊        | 359/2048 [00:00<00:03, 452.55it/s]
Adding requests:  20%|█▉        | 407/2048 [00:00<00:03, 458.70it/s]
Adding requests:  22%|██▏       | 454/2048 [00:01<00:03, 460.51it/s]
Adding requests:  24%|██▍       | 501/2048 [00:01<00:03, 460.48it/s]
Adding requests:  27%|██▋       | 548/2048 [00:01<00:03, 451.46it/s]
Adding requests:  29%|██▉       | 596/2048 [00:01<00:03, 458.47it/s]
Adding requests:  31%|███▏      | 644/2048 [00:01<00:03, 463.85it/s]
Adding requests:  34%|███▍      | 693/2048 [00:01<00:02, 471.04it/s]
Adding requests:  36%|███▌      | 741/2048 [00:01<00:02, 470.08it/s]
Adding requests:  39%|███▊      | 789/2048 [00:01<00:02, 468.90it/s]
Adding requests:  41%|████      | 836/2048 [00:01<00:02, 459.90it/s]
Adding requests:  43%|████▎     | 883/2048 [00:01<00:02, 462.56it/s]
Adding requests:  46%|████▌     | 932/2048 [00:02<00:02, 468.51it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:02, 467.85it/s]
Adding requests:  50%|█████     | 1028/2048 [00:02<00:02, 472.11it/s]
Adding requests:  53%|█████▎    | 1076/2048 [00:02<00:02, 467.29it/s]
Adding requests:  55%|█████▍    | 1123/2048 [00:02<00:01, 467.17it/s]
Adding requests:  57%|█████▋    | 1172/2048 [00:02<00:01, 472.99it/s]
Adding requests:  60%|█████▉    | 1222/2048 [00:02<00:01, 478.92it/s]
Adding requests:  62%|██████▏   | 1270/2048 [00:02<00:01, 473.44it/s]
Adding requests:  64%|██████▍   | 1319/2048 [00:02<00:01, 475.76it/s]
Adding requests:  67%|██████▋   | 1367/2048 [00:02<00:01, 475.38it/s]
Adding requests:  69%|██████▉   | 1416/2048 [00:03<00:01, 475.97it/s]
Adding requests:  72%|███████▏  | 1465/2048 [00:03<00:01, 478.46it/s]
Adding requests:  74%|███████▍  | 1514/2048 [00:03<00:01, 479.92it/s]
Adding requests:  76%|███████▋  | 1562/2048 [00:03<00:01, 479.29it/s]
Adding requests:  79%|███████▊  | 1610/2048 [00:03<00:00, 472.92it/s]
Adding requests:  81%|████████  | 1658/2048 [00:03<00:00, 472.45it/s]
Adding requests:  83%|████████▎ | 1706/2048 [00:03<00:00, 471.71it/s]
Adding requests:  86%|████████▌ | 1754/2048 [00:03<00:00, 472.95it/s]
Adding requests:  88%|████████▊ | 1802/2048 [00:03<00:00, 470.53it/s]
Adding requests:  90%|█████████ | 1850/2048 [00:03<00:00, 472.45it/s]
Adding requests:  93%|█████████▎| 1898/2048 [00:04<00:00, 473.07it/s]
Adding requests:  95%|█████████▌| 1946/2048 [00:04<00:00, 473.66it/s]
Adding requests:  97%|█████████▋| 1994/2048 [00:04<00:00, 462.48it/s]
Adding requests: 100%|█████████▉| 2042/2048 [00:04<00:00, 467.15it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 465.45it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:00<00:00, 8281.65it/s, est. speed input: 8480839.52 toks/s, output: 8281.77 toks/s]
Processed prompts: 100%|█████████▉| 2047/2048 [00:03<00:00, 547.36it/s, est. speed input: 672625.93 toks/s, output: 656.86 toks/s]   
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 547.36it/s, est. speed input: 672933.11 toks/s, output: 657.16 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 657.14it/s, est. speed input: 672933.11 toks/s, output: 657.16 toks/s]
[rank0]:[W128 13:34:47.391156652 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.9s

测试结果:
  Requests/s:   272.41
  Tokens/s:     279225.06
  Total Reqs:   2048
  Elapsed:      7.52s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     278952.64

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:35:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3230790) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3230790) WARNING 01-28 13:35:29 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3230790) WARNING 01-28 13:35:39 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 289.14 requests/s, 296364.72 total tokens/s, 289.14 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 13:35:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:35:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:35:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:35:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:35:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:35:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:35:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:35:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:35:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:35:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:35:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:35:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:35:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:35:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:35:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:35:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:35:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:35:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:35:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:35:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:35:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:35:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:35:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:35:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:35:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:35:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:35:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:35:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3230790) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3230790) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.80it/s]
(EngineCore_DP0 pid=3230790) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.80it/s]
(EngineCore_DP0 pid=3230790) 
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3230790) [2026-01-28 13:35:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3230790) [rank0]:W0128 13:35:34.288000 3230790 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3230790) [rank0]:W0128 13:35:34.370000 3230790 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3230790) [rank0]:W0128 13:35:35.503000 3230790 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3230790) [rank0]:W0128 13:35:35.633000 3230790 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3230790) 2026-01-28 13:35:39,436 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3230790) 2026-01-28 13:35:39,465 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3230790) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 14.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 15.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 14.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.10it/s]
(EngineCore_DP0 pid=3230790) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 13.35it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 14.62it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 14.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 14.91it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/4096 [00:00<00:10, 401.12it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 426.11it/s]
Adding requests:   3%|▎         | 132/4096 [00:00<00:09, 437.85it/s]
Adding requests:   4%|▍         | 176/4096 [00:00<00:08, 438.39it/s]
Adding requests:   5%|▌         | 223/4096 [00:00<00:08, 447.22it/s]
Adding requests:   7%|▋         | 271/4096 [00:00<00:08, 457.01it/s]
Adding requests:   8%|▊         | 317/4096 [00:00<00:08, 443.46it/s]
Adding requests:   9%|▉         | 364/4096 [00:00<00:08, 449.79it/s]
Adding requests:  10%|█         | 411/4096 [00:00<00:08, 454.00it/s]
Adding requests:  11%|█         | 459/4096 [00:01<00:07, 460.10it/s]
Adding requests:  12%|█▏        | 506/4096 [00:01<00:08, 444.58it/s]
Adding requests:  13%|█▎        | 551/4096 [00:01<00:07, 445.12it/s]
Adding requests:  15%|█▍        | 599/4096 [00:01<00:07, 454.41it/s]
Adding requests:  16%|█▌        | 646/4096 [00:01<00:07, 458.02it/s]
Adding requests:  17%|█▋        | 695/4096 [00:01<00:07, 466.68it/s]
Adding requests:  18%|█▊        | 742/4096 [00:01<00:07, 460.47it/s]
Adding requests:  19%|█▉        | 789/4096 [00:01<00:07, 462.75it/s]
Adding requests:  20%|██        | 836/4096 [00:01<00:07, 455.06it/s]
Adding requests:  22%|██▏       | 885/4096 [00:01<00:06, 464.67it/s]
Adding requests:  23%|██▎       | 934/4096 [00:02<00:06, 471.07it/s]
Adding requests:  24%|██▍       | 982/4096 [00:02<00:06, 471.65it/s]
Adding requests:  25%|██▌       | 1031/4096 [00:02<00:06, 475.08it/s]
Adding requests:  26%|██▋       | 1079/4096 [00:02<00:06, 471.21it/s]
Adding requests:  28%|██▊       | 1127/4096 [00:02<00:06, 467.86it/s]
Adding requests:  29%|██▊       | 1176/4096 [00:02<00:06, 473.54it/s]
Adding requests:  30%|██▉       | 1226/4096 [00:02<00:05, 478.78it/s]
Adding requests:  31%|███       | 1274/4096 [00:02<00:05, 474.33it/s]
Adding requests:  32%|███▏      | 1322/4096 [00:02<00:05, 475.97it/s]
Adding requests:  33%|███▎      | 1371/4096 [00:02<00:05, 478.99it/s]
Adding requests:  35%|███▍      | 1419/4096 [00:03<00:05, 478.89it/s]
Adding requests:  36%|███▌      | 1468/4096 [00:03<00:05, 480.45it/s]
Adding requests:  37%|███▋      | 1517/4096 [00:03<00:05, 482.94it/s]
Adding requests:  38%|███▊      | 1566/4096 [00:03<00:05, 480.90it/s]
Adding requests:  39%|███▉      | 1616/4096 [00:03<00:05, 484.53it/s]
Adding requests:  41%|████      | 1665/4096 [00:03<00:05, 480.67it/s]
Adding requests:  42%|████▏     | 1714/4096 [00:03<00:04, 481.39it/s]
Adding requests:  43%|████▎     | 1763/4096 [00:03<00:04, 479.71it/s]
Adding requests:  44%|████▍     | 1811/4096 [00:03<00:04, 479.27it/s]
Adding requests:  45%|████▌     | 1859/4096 [00:03<00:04, 468.30it/s]
Adding requests:  47%|████▋     | 1906/4096 [00:04<00:04, 468.08it/s]
Adding requests:  48%|████▊     | 1954/4096 [00:04<00:04, 469.76it/s]
Adding requests:  49%|████▉     | 2003/4096 [00:04<00:04, 474.84it/s]
Adding requests:  50%|█████     | 2051/4096 [00:04<00:04, 474.67it/s]
Adding requests:  51%|█████▏    | 2100/4096 [00:04<00:04, 478.25it/s]
Adding requests:  52%|█████▏    | 2148/4096 [00:04<00:04, 471.78it/s]
Adding requests:  54%|█████▎    | 2196/4096 [00:04<00:04, 464.15it/s]
Adding requests:  55%|█████▍    | 2245/4096 [00:04<00:03, 469.65it/s]
Adding requests:  56%|█████▌    | 2293/4096 [00:04<00:03, 471.68it/s]
Adding requests:  57%|█████▋    | 2341/4096 [00:05<00:03, 472.85it/s]
Adding requests:  58%|█████▊    | 2390/4096 [00:05<00:03, 476.07it/s]
Adding requests:  60%|█████▉    | 2438/4096 [00:05<00:03, 472.00it/s]
Adding requests:  61%|██████    | 2486/4096 [00:05<00:03, 474.00it/s]
Adding requests:  62%|██████▏   | 2534/4096 [00:05<00:03, 465.36it/s]
Adding requests:  63%|██████▎   | 2582/4096 [00:05<00:03, 467.28it/s]
Adding requests:  64%|██████▍   | 2630/4096 [00:05<00:03, 470.61it/s]
Adding requests:  65%|██████▌   | 2678/4096 [00:05<00:03, 472.49it/s]
Adding requests:  67%|██████▋   | 2726/4096 [00:05<00:02, 474.57it/s]
Adding requests:  68%|██████▊   | 2774/4096 [00:05<00:02, 475.86it/s]
Adding requests:  69%|██████▉   | 2822/4096 [00:06<00:02, 469.14it/s]
Adding requests:  70%|███████   | 2871/4096 [00:06<00:02, 473.95it/s]
Adding requests:  71%|███████▏  | 2920/4096 [00:06<00:02, 475.73it/s]
Adding requests:  72%|███████▏  | 2968/4096 [00:06<00:02, 473.88it/s]
Adding requests:  74%|███████▎  | 3017/4096 [00:06<00:02, 476.60it/s]
Adding requests:  75%|███████▍  | 3065/4096 [00:06<00:02, 460.17it/s]
Adding requests:  76%|███████▌  | 3114/4096 [00:06<00:02, 468.00it/s]
Adding requests:  77%|███████▋  | 3162/4096 [00:06<00:01, 469.86it/s]
Adding requests:  78%|███████▊  | 3210/4096 [00:06<00:01, 470.04it/s]
Adding requests:  80%|███████▉  | 3259/4096 [00:06<00:01, 475.49it/s]
Adding requests:  81%|████████  | 3308/4096 [00:07<00:01, 477.43it/s]
Adding requests:  82%|████████▏ | 3357/4096 [00:07<00:01, 480.42it/s]
Adding requests:  83%|████████▎ | 3406/4096 [00:07<00:01, 479.19it/s]
Adding requests:  84%|████████▍ | 3454/4096 [00:07<00:01, 477.96it/s]
Adding requests:  85%|████████▌ | 3502/4096 [00:07<00:01, 474.36it/s]
Adding requests:  87%|████████▋ | 3551/4096 [00:07<00:01, 478.16it/s]
Adding requests:  88%|████████▊ | 3599/4096 [00:07<00:01, 476.31it/s]
Adding requests:  89%|████████▉ | 3647/4096 [00:07<00:00, 473.48it/s]
Adding requests:  90%|█████████ | 3695/4096 [00:07<00:00, 474.87it/s]
Adding requests:  91%|█████████▏| 3743/4096 [00:07<00:00, 474.11it/s]
Adding requests:  93%|█████████▎| 3794/4096 [00:08<00:00, 482.73it/s]
Adding requests:  94%|█████████▍| 3844/4096 [00:08<00:00, 485.83it/s]
Adding requests:  95%|█████████▌| 3893/4096 [00:08<00:00, 484.93it/s]
Adding requests:  96%|█████████▌| 3942/4096 [00:08<00:00, 483.95it/s]
Adding requests:  97%|█████████▋| 3991/4096 [00:08<00:00, 480.04it/s]
Adding requests:  99%|█████████▊| 4040/4096 [00:08<00:00, 479.77it/s]
Adding requests: 100%|█████████▉| 4089/4096 [00:08<00:00, 480.87it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 470.67it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  61%|██████▏   | 2517/4096 [00:00<00:00, 15971.13it/s, est. speed input: 16355183.27 toks/s, output: 15971.33 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:05<00:00, 15971.13it/s, est. speed input: 767924.48 toks/s, output: 749.93 toks/s]    
Processed prompts: 100%|██████████| 4096/4096 [00:05<00:00, 749.91it/s, est. speed input: 767924.48 toks/s, output: 749.93 toks/s]  
[rank0]:[W128 13:35:56.917152635 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.7s

测试结果:
  Requests/s:   289.14
  Tokens/s:     296364.72
  Total Reqs:   4096
  Elapsed:      14.17s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     296075.59


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,32.8968,16876.0671,3.8910
1024,1024,1,128,128,32.9677,33791.9024,3.8826
2048,1024,2,256,128,66.0222,67672.7846,3.8775
4096,1024,4,512,128,133.0192,136344.6435,3.8491
8192,1024,8,1024,128,240.6854,246702.5154,4.2545
16384,1024,16,2048,128,272.4147,279225.0590,7.5179
32768,1024,32,4096,128,289.1363,296364.7242,14.1663

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:36:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3232057) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3232057) WARNING 01-28 13:36:21 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3232057) WARNING 01-28 13:36:31 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 33.12 requests/s, 16992.05 total tokens/s, 33.12 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:36:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:36:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:36:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:36:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:36:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:36:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:36:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:36:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:36:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:36:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:36:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:36:12] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:36:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:12] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:12] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:36:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:36:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:36:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:36:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:36:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3232057) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3232057) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=3232057) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=3232057) 
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3232057) [2026-01-28 13:36:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3232057) 2026-01-28 13:36:31,219 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3232057) 2026-01-28 13:36:31,244 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3232057) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.06it/s]
(EngineCore_DP0 pid=3232057) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.23it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  57%|█████▋    | 73/128 [00:00<00:00, 727.48it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 747.54it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:25,  5.01it/s, est. speed input: 2564.64 toks/s, output: 5.01 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:06, 18.69it/s, est. speed input: 8221.54 toks/s, output: 16.06 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 25.52it/s, est. speed input: 10946.82 toks/s, output: 21.38 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 29.39it/s, est. speed input: 12541.98 toks/s, output: 24.50 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 31.77it/s, est. speed input: 13591.69 toks/s, output: 26.55 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 33.36it/s, est. speed input: 14347.64 toks/s, output: 28.02 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 34.31it/s, est. speed input: 14893.45 toks/s, output: 29.09 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 35.00it/s, est. speed input: 15324.44 toks/s, output: 29.93 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 35.46it/s, est. speed input: 15665.66 toks/s, output: 30.60 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 35.73it/s, est. speed input: 15937.38 toks/s, output: 31.13 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 35.92it/s, est. speed input: 16164.95 toks/s, output: 31.57 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 36.03it/s, est. speed input: 16353.67 toks/s, output: 31.94 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 36.10it/s, est. speed input: 16514.17 toks/s, output: 32.25 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 36.19it/s, est. speed input: 16657.45 toks/s, output: 32.53 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 36.26it/s, est. speed input: 16783.53 toks/s, output: 32.78 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 36.25it/s, est. speed input: 16888.52 toks/s, output: 32.99 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 36.32it/s, est. speed input: 16988.40 toks/s, output: 33.18 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 36.30it/s, est. speed input: 17072.16 toks/s, output: 33.34 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 36.29it/s, est. speed input: 17148.08 toks/s, output: 33.49 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 36.32it/s, est. speed input: 17219.06 toks/s, output: 33.63 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 36.35it/s, est. speed input: 17284.53 toks/s, output: 33.76 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 36.34it/s, est. speed input: 17342.27 toks/s, output: 33.87 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 36.34it/s, est. speed input: 17395.18 toks/s, output: 33.97 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 36.33it/s, est. speed input: 17443.25 toks/s, output: 34.07 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 36.43it/s, est. speed input: 17494.23 toks/s, output: 34.17 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 36.41it/s, est. speed input: 17536.48 toks/s, output: 34.25 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 36.43it/s, est. speed input: 17577.39 toks/s, output: 34.33 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 36.41it/s, est. speed input: 17613.56 toks/s, output: 34.40 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 36.37it/s, est. speed input: 17645.78 toks/s, output: 34.46 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 36.34it/s, est. speed input: 17676.07 toks/s, output: 34.52 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 36.27it/s, est. speed input: 17701.66 toks/s, output: 34.57 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 36.30it/s, est. speed input: 17729.51 toks/s, output: 34.63 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.30it/s, est. speed input: 17750.31 toks/s, output: 34.67 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.67it/s, est. speed input: 17750.31 toks/s, output: 34.67 toks/s]
[rank0]:[W128 13:36:37.945509515 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.9s

测试结果:
  Requests/s:   33.12
  Tokens/s:     16992.05
  Total Reqs:   128
  Elapsed:      3.86s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16958.92

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:36:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3233261) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3233261) WARNING 01-28 13:37:02 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3233261) WARNING 01-28 13:37:11 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 33.00 requests/s, 33826.76 total tokens/s, 33.00 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:36:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:36:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:36:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:36:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:36:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:36:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:36:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:36:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:36:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:36:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:36:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:36:53] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:36:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:53] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:53] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:36:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:36:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:36:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:36:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:36:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:36:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3233261) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3233261) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=3233261) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=3233261) 
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3233261) [2026-01-28 13:36:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3233261) 2026-01-28 13:37:11,913 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3233261) 2026-01-28 13:37:11,939 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3233261) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.89it/s]
(EngineCore_DP0 pid=3233261) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.96it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 397.95it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 426.43it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 219.86it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 251.15it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:01, 90.85it/s, est. speed input: 93039.58 toks/s, output: 90.85 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:02, 48.04it/s, est. speed input: 52934.17 toks/s, output: 51.69 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 43.30it/s, est. speed input: 48182.00 toks/s, output: 47.05 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 41.00it/s, est. speed input: 45955.38 toks/s, output: 44.88 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 39.53it/s, est. speed input: 44502.72 toks/s, output: 43.46 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:02, 38.49it/s, est. speed input: 43439.79 toks/s, output: 42.42 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 37.89it/s, est. speed input: 42787.15 toks/s, output: 41.78 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 37.38it/s, est. speed input: 42233.90 toks/s, output: 41.24 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 37.10it/s, est. speed input: 41813.43 toks/s, output: 40.83 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 36.87it/s, est. speed input: 41450.21 toks/s, output: 40.48 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 36.71it/s, est. speed input: 41138.80 toks/s, output: 40.17 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 36.47it/s, est. speed input: 40839.53 toks/s, output: 39.88 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 36.43it/s, est. speed input: 40610.99 toks/s, output: 39.66 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 36.37it/s, est. speed input: 40399.61 toks/s, output: 39.45 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 36.38it/s, est. speed input: 40225.57 toks/s, output: 39.28 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 36.38it/s, est. speed input: 40066.97 toks/s, output: 39.13 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 36.28it/s, est. speed input: 39906.07 toks/s, output: 38.97 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 36.25it/s, est. speed input: 39769.34 toks/s, output: 38.84 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 36.25it/s, est. speed input: 39647.03 toks/s, output: 38.72 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 36.21it/s, est. speed input: 39530.28 toks/s, output: 38.60 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 36.18it/s, est. speed input: 39422.40 toks/s, output: 38.50 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 36.20it/s, est. speed input: 39328.50 toks/s, output: 38.41 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 36.20it/s, est. speed input: 39241.68 toks/s, output: 38.32 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 36.20it/s, est. speed input: 39159.57 toks/s, output: 38.24 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 36.16it/s, est. speed input: 39078.91 toks/s, output: 38.16 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 36.24it/s, est. speed input: 39017.36 toks/s, output: 38.10 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 36.30it/s, est. speed input: 38961.18 toks/s, output: 38.05 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.30it/s, est. speed input: 38917.30 toks/s, output: 38.01 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.00it/s, est. speed input: 38917.30 toks/s, output: 38.01 toks/s]
[rank0]:[W128 13:37:17.755929852 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.8s

测试结果:
  Requests/s:   33.00
  Tokens/s:     33826.76
  Total Reqs:   128
  Elapsed:      3.88s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33793.76

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:37:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3234385) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3234385) WARNING 01-28 13:37:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3234385) WARNING 01-28 13:37:52 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 70.22 requests/s, 71971.01 total tokens/s, 70.22 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 13:37:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:37:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:37:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:37:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:37:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:37:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:37:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:37:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:37:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:37:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:37:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:37:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:37:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:37:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:37:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:37:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:37:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:37:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:37:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:37:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:37:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:37:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:37:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:37:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:37:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:37:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:37:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:37:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:34] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3234385) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3234385) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.59it/s]
(EngineCore_DP0 pid=3234385) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.59it/s]
(EngineCore_DP0 pid=3234385) 
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3234385) [2026-01-28 13:37:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3234385) 2026-01-28 13:37:52,055 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3234385) 2026-01-28 13:37:52,082 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3234385) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 15.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 13.81it/s]
(EngineCore_DP0 pid=3234385) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.34it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.33it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  16%|█▌        | 41/256 [00:00<00:00, 401.49it/s]
Adding requests:  34%|███▎      | 86/256 [00:00<00:00, 428.47it/s]
Adding requests:  52%|█████▏    | 132/256 [00:00<00:00, 439.81it/s]
Adding requests:  69%|██████▉   | 177/256 [00:00<00:00, 439.51it/s]
Adding requests:  88%|████████▊ | 225/256 [00:00<00:00, 450.41it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 446.12it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:00<00:00, 344.00it/s, est. speed input: 352279.57 toks/s, output: 344.01 toks/s]
Processed prompts:  30%|███       | 77/256 [00:00<00:01, 116.42it/s, est. speed input: 133690.45 toks/s, output: 130.56 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:00<00:01, 96.54it/s, est. speed input: 113468.32 toks/s, output: 110.81 toks/s] 
Processed prompts:  43%|████▎     | 110/256 [00:01<00:01, 89.56it/s, est. speed input: 106375.82 toks/s, output: 103.88 toks/s]
Processed prompts:  47%|████▋     | 121/256 [00:01<00:01, 87.31it/s, est. speed input: 103534.38 toks/s, output: 101.11 toks/s]
Processed prompts:  51%|█████     | 131/256 [00:01<00:01, 83.65it/s, est. speed input: 100514.63 toks/s, output: 98.16 toks/s] 
Processed prompts:  55%|█████▍    | 140/256 [00:01<00:01, 78.78it/s, est. speed input: 97332.97 toks/s, output: 95.05 toks/s] 
Processed prompts:  58%|█████▊    | 149/256 [00:01<00:01, 79.38it/s, est. speed input: 96356.37 toks/s, output: 94.10 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:01<00:01, 75.34it/s, est. speed input: 94013.09 toks/s, output: 91.81 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:01<00:01, 74.70it/s, est. speed input: 92842.39 toks/s, output: 90.67 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:01<00:01, 74.05it/s, est. speed input: 91765.21 toks/s, output: 89.61 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:02<00:01, 73.68it/s, est. speed input: 90836.36 toks/s, output: 88.71 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:02<00:00, 73.40it/s, est. speed input: 90000.60 toks/s, output: 87.89 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:02<00:00, 73.20it/s, est. speed input: 89247.39 toks/s, output: 87.16 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:02<00:00, 72.99it/s, est. speed input: 88551.57 toks/s, output: 86.48 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:02<00:00, 72.78it/s, est. speed input: 87906.16 toks/s, output: 85.85 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:02<00:00, 72.73it/s, est. speed input: 87331.50 toks/s, output: 85.28 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:02<00:00, 72.68it/s, est. speed input: 86802.03 toks/s, output: 84.77 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:02<00:00, 72.66it/s, est. speed input: 86317.10 toks/s, output: 84.29 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:02<00:00, 72.68it/s, est. speed input: 85871.94 toks/s, output: 83.86 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:03<00:00, 72.68it/s, est. speed input: 85458.70 toks/s, output: 83.46 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 72.68it/s, est. speed input: 85361.23 toks/s, output: 83.36 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 83.36it/s, est. speed input: 85361.23 toks/s, output: 83.36 toks/s]
[rank0]:[W128 13:37:57.005210893 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.3s

测试结果:
  Requests/s:   70.22
  Tokens/s:     71971.01
  Total Reqs:   256
  Elapsed:      3.65s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     71900.80

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:38:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3235518) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3235518) WARNING 01-28 13:38:23 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3235518) WARNING 01-28 13:38:33 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 133.26 requests/s, 136591.12 total tokens/s, 133.26 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 13:38:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:38:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:38:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:38:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:38:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:38:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:38:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:38:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:38:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:38:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:38:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:38:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:38:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:38:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:38:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:38:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:38:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:38:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:16] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:16] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:16] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:16] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:16] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3235518) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3235518) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=3235518) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=3235518) 
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3235518) [2026-01-28 13:38:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3235518) 2026-01-28 13:38:33,579 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3235518) 2026-01-28 13:38:33,604 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3235518) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.36it/s]
(EngineCore_DP0 pid=3235518) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 16.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.43it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 41/512 [00:00<00:01, 401.78it/s]
Adding requests:  17%|█▋        | 86/512 [00:00<00:00, 426.82it/s]
Adding requests:  26%|██▌       | 131/512 [00:00<00:00, 437.10it/s]
Adding requests:  34%|███▍      | 175/512 [00:00<00:00, 437.27it/s]
Adding requests:  43%|████▎     | 222/512 [00:00<00:00, 446.45it/s]
Adding requests:  53%|█████▎    | 270/512 [00:00<00:00, 455.04it/s]
Adding requests:  62%|██████▏   | 316/512 [00:00<00:00, 453.03it/s]
Adding requests:  71%|███████   | 364/512 [00:00<00:00, 458.45it/s]
Adding requests:  80%|████████  | 411/512 [00:00<00:00, 461.45it/s]
Adding requests:  90%|████████▉ | 459/512 [00:01<00:00, 464.72it/s]
Adding requests:  99%|█████████▉| 506/512 [00:01<00:00, 465.24it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 453.93it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:00<00:00, 1326.94it/s, est. speed input: 1358882.20 toks/s, output: 1326.97 toks/s]
Processed prompts:  54%|█████▎    | 275/512 [00:01<00:01, 232.71it/s, est. speed input: 273195.20 toks/s, output: 266.79 toks/s]   
Processed prompts:  66%|██████▌   | 339/512 [00:01<00:00, 197.74it/s, est. speed input: 234860.08 toks/s, output: 229.35 toks/s]
Processed prompts:  74%|███████▍  | 380/512 [00:01<00:00, 181.63it/s, est. speed input: 219226.00 toks/s, output: 214.09 toks/s]
Processed prompts:  80%|████████  | 411/512 [00:01<00:00, 171.88it/s, est. speed input: 210537.01 toks/s, output: 205.60 toks/s]
Processed prompts:  85%|████████▌ | 436/512 [00:02<00:00, 167.52it/s, est. speed input: 206095.13 toks/s, output: 201.26 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:02<00:00, 157.81it/s, est. speed input: 200177.82 toks/s, output: 195.49 toks/s]
Processed prompts:  93%|█████████▎| 477/512 [00:02<00:00, 159.99it/s, est. speed input: 199025.36 toks/s, output: 194.36 toks/s]
Processed prompts:  97%|█████████▋| 495/512 [00:02<00:00, 152.96it/s, est. speed input: 195477.48 toks/s, output: 190.90 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 150.44it/s, est. speed input: 193244.33 toks/s, output: 188.71 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 150.44it/s, est. speed input: 193244.33 toks/s, output: 188.71 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 188.71it/s, est. speed input: 193244.33 toks/s, output: 188.71 toks/s]
[rank0]:[W128 13:38:39.898055497 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.8s

测试结果:
  Requests/s:   133.26
  Tokens/s:     136591.12
  Total Reqs:   512
  Elapsed:      3.84s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     136457.86

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:38:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3236810) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3236810) WARNING 01-28 13:39:07 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3236810) WARNING 01-28 13:39:17 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 239.01 requests/s, 244982.61 total tokens/s, 239.01 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 13:38:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:38:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:38:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:38:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:38:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:38:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:38:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:38:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:38:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:38:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:38:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:38:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:38:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:38:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:38:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:38:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:38:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:38:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:38:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3236810) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3236810) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=3236810) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=3236810) 
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3236810) [2026-01-28 13:39:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3236810) 2026-01-28 13:39:17,409 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3236810) 2026-01-28 13:39:17,435 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3236810) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 15.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 16.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 14.82it/s]
(EngineCore_DP0 pid=3236810) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 16.62it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.73it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 37/1024 [00:00<00:02, 367.07it/s]
Adding requests:   8%|▊         | 82/1024 [00:00<00:02, 409.98it/s]
Adding requests:  12%|█▏        | 127/1024 [00:00<00:02, 426.65it/s]
Adding requests:  17%|█▋        | 172/1024 [00:00<00:01, 432.61it/s]
Adding requests:  21%|██        | 216/1024 [00:00<00:01, 418.57it/s]
Adding requests:  26%|██▌       | 264/1024 [00:00<00:01, 437.34it/s]
Adding requests:  30%|███       | 308/1024 [00:00<00:01, 437.59it/s]
Adding requests:  35%|███▍      | 355/1024 [00:00<00:01, 445.94it/s]
Adding requests:  39%|███▉      | 402/1024 [00:00<00:01, 451.43it/s]
Adding requests:  44%|████▍     | 449/1024 [00:01<00:01, 456.51it/s]
Adding requests:  48%|████▊     | 496/1024 [00:01<00:01, 458.19it/s]
Adding requests:  53%|█████▎    | 542/1024 [00:01<00:01, 452.17it/s]
Adding requests:  58%|█████▊    | 591/1024 [00:01<00:00, 462.07it/s]
Adding requests:  62%|██████▏   | 638/1024 [00:01<00:00, 464.09it/s]
Adding requests:  67%|██████▋   | 687/1024 [00:01<00:00, 468.82it/s]
Adding requests:  72%|███████▏  | 735/1024 [00:01<00:00, 470.54it/s]
Adding requests:  76%|███████▋  | 783/1024 [00:01<00:00, 456.45it/s]
Adding requests:  81%|████████  | 829/1024 [00:01<00:00, 448.83it/s]
Adding requests:  86%|████████▌ | 876/1024 [00:01<00:00, 453.18it/s]
Adding requests:  90%|█████████ | 922/1024 [00:02<00:00, 447.15it/s]
Adding requests:  95%|█████████▍| 969/1024 [00:02<00:00, 452.19it/s]
Adding requests:  99%|█████████▉| 1017/1024 [00:02<00:00, 459.41it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 449.88it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:00<00:00, 4621.00it/s, est. speed input: 4732172.17 toks/s, output: 4621.08 toks/s]
Processed prompts:  99%|█████████▉| 1017/1024 [00:01<00:00, 440.34it/s, est. speed input: 529148.06 toks/s, output: 516.74 toks/s]  
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 440.34it/s, est. speed input: 522480.01 toks/s, output: 510.23 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 510.21it/s, est. speed input: 522480.01 toks/s, output: 510.23 toks/s]
[rank0]:[W128 13:39:24.302075404 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.4s

测试结果:
  Requests/s:   239.01
  Tokens/s:     244982.61
  Total Reqs:   1024
  Elapsed:      4.28s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     244743.61

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:39:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3238059) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3238059) WARNING 01-28 13:39:57 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3238059) WARNING 01-28 13:40:06 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 271.81 requests/s, 278605.29 total tokens/s, 271.81 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 13:39:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:39:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:39:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:39:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:39:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:39:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:39:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:39:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:39:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:39:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:39:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:39:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:39:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:39:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:39:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:39:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:39:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:39:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:39:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:39:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:39:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:39:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:39:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:39:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:39:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:39:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:39:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:39:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:49] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:49] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:49] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:49] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:49] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3238059) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3238059) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=3238059) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.59it/s]
(EngineCore_DP0 pid=3238059) 
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3238059) [2026-01-28 13:39:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3238059) 2026-01-28 13:40:06,824 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3238059) 2026-01-28 13:40:06,850 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3238059) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 15.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 16.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.08it/s]
(EngineCore_DP0 pid=3238059) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 15.65it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 15.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 15.58it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 41/2048 [00:00<00:04, 402.41it/s]
Adding requests:   4%|▍         | 86/2048 [00:00<00:04, 427.99it/s]
Adding requests:   6%|▋         | 131/2048 [00:00<00:04, 435.77it/s]
Adding requests:   9%|▊         | 175/2048 [00:00<00:04, 436.24it/s]
Adding requests:  11%|█         | 222/2048 [00:00<00:04, 446.12it/s]
Adding requests:  13%|█▎        | 270/2048 [00:00<00:03, 456.13it/s]
Adding requests:  15%|█▌        | 316/2048 [00:00<00:03, 454.20it/s]
Adding requests:  18%|█▊        | 364/2048 [00:00<00:03, 459.70it/s]
Adding requests:  20%|██        | 411/2048 [00:00<00:03, 462.73it/s]
Adding requests:  22%|██▏       | 459/2048 [00:01<00:03, 465.61it/s]
Adding requests:  25%|██▍       | 506/2048 [00:01<00:03, 466.14it/s]
Adding requests:  27%|██▋       | 553/2048 [00:01<00:03, 460.35it/s]
Adding requests:  29%|██▉       | 601/2048 [00:01<00:03, 465.69it/s]
Adding requests:  32%|███▏      | 649/2048 [00:01<00:02, 469.51it/s]
Adding requests:  34%|███▍      | 699/2048 [00:01<00:02, 476.48it/s]
Adding requests:  36%|███▋      | 747/2048 [00:01<00:02, 475.24it/s]
Adding requests:  39%|███▉      | 795/2048 [00:01<00:02, 473.30it/s]
Adding requests:  41%|████      | 843/2048 [00:01<00:02, 464.77it/s]
Adding requests:  44%|████▎     | 893/2048 [00:01<00:02, 472.07it/s]
Adding requests:  46%|████▌     | 941/2048 [00:02<00:02, 474.06it/s]
Adding requests:  48%|████▊     | 989/2048 [00:02<00:02, 458.82it/s]
Adding requests:  51%|█████     | 1035/2048 [00:02<00:02, 450.39it/s]
Adding requests:  53%|█████▎    | 1081/2048 [00:02<00:02, 444.48it/s]
Adding requests:  55%|█████▍    | 1126/2048 [00:02<00:02, 443.07it/s]
Adding requests:  57%|█████▋    | 1173/2048 [00:02<00:01, 449.56it/s]
Adding requests:  60%|█████▉    | 1222/2048 [00:02<00:01, 458.68it/s]
Adding requests:  62%|██████▏   | 1268/2048 [00:02<00:01, 455.20it/s]
Adding requests:  64%|██████▍   | 1315/2048 [00:02<00:01, 457.16it/s]
Adding requests:  66%|██████▋   | 1361/2048 [00:02<00:01, 456.83it/s]
Adding requests:  69%|██████▉   | 1408/2048 [00:03<00:01, 459.20it/s]
Adding requests:  71%|███████   | 1454/2048 [00:03<00:01, 455.88it/s]
Adding requests:  73%|███████▎  | 1503/2048 [00:03<00:01, 464.33it/s]
Adding requests:  76%|███████▌  | 1550/2048 [00:03<00:01, 464.85it/s]
Adding requests:  78%|███████▊  | 1597/2048 [00:03<00:00, 463.16it/s]
Adding requests:  80%|████████  | 1645/2048 [00:03<00:00, 466.39it/s]
Adding requests:  83%|████████▎ | 1692/2048 [00:03<00:00, 454.31it/s]
Adding requests:  85%|████████▌ | 1741/2048 [00:03<00:00, 462.62it/s]
Adding requests:  87%|████████▋ | 1788/2048 [00:03<00:00, 461.24it/s]
Adding requests:  90%|████████▉ | 1836/2048 [00:03<00:00, 466.60it/s]
Adding requests:  92%|█████████▏| 1884/2048 [00:04<00:00, 468.89it/s]
Adding requests:  94%|█████████▍| 1932/2048 [00:04<00:00, 471.17it/s]
Adding requests:  97%|█████████▋| 1980/2048 [00:04<00:00, 473.30it/s]
Adding requests:  99%|█████████▉| 2029/2048 [00:04<00:00, 477.87it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 461.55it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:00<00:00, 8302.21it/s, est. speed input: 8501927.77 toks/s, output: 8302.31 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 8302.21it/s, est. speed input: 677387.21 toks/s, output: 661.51 toks/s]  
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 661.48it/s, est. speed input: 677387.21 toks/s, output: 661.51 toks/s] 
[rank0]:[W128 13:40:16.041402372 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.7s

测试结果:
  Requests/s:   271.81
  Tokens/s:     278605.29
  Total Reqs:   2048
  Elapsed:      7.53s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     278333.48

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:40:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3239469) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3239469) WARNING 01-28 13:40:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3239469) WARNING 01-28 13:41:09 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 287.13 requests/s, 294305.09 total tokens/s, 287.13 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 13:40:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:40:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:40:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:40:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:40:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:40:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:40:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:40:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:40:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:40:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:40:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:40:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:40:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:40:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:40:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:40:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:40:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:40:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:40:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:40:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:40:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:40:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:40:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:40:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:40:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:40:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:40:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:40:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:51] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3239469) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3239469) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.59it/s]
(EngineCore_DP0 pid=3239469) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.59it/s]
(EngineCore_DP0 pid=3239469) 
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3239469) [2026-01-28 13:40:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3239469) [rank0]:W0128 13:41:04.165000 3239469 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3239469) [rank0]:W0128 13:41:04.248000 3239469 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3239469) [rank0]:W0128 13:41:05.359000 3239469 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3239469) [rank0]:W0128 13:41:05.487000 3239469 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3239469) 2026-01-28 13:41:09,091 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3239469) 2026-01-28 13:41:09,119 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3239469) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 14.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 14.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 14.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 13.93it/s]
(EngineCore_DP0 pid=3239469) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 16.08it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.36it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 16.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 16.39it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/4096 [00:00<00:10, 401.99it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 424.77it/s]
Adding requests:   3%|▎         | 131/4096 [00:00<00:09, 434.82it/s]
Adding requests:   4%|▍         | 175/4096 [00:00<00:08, 435.98it/s]
Adding requests:   5%|▌         | 221/4096 [00:00<00:08, 443.20it/s]
Adding requests:   7%|▋         | 269/4096 [00:00<00:08, 455.11it/s]
Adding requests:   8%|▊         | 315/4096 [00:00<00:08, 451.26it/s]
Adding requests:   9%|▉         | 361/4096 [00:00<00:08, 447.66it/s]
Adding requests:  10%|▉         | 408/4096 [00:00<00:08, 452.60it/s]
Adding requests:  11%|█         | 454/4096 [00:01<00:08, 449.34it/s]
Adding requests:  12%|█▏        | 500/4096 [00:01<00:07, 451.48it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:07, 450.01it/s]
Adding requests:  15%|█▍        | 595/4096 [00:01<00:07, 460.98it/s]
Adding requests:  16%|█▌        | 643/4096 [00:01<00:07, 465.43it/s]
Adding requests:  17%|█▋        | 693/4096 [00:01<00:07, 473.51it/s]
Adding requests:  18%|█▊        | 741/4096 [00:01<00:07, 471.54it/s]
Adding requests:  19%|█▉        | 789/4096 [00:01<00:07, 471.85it/s]
Adding requests:  20%|██        | 837/4096 [00:01<00:07, 455.64it/s]
Adding requests:  22%|██▏       | 886/4096 [00:01<00:06, 464.54it/s]
Adding requests:  23%|██▎       | 935/4096 [00:02<00:06, 468.67it/s]
Adding requests:  24%|██▍       | 984/4096 [00:02<00:06, 472.33it/s]
Adding requests:  25%|██▌       | 1033/4096 [00:02<00:06, 474.64it/s]
Adding requests:  26%|██▋       | 1081/4096 [00:02<00:06, 472.31it/s]
Adding requests:  28%|██▊       | 1129/4096 [00:02<00:06, 459.67it/s]
Adding requests:  29%|██▊       | 1176/4096 [00:02<00:06, 462.34it/s]
Adding requests:  30%|██▉       | 1225/4096 [00:02<00:06, 470.39it/s]
Adding requests:  31%|███       | 1273/4096 [00:02<00:06, 464.61it/s]
Adding requests:  32%|███▏      | 1321/4096 [00:02<00:05, 469.06it/s]
Adding requests:  33%|███▎      | 1370/4096 [00:02<00:05, 474.09it/s]
Adding requests:  35%|███▍      | 1418/4096 [00:03<00:05, 474.73it/s]
Adding requests:  36%|███▌      | 1467/4096 [00:03<00:05, 478.85it/s]
Adding requests:  37%|███▋      | 1516/4096 [00:03<00:05, 480.60it/s]
Adding requests:  38%|███▊      | 1565/4096 [00:03<00:05, 479.98it/s]
Adding requests:  39%|███▉      | 1615/4096 [00:03<00:05, 483.61it/s]
Adding requests:  41%|████      | 1664/4096 [00:03<00:05, 480.05it/s]
Adding requests:  42%|████▏     | 1713/4096 [00:03<00:04, 479.79it/s]
Adding requests:  43%|████▎     | 1761/4096 [00:03<00:04, 477.71it/s]
Adding requests:  44%|████▍     | 1810/4096 [00:03<00:04, 478.76it/s]
Adding requests:  45%|████▌     | 1859/4096 [00:03<00:04, 479.24it/s]
Adding requests:  47%|████▋     | 1907/4096 [00:04<00:04, 476.58it/s]
Adding requests:  48%|████▊     | 1955/4096 [00:04<00:04, 465.12it/s]
Adding requests:  49%|████▉     | 2004/4096 [00:04<00:04, 471.31it/s]
Adding requests:  50%|█████     | 2052/4096 [00:04<00:04, 472.48it/s]
Adding requests:  51%|█████▏    | 2101/4096 [00:04<00:04, 476.22it/s]
Adding requests:  52%|█████▏    | 2149/4096 [00:04<00:04, 470.44it/s]
Adding requests:  54%|█████▎    | 2197/4096 [00:04<00:04, 468.19it/s]
Adding requests:  55%|█████▍    | 2246/4096 [00:04<00:03, 472.47it/s]
Adding requests:  56%|█████▌    | 2294/4096 [00:04<00:03, 472.44it/s]
Adding requests:  57%|█████▋    | 2342/4096 [00:05<00:03, 471.64it/s]
Adding requests:  58%|█████▊    | 2391/4096 [00:05<00:03, 474.09it/s]
Adding requests:  60%|█████▉    | 2439/4096 [00:05<00:03, 474.13it/s]
Adding requests:  61%|██████    | 2488/4096 [00:05<00:03, 476.70it/s]
Adding requests:  62%|██████▏   | 2536/4096 [00:05<00:03, 475.76it/s]
Adding requests:  63%|██████▎   | 2585/4096 [00:05<00:03, 478.81it/s]
Adding requests:  64%|██████▍   | 2633/4096 [00:05<00:03, 478.68it/s]
Adding requests:  65%|██████▌   | 2681/4096 [00:05<00:02, 478.42it/s]
Adding requests:  67%|██████▋   | 2729/4096 [00:05<00:02, 477.40it/s]
Adding requests:  68%|██████▊   | 2777/4096 [00:05<00:02, 474.54it/s]
Adding requests:  69%|██████▉   | 2825/4096 [00:06<00:02, 470.64it/s]
Adding requests:  70%|███████   | 2873/4096 [00:06<00:02, 468.69it/s]
Adding requests:  71%|███████▏  | 2921/4096 [00:06<00:02, 471.04it/s]
Adding requests:  72%|███████▏  | 2969/4096 [00:06<00:02, 470.11it/s]
Adding requests:  74%|███████▎  | 3017/4096 [00:06<00:02, 472.06it/s]
Adding requests:  75%|███████▍  | 3065/4096 [00:06<00:02, 469.69it/s]
Adding requests:  76%|███████▌  | 3112/4096 [00:06<00:02, 466.66it/s]
Adding requests:  77%|███████▋  | 3159/4096 [00:06<00:02, 455.56it/s]
Adding requests:  78%|███████▊  | 3205/4096 [00:06<00:02, 444.55it/s]
Adding requests:  79%|███████▉  | 3254/4096 [00:06<00:01, 455.89it/s]
Adding requests:  81%|████████  | 3302/4096 [00:07<00:01, 460.24it/s]
Adding requests:  82%|████████▏ | 3350/4096 [00:07<00:01, 464.75it/s]
Adding requests:  83%|████████▎ | 3397/4096 [00:07<00:01, 465.99it/s]
Adding requests:  84%|████████▍ | 3445/4096 [00:07<00:01, 468.75it/s]
Adding requests:  85%|████████▌ | 3492/4096 [00:07<00:01, 463.72it/s]
Adding requests:  86%|████████▋ | 3541/4096 [00:07<00:01, 469.33it/s]
Adding requests:  88%|████████▊ | 3588/4096 [00:07<00:01, 468.09it/s]
Adding requests:  89%|████████▊ | 3635/4096 [00:07<00:00, 468.51it/s]
Adding requests:  90%|████████▉ | 3683/4096 [00:07<00:00, 469.66it/s]
Adding requests:  91%|█████████ | 3731/4096 [00:07<00:00, 471.40it/s]
Adding requests:  92%|█████████▏| 3781/4096 [00:08<00:00, 479.65it/s]
Adding requests:  93%|█████████▎| 3829/4096 [00:08<00:00, 479.15it/s]
Adding requests:  95%|█████████▍| 3879/4096 [00:08<00:00, 485.02it/s]
Adding requests:  96%|█████████▌| 3928/4096 [00:08<00:00, 482.44it/s]
Adding requests:  97%|█████████▋| 3977/4096 [00:08<00:00, 482.28it/s]
Adding requests:  98%|█████████▊| 4026/4096 [00:08<00:00, 479.27it/s]
Adding requests:  99%|█████████▉| 4074/4096 [00:08<00:00, 474.57it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 468.74it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  61%|██████▏   | 2517/4096 [00:00<00:00, 13325.22it/s, est. speed input: 13645625.16 toks/s, output: 13325.37 toks/s]
Processed prompts:  94%|█████████▍| 3850/4096 [00:04<00:00, 666.22it/s, est. speed input: 838427.98 toks/s, output: 818.78 toks/s]      
Processed prompts: 100%|██████████| 4096/4096 [00:05<00:00, 666.22it/s, est. speed input: 759154.29 toks/s, output: 741.36 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:05<00:00, 741.34it/s, est. speed input: 759154.29 toks/s, output: 741.36 toks/s]
[rank0]:[W128 13:41:26.629298238 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.7s

测试结果:
  Requests/s:   287.13
  Tokens/s:     294305.09
  Total Reqs:   4096
  Elapsed:      14.27s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     294017.96


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,33.1229,16992.0475,3.8644
1024,1024,1,128,128,33.0017,33826.7577,3.8786
2048,1024,2,256,128,70.2156,71971.0126,3.6459
4096,1024,4,512,128,133.2596,136591.1239,3.8421
8192,1024,8,1024,128,239.0074,244982.6128,4.2844
16384,1024,16,2048,128,271.8100,278605.2917,7.5347
32768,1024,32,4096,128,287.1269,294305.0883,14.2655

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:41:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3240825) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3240825) WARNING 01-28 13:41:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3240825) WARNING 01-28 13:42:00 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 33.60 requests/s, 17238.19 total tokens/s, 33.60 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:41:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:41:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:41:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:41:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:41:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:41:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:41:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:41:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:41:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:41:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:41:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:41:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:41:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:41:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:41:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:41:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:41:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:41:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:41:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:41:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:41:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:41:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:41:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:41:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:41:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:41:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:41:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:41:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3240825) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3240825) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]
(EngineCore_DP0 pid=3240825) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]
(EngineCore_DP0 pid=3240825) 
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3240825) [2026-01-28 13:41:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17735680 bytes
(EngineCore_DP0 pid=3240825) 2026-01-28 13:42:00,932 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3240825) 2026-01-28 13:42:00,958 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3240825) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.05it/s]
(EngineCore_DP0 pid=3240825) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.36it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  57%|█████▋    | 73/128 [00:00<00:00, 729.46it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 335.52it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 48.78it/s, est. speed input: 24977.30 toks/s, output: 48.78 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:02, 41.00it/s, est. speed input: 21506.11 toks/s, output: 42.00 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 39.07it/s, est. speed input: 20577.64 toks/s, output: 40.19 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 38.33it/s, est. speed input: 20209.59 toks/s, output: 39.47 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 37.86it/s, est. speed input: 19971.50 toks/s, output: 39.01 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 37.62it/s, est. speed input: 19823.06 toks/s, output: 38.72 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 37.41it/s, est. speed input: 19700.34 toks/s, output: 38.48 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:02, 37.29it/s, est. speed input: 19613.03 toks/s, output: 38.31 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:02, 37.28it/s, est. speed input: 19557.39 toks/s, output: 38.20 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 37.23it/s, est. speed input: 19503.77 toks/s, output: 38.09 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 37.13it/s, est. speed input: 19450.57 toks/s, output: 37.99 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 37.06it/s, est. speed input: 19405.07 toks/s, output: 37.90 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 37.04it/s, est. speed input: 19370.48 toks/s, output: 37.83 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 36.98it/s, est. speed input: 19335.97 toks/s, output: 37.77 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 36.99it/s, est. speed input: 19310.46 toks/s, output: 37.72 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 37.02it/s, est. speed input: 19291.34 toks/s, output: 37.68 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 36.91it/s, est. speed input: 19261.07 toks/s, output: 37.62 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 36.94it/s, est. speed input: 19244.48 toks/s, output: 37.59 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 36.95it/s, est. speed input: 19228.18 toks/s, output: 37.55 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 37.02it/s, est. speed input: 19219.27 toks/s, output: 37.54 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 37.06it/s, est. speed input: 19209.53 toks/s, output: 37.52 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 37.05it/s, est. speed input: 19198.37 toks/s, output: 37.50 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 37.07it/s, est. speed input: 19190.12 toks/s, output: 37.48 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 37.04it/s, est. speed input: 19179.47 toks/s, output: 37.46 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 36.99it/s, est. speed input: 19167.76 toks/s, output: 37.44 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 37.01it/s, est. speed input: 19160.21 toks/s, output: 37.42 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 36.95it/s, est. speed input: 19148.56 toks/s, output: 37.40 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:03<00:00, 36.97it/s, est. speed input: 19142.04 toks/s, output: 37.39 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 36.98it/s, est. speed input: 19135.23 toks/s, output: 37.37 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 37.00it/s, est. speed input: 19129.89 toks/s, output: 37.36 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 37.03it/s, est. speed input: 19125.33 toks/s, output: 37.35 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.03it/s, est. speed input: 19125.23 toks/s, output: 37.35 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.35it/s, est. speed input: 19125.23 toks/s, output: 37.35 toks/s]
[rank0]:[W128 13:42:07.605391059 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.7s

测试结果:
  Requests/s:   33.60
  Tokens/s:     17238.19
  Total Reqs:   128
  Elapsed:      3.81s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     17204.59

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:42:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3242166) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3242166) WARNING 01-28 13:42:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3242166) WARNING 01-28 13:42:41 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.77 requests/s, 33591.92 total tokens/s, 32.77 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:42:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:42:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:42:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:42:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:42:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:42:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:42:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:42:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:42:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:42:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:42:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:42:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:42:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:42:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:42:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:42:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:42:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:42:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3242166) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3242166) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]
(EngineCore_DP0 pid=3242166) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]
(EngineCore_DP0 pid=3242166) 
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3242166) [2026-01-28 13:42:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17735680 bytes
(EngineCore_DP0 pid=3242166) 2026-01-28 13:42:41,742 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3242166) 2026-01-28 13:42:41,767 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3242166) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.81it/s]
(EngineCore_DP0 pid=3242166) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.90it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  32%|███▏      | 41/128 [00:00<00:00, 403.70it/s]
Adding requests:  67%|██████▋   | 86/128 [00:00<00:00, 428.07it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 428.95it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:04, 26.59it/s, est. speed input: 27226.24 toks/s, output: 26.59 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 31.88it/s, est. speed input: 31833.71 toks/s, output: 31.09 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 33.57it/s, est. speed input: 33365.02 toks/s, output: 32.58 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:03, 34.54it/s, est. speed input: 34245.22 toks/s, output: 33.44 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:03, 35.04it/s, est. speed input: 34751.46 toks/s, output: 33.94 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 35.39it/s, est. speed input: 35114.26 toks/s, output: 34.29 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 35.54it/s, est. speed input: 35341.06 toks/s, output: 34.51 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 35.64it/s, est. speed input: 35511.68 toks/s, output: 34.68 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:02, 35.73it/s, est. speed input: 35654.91 toks/s, output: 34.82 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:02, 35.80it/s, est. speed input: 35769.07 toks/s, output: 34.93 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 35.84it/s, est. speed input: 35861.83 toks/s, output: 35.02 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 35.82it/s, est. speed input: 35927.61 toks/s, output: 35.09 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 35.84it/s, est. speed input: 35989.32 toks/s, output: 35.15 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:02, 35.91it/s, est. speed input: 36057.03 toks/s, output: 35.21 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 35.91it/s, est. speed input: 36105.20 toks/s, output: 35.26 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 35.90it/s, est. speed input: 36145.14 toks/s, output: 35.30 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 35.79it/s, est. speed input: 36160.01 toks/s, output: 35.31 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:01, 35.42it/s, est. speed input: 36116.66 toks/s, output: 35.27 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:01, 35.54it/s, est. speed input: 36146.93 toks/s, output: 35.30 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 35.69it/s, est. speed input: 36184.51 toks/s, output: 35.34 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 35.88it/s, est. speed input: 36232.40 toks/s, output: 35.38 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 35.82it/s, est. speed input: 36246.68 toks/s, output: 35.40 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:01, 35.90it/s, est. speed input: 36276.80 toks/s, output: 35.43 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 35.96it/s, est. speed input: 36305.04 toks/s, output: 35.45 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 35.75it/s, est. speed input: 36297.67 toks/s, output: 35.45 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 35.88it/s, est. speed input: 36326.02 toks/s, output: 35.47 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:03<00:00, 35.89it/s, est. speed input: 36343.28 toks/s, output: 35.49 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:03<00:00, 35.89it/s, est. speed input: 36358.17 toks/s, output: 35.51 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:03<00:00, 35.92it/s, est. speed input: 36374.40 toks/s, output: 35.52 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 35.94it/s, est. speed input: 36390.26 toks/s, output: 35.54 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 35.30it/s, est. speed input: 36333.15 toks/s, output: 35.48 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 35.46it/s, est. speed input: 36344.60 toks/s, output: 35.49 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.46it/s, est. speed input: 36345.49 toks/s, output: 35.49 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.49it/s, est. speed input: 36345.49 toks/s, output: 35.49 toks/s]
[rank0]:[W128 13:42:47.559534142 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.2s

测试结果:
  Requests/s:   32.77
  Tokens/s:     33591.92
  Total Reqs:   128
  Elapsed:      3.91s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33559.14

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:42:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3243392) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3243392) WARNING 01-28 13:43:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3243392) WARNING 01-28 13:43:22 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 66.56 requests/s, 68224.36 total tokens/s, 66.56 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 13:42:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:42:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:42:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:42:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:42:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:42:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:42:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:42:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:42:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:42:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:43:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:43:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:43:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:43:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:43:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:43:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:43:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:43:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:43:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3243392) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3243392) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]
(EngineCore_DP0 pid=3243392) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]
(EngineCore_DP0 pid=3243392) 
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3243392) [2026-01-28 13:43:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17735680 bytes
(EngineCore_DP0 pid=3243392) 2026-01-28 13:43:22,257 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3243392) 2026-01-28 13:43:22,283 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3243392) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 15.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 12.64it/s]
(EngineCore_DP0 pid=3243392) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 13.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 13.40it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:51,  4.96it/s]
Adding requests:  16%|█▋        | 42/256 [00:00<00:01, 172.70it/s]
Adding requests:  34%|███▍      | 87/256 [00:00<00:00, 274.32it/s]
Adding requests:  52%|█████▏    | 132/256 [00:00<00:00, 334.07it/s]
Adding requests:  69%|██████▉   | 176/256 [00:00<00:00, 368.43it/s]
Adding requests:  87%|████████▋ | 222/256 [00:00<00:00, 397.47it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 329.88it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:00<00:00, 336.55it/s, est. speed input: 344646.42 toks/s, output: 336.55 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:00<00:01, 114.06it/s, est. speed input: 131172.71 toks/s, output: 128.10 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:00<00:01, 99.48it/s, est. speed input: 115632.31 toks/s, output: 112.92 toks/s] 
Processed prompts:  43%|████▎     | 109/256 [00:01<00:01, 91.33it/s, est. speed input: 107903.35 toks/s, output: 105.37 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:01<00:01, 84.64it/s, est. speed input: 102426.86 toks/s, output: 100.03 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:01<00:01, 81.77it/s, est. speed input: 99561.96 toks/s, output: 97.23 toks/s]  
Processed prompts:  54%|█████▍    | 139/256 [00:01<00:01, 81.69it/s, est. speed input: 98324.49 toks/s, output: 96.02 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:01<00:01, 77.19it/s, est. speed input: 95621.62 toks/s, output: 93.38 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:01<00:01, 76.12it/s, est. speed input: 94253.20 toks/s, output: 92.04 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:01, 75.23it/s, est. speed input: 93041.57 toks/s, output: 90.86 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:01, 74.66it/s, est. speed input: 92000.93 toks/s, output: 89.84 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:02<00:01, 74.21it/s, est. speed input: 91070.14 toks/s, output: 88.94 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:00, 73.74it/s, est. speed input: 90204.57 toks/s, output: 88.09 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:02<00:00, 73.38it/s, est. speed input: 89419.19 toks/s, output: 87.32 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:02<00:00, 73.28it/s, est. speed input: 88738.31 toks/s, output: 86.66 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 73.11it/s, est. speed input: 88100.37 toks/s, output: 86.04 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:02<00:00, 72.99it/s, est. speed input: 87516.42 toks/s, output: 85.47 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 72.85it/s, est. speed input: 86972.33 toks/s, output: 84.93 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 72.82it/s, est. speed input: 86480.69 toks/s, output: 84.45 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 72.74it/s, est. speed input: 86018.01 toks/s, output: 84.00 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:03<00:00, 72.76it/s, est. speed input: 85600.37 toks/s, output: 83.59 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 72.76it/s, est. speed input: 85415.16 toks/s, output: 83.41 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 83.41it/s, est. speed input: 85415.16 toks/s, output: 83.41 toks/s]
[rank0]:[W128 13:43:28.227770967 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.5s

测试结果:
  Requests/s:   66.56
  Tokens/s:     68224.36
  Total Reqs:   256
  Elapsed:      3.85s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     68157.80

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:43:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3244584) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3244584) WARNING 01-28 13:43:54 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3244584) WARNING 01-28 13:44:03 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 132.43 requests/s, 135736.96 total tokens/s, 132.43 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 13:43:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:43:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:43:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:43:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:43:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:43:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:43:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:43:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:43:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:43:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:43:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:43:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:43:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:43:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:43:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:43:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:43:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:43:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:43:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3244584) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3244584) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.36it/s]
(EngineCore_DP0 pid=3244584) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.35it/s]
(EngineCore_DP0 pid=3244584) 
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3244584) [2026-01-28 13:43:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17735680 bytes
(EngineCore_DP0 pid=3244584) 2026-01-28 13:44:03,844 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3244584) 2026-01-28 13:44:03,869 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3244584) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 13.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.21it/s]
(EngineCore_DP0 pid=3244584) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 16.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.28it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 40/512 [00:00<00:01, 397.67it/s]
Adding requests:  17%|█▋        | 85/512 [00:00<00:01, 422.79it/s]
Adding requests:  25%|██▌       | 130/512 [00:00<00:00, 432.98it/s]
Adding requests:  34%|███▍      | 174/512 [00:00<00:00, 432.95it/s]
Adding requests:  43%|████▎     | 220/512 [00:00<00:00, 440.40it/s]
Adding requests:  52%|█████▏    | 268/512 [00:00<00:00, 450.74it/s]
Adding requests:  61%|██████▏   | 314/512 [00:00<00:00, 449.26it/s]
Adding requests:  70%|███████   | 360/512 [00:00<00:00, 451.16it/s]
Adding requests:  80%|███████▉  | 408/512 [00:00<00:00, 457.39it/s]
Adding requests:  89%|████████▊ | 454/512 [00:01<00:00, 458.18it/s]
Adding requests:  98%|█████████▊| 500/512 [00:01<00:00, 457.54it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 448.73it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:00<00:00, 1349.10it/s, est. speed input: 1381575.24 toks/s, output: 1349.13 toks/s]
Processed prompts:  55%|█████▍    | 281/512 [00:01<00:00, 233.88it/s, est. speed input: 274918.92 toks/s, output: 268.47 toks/s]   
Processed prompts:  68%|██████▊   | 346/512 [00:01<00:00, 191.46it/s, est. speed input: 229551.48 toks/s, output: 224.17 toks/s]
Processed prompts:  76%|███████▌  | 387/512 [00:01<00:00, 179.58it/s, est. speed input: 216829.23 toks/s, output: 211.75 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:02<00:00, 170.07it/s, est. speed input: 208375.95 toks/s, output: 203.49 toks/s]
Processed prompts:  87%|████████▋ | 443/512 [00:02<00:00, 165.66it/s, est. speed input: 203971.27 toks/s, output: 199.19 toks/s]
Processed prompts:  91%|█████████ | 465/512 [00:02<00:00, 161.76it/s, est. speed input: 200498.38 toks/s, output: 195.80 toks/s]
Processed prompts:  95%|█████████▍| 484/512 [00:02<00:00, 155.85it/s, est. speed input: 196895.31 toks/s, output: 192.28 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:02<00:00, 149.18it/s, est. speed input: 193317.33 toks/s, output: 188.79 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 149.18it/s, est. speed input: 192454.77 toks/s, output: 187.94 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 187.94it/s, est. speed input: 192454.77 toks/s, output: 187.94 toks/s]
[rank0]:[W128 13:44:10.226812661 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.0s

测试结果:
  Requests/s:   132.43
  Tokens/s:     135736.96
  Total Reqs:   512
  Elapsed:      3.87s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     135604.53

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:44:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3245758) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3245758) WARNING 01-28 13:44:38 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3245758) WARNING 01-28 13:44:47 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 236.26 requests/s, 242164.40 total tokens/s, 236.26 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 13:44:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:44:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:44:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:44:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:44:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:44:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:44:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:44:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:44:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:44:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:44:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:44:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:44:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:44:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:44:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:44:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:44:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:44:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:44:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:44:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:44:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:44:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:44:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:44:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:44:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:44:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:44:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:44:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:30] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3245758) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3245758) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.46it/s]
(EngineCore_DP0 pid=3245758) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.46it/s]
(EngineCore_DP0 pid=3245758) 
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3245758) [2026-01-28 13:44:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17735680 bytes
(EngineCore_DP0 pid=3245758) 2026-01-28 13:44:47,929 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3245758) 2026-01-28 13:44:47,955 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3245758) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 15.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 16.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 14.67it/s]
(EngineCore_DP0 pid=3245758) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 16.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 14.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 14.46it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 397.95it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 424.62it/s]
Adding requests:  13%|█▎        | 129/1024 [00:00<00:02, 431.42it/s]
Adding requests:  17%|█▋        | 173/1024 [00:00<00:01, 433.59it/s]
Adding requests:  21%|██▏       | 218/1024 [00:00<00:01, 437.26it/s]
Adding requests:  26%|██▌       | 266/1024 [00:00<00:01, 449.88it/s]
Adding requests:  30%|███       | 311/1024 [00:00<00:01, 448.66it/s]
Adding requests:  35%|███▍      | 358/1024 [00:00<00:01, 452.17it/s]
Adding requests:  40%|███▉      | 405/1024 [00:00<00:01, 455.09it/s]
Adding requests:  44%|████▍     | 452/1024 [00:01<00:01, 458.91it/s]
Adding requests:  49%|████▊     | 499/1024 [00:01<00:01, 460.72it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 454.67it/s]
Adding requests:  58%|█████▊    | 595/1024 [00:01<00:00, 464.06it/s]
Adding requests:  63%|██████▎   | 643/1024 [00:01<00:00, 467.24it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:01<00:00, 473.93it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:01<00:00, 475.02it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:01<00:00, 472.28it/s]
Adding requests:  82%|████████▏ | 836/1024 [00:01<00:00, 461.19it/s]
Adding requests:  86%|████████▋ | 885/1024 [00:01<00:00, 468.14it/s]
Adding requests:  91%|█████████ | 934/1024 [00:02<00:00, 474.21it/s]
Adding requests:  96%|█████████▌| 982/1024 [00:02<00:00, 473.06it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 460.58it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:00<00:00, 4071.68it/s, est. speed input: 4169637.90 toks/s, output: 4071.74 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:01<00:00, 442.89it/s, est. speed input: 534849.61 toks/s, output: 522.31 toks/s]   
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 442.89it/s, est. speed input: 497014.95 toks/s, output: 485.37 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 485.34it/s, est. speed input: 497014.95 toks/s, output: 485.37 toks/s]
[rank0]:[W128 13:44:54.898158636 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.6s

测试结果:
  Requests/s:   236.26
  Tokens/s:     242164.40
  Total Reqs:   1024
  Elapsed:      4.33s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     241928.15

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:45:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3247075) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3247075) WARNING 01-28 13:45:27 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3247075) WARNING 01-28 13:45:37 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 265.52 requests/s, 272153.66 total tokens/s, 265.52 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 13:45:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:45:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:45:12] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:45:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:45:12] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:45:12] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:45:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:45:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:45:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:45:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:45:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:45:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:45:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:45:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:45:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:45:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:45:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:45:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:45:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:45:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:45:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:45:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:45:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:45:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:45:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:45:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:45:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:45:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3247075) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3247075) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=3247075) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=3247075) 
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3247075) [2026-01-28 13:45:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17735680 bytes
(EngineCore_DP0 pid=3247075) 2026-01-28 13:45:37,322 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3247075) 2026-01-28 13:45:37,348 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3247075) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 10.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 13.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 13.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 12.44it/s]
(EngineCore_DP0 pid=3247075) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 16.26it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.62it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.59it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 40/2048 [00:00<00:05, 398.01it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 422.27it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:04, 430.17it/s]
Adding requests:   8%|▊         | 172/2048 [00:00<00:04, 432.52it/s]
Adding requests:  11%|█         | 217/2048 [00:00<00:04, 438.53it/s]
Adding requests:  13%|█▎        | 265/2048 [00:00<00:03, 450.47it/s]
Adding requests:  15%|█▌        | 311/2048 [00:00<00:03, 450.33it/s]
Adding requests:  17%|█▋        | 357/2048 [00:00<00:03, 452.32it/s]
Adding requests:  20%|█▉        | 404/2048 [00:00<00:03, 456.73it/s]
Adding requests:  22%|██▏       | 450/2048 [00:01<00:03, 456.91it/s]
Adding requests:  24%|██▍       | 496/2048 [00:01<00:03, 450.14it/s]
Adding requests:  26%|██▋       | 542/2048 [00:01<00:03, 435.61it/s]
Adding requests:  29%|██▊       | 588/2048 [00:01<00:03, 440.52it/s]
Adding requests:  31%|███       | 633/2048 [00:01<00:03, 434.08it/s]
Adding requests:  33%|███▎      | 678/2048 [00:01<00:03, 438.54it/s]
Adding requests:  35%|███▌      | 723/2048 [00:01<00:03, 440.40it/s]
Adding requests:  38%|███▊      | 768/2048 [00:01<00:02, 439.52it/s]
Adding requests:  40%|███▉      | 812/2048 [00:01<00:02, 432.89it/s]
Adding requests:  42%|████▏     | 857/2048 [00:01<00:02, 435.55it/s]
Adding requests:  44%|████▍     | 906/2048 [00:02<00:02, 449.83it/s]
Adding requests:  46%|████▋     | 952/2048 [00:02<00:02, 452.68it/s]
Adding requests:  49%|████▉     | 999/2048 [00:02<00:02, 457.36it/s]
Adding requests:  51%|█████     | 1047/2048 [00:02<00:02, 462.80it/s]
Adding requests:  53%|█████▎    | 1094/2048 [00:02<00:02, 461.68it/s]
Adding requests:  56%|█████▌    | 1141/2048 [00:02<00:01, 458.34it/s]
Adding requests:  58%|█████▊    | 1187/2048 [00:02<00:01, 457.94it/s]
Adding requests:  60%|██████    | 1235/2048 [00:02<00:01, 464.11it/s]
Adding requests:  63%|██████▎   | 1282/2048 [00:02<00:01, 460.66it/s]
Adding requests:  65%|██████▍   | 1330/2048 [00:02<00:01, 464.81it/s]
Adding requests:  67%|██████▋   | 1378/2048 [00:03<00:01, 469.02it/s]
Adding requests:  70%|██████▉   | 1426/2048 [00:03<00:01, 470.27it/s]
Adding requests:  72%|███████▏  | 1474/2048 [00:03<00:01, 473.13it/s]
Adding requests:  74%|███████▍  | 1522/2048 [00:03<00:01, 474.31it/s]
Adding requests:  77%|███████▋  | 1570/2048 [00:03<00:01, 475.22it/s]
Adding requests:  79%|███████▉  | 1619/2048 [00:03<00:00, 477.03it/s]
Adding requests:  81%|████████▏ | 1667/2048 [00:03<00:00, 474.55it/s]
Adding requests:  84%|████████▎ | 1715/2048 [00:03<00:00, 465.02it/s]
Adding requests:  86%|████████▌ | 1762/2048 [00:03<00:00, 465.51it/s]
Adding requests:  88%|████████▊ | 1809/2048 [00:03<00:00, 464.98it/s]
Adding requests:  91%|█████████ | 1857/2048 [00:04<00:00, 467.82it/s]
Adding requests:  93%|█████████▎| 1904/2048 [00:04<00:00, 467.02it/s]
Adding requests:  95%|█████████▌| 1952/2048 [00:04<00:00, 468.39it/s]
Adding requests:  98%|█████████▊| 2000/2048 [00:04<00:00, 469.99it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 471.16it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 456.54it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:00<00:00, 8512.95it/s, est. speed input: 8717726.89 toks/s, output: 8513.06 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 8512.95it/s, est. speed input: 650118.60 toks/s, output: 634.88 toks/s]  
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 634.85it/s, est. speed input: 650118.60 toks/s, output: 634.88 toks/s] 
[rank0]:[W128 13:45:47.760156040 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.9s

测试结果:
  Requests/s:   265.52
  Tokens/s:     272153.66
  Total Reqs:   2048
  Elapsed:      7.71s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     271888.14

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:46:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3248511) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3248511) WARNING 01-28 13:46:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3248511) WARNING 01-28 13:46:40 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 275.47 requests/s, 282359.59 total tokens/s, 275.47 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 13:46:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:46:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:46:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:46:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:46:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:46:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:46:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:46:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:46:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:46:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:46:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:46:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:46:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:46:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:46:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:46:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:46:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:46:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:46:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:46:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:46:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:46:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:46:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:46:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:46:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:46:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:46:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:46:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3248511) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3248511) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=3248511) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=3248511) 
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3248511) [2026-01-28 13:46:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17735680 bytes
(EngineCore_DP0 pid=3248511) [rank0]:W0128 13:46:35.015000 3248511 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3248511) [rank0]:W0128 13:46:35.099000 3248511 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3248511) [rank0]:W0128 13:46:36.220000 3248511 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3248511) [rank0]:W0128 13:46:36.349000 3248511 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3248511) 2026-01-28 13:46:40,092 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3248511) 2026-01-28 13:46:40,121 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3248511) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 10.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 11.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 12.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 13.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 14.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 12.96it/s]
(EngineCore_DP0 pid=3248511) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 15.98it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.34it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 16.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 16.21it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/4096 [00:00<00:10, 401.58it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 425.75it/s]
Adding requests:   3%|▎         | 131/4096 [00:00<00:09, 435.80it/s]
Adding requests:   4%|▍         | 176/4096 [00:00<00:08, 438.11it/s]
Adding requests:   5%|▌         | 223/4096 [00:00<00:08, 447.47it/s]
Adding requests:   7%|▋         | 271/4096 [00:00<00:08, 457.75it/s]
Adding requests:   8%|▊         | 317/4096 [00:00<00:08, 454.31it/s]
Adding requests:   9%|▉         | 366/4096 [00:00<00:08, 462.48it/s]
Adding requests:  10%|█         | 413/4096 [00:00<00:08, 457.98it/s]
Adding requests:  11%|█         | 460/4096 [00:01<00:07, 460.66it/s]
Adding requests:  12%|█▏        | 507/4096 [00:01<00:07, 462.22it/s]
Adding requests:  14%|█▎        | 554/4096 [00:01<00:07, 459.20it/s]
Adding requests:  15%|█▍        | 602/4096 [00:01<00:07, 463.10it/s]
Adding requests:  16%|█▌        | 652/4096 [00:01<00:07, 472.64it/s]
Adding requests:  17%|█▋        | 702/4096 [00:01<00:07, 480.30it/s]
Adding requests:  18%|█▊        | 751/4096 [00:01<00:06, 478.33it/s]
Adding requests:  20%|█▉        | 799/4096 [00:01<00:06, 476.31it/s]
Adding requests:  21%|██        | 847/4096 [00:01<00:06, 467.90it/s]
Adding requests:  22%|██▏       | 897/4096 [00:01<00:06, 476.88it/s]
Adding requests:  23%|██▎       | 945/4096 [00:02<00:06, 469.60it/s]
Adding requests:  24%|██▍       | 994/4096 [00:02<00:06, 474.47it/s]
Adding requests:  25%|██▌       | 1043/4096 [00:02<00:06, 477.50it/s]
Adding requests:  27%|██▋       | 1091/4096 [00:02<00:06, 476.93it/s]
Adding requests:  28%|██▊       | 1139/4096 [00:02<00:06, 470.34it/s]
Adding requests:  29%|██▉       | 1191/4096 [00:02<00:06, 484.09it/s]
Adding requests:  30%|███       | 1240/4096 [00:02<00:05, 484.09it/s]
Adding requests:  31%|███▏      | 1289/4096 [00:02<00:05, 480.52it/s]
Adding requests:  33%|███▎      | 1338/4096 [00:02<00:05, 483.24it/s]
Adding requests:  34%|███▍      | 1387/4096 [00:02<00:05, 484.12it/s]
Adding requests:  35%|███▌      | 1436/4096 [00:03<00:05, 464.82it/s]
Adding requests:  36%|███▋      | 1485/4096 [00:03<00:05, 471.85it/s]
Adding requests:  37%|███▋      | 1535/4096 [00:03<00:05, 478.63it/s]
Adding requests:  39%|███▊      | 1584/4096 [00:03<00:05, 480.91it/s]
Adding requests:  40%|███▉      | 1635/4096 [00:03<00:05, 487.71it/s]
Adding requests:  41%|████      | 1684/4096 [00:03<00:05, 475.96it/s]
Adding requests:  42%|████▏     | 1733/4096 [00:03<00:04, 479.95it/s]
Adding requests:  44%|████▎     | 1782/4096 [00:03<00:04, 474.83it/s]
Adding requests:  45%|████▍     | 1832/4096 [00:03<00:04, 479.68it/s]
Adding requests:  46%|████▌     | 1881/4096 [00:03<00:04, 480.18it/s]
Adding requests:  47%|████▋     | 1930/4096 [00:04<00:04, 479.20it/s]
Adding requests:  48%|████▊     | 1979/4096 [00:04<00:04, 481.09it/s]
Adding requests:  50%|████▉     | 2028/4096 [00:04<00:04, 471.63it/s]
Adding requests:  51%|█████     | 2078/4096 [00:04<00:04, 477.56it/s]
Adding requests:  52%|█████▏    | 2126/4096 [00:04<00:04, 473.95it/s]
Adding requests:  53%|█████▎    | 2174/4096 [00:04<00:04, 471.48it/s]
Adding requests:  54%|█████▍    | 2222/4096 [00:04<00:03, 473.44it/s]
Adding requests:  55%|█████▌    | 2270/4096 [00:04<00:03, 472.88it/s]
Adding requests:  57%|█████▋    | 2320/4096 [00:04<00:03, 478.73it/s]
Adding requests:  58%|█████▊    | 2368/4096 [00:05<00:03, 477.85it/s]
Adding requests:  59%|█████▉    | 2417/4096 [00:05<00:03, 479.19it/s]
Adding requests:  60%|██████    | 2466/4096 [00:05<00:03, 480.16it/s]
Adding requests:  61%|██████▏   | 2515/4096 [00:05<00:03, 478.46it/s]
Adding requests:  63%|██████▎   | 2565/4096 [00:05<00:03, 483.74it/s]
Adding requests:  64%|██████▍   | 2614/4096 [00:05<00:03, 483.07it/s]
Adding requests:  65%|██████▌   | 2664/4096 [00:05<00:02, 486.19it/s]
Adding requests:  66%|██████▌   | 2713/4096 [00:05<00:02, 479.93it/s]
Adding requests:  67%|██████▋   | 2762/4096 [00:05<00:02, 480.31it/s]
Adding requests:  69%|██████▊   | 2811/4096 [00:05<00:02, 477.33it/s]
Adding requests:  70%|██████▉   | 2860/4096 [00:06<00:02, 479.26it/s]
Adding requests:  71%|███████   | 2909/4096 [00:06<00:02, 482.12it/s]
Adding requests:  72%|███████▏  | 2958/4096 [00:06<00:02, 477.07it/s]
Adding requests:  73%|███████▎  | 3007/4096 [00:06<00:02, 479.56it/s]
Adding requests:  75%|███████▍  | 3056/4096 [00:06<00:02, 480.88it/s]
Adding requests:  76%|███████▌  | 3105/4096 [00:06<00:02, 477.75it/s]
Adding requests:  77%|███████▋  | 3154/4096 [00:06<00:01, 479.28it/s]
Adding requests:  78%|███████▊  | 3202/4096 [00:06<00:01, 479.45it/s]
Adding requests:  79%|███████▉  | 3252/4096 [00:06<00:01, 484.71it/s]
Adding requests:  81%|████████  | 3301/4096 [00:06<00:01, 474.28it/s]
Adding requests:  82%|████████▏ | 3350/4096 [00:07<00:01, 478.15it/s]
Adding requests:  83%|████████▎ | 3398/4096 [00:07<00:01, 477.46it/s]
Adding requests:  84%|████████▍ | 3447/4096 [00:07<00:01, 479.62it/s]
Adding requests:  85%|████████▌ | 3495/4096 [00:07<00:01, 476.42it/s]
Adding requests:  87%|████████▋ | 3544/4096 [00:07<00:01, 479.49it/s]
Adding requests:  88%|████████▊ | 3593/4096 [00:07<00:01, 480.56it/s]
Adding requests:  89%|████████▉ | 3642/4096 [00:07<00:00, 475.81it/s]
Adding requests:  90%|█████████ | 3692/4096 [00:07<00:00, 480.45it/s]
Adding requests:  91%|█████████▏| 3741/4096 [00:07<00:00, 476.60it/s]
Adding requests:  93%|█████████▎| 3793/4096 [00:07<00:00, 487.60it/s]
Adding requests:  94%|█████████▍| 3842/4096 [00:08<00:00, 488.02it/s]
Adding requests:  95%|█████████▌| 3892/4096 [00:08<00:00, 490.40it/s]
Adding requests:  96%|█████████▌| 3942/4096 [00:08<00:00, 488.55it/s]
Adding requests:  97%|█████████▋| 3991/4096 [00:08<00:00, 485.03it/s]
Adding requests:  99%|█████████▊| 4040/4096 [00:08<00:00, 484.94it/s]
Adding requests: 100%|█████████▉| 4089/4096 [00:08<00:00, 485.80it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 475.83it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  58%|█████▊    | 2363/4096 [00:00<00:00, 14168.57it/s, est. speed input: 14509319.35 toks/s, output: 14168.75 toks/s]
Processed prompts:  92%|█████████▏| 3780/4096 [00:05<00:00, 596.74it/s, est. speed input: 744869.69 toks/s, output: 727.41 toks/s]      
Processed prompts: 100%|██████████| 4096/4096 [00:06<00:00, 596.74it/s, est. speed input: 670127.69 toks/s, output: 654.42 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:06<00:00, 654.41it/s, est. speed input: 670127.69 toks/s, output: 654.42 toks/s]
[rank0]:[W128 13:46:58.393020037 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 70.8s

测试结果:
  Requests/s:   275.47
  Tokens/s:     282359.59
  Total Reqs:   4096
  Elapsed:      14.87s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     282084.11


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,33.6027,17238.1910,3.8092
1024,1024,1,128,128,32.7726,33591.9158,3.9057
2048,1024,2,256,128,66.5604,68224.3622,3.8461
4096,1024,4,512,128,132.4263,135736.9576,3.8663
8192,1024,8,1024,128,236.2580,242164.4033,4.3342
16384,1024,16,2048,128,265.5158,272153.6576,7.7133
32768,1024,32,4096,128,275.4728,282359.5862,14.8690

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 0 失败
============================================================
