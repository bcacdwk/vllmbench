# A100 Benchmark æœ€ç»ˆæŠ¥å‘Š

**æ—¥å¿—æ–‡ä»¶**: `prepare_bench_20260125_155107.log`  
**æ—¶é—´**: 2026-01-26  
**æ€»è€—æ—¶**: 9.08 å°æ—¶ (32700ç§’)  
**GPU**: NVIDIA A100 80GB PCIe (CC 8.0, Ampere)

---

## ğŸ“Š æœ€ç»ˆç»“æœæ±‡æ€»

### æ€»ä½“æƒ…å†µ

| ä»»åŠ¡ | çŠ¶æ€ | æˆåŠŸæ•° | å¤±è´¥æ•° | è¯´æ˜ |
|------|------|--------|--------|------|
| Task 3: ç¦»çº¿ç²—è°ƒä¼˜ | â­ï¸ è·³è¿‡ | - | - | ä¹‹å‰å·²å®Œæˆ |
| Task 4: ç¦»çº¿ç»†è°ƒä¼˜ | âœ… æˆåŠŸ | 16 | 0 | FP8 è¢«æ­£ç¡®è·³è¿‡ |
| Task 5: ç®€å• Benchmark | âœ… æˆåŠŸ | 2 | 0 | INT8 é€šè¿‡ï¼ŒFP8 è·³è¿‡ |
| Task 6: Prefill Benchmark | âš ï¸ éƒ¨åˆ†å¤±è´¥ | 155 | 5 | Qwen2.5-7B M=65536 å¤±è´¥ |
| Task 7: Decode Benchmark | âœ… æˆåŠŸ | 80 | 0 | å…¨éƒ¨é€šè¿‡ |

---

## 1. è°ƒä¼˜ç»“æœ (Task 4)

### Triton Kernel è°ƒä¼˜æ–‡ä»¶ âœ…

æ‰€æœ‰ 12 ä¸ªè°ƒä¼˜æ–‡ä»¶å·²ç”Ÿæˆä¸”æ­£å¸¸ï¼š

| Kernel | æ–‡ä»¶ | å¤§å° |
|--------|------|------|
| quant_only | `quant_only_tuned_Llama3.2-1B.py` | 6,696 bytes |
| quant_only | `quant_only_tuned_Llama3.2-3B.py` | 6,640 bytes |
| quant_only | `quant_only_tuned_Qwen2.5-7B.py` | 6,643 bytes |
| quant_only | `quant_only_tuned_Qwen2.5-14B.py` | 6,284 bytes |
| quant_slide | `quant_slide_tuned_Llama3.2-1B.py` | 12,640 bytes |
| quant_slide | `quant_slide_tuned_Llama3.2-3B.py` | 12,361 bytes |
| quant_slide | `quant_slide_tuned_Qwen2.5-7B.py` | 11,730 bytes |
| quant_slide | `quant_slide_tuned_Qwen2.5-14B.py` | 11,845 bytes |
| dequant_bias | `dequant_bias_tuned_Llama3.2-1B.py` | 4,641 bytes |
| dequant_bias | `dequant_bias_tuned_Llama3.2-3B.py` | 4,634 bytes |
| dequant_bias | `dequant_bias_tuned_Qwen2.5-7B.py` | 4,469 bytes |
| dequant_bias | `dequant_bias_tuned_Qwen2.5-14B.py` | 4,701 bytes |

**è·¯å¾„**: `/root/vllmbench/slidesparse/csrc/*/build/A100_cc80_py312_cu129_x86_64/`

---

## 2. Prefill Benchmark ç»“æœ (Task 6)

### ç»“æœç»Ÿè®¡

**é…ç½®**:
- M åˆ—è¡¨: `[512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]`
- æ¨¡å‹: `Llama3.2-1B-INT8`, `Llama3.2-3B-INT8`, `Qwen2.5-7B-INT8`, `Qwen2.5-14B-INT8`
- Backend: `cuBLASLt`, `cuSPARSELt (2:4, 2:6, 2:8, 2:10)`

| Backend | Llama3.2-1B | Llama3.2-3B | Qwen2.5-7B | Qwen2.5-14B | Total |
|---------|-------------|-------------|------------|-------------|-------|
| cuBLASLt | 8/8 âœ… | 8/8 âœ… | **7/8** âš ï¸ | 8/8 âœ… | 31/32 |
| cuSPARSELt 2:4 | 8/8 âœ… | 8/8 âœ… | **7/8** âš ï¸ | 8/8 âœ… | 31/32 |
| cuSPARSELt 2:6 | 8/8 âœ… | 8/8 âœ… | **7/8** âš ï¸ | 8/8 âœ… | 31/32 |
| cuSPARSELt 2:8 | 8/8 âœ… | 8/8 âœ… | **7/8** âš ï¸ | 8/8 âœ… | 31/32 |
| cuSPARSELt 2:10 | 8/8 âœ… | 8/8 âœ… | **7/8** âš ï¸ | 8/8 âœ… | 31/32 |
| **Total** | 40/40 | 40/40 | **35/40** | 40/40 | **155/160** |

### âŒ å¤±è´¥çš„æµ‹è¯• (5ä¸ª)

| # | æ¨¡å‹ | M å€¼ | Backend | é”™è¯¯ç±»å‹ |
|---|------|------|---------|----------|
| 1 | Qwen2.5-7B-INT8 | 65536 | cuBLASLt | CUDA illegal memory access |
| 2 | Qwen2.5-7B-INT8 | 65536 | cuSPARSELt (2:4) | CUDA illegal memory access |
| 3 | Qwen2.5-7B-INT8 | 65536 | cuSPARSELt (2:6) | Triton CUDA illegal memory access |
| 4 | Qwen2.5-7B-INT8 | 65536 | cuSPARSELt (2:8) | Triton CUDA illegal memory access |
| 5 | Qwen2.5-7B-INT8 | 65536 | cuSPARSELt (2:10) | Triton CUDA illegal memory access |

### ç»“æœæ–‡ä»¶ä½ç½®

- **JSON**: `throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/{backend}/json/`
- **CSV**: `throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/{backend}/`

âš ï¸ **æ³¨æ„**: `Qwen2.5-7B-INT8_prefill.csv` è¢«å¤±è´¥çš„æµ‹è¯•è¦†ç›–ï¼Œåªæœ‰å¤±è´¥è®°å½•ã€‚å®Œæ•´æ•°æ®ä¿å­˜åœ¨ JSON æ–‡ä»¶ä¸­ã€‚

---

## 3. Decode Benchmark ç»“æœ (Task 7)

### ç»“æœç»Ÿè®¡ âœ… å…¨éƒ¨é€šè¿‡

**é…ç½®**:
- M åˆ—è¡¨: `[64, 128, 256, 512]`
- æ¨¡å‹: `Llama3.2-1B-INT8`, `Llama3.2-3B-INT8`, `Qwen2.5-7B-INT8`, `Qwen2.5-14B-INT8`
- Backend: `cuBLASLt`, `cuSPARSELt (2:4, 2:6, 2:8, 2:10)`

| Backend | Llama3.2-1B | Llama3.2-3B | Qwen2.5-7B | Qwen2.5-14B | Total |
|---------|-------------|-------------|------------|-------------|-------|
| cuBLASLt | 4/4 âœ… | 4/4 âœ… | 4/4 âœ… | 4/4 âœ… | 16/16 |
| cuSPARSELt 2:4 | 4/4 âœ… | 4/4 âœ… | 4/4 âœ… | 4/4 âœ… | 16/16 |
| cuSPARSELt 2:6 | 4/4 âœ… | 4/4 âœ… | 4/4 âœ… | 4/4 âœ… | 16/16 |
| cuSPARSELt 2:8 | 4/4 âœ… | 4/4 âœ… | 4/4 âœ… | 4/4 âœ… | 16/16 |
| cuSPARSELt 2:10 | 4/4 âœ… | 4/4 âœ… | 4/4 âœ… | 4/4 âœ… | 16/16 |
| **Total** | 20/20 | 20/20 | 20/20 | 20/20 | **80/80** |

---

## 4. é”™è¯¯æ ¹æœ¬åŸå› åˆ†æ

### ğŸ” å…³é”®å‘ç°

1. **æˆ‘ä»¬çš„ Triton kernel æœ¬èº«å®Œå…¨æ²¡æœ‰é—®é¢˜**
   - `quant_only_int8` ç›´æ¥è°ƒç”¨æµ‹è¯•ï¼šM=65536 å…¨éƒ¨é€šè¿‡
   - `quant_slide_int8` ç›´æ¥è°ƒç”¨æµ‹è¯•ï¼šM=65536 å…¨éƒ¨é€šè¿‡
   - `dequant_bias` å•ç‹¬æµ‹è¯•ä¹Ÿé€šè¿‡

2. **é—®é¢˜å‡ºåœ¨ PyTorch Inductor ç”Ÿæˆçš„èåˆ kernel ä¸­**
   - é”™è¯¯å‘ç”Ÿåœ¨ `triton_poi_fused_mul_quant_only_int8_silu_slice_1`
   - è¿™æ˜¯ Inductor è¯•å›¾èåˆ `mul + silu + slice + quant_only_int8` ç”Ÿæˆçš„ kernel
   - é”™è¯¯å‘ç”Ÿåœ¨ Inductor çš„ **autotune é˜¶æ®µ**

3. **åªæœ‰ Qwen2.5-7B åœ¨ M=65536 å¤±è´¥**
   - å…¶ä»–æ¨¡å‹ (Llama3.2-1B, Llama3.2-3B, Qwen2.5-14B) åœ¨ M=65536 éƒ½é€šè¿‡
   - è¿™ä¸ Qwen2.5-7B çš„ `intermediate_size=18944` æœ‰å…³

### ğŸ¯ æ ¹æœ¬åŸå› 

é—®é¢˜æœ€å¯èƒ½å‘ç”Ÿåœ¨ PyTorch Inductor çš„ `triton_heuristics.pointwise` autotuning ä¸­ã€‚

å½“ `xnumel = M Ã— K = 65536 Ã— 18944 = 1,241,513,984` æ—¶ï¼ŒæŸäº› autotune é…ç½®å¯èƒ½åœ¨ A100 çš„ sm_80 æ¶æ„ä¸Šäº§ç”Ÿè¶Šç•Œè®¿é—®ã€‚è¿™ä¸ INT32 ç´¢å¼•æº¢å‡ºæœ‰å…³ï¼ˆä½†ä¸æ˜¯æˆ‘ä»¬ä»£ç çš„é—®é¢˜ï¼‰ã€‚

**æŠ€æœ¯ç»†èŠ‚**:
- Qwen2.5-7B: `intermediate_size=18944`, `hidden_size=3584`
- M=65536 Ã— N=18944 Ã— 2 (gate_up_proj) = 2,483,027,968 > INT32_MAX (2,147,483,647)

---

## 5. A100 FP8 æ”¯æŒè¯´æ˜

- A100 æ˜¯ Ampere æ¶æ„ (Compute Capability 8.0)ï¼Œ**ä¸æ”¯æŒåŸç”Ÿ FP8**
- FP8 éœ€è¦ Ada Lovelace (CC 8.9+) æˆ– Hopper (CC 9.0+)
- æ‰€æœ‰ FP8 ç›¸å…³æµ‹è¯•è¢«æ­£ç¡®è·³è¿‡ï¼Œæ²¡æœ‰å´©æºƒ
- è­¦å‘Šä¿¡æ¯: `[WARNING] GPU A100 (cc80) ä¸æ”¯æŒåŸç”Ÿ FP8ï¼Œè·³è¿‡...`

---

## 6. å»ºè®®

### çŸ­æœŸæ–¹æ¡ˆ (æ¨è)
1. **ä» benchmark M åˆ—è¡¨ä¸­ç§»é™¤ 65536** - æœ€ç®€å•æœ‰æ•ˆ
   - M=65536 æ˜¯æç«¯è¾¹ç•Œç”¨ä¾‹ (65536 tokens = çº¦ 50,000 å­— prompt)
   - å®é™…ç”Ÿäº§ä¸­å¾ˆå°‘é‡åˆ°
   
2. æˆ–è®¾ç½®ç¯å¢ƒå˜é‡: `TORCHINDUCTOR_MAX_AUTOTUNE=0`

### é•¿æœŸæ–¹æ¡ˆ
1. å‘ PyTorch å›¢é˜ŸæŠ¥å‘Šæ­¤ Inductor bug
2. è°ƒæŸ¥ Inductor åœ¨ sm_80 ä¸Šç”Ÿæˆçš„ç‰¹å®š kernel é…ç½®
3. ç­‰å¾… PyTorch æ›´æ–°ä¿®å¤

---

## 7. æ–‡ä»¶æ¸…å•

### è°ƒä¼˜ç»“æœ
```
slidesparse/csrc/quant_only_triton/build/A100_cc80_py312_cu129_x86_64/
  â”œâ”€â”€ quant_only_tuned_Llama3.2-1B.py
  â”œâ”€â”€ quant_only_tuned_Llama3.2-3B.py
  â”œâ”€â”€ quant_only_tuned_Qwen2.5-7B.py
  â””â”€â”€ quant_only_tuned_Qwen2.5-14B.py

slidesparse/csrc/fused_quant_slide_triton/build/A100_cc80_py312_cu129_x86_64/
  â”œâ”€â”€ quant_slide_tuned_Llama3.2-1B.py
  â”œâ”€â”€ quant_slide_tuned_Llama3.2-3B.py
  â”œâ”€â”€ quant_slide_tuned_Qwen2.5-7B.py
  â””â”€â”€ quant_slide_tuned_Qwen2.5-14B.py

slidesparse/csrc/fused_dequant_bias_triton/build/A100_cc80_py312_cu129_x86_64/
  â”œâ”€â”€ dequant_bias_tuned_Llama3.2-1B.py
  â”œâ”€â”€ dequant_bias_tuned_Llama3.2-3B.py
  â”œâ”€â”€ dequant_bias_tuned_Qwen2.5-7B.py
  â””â”€â”€ dequant_bias_tuned_Qwen2.5-14B.py
```

### Benchmark ç»“æœ
```
slidesparse/tools/throughput_benchmark_results/
  â”œâ”€â”€ prefill/A100_cc80_INT8_py312_cu129_x86_64/
  â”‚   â”œâ”€â”€ cublaslt/         (31 JSON, 4 CSV)
  â”‚   â”œâ”€â”€ cusparselt/2_4/   (31 JSON, 4 CSV)
  â”‚   â”œâ”€â”€ cusparselt/2_6/   (31 JSON, 4 CSV)
  â”‚   â”œâ”€â”€ cusparselt/2_8/   (31 JSON, 4 CSV)
  â”‚   â””â”€â”€ cusparselt/2_10/  (31 JSON, 4 CSV)
  â””â”€â”€ decode/A100_cc80_INT8_py312_cu129_x86_64/
      â”œâ”€â”€ cublaslt/         (16 JSON, 4 CSV)
      â”œâ”€â”€ cusparselt/2_4/   (16 JSON, 4 CSV)
      â”œâ”€â”€ cusparselt/2_6/   (16 JSON, 4 CSV)
      â”œâ”€â”€ cusparselt/2_8/   (16 JSON, 4 CSV)
      â””â”€â”€ cusparselt/2_10/  (16 JSON, 4 CSV)
```

---

## 8. é‡è·‘å¤±è´¥æµ‹è¯•çš„å‘½ä»¤ (å¦‚éœ€è¦)

```bash
# ä»…é‡è·‘å¤±è´¥çš„æµ‹è¯•
python throughput_benchmark.py \
  --model qwen2.5-7b-int8 \
  --backend cublaslt,cusparselt \
  --stage prefill \
  --sparsity 2_4,2_6,2_8,2_10 \
  --M 65536
```

âš ï¸ **æ³¨æ„**: æ­¤å‘½ä»¤ä¼šå†æ¬¡å¤±è´¥ï¼Œé™¤éä¿®æ”¹ PyTorch Inductor æˆ–è·³è¿‡ M=65536ã€‚

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2026-01-26
