A100 SlideSparse Benchmark é‡è¯•æ—¥å¿—
å¼€å§‹æ—¶é—´: 2026-01-26 02:26:57
æ€»æµ‹è¯•æ•°: 5
æœ€å¤§é‡è¯•: 3
GPU ID: 0

======================================================================

======================================================================
Test: qwen2.5-7b-int8 | cublaslt | prefill | M=[65536]
Attempt: 2
Time: 2026-01-26 02:37:08
Duration: 83.1s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cublaslt --M 65536 --gpu-mem 0.8 --gpu-id 0

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023714.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuBLASLt [INT32 output]                         â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:38:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=747501)[0;0m ERROR 01-26 02:38:27 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:38:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:38:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:38:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:38:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:38:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:38:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:38:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:38:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:38:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:38:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:38:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:38:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:38:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:38:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:38:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:38:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=747501)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=747501)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=747501)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=747501)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=747501)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=747501)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=747501)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=747501)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=747501)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=747501)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=747501)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747501)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=747501)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=747501)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=747501)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=747501)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747501)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=747501)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=747501)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=747501)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=747501)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=747501)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=747501)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=747501)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=747501)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=747501)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:38:28.928930220 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023714.log


======================================================================
Test: qwen2.5-7b-int8 | cublaslt | prefill | M=[65536]
Attempt: 3
Time: 2026-01-26 02:38:43
Duration: 83.1s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cublaslt --M 65536 --gpu-mem 0.8 --gpu-id 0

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cublaslt']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023848.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuBLASLt | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuBLASLt [INT32 output]                         â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:39:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=748177)[0;0m ERROR 01-26 02:40:02 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:39:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:39:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:39:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:39:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:39:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:39:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:39:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:39:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:39:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:40:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:40:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:40:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:40:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:40:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:40:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:40:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:40:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:40:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=748177)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=748177)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=748177)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=748177)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=748177)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=748177)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=748177)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=748177)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=748177)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=748177)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=748177)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=748177)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=748177)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=748177)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=748177)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=748177)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=748177)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=748177)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=748177)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=748177)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=748177)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=748177)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=748177)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=748177)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=748177)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=748177)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:40:03.684754998 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_023848.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_4) | prefill | M=[65536]
Attempt: 1
Time: 2026-01-26 02:40:17
Duration: 84.6s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.8 --gpu-id 0 --sparsity 2_4

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_4']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024023.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_4) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:4)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:41:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=748851)[0;0m ERROR 01-26 02:41:38 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:41:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:41:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:41:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:41:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:41:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:41:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:41:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:41:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:41:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:41:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:41:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:41:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:41:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:41:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:41:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:41:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:41:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=748851)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=748851)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=748851)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=748851)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=748851)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=748851)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=748851)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=748851)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=748851)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=748851)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=748851)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=748851)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=748851)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=748851)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=748851)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=748851)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=748851)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=748851)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=748851)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=748851)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=748851)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=748851)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=748851)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=748851)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=748851)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=748851)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:41:39.703311777 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024023.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_4) | prefill | M=[65536]
Attempt: 2
Time: 2026-01-26 02:41:54
Duration: 85.2s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.8 --gpu-id 0 --sparsity 2_4

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_4']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024159.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_4) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:4)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:43:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=749538)[0;0m ERROR 01-26 02:43:15 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:43:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:43:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:43:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:43:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:43:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:43:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:43:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:43:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:43:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:43:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:43:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:43:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:43:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:43:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:43:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:43:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:43:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=749538)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=749538)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=749538)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=749538)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=749538)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=749538)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=749538)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=749538)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=749538)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=749538)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=749538)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=749538)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=749538)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=749538)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=749538)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=749538)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=749538)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=749538)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=749538)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=749538)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=749538)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=749538)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=749538)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=749538)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=749538)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=749538)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:43:16.571180965 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024159.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_4) | prefill | M=[65536]
Attempt: 3
Time: 2026-01-26 02:43:30
Duration: 81.4s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.8 --gpu-id 0 --sparsity 2_4

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_4']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024336.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_4) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:4)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:44:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=750201)[0;0m ERROR 01-26 02:44:48 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:44:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:44:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:44:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:44:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:44:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:44:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:44:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:44:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:44:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:44:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:44:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:44:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:44:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:44:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:44:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:44:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:44:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=750201)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=750201)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=750201)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=750201)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=750201)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=750201)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=750201)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=750201)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=750201)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=750201)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=750201)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=750201)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=750201)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=750201)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=750201)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=750201)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=750201)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=750201)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=750201)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=750201)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=750201)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=750201)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=750201)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=750201)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=750201)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=750201)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:44:49.582566904 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024336.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_6) | prefill | M=[65536]
Attempt: 1
Time: 2026-01-26 02:45:03
Duration: 83.0s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.8 --gpu-id 0 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024508.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:46:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=750874)[0;0m ERROR 01-26 02:46:23 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:46:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:46:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:46:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:46:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:46:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:46:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:46:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:46:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:46:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:46:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:46:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:46:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:46:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:46:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:46:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:46:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:46:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=750874)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=750874)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=750874)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=750874)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=750874)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=750874)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=750874)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=750874)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=750874)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=750874)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=750874)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=750874)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=750874)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=750874)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=750874)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=750874)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=750874)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=750874)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=750874)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=750874)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=750874)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=750874)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=750874)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=750874)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=750874)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=750874)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:46:23.094172724 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024508.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_6) | prefill | M=[65536]
Attempt: 2
Time: 2026-01-26 02:46:38
Duration: 82.0s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.8 --gpu-id 0 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024643.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:47:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=751537)[0;0m ERROR 01-26 02:47:56 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:47:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:47:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:47:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:47:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:47:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:47:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:47:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:47:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:47:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:47:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:47:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:47:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:47:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:47:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:47:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:47:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:47:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=751537)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=751537)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=751537)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=751537)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=751537)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=751537)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=751537)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=751537)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=751537)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=751537)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=751537)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=751537)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=751537)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=751537)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=751537)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=751537)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=751537)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=751537)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=751537)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=751537)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=751537)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=751537)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=751537)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=751537)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=751537)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=751537)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:47:57.583646826 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024643.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_6) | prefill | M=[65536]
Attempt: 3
Time: 2026-01-26 02:48:11
Duration: 83.2s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.8 --gpu-id 0 --sparsity 2_6

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_6']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024816.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:6)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:49:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=752209)[0;0m ERROR 01-26 02:49:31 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:49:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:49:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:49:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:49:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:49:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:49:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:49:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:49:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:49:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:49:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:49:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:49:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:49:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:49:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=752209)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=752209)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=752209)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=752209)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=752209)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=752209)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=752209)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=752209)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=752209)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=752209)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=752209)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=752209)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=752209)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=752209)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=752209)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=752209)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=752209)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=752209)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=752209)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=752209)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=752209)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=752209)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=752209)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=752209)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=752209)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=752209)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:49:31.274889652 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024816.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | prefill | M=[65536]
Attempt: 1
Time: 2026-01-26 02:49:46
Duration: 84.7s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.8 --gpu-id 0 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024951.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:50:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=752887)[0;0m ERROR 01-26 02:51:07 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:50:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:50:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:50:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:50:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:50:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:50:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:50:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:50:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:50:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:51:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:51:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:51:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:51:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:51:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:51:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:51:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:51:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=752887)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=752887)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=752887)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=752887)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=752887)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=752887)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=752887)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=752887)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=752887)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=752887)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=752887)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=752887)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=752887)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=752887)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=752887)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=752887)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=752887)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=752887)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=752887)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=752887)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=752887)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=752887)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=752887)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=752887)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=752887)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=752887)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:51:07.403609191 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_024951.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | prefill | M=[65536]
Attempt: 2
Time: 2026-01-26 02:51:22
Duration: 82.6s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.8 --gpu-id 0 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025127.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:52:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=753559)[0;0m ERROR 01-26 02:52:41 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:52:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:52:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:52:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:52:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:52:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:52:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:52:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:52:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:52:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:52:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:52:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:52:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:52:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:52:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:52:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:52:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:52:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=753559)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=753559)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=753559)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=753559)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=753559)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=753559)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=753559)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=753559)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=753559)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=753559)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=753559)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=753559)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=753559)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=753559)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=753559)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=753559)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=753559)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=753559)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=753559)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=753559)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=753559)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=753559)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=753559)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=753559)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=753559)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=753559)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:52:42.521461913 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025127.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_8) | prefill | M=[65536]
Attempt: 3
Time: 2026-01-26 02:52:56
Duration: 84.2s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.8 --gpu-id 0 --sparsity 2_8

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_8']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025301.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:8)                                â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:54:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=754239)[0;0m ERROR 01-26 02:54:17 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:54:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:54:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:54:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:54:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:54:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:54:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:54:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:54:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:54:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:54:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:54:16] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:54:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:54:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:54:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:54:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:54:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:54:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=754239)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=754239)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=754239)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=754239)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=754239)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=754239)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=754239)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=754239)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=754239)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=754239)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=754239)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=754239)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=754239)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=754239)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=754239)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=754239)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=754239)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=754239)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=754239)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=754239)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=754239)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=754239)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=754239)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=754239)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=754239)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=754239)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:54:17.174476149 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025301.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | prefill | M=[65536]
Attempt: 1
Time: 2026-01-26 02:54:32
Duration: 83.6s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.8 --gpu-id 0 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025437.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:55:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=754910)[0;0m ERROR 01-26 02:55:52 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:55:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:55:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:55:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:55:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:55:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:55:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:55:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:55:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:55:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:55:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:55:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:55:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:55:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:55:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:55:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:55:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:55:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:55:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:55:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:55:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:55:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:55:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:55:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:55:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:55:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:55:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:55:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:55:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=754910)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=754910)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=754910)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=754910)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=754910)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=754910)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=754910)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=754910)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=754910)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=754910)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=754910)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=754910)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=754910)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=754910)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=754910)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=754910)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=754910)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=754910)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=754910)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=754910)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=754910)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=754910)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=754910)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=754910)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=754910)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=754910)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:55:52.311283211 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025437.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | prefill | M=[65536]
Attempt: 2
Time: 2026-01-26 02:56:07
Duration: 85.4s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.8 --gpu-id 0 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025612.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:57:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=755590)[0;0m ERROR 01-26 02:57:29 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:57:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:57:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:57:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:57:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:57:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:57:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:57:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:57:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:57:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:57:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:57:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:57:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:57:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:57:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:57:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:57:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:57:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:57:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:57:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:57:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:57:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:57:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:57:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:57:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:57:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:57:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:57:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:57:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=755590)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=755590)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=755590)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=755590)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=755590)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=755590)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=755590)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=755590)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=755590)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=755590)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=755590)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=755590)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=755590)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=755590)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=755590)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=755590)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=755590)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=755590)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=755590)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=755590)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=755590)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=755590)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=755590)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=755590)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=755590)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=755590)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:57:29.260948943 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025612.log


======================================================================
Test: qwen2.5-7b-int8 | cusparselt (2_10) | prefill | M=[65536]
Attempt: 3
Time: 2026-01-26 02:57:44
Duration: 83.4s
Exit Code: 1
Command: python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --stage prefill --backend cusparselt --M 65536 --gpu-mem 0.8 --gpu-id 0 --sparsity 2_10

STDOUT:

[0;36m============================================================[0m
[0;36m  SlideSparse vLLM Throughput Benchmark[0m
[0;36m============================================================[0m


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hardware Information                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU:              NVIDIA A100 80GB PCIe                     â”‚â”‚
â”‚ GPU (short):      A100                                      â”‚
â”‚ Memory:           79.3 GB                                    â”‚
â”‚ CC:               cc80 (Ampere)                              â”‚â”‚
â”‚ SM Code:          sm_80                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Runtime:     12.9                                      â”‚
â”‚ CUDA Driver:      13.0                                      â”‚
â”‚ Driver:           580.95.05                                 â”‚
â”‚ PyTorch:          2.9.0+cu129                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Triton:           âœ“ supported                               â”‚â”‚
â”‚ FP8 Support:      âœ—                                         â”‚
â”‚ INT8 Support:     âœ“                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æµ‹è¯•é…ç½®:
  æ¨¡å‹:             ['qwen2.5-7b-int8']
  Backends:         ['cusparselt']
  Sparsities:       ['2_10']
  Stages:           ['prefill']
  M_prefill:        [65536]
  M_decode:         [65536]
  GPU å†…å­˜åˆ©ç”¨ç‡:   0.8

è¾“å‡ºç›®å½•ç»“æ„:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[0;34m[INFO][0m æ—¥å¿—æ–‡ä»¶: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025749.log


[0;36m============================================================[0m
[0;36m  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill[0m
[0;36m============================================================[0m

[0;34m[INFO][0m Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[0;34m[INFO][0m Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/1] æµ‹è¯• M=65536
============================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµ‹è¯•å‚æ•°                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹:     Qwen2.5-7B-INT8                                 â”‚
â”‚ Backend:  cuSPARSELt (2:10)                               â”‚
â”‚ é˜¶æ®µ:     prefill                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GEMM M ç»´åº¦ (ç²¾ç¡®æ§åˆ¶):
â”‚   ç›®æ ‡ M        = 65536
â”‚   M_prefill     = 65536 (= 64 x 1024)
â”‚   M_decode      = 64
â”‚   batched_tokens = 65536 (æ§åˆ¶ M çš„å…³é”®å‚æ•°)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vLLM å‚æ•°:
â”‚   --input-len              = 1024
â”‚   --output-len             = 1
â”‚   --num-prompts            = 8192
â”‚   --max-num-seqs           = 64
â”‚   --max-model-len          = 1025
â”‚   --max-num-batched-tokens = 65536
â”‚   --no-enable-chunked-prefill
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿­ä»£æ¬¡æ•°:
â”‚   N_prefill = 128
â”‚   N_decode  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[0;34m[INFO][0m å¼€å§‹æµ‹è¯•...

â”€â”€â”€ STDOUT â”€â”€â”€
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:58:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=756267)[0;0m ERROR 01-26 02:59:04 [core.py:866] ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.


â”€â”€â”€ STDERR â”€â”€â”€
[2026-01-26 02:58:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:58:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:58:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:58:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:58:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:58:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:58:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:58:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:58:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:58:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:58:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:58:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:58:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:58:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:59:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:59:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:59:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:59:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:59:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:59:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:59:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:59:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:59:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[0;36m(EngineCore_DP0 pid=756267)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=756267)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=756267)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=756267)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=756267)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=756267)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=756267)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=756267)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=756267)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=756267)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=756267)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=756267)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=756267)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=756267)[0;0m   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=756267)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=756267)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=756267)[0;0m   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=756267)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=756267)[0;0m   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[0;36m(EngineCore_DP0 pid=756267)[0;0m     self.driver_worker.init_device()
[0;36m(EngineCore_DP0 pid=756267)[0;0m   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
[0;36m(EngineCore_DP0 pid=756267)[0;0m     self.worker.init_device()  # type: ignore
[0;36m(EngineCore_DP0 pid=756267)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=756267)[0;0m   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[0;36m(EngineCore_DP0 pid=756267)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=756267)[0;0m ValueError: Free memory on device (11.35/79.25 GiB) on startup is less than desired GPU memory utilization (0.8, 63.4 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 02:59:04.228062945 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[0;31m[ERROR][0m æµ‹è¯•å¤±è´¥: M=65536 (exit code: 1)


[0;35m------------------------------------------------------------[0m
[0;35m  ç”Ÿæˆ CSV: Qwen2.5-7B-INT8[0m
[0;35m------------------------------------------------------------[0m
[0;32m[SUCCESS][0m CSV ä¿å­˜åˆ°: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

é¢„è§ˆ:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[0;34m[INFO][0m å®Œæˆ: 0 æˆåŠŸ, 1 å¤±è´¥


[0;36m============================================================[0m
[0;36m  Benchmark å®Œæˆ![0m
[0;36m============================================================[0m


æ€»è®¡: [0;32m0 æˆåŠŸ[0m, [0;31m1 å¤±è´¥[0m
============================================================

[0;34m[INFO][0m æ—¥å¿—å·²ä¿å­˜: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260126_025749.log


======================================================================
æµ‹è¯•æ€»ç»“
======================================================================
ç»“æŸæ—¶é—´: 2026-01-26 02:59:09
æˆåŠŸ: 0
å¤±è´¥: 5
è·³è¿‡: 0

è¯¦ç»†ç»“æœ:
  âŒ å¤±è´¥ qwen2.5-7b-int8 | cublaslt | M=[65536]
  âŒ å¤±è´¥ qwen2.5-7b-int8 | cusparselt (2_4) | M=[65536]
  âŒ å¤±è´¥ qwen2.5-7b-int8 | cusparselt (2_6) | M=[65536]
  âŒ å¤±è´¥ qwen2.5-7b-int8 | cusparselt (2_8) | M=[65536]
  âŒ å¤±è´¥ qwen2.5-7b-int8 | cusparselt (2_10) | M=[65536]
