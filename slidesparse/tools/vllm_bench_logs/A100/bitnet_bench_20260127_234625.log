======================================================================
BitNet Benchmark Log
Started: 2026-01-27 23:46:25
======================================================================

Hardware:
  GPU: NVIDIA A100 80GB PCIe (cc80)
  Python: py312
  CUDA: cu129
  Arch: x86_64

[INFO] 日志文件: /root/vllmbench/slidesparse/tools/bitnet_bench_20260127_234625.log

======================================================================
TASK 1: 基础模型准备 (下载 + 量化)
Started: 2026-01-27 23:46:25
======================================================================


------------------------------------------------------------
  Step 1: 下载 BitNet BF16 模型
------------------------------------------------------------
[INFO] BitNet-2B-BF16 已存在，跳过下载

------------------------------------------------------------
  Step 2: 量化为 BitNet-2B-INT8
------------------------------------------------------------
[INFO] BitNet-2B-INT8 已存在，跳过量化

------------------------------------------------------------
  Step 2: 量化为 BitNet-2B-FP8
------------------------------------------------------------
[INFO] BitNet-2B-FP8 已存在，跳过量化

[INFO] 基础模型准备统计: 成功 2, 失败 0

------------------------------------------------------------
  验证基础模型
------------------------------------------------------------
[SUCCESS]   ✓ BitNet-2B-INT8
[SUCCESS]   ✓ BitNet-2B-FP8

----------------------------------------------------------------------
TASK 1: 基础模型准备 (下载 + 量化) - SUCCESS
Duration: 0.0 seconds (0.0 minutes)
----------------------------------------------------------------------


======================================================================
TASK 2: SlideSparse 转换 (prune + slide)
Started: 2026-01-27 23:46:25
======================================================================

[INFO] 跳过已存在: BitNet-2B-INT8-SlideSparse-2_4
[INFO] 跳过已存在: BitNet-2B-INT8-SlideSparse-2_6
[INFO] 跳过已存在: BitNet-2B-INT8-SlideSparse-2_8
[INFO] 跳过已存在: BitNet-2B-INT8-SlideSparse-2_10
[INFO] 跳过已存在: BitNet-2B-FP8-SlideSparse-2_4
[INFO] 跳过已存在: BitNet-2B-FP8-SlideSparse-2_6
[INFO] 跳过已存在: BitNet-2B-FP8-SlideSparse-2_8
[INFO] 跳过已存在: BitNet-2B-FP8-SlideSparse-2_10

[INFO] SlideSparse 转换统计: 成功 0, 跳过 8, 失败 0

------------------------------------------------------------
  验证 SlideSparse 模型
------------------------------------------------------------
[SUCCESS]   ✓ BitNet-2B-INT8-SlideSparse-2_4
[SUCCESS]   ✓ BitNet-2B-INT8-SlideSparse-2_6
[SUCCESS]   ✓ BitNet-2B-INT8-SlideSparse-2_8
[SUCCESS]   ✓ BitNet-2B-INT8-SlideSparse-2_10
[SUCCESS]   ✓ BitNet-2B-FP8-SlideSparse-2_4
[SUCCESS]   ✓ BitNet-2B-FP8-SlideSparse-2_6
[SUCCESS]   ✓ BitNet-2B-FP8-SlideSparse-2_8
[SUCCESS]   ✓ BitNet-2B-FP8-SlideSparse-2_10

----------------------------------------------------------------------
TASK 2: SlideSparse 转换 (prune + slide) - SUCCESS
Duration: 0.0 seconds (0.0 minutes)
----------------------------------------------------------------------


======================================================================
TASK 3: 离线调优 (粗调优 + 细调优)
Started: 2026-01-27 23:46:25
======================================================================


------------------------------------------------------------
  粗调优: cuBLASLt + Triton quant_only
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/offline_autotune_algsearch.py --model BitNet-2B --dtype all --m_list 256,1024,4096,16384,32768 --Lmax 10 --warmup 25 --repeat 50 --kernels 1,0,0,0,1


============================================================
  SlideSparse 统一离线调优
============================================================

  GPU:           NVIDIA A100 80GB PCIe (cc80)
  Python:        py312
  CUDA:          cu129
  Arch:          x86_64

  数据类型:      ['int8', 'fp8']
  输出类型:      bf16
  高精度累加:    否
  模型 (base):   ['BitNet-2B']
  Lmax:          10
  M-quick:       否
  M 列表:        [256, 1024, 4096, 16384, 32768]
  Warmup/Repeat: 25/50

  Kernel 调优:
    ✓ cuBLASLt GEMM
    ✗ cuSPARSELt GEMM
    ✗ Triton Dequant + Bias
    ✗ Triton Quant + Slide
    ✓ Triton Quant Only

============================================================
  Step 0: 编译 CUDA 扩展
============================================================


------------------------------------------------------------
  编译 cublaslt
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/cublaslt_gemm/build_cublaslt.py build --force
[SUCCESS] cublaslt 编译成功

------------------------------------------------------------
  编译 cusparselt
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/cusparselt_gemm/build_cusparselt.py build --force
[SUCCESS] cusparselt 编译成功

------------------------------------------------------------
  编译 compress
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/build_compress.py build --force
[SUCCESS] compress 编译成功

============================================================
  Step 1: cuBLASLt GEMM
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] dtype=int8, outdtype=int32
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/search/cuBLASLt_AlgSearch/alg_search.py --dtype int8 --outdtype int32 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --compile --Lmax 10 --m_list 256,1024,4096,16384,32768
[SUCCESS] cuBLASLt GEMM (int8) 完成
[WARNING] GPU A100 (cc80) 不支持原生 FP8，跳过 cuBLASLt GEMM FP8 调优

============================================================
  Step 2: cuSPARSELt GEMM [跳过]
============================================================


============================================================
  Step 3: Triton Dequant + Bias [跳过]
============================================================


============================================================
  Step 4: Triton Quant + Slide [跳过]
============================================================


============================================================
  Step 5: Triton Quant Only
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/quant_only_triton/autotune_autogen_quant_only.py --model BitNet-2B-INT8 --warmup 25 --repeat 50 --Lmax 10 --m_list 256,1024,4096,16384,32768
[SUCCESS] Triton Quant Only 完成

============================================================
  调优总结
============================================================

  cuBLASLt GEMM: [全部成功] (2/2)
  cuSPARSELt GEMM: [跳过]
  Triton Dequant + Bias: [跳过]
  Triton Quant + Slide: [跳过]
  Triton Quant Only: [全部成功] (1/1)

总计: 成功 3, 失败 0, 跳过 3
[SUCCESS] 粗调优完成 (215.5s)

------------------------------------------------------------
  细调优: cuSPARSELt + Triton Dequant/QuantSlide
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/offline_autotune_algsearch.py --model BitNet-2B --dtype all --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768 --Lmax 10 --warmup 25 --repeat 50 --kernels 0,1,1,1,0


============================================================
  SlideSparse 统一离线调优
============================================================

  GPU:           NVIDIA A100 80GB PCIe (cc80)
  Python:        py312
  CUDA:          cu129
  Arch:          x86_64

  数据类型:      ['int8', 'fp8']
  输出类型:      bf16
  高精度累加:    否
  模型 (base):   ['BitNet-2B']
  Lmax:          10
  M-quick:       否
  M 列表:        [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]
  Warmup/Repeat: 25/50

  Kernel 调优:
    ✗ cuBLASLt GEMM
    ✓ cuSPARSELt GEMM
    ✓ Triton Dequant + Bias
    ✓ Triton Quant + Slide
    ✗ Triton Quant Only

============================================================
  Step 0: 编译 CUDA 扩展
============================================================


------------------------------------------------------------
  编译 cublaslt
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/cublaslt_gemm/build_cublaslt.py build --force
[SUCCESS] cublaslt 编译成功

------------------------------------------------------------
  编译 cusparselt
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/cusparselt_gemm/build_cusparselt.py build --force
[SUCCESS] cusparselt 编译成功

------------------------------------------------------------
  编译 compress
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/build_compress.py build --force
[SUCCESS] compress 编译成功

============================================================
  Step 1: cuBLASLt GEMM [跳过]
============================================================


============================================================
  Step 2: cuSPARSELt GEMM
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] dtype=int8, outdtype=bf16
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/search/cuSPARSELt_AlgSearch/alg_search.py --dtype int8 --outdtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --compile --Lmax 10 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768
[SUCCESS] cuSPARSELt GEMM (int8) 完成
[WARNING] GPU A100 (cc80) 不支持原生 FP8，跳过 cuSPARSELt GEMM FP8 调优

============================================================
  Step 3: Triton Dequant + Bias
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/autotune_autogen_dequant_bias.py --model BitNet-2B-INT8 --warmup 25 --repeat 50 --Lmax 10 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768
[SUCCESS] Triton Dequant + Bias 完成

============================================================
  Step 4: Triton Quant + Slide
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/autotune_autogen_quant_slide.py --model BitNet-2B-INT8 --warmup 25 --repeat 50 --Lmax 10 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768
[SUCCESS] Triton Quant + Slide 完成

============================================================
  Step 5: Triton Quant Only [跳过]
============================================================


============================================================
  调优总结
============================================================

  cuBLASLt GEMM: [跳过]
  cuSPARSELt GEMM: [全部成功] (2/2)
  Triton Dequant + Bias: [全部成功] (1/1)
  Triton Quant + Slide: [全部成功] (1/1)
  Triton Quant Only: [跳过]

总计: 成功 4, 失败 0, 跳过 2
[SUCCESS] 细调优完成 (284.9s)

----------------------------------------------------------------------
TASK 3: 离线调优 (粗调优 + 细调优) - SUCCESS
Duration: 500.4 seconds (8.3 minutes)
----------------------------------------------------------------------


======================================================================
TASK 4: 完整 Prefill Benchmark
Started: 2026-01-27 23:54:46
======================================================================


------------------------------------------------------------
  Prefill Benchmark: bitnet1.58-2b-int8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-int8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA A100 80GB PCIe                     ││
│ GPU (short):      A100                                      │
│ Memory:           79.3 GB                                    │
│ CC:               cc80 (Ampere)                              ││
│ SM Code:          sm_80                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.95.05                                 │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✗                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-int8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['prefill']
  M_prefill:        [512, 1024, 2048, 4096, 8192, 16384, 32768]
  M_decode:         [512, 1024, 2048, 4096, 8192, 16384, 32768]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260127_235452.log


============================================================
  BitNet-2B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 23:55:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2205821) WARNING 01-27 23:55:17 [backends.py:609] Failed to read file <frozen os>
Throughput: 18.01 requests/s, 9239.13 total tokens/s, 18.01 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-27 23:55:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-27 23:55:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-27 23:55:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-27 23:55:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-27 23:55:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 23:55:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 23:55:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 23:55:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 23:55:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 23:55:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-27 23:55:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-27 23:55:07] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-27 23:55:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:07] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:07] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-27 23:55:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 23:55:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 23:55:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 23:55:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 23:55:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2205821) [2026-01-27 23:55:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2205821) [2026-01-27 23:55:08] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2205821) [2026-01-27 23:55:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2205821) [2026-01-27 23:55:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2205821) [2026-01-27 23:55:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2205821) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2205821) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.72it/s]
(EngineCore_DP0 pid=2205821) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=2205821) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2205821) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.58it/s]
(EngineCore_DP0 pid=2205821) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.27it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  51%|█████     | 65/128 [00:00<00:00, 644.00it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 658.21it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:53,  2.36it/s, est. speed input: 1209.22 toks/s, output: 2.36 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:19,  6.53it/s, est. speed input: 2841.71 toks/s, output: 5.55 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:12,  9.72it/s, est. speed input: 3931.82 toks/s, output: 7.68 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:09, 12.12it/s, est. speed input: 4710.68 toks/s, output: 9.20 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:08, 14.03it/s, est. speed input: 5319.44 toks/s, output: 10.39 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:01<00:07, 16.23it/s, est. speed input: 6051.46 toks/s, output: 11.82 toks/s]
Processed prompts:  11%|█         | 14/128 [00:01<00:06, 17.09it/s, est. speed input: 6412.79 toks/s, output: 12.52 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:06, 18.22it/s, est. speed input: 6879.04 toks/s, output: 13.44 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:05, 18.53it/s, est. speed input: 7109.91 toks/s, output: 13.89 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:05, 18.57it/s, est. speed input: 7287.28 toks/s, output: 14.23 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 18.87it/s, est. speed input: 7467.08 toks/s, output: 14.58 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:05, 19.49it/s, est. speed input: 7727.70 toks/s, output: 15.09 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 19.61it/s, est. speed input: 7863.77 toks/s, output: 15.36 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:04, 19.74it/s, est. speed input: 8044.12 toks/s, output: 15.71 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:04, 19.62it/s, est. speed input: 8134.91 toks/s, output: 15.89 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:04, 19.51it/s, est. speed input: 8216.63 toks/s, output: 16.05 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:04, 19.77it/s, est. speed input: 8352.68 toks/s, output: 16.31 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:04, 19.87it/s, est. speed input: 8469.23 toks/s, output: 16.54 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 19.77it/s, est. speed input: 8528.69 toks/s, output: 16.66 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 19.85it/s, est. speed input: 8622.74 toks/s, output: 16.84 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 19.45it/s, est. speed input: 8651.65 toks/s, output: 16.90 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:04, 19.09it/s, est. speed input: 8675.65 toks/s, output: 16.94 toks/s]
Processed prompts:  41%|████      | 52/128 [00:03<00:04, 18.95it/s, est. speed input: 8704.91 toks/s, output: 17.00 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:03, 18.78it/s, est. speed input: 8728.70 toks/s, output: 17.05 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:03, 18.93it/s, est. speed input: 8765.58 toks/s, output: 17.12 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:03, 19.38it/s, est. speed input: 8833.50 toks/s, output: 17.25 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 19.22it/s, est. speed input: 8857.18 toks/s, output: 17.30 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 19.42it/s, est. speed input: 8895.07 toks/s, output: 17.37 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 19.70it/s, est. speed input: 8951.88 toks/s, output: 17.48 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:03, 19.25it/s, est. speed input: 8960.49 toks/s, output: 17.50 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:03<00:03, 19.31it/s, est. speed input: 8986.66 toks/s, output: 17.55 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:02, 19.55it/s, est. speed input: 9031.54 toks/s, output: 17.64 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:02, 19.78it/s, est. speed input: 9077.58 toks/s, output: 17.73 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 19.91it/s, est. speed input: 9119.80 toks/s, output: 17.81 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 19.99it/s, est. speed input: 9158.73 toks/s, output: 17.89 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:04<00:02, 20.06it/s, est. speed input: 9196.05 toks/s, output: 17.96 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:04<00:02, 20.00it/s, est. speed input: 9226.18 toks/s, output: 18.02 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:04<00:01, 19.90it/s, est. speed input: 9242.61 toks/s, output: 18.05 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:01, 19.91it/s, est. speed input: 9261.43 toks/s, output: 18.09 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:01, 19.74it/s, est. speed input: 9273.70 toks/s, output: 18.11 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:01, 19.69it/s, est. speed input: 9287.82 toks/s, output: 18.14 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 19.88it/s, est. speed input: 9316.79 toks/s, output: 18.20 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:05<00:01, 19.73it/s, est. speed input: 9334.41 toks/s, output: 18.23 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:05<00:01, 19.74it/s, est. speed input: 9348.55 toks/s, output: 18.26 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:05<00:01, 19.64it/s, est. speed input: 9358.52 toks/s, output: 18.28 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:05<00:01, 19.72it/s, est. speed input: 9372.90 toks/s, output: 18.31 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:00, 19.42it/s, est. speed input: 9376.45 toks/s, output: 18.31 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:06<00:00, 19.57it/s, est. speed input: 9390.18 toks/s, output: 18.34 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 19.73it/s, est. speed input: 9410.71 toks/s, output: 18.38 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:06<00:00, 19.67it/s, est. speed input: 9419.78 toks/s, output: 18.40 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:06<00:00, 19.83it/s, est. speed input: 9439.90 toks/s, output: 18.44 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:06<00:00, 19.76it/s, est. speed input: 9448.86 toks/s, output: 18.45 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:06<00:00, 19.89it/s, est. speed input: 9467.91 toks/s, output: 18.49 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 19.86it/s, est. speed input: 9482.62 toks/s, output: 18.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 19.86it/s, est. speed input: 9482.62 toks/s, output: 18.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 18.52it/s, est. speed input: 9482.62 toks/s, output: 18.52 toks/s]
[rank0]:[W127 23:55:37.380737536 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.7s

测试结果:
  Requests/s:   18.01
  Tokens/s:     9239.13
  Total Reqs:   128
  Elapsed:      7.11s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     9221.12

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 23:55:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2206901) WARNING 01-27 23:56:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 17.88 requests/s, 18325.34 total tokens/s, 17.88 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-27 23:55:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-27 23:55:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-27 23:55:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-27 23:55:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-27 23:55:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 23:55:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 23:55:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 23:55:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 23:55:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 23:55:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-27 23:55:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-27 23:55:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-27 23:55:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:55:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-27 23:55:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 23:55:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 23:55:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 23:55:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 23:55:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2206901) [2026-01-27 23:55:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2206901) [2026-01-27 23:55:56] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2206901) [2026-01-27 23:55:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2206901) [2026-01-27 23:55:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2206901) [2026-01-27 23:55:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2206901) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2206901) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.77it/s]
(EngineCore_DP0 pid=2206901) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.77it/s]
(EngineCore_DP0 pid=2206901) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2206901) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.65it/s]
(EngineCore_DP0 pid=2206901) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.93it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  28%|██▊       | 36/128 [00:00<00:00, 358.16it/s]
Adding requests:  57%|█████▋    | 73/128 [00:00<00:00, 364.06it/s]
Adding requests:  86%|████████▌ | 110/128 [00:00<00:00, 358.83it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 360.46it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 16.86it/s, est. speed input: 17270.40 toks/s, output: 16.86 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:06, 18.00it/s, est. speed input: 18245.23 toks/s, output: 17.82 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:06, 18.16it/s, est. speed input: 18425.82 toks/s, output: 17.99 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:06, 18.05it/s, est. speed input: 18400.47 toks/s, output: 17.97 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:06, 17.91it/s, est. speed input: 18335.69 toks/s, output: 17.91 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:06, 17.67it/s, est. speed input: 18216.35 toks/s, output: 17.79 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:06, 17.81it/s, est. speed input: 18261.51 toks/s, output: 17.83 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:06, 18.14it/s, est. speed input: 18389.89 toks/s, output: 17.96 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:05, 18.46it/s, est. speed input: 18522.36 toks/s, output: 18.09 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:05, 18.47it/s, est. speed input: 18563.70 toks/s, output: 18.13 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:05, 18.76it/s, est. speed input: 18678.69 toks/s, output: 18.24 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:05, 18.92it/s, est. speed input: 18764.53 toks/s, output: 18.32 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:05, 18.88it/s, est. speed input: 18801.04 toks/s, output: 18.36 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 18.70it/s, est. speed input: 18794.83 toks/s, output: 18.35 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 18.64it/s, est. speed input: 18806.09 toks/s, output: 18.37 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:05, 18.71it/s, est. speed input: 18838.19 toks/s, output: 18.40 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:05, 18.79it/s, est. speed input: 18872.28 toks/s, output: 18.43 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:04, 18.89it/s, est. speed input: 18910.12 toks/s, output: 18.47 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:04, 18.95it/s, est. speed input: 18942.20 toks/s, output: 18.50 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:04, 18.98it/s, est. speed input: 18970.50 toks/s, output: 18.53 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:04, 19.05it/s, est. speed input: 19002.79 toks/s, output: 18.56 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:04, 19.18it/s, est. speed input: 19044.31 toks/s, output: 18.60 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 18.76it/s, est. speed input: 19010.18 toks/s, output: 18.56 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 18.82it/s, est. speed input: 19026.15 toks/s, output: 18.58 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:04, 18.84it/s, est. speed input: 19038.47 toks/s, output: 18.59 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:03, 19.01it/s, est. speed input: 19070.13 toks/s, output: 18.62 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:02<00:03, 18.95it/s, est. speed input: 19076.82 toks/s, output: 18.63 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:03, 19.16it/s, est. speed input: 19112.50 toks/s, output: 18.66 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:03, 19.23it/s, est. speed input: 19137.61 toks/s, output: 18.69 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:03, 19.22it/s, est. speed input: 19154.17 toks/s, output: 18.71 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 19.31it/s, est. speed input: 19180.11 toks/s, output: 18.73 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:03, 19.32it/s, est. speed input: 19199.18 toks/s, output: 18.75 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 19.29it/s, est. speed input: 19213.45 toks/s, output: 18.76 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:03, 19.01it/s, est. speed input: 19201.67 toks/s, output: 18.75 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:03<00:03, 18.78it/s, est. speed input: 19186.94 toks/s, output: 18.74 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:03<00:03, 18.54it/s, est. speed input: 19165.55 toks/s, output: 18.72 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:03<00:02, 18.57it/s, est. speed input: 19162.92 toks/s, output: 18.71 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:02, 18.66it/s, est. speed input: 19167.43 toks/s, output: 18.72 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:02, 18.83it/s, est. speed input: 19180.84 toks/s, output: 18.73 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:02, 18.88it/s, est. speed input: 19187.36 toks/s, output: 18.74 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 19.00it/s, est. speed input: 19200.75 toks/s, output: 18.75 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:04<00:02, 19.08it/s, est. speed input: 19212.97 toks/s, output: 18.76 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:04<00:02, 18.61it/s, est. speed input: 19183.31 toks/s, output: 18.73 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:04<00:02, 18.59it/s, est. speed input: 19178.70 toks/s, output: 18.73 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:04<00:02, 18.80it/s, est. speed input: 19191.47 toks/s, output: 18.74 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:04<00:01, 19.02it/s, est. speed input: 19208.78 toks/s, output: 18.76 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:01, 19.13it/s, est. speed input: 19222.47 toks/s, output: 18.77 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:01, 19.16it/s, est. speed input: 19232.22 toks/s, output: 18.78 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 18.98it/s, est. speed input: 19227.39 toks/s, output: 18.78 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:05<00:01, 19.01it/s, est. speed input: 19233.82 toks/s, output: 18.78 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:05<00:01, 19.05it/s, est. speed input: 19241.00 toks/s, output: 18.79 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:05<00:01, 19.18it/s, est. speed input: 19254.07 toks/s, output: 18.80 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:05<00:01, 19.17it/s, est. speed input: 19260.49 toks/s, output: 18.81 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:05<00:01, 18.99it/s, est. speed input: 19256.41 toks/s, output: 18.81 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:05<00:00, 18.82it/s, est. speed input: 19249.40 toks/s, output: 18.80 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:05<00:00, 18.94it/s, est. speed input: 19256.78 toks/s, output: 18.81 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:06<00:00, 19.09it/s, est. speed input: 19268.32 toks/s, output: 18.82 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:06<00:00, 19.16it/s, est. speed input: 19276.99 toks/s, output: 18.83 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:06<00:00, 19.12it/s, est. speed input: 19280.20 toks/s, output: 18.83 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:06<00:00, 19.13it/s, est. speed input: 19286.03 toks/s, output: 18.83 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:06<00:00, 19.07it/s, est. speed input: 19287.32 toks/s, output: 18.84 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:06<00:00, 18.84it/s, est. speed input: 19278.98 toks/s, output: 18.83 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:06<00:00, 18.41it/s, est. speed input: 19255.28 toks/s, output: 18.80 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 18.74it/s, est. speed input: 19266.71 toks/s, output: 18.82 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 18.74it/s, est. speed input: 19266.71 toks/s, output: 18.82 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 18.81it/s, est. speed input: 19266.71 toks/s, output: 18.82 toks/s]
[rank0]:[W127 23:56:25.721100113 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.4s

测试结果:
  Requests/s:   17.88
  Tokens/s:     18325.34
  Total Reqs:   128
  Elapsed:      7.16s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     18307.46

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 23:56:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2207819) WARNING 01-27 23:56:53 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.35 requests/s, 37260.34 total tokens/s, 36.35 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-27 23:56:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-27 23:56:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-27 23:56:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-27 23:56:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:56:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:56:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:56:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:56:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:56:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-27 23:56:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 23:56:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 23:56:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 23:56:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 23:56:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 23:56:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-27 23:56:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-27 23:56:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-27 23:56:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:56:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:56:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:56:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:56:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:56:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-27 23:56:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 23:56:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 23:56:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 23:56:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 23:56:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2207819) [2026-01-27 23:56:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2207819) [2026-01-27 23:56:44] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2207819) [2026-01-27 23:56:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2207819) [2026-01-27 23:56:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2207819) [2026-01-27 23:56:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2207819) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2207819) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.79it/s]
(EngineCore_DP0 pid=2207819) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.79it/s]
(EngineCore_DP0 pid=2207819) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2207819) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  9.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.07it/s]
(EngineCore_DP0 pid=2207819) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  8.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.09it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 36/256 [00:00<00:00, 355.96it/s]
Adding requests:  29%|██▉       | 74/256 [00:00<00:00, 367.13it/s]
Adding requests:  43%|████▎     | 111/256 [00:00<00:00, 349.01it/s]
Adding requests:  57%|█████▋    | 147/256 [00:00<00:00, 350.32it/s]
Adding requests:  71%|███████▏  | 183/256 [00:00<00:00, 348.30it/s]
Adding requests:  86%|████████▌ | 220/256 [00:00<00:00, 353.63it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 359.96it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:00<00:01, 166.66it/s, est. speed input: 170681.86 toks/s, output: 166.67 toks/s]
Processed prompts:  16%|█▌        | 41/256 [00:00<00:03, 63.61it/s, est. speed input: 73070.21 toks/s, output: 71.36 toks/s]   
Processed prompts:  20%|█▉        | 51/256 [00:00<00:03, 53.05it/s, est. speed input: 62389.40 toks/s, output: 60.93 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:01<00:04, 46.82it/s, est. speed input: 56794.50 toks/s, output: 55.46 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:01<00:04, 44.66it/s, est. speed input: 54515.68 toks/s, output: 53.24 toks/s]
Processed prompts:  27%|██▋       | 69/256 [00:01<00:04, 45.31it/s, est. speed input: 54088.40 toks/s, output: 52.82 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:04, 40.86it/s, est. speed input: 51570.42 toks/s, output: 50.36 toks/s]
Processed prompts:  31%|███       | 79/256 [00:01<00:04, 42.34it/s, est. speed input: 51383.21 toks/s, output: 50.18 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:04, 39.10it/s, est. speed input: 49716.12 toks/s, output: 48.55 toks/s]
Processed prompts:  35%|███▍      | 89/256 [00:01<00:04, 41.04it/s, est. speed input: 49648.38 toks/s, output: 48.48 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:01<00:04, 38.08it/s, est. speed input: 48335.43 toks/s, output: 47.20 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 38.23it/s, est. speed input: 47906.72 toks/s, output: 46.78 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:04, 38.10it/s, est. speed input: 47460.42 toks/s, output: 46.35 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:03, 38.13it/s, est. speed input: 47080.55 toks/s, output: 45.98 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:03, 37.68it/s, est. speed input: 46643.93 toks/s, output: 45.55 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:03, 37.37it/s, est. speed input: 46248.20 toks/s, output: 45.16 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 36.97it/s, est. speed input: 45852.68 toks/s, output: 44.78 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:02<00:03, 36.33it/s, est. speed input: 45429.30 toks/s, output: 44.36 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:02<00:03, 36.38it/s, est. speed input: 45120.27 toks/s, output: 44.06 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:02<00:03, 35.80it/s, est. speed input: 44738.81 toks/s, output: 43.69 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 35.38it/s, est. speed input: 44382.35 toks/s, output: 43.34 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 36.30it/s, est. speed input: 44227.44 toks/s, output: 43.19 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:03, 36.93it/s, est. speed input: 44076.46 toks/s, output: 43.04 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:02, 37.62it/s, est. speed input: 43962.28 toks/s, output: 42.93 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:02, 37.66it/s, est. speed input: 43802.73 toks/s, output: 42.78 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 37.76it/s, est. speed input: 43660.16 toks/s, output: 42.64 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:03<00:02, 37.96it/s, est. speed input: 43538.96 toks/s, output: 42.52 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:03<00:02, 37.55it/s, est. speed input: 43367.06 toks/s, output: 42.35 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:03<00:02, 37.41it/s, est. speed input: 43219.03 toks/s, output: 42.21 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 37.46it/s, est. speed input: 43094.15 toks/s, output: 42.08 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 37.30it/s, est. speed input: 42956.17 toks/s, output: 41.95 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:02, 37.72it/s, est. speed input: 42876.05 toks/s, output: 41.87 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:01, 37.83it/s, est. speed input: 42783.34 toks/s, output: 41.78 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 37.43it/s, est. speed input: 42651.50 toks/s, output: 41.65 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 36.99it/s, est. speed input: 42510.75 toks/s, output: 41.51 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:04<00:01, 37.13it/s, est. speed input: 42416.38 toks/s, output: 41.42 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:04<00:01, 36.90it/s, est. speed input: 42297.66 toks/s, output: 41.31 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:04<00:01, 37.31it/s, est. speed input: 42232.08 toks/s, output: 41.24 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:05<00:01, 37.47it/s, est. speed input: 42158.87 toks/s, output: 41.17 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:05<00:01, 37.80it/s, est. speed input: 42105.52 toks/s, output: 41.12 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:05<00:01, 38.03it/s, est. speed input: 42053.38 toks/s, output: 41.07 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:05<00:01, 37.93it/s, est. speed input: 41984.81 toks/s, output: 41.00 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:05<00:00, 37.95it/s, est. speed input: 41924.72 toks/s, output: 40.94 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:05<00:00, 38.04it/s, est. speed input: 41873.03 toks/s, output: 40.89 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:05<00:00, 37.76it/s, est. speed input: 41799.20 toks/s, output: 40.82 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:05<00:00, 37.56it/s, est. speed input: 41727.31 toks/s, output: 40.75 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:05<00:00, 36.61it/s, est. speed input: 41602.86 toks/s, output: 40.63 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:05<00:00, 36.66it/s, est. speed input: 41530.62 toks/s, output: 40.56 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:06<00:00, 37.12it/s, est. speed input: 41489.55 toks/s, output: 40.52 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:06<00:00, 37.65it/s, est. speed input: 41462.88 toks/s, output: 40.49 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:06<00:00, 37.95it/s, est. speed input: 41432.35 toks/s, output: 40.46 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 37.95it/s, est. speed input: 41414.22 toks/s, output: 40.44 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 40.44it/s, est. speed input: 41414.22 toks/s, output: 40.44 toks/s]
[rank0]:[W127 23:57:19.049744300 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.4s

测试结果:
  Requests/s:   36.35
  Tokens/s:     37260.34
  Total Reqs:   256
  Elapsed:      7.04s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     37223.99

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 23:57:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2208844) WARNING 01-27 23:57:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 49.64 requests/s, 50882.74 total tokens/s, 49.64 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-27 23:57:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-27 23:57:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-27 23:57:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-27 23:57:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:57:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:57:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:57:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:57:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:57:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-27 23:57:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 23:57:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 23:57:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 23:57:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 23:57:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 23:57:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-27 23:57:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-27 23:57:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-27 23:57:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:57:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:57:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:57:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:57:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:57:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-27 23:57:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 23:57:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 23:57:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 23:57:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 23:57:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2208844) [2026-01-27 23:57:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2208844) [2026-01-27 23:57:40] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2208844) [2026-01-27 23:57:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2208844) [2026-01-27 23:57:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2208844) [2026-01-27 23:57:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2208844) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2208844) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.64it/s]
(EngineCore_DP0 pid=2208844) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.63it/s]
(EngineCore_DP0 pid=2208844) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2208844) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  9.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  9.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.00it/s]
(EngineCore_DP0 pid=2208844) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  8.02it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.56it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.67it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 39/512 [00:00<00:01, 383.52it/s]
Adding requests:  16%|█▌        | 80/512 [00:00<00:01, 397.39it/s]
Adding requests:  23%|██▎       | 120/512 [00:00<00:01, 386.69it/s]
Adding requests:  31%|███       | 159/512 [00:00<00:00, 385.53it/s]
Adding requests:  39%|███▊      | 198/512 [00:00<00:00, 377.77it/s]
Adding requests:  46%|████▋     | 237/512 [00:00<00:00, 380.50it/s]
Adding requests:  54%|█████▍    | 277/512 [00:00<00:00, 384.32it/s]
Adding requests:  62%|██████▏   | 316/512 [00:00<00:00, 376.09it/s]
Adding requests:  69%|██████▉   | 354/512 [00:00<00:00, 374.72it/s]
Adding requests:  77%|███████▋  | 392/512 [00:01<00:00, 371.47it/s]
Adding requests:  84%|████████▍ | 431/512 [00:01<00:00, 374.40it/s]
Adding requests:  92%|█████████▏| 472/512 [00:01<00:00, 382.83it/s]
Adding requests: 100%|█████████▉| 511/512 [00:01<00:00, 384.37it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 381.15it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:00<00:00, 604.77it/s, est. speed input: 619421.77 toks/s, output: 604.81 toks/s]
Processed prompts:  25%|██▍       | 127/512 [00:01<00:04, 83.77it/s, est. speed input: 99092.27 toks/s, output: 96.77 toks/s]  
Processed prompts:  30%|███       | 155/512 [00:01<00:05, 70.69it/s, est. speed input: 84679.46 toks/s, output: 82.69 toks/s]
Processed prompts:  34%|███▍      | 173/512 [00:02<00:05, 67.26it/s, est. speed input: 80744.46 toks/s, output: 78.85 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:02<00:05, 60.49it/s, est. speed input: 75767.55 toks/s, output: 73.99 toks/s]
Processed prompts:  38%|███▊      | 196/512 [00:02<00:05, 60.77it/s, est. speed input: 75048.54 toks/s, output: 73.29 toks/s]
Processed prompts:  40%|████      | 205/512 [00:02<00:05, 59.99it/s, est. speed input: 74074.57 toks/s, output: 72.34 toks/s]
Processed prompts:  42%|████▏     | 213/512 [00:02<00:05, 57.87it/s, est. speed input: 72828.96 toks/s, output: 71.12 toks/s]
Processed prompts:  43%|████▎     | 220/512 [00:03<00:05, 54.61it/s, est. speed input: 71388.06 toks/s, output: 69.71 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:03<00:05, 50.40it/s, est. speed input: 69786.98 toks/s, output: 68.15 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:03<00:05, 50.23it/s, est. speed input: 68914.40 toks/s, output: 67.30 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:03<00:05, 50.14it/s, est. speed input: 68129.57 toks/s, output: 66.53 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:03<00:05, 50.01it/s, est. speed input: 67396.21 toks/s, output: 65.82 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:03<00:05, 49.96it/s, est. speed input: 66733.51 toks/s, output: 65.17 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:04<00:04, 49.97it/s, est. speed input: 66128.86 toks/s, output: 64.58 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:04<00:04, 49.92it/s, est. speed input: 65560.48 toks/s, output: 64.02 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:04<00:04, 49.81it/s, est. speed input: 65022.50 toks/s, output: 63.50 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:04<00:04, 49.80it/s, est. speed input: 64531.12 toks/s, output: 63.02 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:04<00:04, 49.78it/s, est. speed input: 64071.98 toks/s, output: 62.57 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:04<00:04, 49.80it/s, est. speed input: 63647.72 toks/s, output: 62.16 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:05<00:03, 49.95it/s, est. speed input: 63267.76 toks/s, output: 61.78 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:05<00:03, 49.99it/s, est. speed input: 62902.80 toks/s, output: 61.43 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:05<00:03, 50.01it/s, est. speed input: 62557.90 toks/s, output: 61.09 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:05<00:03, 50.01it/s, est. speed input: 62231.43 toks/s, output: 60.77 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:05<00:03, 51.36it/s, est. speed input: 62075.81 toks/s, output: 60.62 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:05<00:03, 50.93it/s, est. speed input: 61777.36 toks/s, output: 60.33 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:06<00:02, 50.67it/s, est. speed input: 61498.94 toks/s, output: 60.06 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:06<00:02, 50.46it/s, est. speed input: 61231.68 toks/s, output: 59.80 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:06<00:02, 50.29it/s, est. speed input: 60975.81 toks/s, output: 59.55 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:06<00:02, 50.12it/s, est. speed input: 60726.97 toks/s, output: 59.30 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:06<00:02, 49.95it/s, est. speed input: 60485.98 toks/s, output: 59.07 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:06<00:02, 49.83it/s, est. speed input: 60255.53 toks/s, output: 58.84 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:06<00:02, 49.84it/s, est. speed input: 60044.33 toks/s, output: 58.64 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:07<00:01, 49.80it/s, est. speed input: 59838.43 toks/s, output: 58.44 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:07<00:01, 49.78it/s, est. speed input: 59642.55 toks/s, output: 58.24 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:07<00:01, 49.80it/s, est. speed input: 59457.66 toks/s, output: 58.06 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:07<00:01, 49.75it/s, est. speed input: 59275.47 toks/s, output: 57.89 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:07<00:01, 51.34it/s, est. speed input: 59229.99 toks/s, output: 57.84 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:07<00:01, 50.92it/s, est. speed input: 59067.18 toks/s, output: 57.68 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:08<00:00, 50.57it/s, est. speed input: 58906.11 toks/s, output: 57.53 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:08<00:00, 50.31it/s, est. speed input: 58750.59 toks/s, output: 57.37 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:08<00:00, 50.14it/s, est. speed input: 58601.46 toks/s, output: 57.23 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:08<00:00, 50.05it/s, est. speed input: 58459.69 toks/s, output: 57.09 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:08<00:00, 49.86it/s, est. speed input: 58314.53 toks/s, output: 56.95 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:08<00:00, 49.79it/s, est. speed input: 58178.91 toks/s, output: 56.82 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 49.79it/s, est. speed input: 58454.23 toks/s, output: 57.08 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 57.08it/s, est. speed input: 58454.23 toks/s, output: 57.08 toks/s]
[rank0]:[W127 23:58:19.469506621 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 59.4s

测试结果:
  Requests/s:   49.64
  Tokens/s:     50882.74
  Total Reqs:   512
  Elapsed:      10.31s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     50833.10

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 23:58:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2209923) WARNING 01-27 23:58:51 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.99 requests/s, 53294.68 total tokens/s, 51.99 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-27 23:58:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-27 23:58:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-27 23:58:34] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-27 23:58:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:58:34] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:58:34] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:58:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:58:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:58:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-27 23:58:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 23:58:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 23:58:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 23:58:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 23:58:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 23:58:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-27 23:58:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-27 23:58:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-27 23:58:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:58:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:58:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:58:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:58:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:58:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-27 23:58:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 23:58:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 23:58:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 23:58:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 23:58:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2209923) [2026-01-27 23:58:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2209923) [2026-01-27 23:58:42] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2209923) [2026-01-27 23:58:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2209923) [2026-01-27 23:58:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2209923) [2026-01-27 23:58:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2209923) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2209923) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.69it/s]
(EngineCore_DP0 pid=2209923) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.69it/s]
(EngineCore_DP0 pid=2209923) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2209923) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  2.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  3.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  5.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  6.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  6.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  5.30it/s]
(EngineCore_DP0 pid=2209923) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  8.18it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  8.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.69it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.67it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 36/1024 [00:00<00:02, 356.53it/s]
Adding requests:   7%|▋         | 75/1024 [00:00<00:02, 371.81it/s]
Adding requests:  11%|█         | 113/1024 [00:00<00:02, 372.44it/s]
Adding requests:  15%|█▍        | 152/1024 [00:00<00:02, 377.17it/s]
Adding requests:  19%|█▊        | 191/1024 [00:00<00:02, 381.57it/s]
Adding requests:  23%|██▎       | 231/1024 [00:00<00:02, 386.04it/s]
Adding requests:  26%|██▋       | 270/1024 [00:00<00:01, 378.14it/s]
Adding requests:  30%|███       | 308/1024 [00:00<00:01, 370.94it/s]
Adding requests:  34%|███▍      | 348/1024 [00:00<00:01, 378.94it/s]
Adding requests:  38%|███▊      | 387/1024 [00:01<00:01, 382.09it/s]
Adding requests:  42%|████▏     | 426/1024 [00:01<00:01, 383.29it/s]
Adding requests:  46%|████▌     | 467/1024 [00:01<00:01, 388.48it/s]
Adding requests:  49%|████▉     | 506/1024 [00:01<00:01, 382.78it/s]
Adding requests:  53%|█████▎    | 545/1024 [00:01<00:01, 372.69it/s]
Adding requests:  57%|█████▋    | 587/1024 [00:01<00:01, 385.54it/s]
Adding requests:  61%|██████    | 626/1024 [00:01<00:01, 374.51it/s]
Adding requests:  65%|██████▍   | 665/1024 [00:01<00:00, 377.60it/s]
Adding requests:  69%|██████▉   | 708/1024 [00:01<00:00, 390.29it/s]
Adding requests:  73%|███████▎  | 748/1024 [00:01<00:00, 384.96it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:02<00:00, 388.05it/s]
Adding requests:  81%|████████  | 827/1024 [00:02<00:00, 385.85it/s]
Adding requests:  85%|████████▍ | 869/1024 [00:02<00:00, 394.10it/s]
Adding requests:  89%|████████▉ | 912/1024 [00:02<00:00, 403.07it/s]
Adding requests:  93%|█████████▎| 953/1024 [00:02<00:00, 404.04it/s]
Adding requests:  97%|█████████▋| 995/1024 [00:02<00:00, 406.09it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 386.90it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:00<00:01, 674.98it/s, est. speed input: 691341.21 toks/s, output: 675.00 toks/s]
Processed prompts:  20%|██        | 206/1024 [00:01<00:06, 120.14it/s, est. speed input: 147370.50 toks/s, output: 143.92 toks/s]
Processed prompts:  23%|██▎       | 238/1024 [00:02<00:08, 93.64it/s, est. speed input: 119199.35 toks/s, output: 116.41 toks/s] 
Processed prompts:  25%|██▌       | 258/1024 [00:02<00:09, 78.84it/s, est. speed input: 105475.69 toks/s, output: 103.00 toks/s]
Processed prompts:  27%|██▋       | 272/1024 [00:02<00:09, 80.38it/s, est. speed input: 104788.19 toks/s, output: 102.33 toks/s]
Processed prompts:  28%|██▊       | 284/1024 [00:02<00:10, 69.66it/s, est. speed input: 98099.53 toks/s, output: 95.80 toks/s]  
Processed prompts:  29%|██▊       | 294/1024 [00:03<00:10, 68.98it/s, est. speed input: 96566.12 toks/s, output: 94.30 toks/s]
Processed prompts:  30%|██▉       | 303/1024 [00:03<00:10, 66.99it/s, est. speed input: 94818.82 toks/s, output: 92.60 toks/s]
Processed prompts:  30%|███       | 311/1024 [00:03<00:11, 63.93it/s, est. speed input: 92966.56 toks/s, output: 90.79 toks/s]
Processed prompts:  31%|███       | 318/1024 [00:03<00:11, 59.69it/s, est. speed input: 90968.57 toks/s, output: 88.84 toks/s]
Processed prompts:  32%|███▏      | 325/1024 [00:03<00:12, 56.28it/s, est. speed input: 89161.07 toks/s, output: 87.07 toks/s]
Processed prompts:  32%|███▏      | 331/1024 [00:03<00:13, 51.79it/s, est. speed input: 87216.85 toks/s, output: 85.17 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:04<00:13, 51.48it/s, est. speed input: 85996.41 toks/s, output: 83.98 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:04<00:13, 51.60it/s, est. speed input: 84785.09 toks/s, output: 82.80 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:04<00:12, 51.73it/s, est. speed input: 83668.34 toks/s, output: 81.71 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:04<00:12, 51.88it/s, est. speed input: 82637.94 toks/s, output: 80.70 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:04<00:12, 51.86it/s, est. speed input: 81653.04 toks/s, output: 79.74 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:04<00:12, 51.96it/s, est. speed input: 80751.92 toks/s, output: 78.86 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:04<00:12, 51.95it/s, est. speed input: 79892.21 toks/s, output: 78.02 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:05<00:12, 51.96it/s, est. speed input: 79088.55 toks/s, output: 77.23 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:05<00:11, 51.92it/s, est. speed input: 78323.98 toks/s, output: 76.49 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:05<00:11, 52.01it/s, est. speed input: 77620.33 toks/s, output: 75.80 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:05<00:11, 51.98it/s, est. speed input: 76941.93 toks/s, output: 75.14 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:05<00:11, 52.00it/s, est. speed input: 76306.44 toks/s, output: 74.52 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:05<00:11, 52.04it/s, est. speed input: 75707.34 toks/s, output: 73.93 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:06<00:11, 52.06it/s, est. speed input: 75137.61 toks/s, output: 73.38 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:06<00:10, 53.86it/s, est. speed input: 74801.79 toks/s, output: 73.05 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:06<00:10, 53.26it/s, est. speed input: 74273.43 toks/s, output: 72.53 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:06<00:10, 52.89it/s, est. speed input: 73774.96 toks/s, output: 72.05 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:06<00:10, 52.56it/s, est. speed input: 73291.31 toks/s, output: 71.57 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:06<00:10, 52.45it/s, est. speed input: 72842.29 toks/s, output: 71.13 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:06<00:10, 52.30it/s, est. speed input: 72405.99 toks/s, output: 70.71 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:07<00:10, 52.24it/s, est. speed input: 71992.98 toks/s, output: 70.31 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:07<00:09, 52.15it/s, est. speed input: 71593.07 toks/s, output: 69.91 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:07<00:09, 52.14it/s, est. speed input: 71214.29 toks/s, output: 69.55 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:07<00:09, 52.18it/s, est. speed input: 70855.43 toks/s, output: 69.19 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:07<00:09, 52.15it/s, est. speed input: 70506.10 toks/s, output: 68.85 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:07<00:09, 52.14it/s, est. speed input: 70170.66 toks/s, output: 68.53 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:08<00:09, 52.06it/s, est. speed input: 69842.37 toks/s, output: 68.21 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:08<00:09, 51.89it/s, est. speed input: 69516.94 toks/s, output: 67.89 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:08<00:08, 52.01it/s, est. speed input: 69222.84 toks/s, output: 67.60 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:08<00:08, 52.01it/s, est. speed input: 68932.75 toks/s, output: 67.32 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:08<00:08, 51.99it/s, est. speed input: 68651.75 toks/s, output: 67.04 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:08<00:08, 52.00it/s, est. speed input: 68381.79 toks/s, output: 66.78 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:08<00:08, 51.97it/s, est. speed input: 68118.84 toks/s, output: 66.52 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:09<00:08, 51.97it/s, est. speed input: 67866.71 toks/s, output: 66.28 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:09<00:07, 52.00it/s, est. speed input: 67624.40 toks/s, output: 66.04 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:09<00:07, 52.05it/s, est. speed input: 67392.21 toks/s, output: 65.81 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:09<00:07, 52.08it/s, est. speed input: 67167.19 toks/s, output: 65.59 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:09<00:07, 52.00it/s, est. speed input: 66943.11 toks/s, output: 65.37 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:09<00:07, 52.02it/s, est. speed input: 66730.27 toks/s, output: 65.17 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:10<00:07, 52.02it/s, est. speed input: 66523.54 toks/s, output: 64.96 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:10<00:07, 51.96it/s, est. speed input: 66318.90 toks/s, output: 64.76 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:10<00:06, 51.96it/s, est. speed input: 66123.36 toks/s, output: 64.57 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:10<00:06, 51.96it/s, est. speed input: 65933.21 toks/s, output: 64.39 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:10<00:06, 51.92it/s, est. speed input: 65746.22 toks/s, output: 64.21 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:10<00:06, 52.04it/s, est. speed input: 65573.36 toks/s, output: 64.04 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:10<00:06, 52.01it/s, est. speed input: 65398.99 toks/s, output: 63.87 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:11<00:06, 51.93it/s, est. speed input: 65225.52 toks/s, output: 63.70 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:11<00:05, 51.91it/s, est. speed input: 65059.58 toks/s, output: 63.53 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:11<00:05, 52.00it/s, est. speed input: 64903.63 toks/s, output: 63.38 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:11<00:05, 51.96it/s, est. speed input: 64746.07 toks/s, output: 63.23 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:11<00:05, 52.00it/s, est. speed input: 64596.32 toks/s, output: 63.08 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:11<00:05, 51.99it/s, est. speed input: 64448.35 toks/s, output: 62.94 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:12<00:05, 51.99it/s, est. speed input: 64304.97 toks/s, output: 62.80 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:12<00:05, 51.90it/s, est. speed input: 64160.28 toks/s, output: 62.66 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:12<00:04, 51.90it/s, est. speed input: 64022.43 toks/s, output: 62.52 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:12<00:04, 51.99it/s, est. speed input: 63892.30 toks/s, output: 62.39 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:12<00:04, 51.89it/s, est. speed input: 63757.52 toks/s, output: 62.26 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:12<00:04, 51.92it/s, est. speed input: 63630.76 toks/s, output: 62.14 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:12<00:04, 51.93it/s, est. speed input: 63506.50 toks/s, output: 62.02 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:13<00:04, 51.92it/s, est. speed input: 63384.33 toks/s, output: 61.90 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:13<00:03, 51.84it/s, est. speed input: 63261.67 toks/s, output: 61.78 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:13<00:03, 51.85it/s, est. speed input: 63144.72 toks/s, output: 61.66 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:13<00:03, 51.86it/s, est. speed input: 63031.02 toks/s, output: 61.55 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:13<00:03, 51.92it/s, est. speed input: 62922.05 toks/s, output: 61.45 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:13<00:03, 51.92it/s, est. speed input: 62813.47 toks/s, output: 61.34 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:14<00:03, 51.85it/s, est. speed input: 62704.40 toks/s, output: 61.23 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:14<00:03, 51.93it/s, est. speed input: 62603.32 toks/s, output: 61.14 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:14<00:02, 51.89it/s, est. speed input: 62499.97 toks/s, output: 61.04 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:14<00:02, 51.89it/s, est. speed input: 62400.18 toks/s, output: 60.94 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:14<00:02, 51.82it/s, est. speed input: 62299.83 toks/s, output: 60.84 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:14<00:02, 51.85it/s, est. speed input: 62204.29 toks/s, output: 60.75 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:14<00:02, 51.88it/s, est. speed input: 62111.67 toks/s, output: 60.66 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:15<00:02, 51.82it/s, est. speed input: 62017.38 toks/s, output: 60.56 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:15<00:01, 51.84it/s, est. speed input: 61927.35 toks/s, output: 60.48 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:15<00:01, 51.89it/s, est. speed input: 61840.63 toks/s, output: 60.39 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:15<00:01, 53.62it/s, est. speed input: 61820.45 toks/s, output: 60.37 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:15<00:01, 53.12it/s, est. speed input: 61736.03 toks/s, output: 60.29 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:15<00:01, 52.77it/s, est. speed input: 61653.55 toks/s, output: 60.21 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:16<00:01, 52.40it/s, est. speed input: 61567.52 toks/s, output: 60.12 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:16<00:01, 52.20it/s, est. speed input: 61485.36 toks/s, output: 60.04 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:16<00:00, 52.01it/s, est. speed input: 61402.97 toks/s, output: 59.96 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:16<00:00, 53.84it/s, est. speed input: 61391.79 toks/s, output: 59.95 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:16<00:00, 53.14it/s, est. speed input: 61311.62 toks/s, output: 59.87 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:16<00:00, 52.77it/s, est. speed input: 61236.86 toks/s, output: 59.80 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:16<00:00, 52.50it/s, est. speed input: 61162.86 toks/s, output: 59.73 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:17<00:00, 54.21it/s, est. speed input: 61154.22 toks/s, output: 59.72 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 54.21it/s, est. speed input: 61513.84 toks/s, output: 60.07 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 60.07it/s, est. speed input: 61513.84 toks/s, output: 60.07 toks/s]
[rank0]:[W127 23:59:32.013693882 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 73.6s

测试结果:
  Requests/s:   51.99
  Tokens/s:     53294.68
  Total Reqs:   1024
  Elapsed:      19.69s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     53242.69

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 23:59:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2211290) WARNING 01-28 00:00:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 52.97 requests/s, 54294.72 total tokens/s, 52.97 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-27 23:59:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-27 23:59:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-27 23:59:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-27 23:59:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:59:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:59:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:59:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:59:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-27 23:59:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-27 23:59:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 23:59:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 23:59:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 23:59:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 23:59:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:00:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:00:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:00:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:00:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:00:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:00:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:00:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:00:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:00:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:00:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:00:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:00:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:00:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:00:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2211290) [2026-01-28 00:00:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2211290) [2026-01-28 00:00:01] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2211290) [2026-01-28 00:00:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2211290) [2026-01-28 00:00:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2211290) [2026-01-28 00:00:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2211290) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2211290) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.74it/s]
(EngineCore_DP0 pid=2211290) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.74it/s]
(EngineCore_DP0 pid=2211290) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2211290) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  9.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  9.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  9.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.99it/s]
(EngineCore_DP0 pid=2211290) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.73it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.73it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  9.04it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  9.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.91it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 384.96it/s]
Adding requests:   4%|▍         | 78/2048 [00:00<00:05, 386.16it/s]
Adding requests:   6%|▌         | 118/2048 [00:00<00:04, 390.66it/s]
Adding requests:   8%|▊         | 158/2048 [00:00<00:04, 381.80it/s]
Adding requests:  10%|▉         | 197/2048 [00:00<00:04, 380.50it/s]
Adding requests:  12%|█▏        | 239/2048 [00:00<00:04, 393.47it/s]
Adding requests:  14%|█▎        | 279/2048 [00:00<00:04, 384.44it/s]
Adding requests:  16%|█▌        | 319/2048 [00:00<00:04, 387.81it/s]
Adding requests:  18%|█▊        | 360/2048 [00:00<00:04, 394.50it/s]
Adding requests:  20%|█▉        | 402/2048 [00:01<00:04, 401.25it/s]
Adding requests:  22%|██▏       | 443/2048 [00:01<00:03, 403.48it/s]
Adding requests:  24%|██▎       | 484/2048 [00:01<00:03, 403.62it/s]
Adding requests:  26%|██▌       | 525/2048 [00:01<00:03, 392.04it/s]
Adding requests:  28%|██▊       | 566/2048 [00:01<00:03, 395.77it/s]
Adding requests:  30%|██▉       | 608/2048 [00:01<00:03, 400.43it/s]
Adding requests:  32%|███▏      | 651/2048 [00:01<00:03, 408.46it/s]
Adding requests:  34%|███▍      | 692/2048 [00:01<00:03, 406.33it/s]
Adding requests:  36%|███▌      | 733/2048 [00:01<00:03, 400.29it/s]
Adding requests:  38%|███▊      | 774/2048 [00:01<00:03, 386.59it/s]
Adding requests:  40%|███▉      | 814/2048 [00:02<00:03, 389.80it/s]
Adding requests:  42%|████▏     | 856/2048 [00:02<00:03, 396.06it/s]
Adding requests:  44%|████▍     | 897/2048 [00:02<00:02, 398.96it/s]
Adding requests:  46%|████▌     | 939/2048 [00:02<00:02, 404.50it/s]
Adding requests:  48%|████▊     | 980/2048 [00:02<00:02, 403.19it/s]
Adding requests:  50%|█████     | 1024/2048 [00:02<00:02, 411.58it/s]
Adding requests:  52%|█████▏    | 1066/2048 [00:02<00:02, 411.07it/s]
Adding requests:  54%|█████▍    | 1108/2048 [00:02<00:02, 396.67it/s]
Adding requests:  56%|█████▌    | 1149/2048 [00:02<00:02, 398.92it/s]
Adding requests:  58%|█████▊    | 1194/2048 [00:02<00:02, 411.87it/s]
Adding requests:  60%|██████    | 1237/2048 [00:03<00:01, 417.02it/s]
Adding requests:  62%|██████▏   | 1279/2048 [00:03<00:01, 411.47it/s]
Adding requests:  65%|██████▍   | 1322/2048 [00:03<00:01, 416.23it/s]
Adding requests:  67%|██████▋   | 1366/2048 [00:03<00:01, 420.77it/s]
Adding requests:  69%|██████▉   | 1409/2048 [00:03<00:01, 414.27it/s]
Adding requests:  71%|███████   | 1451/2048 [00:03<00:01, 410.04it/s]
Adding requests:  73%|███████▎  | 1493/2048 [00:03<00:01, 409.52it/s]
Adding requests:  75%|███████▍  | 1534/2048 [00:03<00:01, 404.96it/s]
Adding requests:  77%|███████▋  | 1575/2048 [00:03<00:01, 395.24it/s]
Adding requests:  79%|███████▉  | 1615/2048 [00:04<00:01, 395.88it/s]
Adding requests:  81%|████████  | 1657/2048 [00:04<00:00, 400.57it/s]
Adding requests:  83%|████████▎ | 1700/2048 [00:04<00:00, 405.62it/s]
Adding requests:  85%|████████▌ | 1742/2048 [00:04<00:00, 407.11it/s]
Adding requests:  87%|████████▋ | 1783/2048 [00:04<00:00, 397.82it/s]
Adding requests:  89%|████████▉ | 1827/2048 [00:04<00:00, 407.79it/s]
Adding requests:  91%|█████████ | 1868/2048 [00:04<00:00, 401.75it/s]
Adding requests:  93%|█████████▎| 1909/2048 [00:04<00:00, 391.16it/s]
Adding requests:  95%|█████████▌| 1949/2048 [00:04<00:00, 388.95it/s]
Adding requests:  97%|█████████▋| 1988/2048 [00:04<00:00, 386.87it/s]
Adding requests:  99%|█████████▉| 2029/2048 [00:05<00:00, 392.03it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 399.70it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:00<00:02, 717.29it/s, est. speed input: 734559.60 toks/s, output: 717.30 toks/s]
Processed prompts:  17%|█▋        | 346/2048 [00:01<00:09, 181.12it/s, est. speed input: 225518.57 toks/s, output: 220.23 toks/s]
Processed prompts:  19%|█▊        | 380/2048 [00:02<00:12, 134.93it/s, est. speed input: 179145.45 toks/s, output: 174.95 toks/s]
Processed prompts:  20%|█▉        | 401/2048 [00:02<00:13, 121.24it/s, est. speed input: 166019.99 toks/s, output: 162.13 toks/s]
Processed prompts:  20%|██        | 417/2048 [00:02<00:15, 105.48it/s, est. speed input: 153861.55 toks/s, output: 150.26 toks/s]
Processed prompts:  21%|██        | 429/2048 [00:03<00:18, 89.14it/s, est. speed input: 142741.02 toks/s, output: 139.40 toks/s] 
Processed prompts:  21%|██▏       | 439/2048 [00:03<00:21, 74.48it/s, est. speed input: 133018.33 toks/s, output: 129.90 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:03<00:24, 65.19it/s, est. speed input: 125785.76 toks/s, output: 122.84 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:03<00:25, 61.67it/s, est. speed input: 120307.72 toks/s, output: 117.49 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:04<00:26, 59.21it/s, est. speed input: 115650.95 toks/s, output: 112.94 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:04<00:27, 57.37it/s, est. speed input: 111585.81 toks/s, output: 108.97 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:04<00:27, 56.08it/s, est. speed input: 108029.88 toks/s, output: 105.50 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:05<00:27, 55.07it/s, est. speed input: 104857.86 toks/s, output: 102.40 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:05<00:27, 54.40it/s, est. speed input: 102049.58 toks/s, output: 99.66 toks/s] 
Processed prompts:  27%|██▋       | 562/2048 [00:05<00:27, 53.97it/s, est. speed input: 99551.57 toks/s, output: 97.22 toks/s] 
Processed prompts:  28%|██▊       | 578/2048 [00:06<00:27, 53.62it/s, est. speed input: 97284.23 toks/s, output: 95.00 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:06<00:27, 53.41it/s, est. speed input: 95245.19 toks/s, output: 93.01 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:06<00:26, 53.28it/s, est. speed input: 93394.33 toks/s, output: 91.21 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:06<00:26, 53.11it/s, est. speed input: 91683.03 toks/s, output: 89.53 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:07<00:26, 53.10it/s, est. speed input: 90140.24 toks/s, output: 88.03 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:07<00:26, 52.97it/s, est. speed input: 88691.76 toks/s, output: 86.61 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:07<00:25, 52.92it/s, est. speed input: 87363.91 toks/s, output: 85.32 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:08<00:25, 52.90it/s, est. speed input: 86136.86 toks/s, output: 84.12 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:08<00:25, 52.89it/s, est. speed input: 84998.25 toks/s, output: 83.01 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:08<00:25, 52.92it/s, est. speed input: 83944.40 toks/s, output: 81.98 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:09<00:24, 52.90it/s, est. speed input: 82953.58 toks/s, output: 81.01 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:09<00:24, 52.83it/s, est. speed input: 82017.47 toks/s, output: 80.09 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:09<00:24, 52.85it/s, est. speed input: 81150.11 toks/s, output: 79.25 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:10<00:23, 52.90it/s, est. speed input: 80340.64 toks/s, output: 78.46 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:10<00:23, 52.80it/s, est. speed input: 79559.49 toks/s, output: 77.69 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:10<00:23, 52.85it/s, est. speed input: 78838.68 toks/s, output: 76.99 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:10<00:22, 52.86it/s, est. speed input: 78155.93 toks/s, output: 76.32 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:11<00:22, 52.81it/s, est. speed input: 77500.95 toks/s, output: 75.68 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:11<00:22, 52.81it/s, est. speed input: 76886.49 toks/s, output: 75.08 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:11<00:22, 52.78it/s, est. speed input: 76298.43 toks/s, output: 74.51 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:12<00:21, 52.76it/s, est. speed input: 75740.61 toks/s, output: 73.97 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:12<00:21, 52.80it/s, est. speed input: 75215.88 toks/s, output: 73.45 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:12<00:20, 53.74it/s, est. speed input: 74817.09 toks/s, output: 73.06 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:13<00:20, 53.50it/s, est. speed input: 74339.17 toks/s, output: 72.60 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:13<00:20, 53.30it/s, est. speed input: 73880.24 toks/s, output: 72.15 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:13<00:19, 54.13it/s, est. speed input: 73537.48 toks/s, output: 71.81 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:13<00:19, 53.65it/s, est. speed input: 73106.66 toks/s, output: 71.39 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:14<00:19, 53.40it/s, est. speed input: 72702.17 toks/s, output: 71.00 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:14<00:19, 53.20it/s, est. speed input: 72311.69 toks/s, output: 70.62 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:14<00:18, 53.11it/s, est. speed input: 71941.79 toks/s, output: 70.26 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:15<00:18, 53.00it/s, est. speed input: 71582.59 toks/s, output: 69.90 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:15<00:18, 52.93it/s, est. speed input: 71237.31 toks/s, output: 69.57 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:15<00:18, 52.88it/s, est. speed input: 70906.30 toks/s, output: 69.24 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:16<00:17, 52.89it/s, est. speed input: 70591.32 toks/s, output: 68.94 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:16<00:17, 52.92it/s, est. speed input: 70289.53 toks/s, output: 68.64 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:16<00:17, 52.90it/s, est. speed input: 69995.60 toks/s, output: 68.35 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:16<00:16, 53.85it/s, est. speed input: 69786.24 toks/s, output: 68.15 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:17<00:16, 53.60it/s, est. speed input: 69514.98 toks/s, output: 67.89 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:17<00:16, 53.43it/s, est. speed input: 69253.40 toks/s, output: 67.63 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:17<00:15, 53.28it/s, est. speed input: 68998.84 toks/s, output: 67.38 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:18<00:15, 53.15it/s, est. speed input: 68750.54 toks/s, output: 67.14 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:18<00:15, 53.07it/s, est. speed input: 68510.63 toks/s, output: 66.90 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:18<00:15, 53.02it/s, est. speed input: 68279.69 toks/s, output: 66.68 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:19<00:14, 53.91it/s, est. speed input: 68117.14 toks/s, output: 66.52 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:19<00:14, 53.44it/s, est. speed input: 67888.62 toks/s, output: 66.30 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:19<00:14, 53.24it/s, est. speed input: 67674.66 toks/s, output: 66.09 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:19<00:13, 53.12it/s, est. speed input: 67468.57 toks/s, output: 65.89 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:20<00:13, 53.04it/s, est. speed input: 67269.11 toks/s, output: 65.69 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:20<00:13, 52.93it/s, est. speed input: 67072.25 toks/s, output: 65.50 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:20<00:12, 52.91it/s, est. speed input: 66884.31 toks/s, output: 65.32 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:21<00:12, 52.85it/s, est. speed input: 66698.99 toks/s, output: 65.14 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:21<00:12, 52.81it/s, est. speed input: 66519.04 toks/s, output: 64.96 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:21<00:12, 52.79it/s, est. speed input: 66344.95 toks/s, output: 64.79 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:22<00:11, 52.76it/s, est. speed input: 66174.16 toks/s, output: 64.62 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:22<00:11, 52.67it/s, est. speed input: 66004.66 toks/s, output: 64.46 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:22<00:11, 52.62it/s, est. speed input: 65840.03 toks/s, output: 64.30 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:22<00:10, 52.67it/s, est. speed input: 65684.32 toks/s, output: 64.14 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:23<00:10, 52.67it/s, est. speed input: 65531.33 toks/s, output: 64.00 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:23<00:10, 52.73it/s, est. speed input: 65384.97 toks/s, output: 63.85 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:23<00:09, 52.80it/s, est. speed input: 65244.06 toks/s, output: 63.71 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:24<00:09, 52.83it/s, est. speed input: 65105.40 toks/s, output: 63.58 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:24<00:09, 52.82it/s, est. speed input: 64968.73 toks/s, output: 63.45 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:24<00:09, 52.80it/s, est. speed input: 64835.02 toks/s, output: 63.32 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:25<00:08, 53.78it/s, est. speed input: 64752.20 toks/s, output: 63.23 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:25<00:08, 53.46it/s, est. speed input: 64623.66 toks/s, output: 63.11 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:25<00:08, 53.16it/s, est. speed input: 64494.66 toks/s, output: 62.98 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:25<00:07, 52.92it/s, est. speed input: 64366.66 toks/s, output: 62.86 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:26<00:07, 52.83it/s, est. speed input: 64245.74 toks/s, output: 62.74 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:26<00:07, 52.77it/s, est. speed input: 64127.28 toks/s, output: 62.62 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:26<00:06, 52.66it/s, est. speed input: 64008.40 toks/s, output: 62.51 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:27<00:06, 52.68it/s, est. speed input: 63896.96 toks/s, output: 62.40 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:27<00:06, 52.58it/s, est. speed input: 63782.59 toks/s, output: 62.29 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:27<00:06, 52.57it/s, est. speed input: 63673.17 toks/s, output: 62.18 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:28<00:05, 52.66it/s, est. speed input: 63570.58 toks/s, output: 62.08 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:28<00:05, 52.68it/s, est. speed input: 63468.55 toks/s, output: 61.98 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:28<00:05, 52.72it/s, est. speed input: 63369.64 toks/s, output: 61.88 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:29<00:04, 52.74it/s, est. speed input: 63272.30 toks/s, output: 61.79 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:29<00:04, 52.78it/s, est. speed input: 63178.28 toks/s, output: 61.70 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:29<00:04, 52.81it/s, est. speed input: 63086.08 toks/s, output: 61.61 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:29<00:03, 52.77it/s, est. speed input: 62993.21 toks/s, output: 61.52 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:30<00:03, 52.76it/s, est. speed input: 62903.13 toks/s, output: 61.43 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:30<00:03, 53.69it/s, est. speed input: 62851.13 toks/s, output: 61.38 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:30<00:02, 53.25it/s, est. speed input: 62758.25 toks/s, output: 61.29 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:31<00:02, 53.03it/s, est. speed input: 62670.31 toks/s, output: 61.20 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:31<00:02, 52.94it/s, est. speed input: 62586.55 toks/s, output: 61.12 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:31<00:02, 52.84it/s, est. speed input: 62503.25 toks/s, output: 61.04 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:32<00:01, 53.65it/s, est. speed input: 62453.53 toks/s, output: 60.99 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:32<00:01, 53.32it/s, est. speed input: 62372.42 toks/s, output: 60.91 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:32<00:01, 53.15it/s, est. speed input: 62294.66 toks/s, output: 60.83 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:32<00:00, 53.01it/s, est. speed input: 62217.85 toks/s, output: 60.76 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:33<00:00, 52.94it/s, est. speed input: 62143.33 toks/s, output: 60.69 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:33<00:00, 53.91it/s, est. speed input: 62105.46 toks/s, output: 60.65 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:33<00:00, 53.91it/s, est. speed input: 62532.31 toks/s, output: 61.07 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:33<00:00, 61.07it/s, est. speed input: 62532.31 toks/s, output: 61.07 toks/s]
[rank0]:[W128 00:01:11.866317286 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 98.9s

测试结果:
  Requests/s:   52.97
  Tokens/s:     54294.72
  Total Reqs:   2048
  Elapsed:      38.66s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     54241.75

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:01:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2213167) WARNING 01-28 00:01:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.24 requests/s, 54567.94 total tokens/s, 53.24 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 00:01:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:01:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:01:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:01:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:01:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:01:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:01:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:01:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:01:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:01:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:01:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:01:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:01:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:01:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:01:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:01:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:01:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:01:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:01:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:01:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:01:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:01:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:01:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:01:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:01:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:01:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:01:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:01:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2213167) [2026-01-28 00:01:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2213167) [2026-01-28 00:01:51] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2213167) [2026-01-28 00:01:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2213167) [2026-01-28 00:01:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2213167) [2026-01-28 00:01:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2213167) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2213167) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.74it/s]
(EngineCore_DP0 pid=2213167) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.74it/s]
(EngineCore_DP0 pid=2213167) 
(EngineCore_DP0 pid=2213167) [rank0]:W0128 00:02:08.912000 2213167 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2213167) [rank0]:W0128 00:02:09.822000 2213167 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2213167) [rank0]:W0128 00:02:12.048000 2213167 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2213167) [rank0]:W0128 00:02:12.238000 2213167 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2213167) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  9.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  9.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  9.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  9.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00,  9.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  9.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.09it/s]
(EngineCore_DP0 pid=2213167) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  9.13it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  9.42it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  9.12it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  9.21it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.27it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 35/4096 [00:00<00:11, 341.64it/s]
Adding requests:   2%|▏         | 74/4096 [00:00<00:10, 365.81it/s]
Adding requests:   3%|▎         | 112/4096 [00:00<00:10, 371.18it/s]
Adding requests:   4%|▎         | 150/4096 [00:00<00:10, 371.41it/s]
Adding requests:   5%|▍         | 188/4096 [00:00<00:10, 373.48it/s]
Adding requests:   6%|▌         | 227/4096 [00:00<00:10, 377.87it/s]
Adding requests:   7%|▋         | 268/4096 [00:00<00:09, 386.15it/s]
Adding requests:   7%|▋         | 307/4096 [00:00<00:09, 383.38it/s]
Adding requests:   8%|▊         | 347/4096 [00:00<00:09, 386.30it/s]
Adding requests:   9%|▉         | 388/4096 [00:01<00:09, 392.76it/s]
Adding requests:  10%|█         | 428/4096 [00:01<00:09, 392.67it/s]
Adding requests:  11%|█▏        | 468/4096 [00:01<00:09, 394.18it/s]
Adding requests:  12%|█▏        | 508/4096 [00:01<00:09, 388.26it/s]
Adding requests:  13%|█▎        | 547/4096 [00:01<00:09, 375.58it/s]
Adding requests:  14%|█▍        | 588/4096 [00:01<00:09, 385.01it/s]
Adding requests:  15%|█▌        | 627/4096 [00:01<00:09, 382.50it/s]
Adding requests:  16%|█▋        | 669/4096 [00:01<00:08, 391.43it/s]
Adding requests:  17%|█▋        | 709/4096 [00:01<00:08, 393.89it/s]
Adding requests:  18%|█▊        | 749/4096 [00:01<00:08, 383.91it/s]
Adding requests:  19%|█▉        | 788/4096 [00:02<00:08, 382.04it/s]
Adding requests:  20%|██        | 827/4096 [00:02<00:08, 381.19it/s]
Adding requests:  21%|██        | 866/4096 [00:02<00:08, 382.91it/s]
Adding requests:  22%|██▏       | 905/4096 [00:02<00:08, 381.16it/s]
Adding requests:  23%|██▎       | 944/4096 [00:02<00:08, 382.33it/s]
Adding requests:  24%|██▍       | 985/4096 [00:02<00:07, 389.19it/s]
Adding requests:  25%|██▌       | 1026/4096 [00:02<00:07, 392.86it/s]
Adding requests:  26%|██▌       | 1067/4096 [00:02<00:07, 397.74it/s]
Adding requests:  27%|██▋       | 1107/4096 [00:02<00:07, 392.91it/s]
Adding requests:  28%|██▊       | 1147/4096 [00:02<00:07, 391.25it/s]
Adding requests:  29%|██▉       | 1187/4096 [00:03<00:07, 378.88it/s]
Adding requests:  30%|██▉       | 1226/4096 [00:03<00:07, 381.81it/s]
Adding requests:  31%|███       | 1265/4096 [00:03<00:07, 381.47it/s]
Adding requests:  32%|███▏      | 1304/4096 [00:03<00:07, 381.79it/s]
Adding requests:  33%|███▎      | 1344/4096 [00:03<00:07, 386.51it/s]
Adding requests:  34%|███▍      | 1383/4096 [00:03<00:07, 383.97it/s]
Adding requests:  35%|███▍      | 1423/4096 [00:03<00:06, 385.81it/s]
Adding requests:  36%|███▌      | 1463/4096 [00:03<00:06, 388.62it/s]
Adding requests:  37%|███▋      | 1504/4096 [00:03<00:06, 394.38it/s]
Adding requests:  38%|███▊      | 1545/4096 [00:04<00:06, 396.12it/s]
Adding requests:  39%|███▊      | 1585/4096 [00:04<00:06, 391.59it/s]
Adding requests:  40%|███▉      | 1628/4096 [00:04<00:06, 402.60it/s]
Adding requests:  41%|████      | 1669/4096 [00:04<00:06, 402.57it/s]
Adding requests:  42%|████▏     | 1710/4096 [00:04<00:05, 397.79it/s]
Adding requests:  43%|████▎     | 1750/4096 [00:04<00:06, 390.96it/s]
Adding requests:  44%|████▎     | 1790/4096 [00:04<00:05, 387.96it/s]
Adding requests:  45%|████▍     | 1833/4096 [00:04<00:05, 397.58it/s]
Adding requests:  46%|████▌     | 1873/4096 [00:04<00:05, 396.25it/s]
Adding requests:  47%|████▋     | 1915/4096 [00:04<00:05, 401.65it/s]
Adding requests:  48%|████▊     | 1957/4096 [00:05<00:05, 403.81it/s]
Adding requests:  49%|████▉     | 1998/4096 [00:05<00:05, 399.54it/s]
Adding requests:  50%|████▉     | 2040/4096 [00:05<00:05, 404.08it/s]
Adding requests:  51%|█████     | 2081/4096 [00:05<00:05, 398.14it/s]
Adding requests:  52%|█████▏    | 2121/4096 [00:05<00:05, 391.42it/s]
Adding requests:  53%|█████▎    | 2161/4096 [00:05<00:04, 389.62it/s]
Adding requests:  54%|█████▎    | 2201/4096 [00:05<00:04, 388.64it/s]
Adding requests:  55%|█████▍    | 2244/4096 [00:05<00:04, 398.29it/s]
Adding requests:  56%|█████▌    | 2285/4096 [00:05<00:04, 399.42it/s]
Adding requests:  57%|█████▋    | 2326/4096 [00:05<00:04, 400.62it/s]
Adding requests:  58%|█████▊    | 2367/4096 [00:06<00:04, 393.37it/s]
Adding requests:  59%|█████▉    | 2407/4096 [00:06<00:04, 372.87it/s]
Adding requests:  60%|█████▉    | 2445/4096 [00:06<00:04, 374.86it/s]
Adding requests:  61%|██████    | 2483/4096 [00:06<00:04, 375.48it/s]
Adding requests:  62%|██████▏   | 2522/4096 [00:06<00:04, 376.93it/s]
Adding requests:  62%|██████▎   | 2560/4096 [00:06<00:04, 375.88it/s]
Adding requests:  63%|██████▎   | 2600/4096 [00:06<00:03, 380.86it/s]
Adding requests:  64%|██████▍   | 2639/4096 [00:06<00:03, 381.21it/s]
Adding requests:  65%|██████▌   | 2680/4096 [00:06<00:03, 388.64it/s]
Adding requests:  66%|██████▋   | 2721/4096 [00:07<00:03, 394.84it/s]
Adding requests:  67%|██████▋   | 2762/4096 [00:07<00:03, 396.94it/s]
Adding requests:  68%|██████▊   | 2802/4096 [00:07<00:03, 396.99it/s]
Adding requests:  69%|██████▉   | 2842/4096 [00:07<00:03, 391.28it/s]
Adding requests:  70%|███████   | 2882/4096 [00:07<00:03, 385.20it/s]
Adding requests:  71%|███████▏  | 2921/4096 [00:07<00:03, 385.25it/s]
Adding requests:  72%|███████▏  | 2960/4096 [00:07<00:02, 380.40it/s]
Adding requests:  73%|███████▎  | 3001/4096 [00:07<00:02, 386.83it/s]
Adding requests:  74%|███████▍  | 3042/4096 [00:07<00:02, 391.20it/s]
Adding requests:  75%|███████▌  | 3082/4096 [00:07<00:02, 392.06it/s]
Adding requests:  76%|███████▌  | 3123/4096 [00:08<00:02, 396.21it/s]
Adding requests:  77%|███████▋  | 3163/4096 [00:08<00:02, 387.45it/s]
Adding requests:  78%|███████▊  | 3202/4096 [00:08<00:02, 380.46it/s]
Adding requests:  79%|███████▉  | 3241/4096 [00:08<00:02, 377.69it/s]
Adding requests:  80%|████████  | 3281/4096 [00:08<00:02, 381.67it/s]
Adding requests:  81%|████████  | 3320/4096 [00:08<00:02, 379.96it/s]
Adding requests:  82%|████████▏ | 3359/4096 [00:08<00:01, 378.51it/s]
Adding requests:  83%|████████▎ | 3397/4096 [00:08<00:01, 376.36it/s]
Adding requests:  84%|████████▍ | 3435/4096 [00:08<00:01, 376.81it/s]
Adding requests:  85%|████████▍ | 3473/4096 [00:08<00:01, 372.25it/s]
Adding requests:  86%|████████▌ | 3514/4096 [00:09<00:01, 383.26it/s]
Adding requests:  87%|████████▋ | 3554/4096 [00:09<00:01, 387.62it/s]
Adding requests:  88%|████████▊ | 3593/4096 [00:09<00:01, 376.90it/s]
Adding requests:  89%|████████▊ | 3631/4096 [00:09<00:01, 368.97it/s]
Adding requests:  90%|████████▉ | 3669/4096 [00:09<00:01, 371.71it/s]
Adding requests:  91%|█████████ | 3710/4096 [00:09<00:01, 380.95it/s]
Adding requests:  92%|█████████▏| 3749/4096 [00:09<00:00, 369.88it/s]
Adding requests:  93%|█████████▎| 3789/4096 [00:09<00:00, 377.82it/s]
Adding requests:  93%|█████████▎| 3829/4096 [00:09<00:00, 383.05it/s]
Adding requests:  95%|█████████▍| 3871/4096 [00:10<00:00, 392.77it/s]
Adding requests:  95%|█████████▌| 3911/4096 [00:10<00:00, 389.18it/s]
Adding requests:  96%|█████████▋| 3951/4096 [00:10<00:00, 392.23it/s]
Adding requests:  97%|█████████▋| 3991/4096 [00:10<00:00, 388.98it/s]
Adding requests:  98%|█████████▊| 4030/4096 [00:10<00:00, 382.50it/s]
Adding requests:  99%|█████████▉| 4071/4096 [00:10<00:00, 388.05it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 386.63it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 540/4096 [00:00<00:00, 4578.89it/s, est. speed input: 4689710.66 toks/s, output: 4579.12 toks/s]
Processed prompts:  24%|██▍       | 998/4096 [00:08<00:31, 98.89it/s, est. speed input: 120380.49 toks/s, output: 117.56 toks/s]    
Processed prompts:  29%|██▉       | 1190/4096 [00:12<00:35, 81.67it/s, est. speed input: 100810.29 toks/s, output: 98.45 toks/s]
Processed prompts:  32%|███▏      | 1297/4096 [00:13<00:36, 76.97it/s, est. speed input: 95656.32 toks/s, output: 93.41 toks/s] 
Processed prompts:  33%|███▎      | 1364/4096 [00:15<00:37, 73.30it/s, est. speed input: 92528.05 toks/s, output: 90.36 toks/s]
Processed prompts:  34%|███▍      | 1409/4096 [00:16<00:40, 66.26it/s, est. speed input: 88489.34 toks/s, output: 86.42 toks/s]
Processed prompts:  35%|███▌      | 1440/4096 [00:16<00:41, 64.41it/s, est. speed input: 87196.95 toks/s, output: 85.15 toks/s]
Processed prompts:  36%|███▌      | 1468/4096 [00:17<00:42, 61.68it/s, est. speed input: 85810.58 toks/s, output: 83.80 toks/s]
Processed prompts:  37%|███▋      | 1500/4096 [00:18<00:43, 60.18it/s, est. speed input: 84764.45 toks/s, output: 82.78 toks/s]
Processed prompts:  37%|███▋      | 1532/4096 [00:18<00:43, 58.68it/s, est. speed input: 83765.75 toks/s, output: 81.80 toks/s]
Processed prompts:  38%|███▊      | 1564/4096 [00:19<00:44, 57.36it/s, est. speed input: 82833.85 toks/s, output: 80.89 toks/s]
Processed prompts:  39%|███▉      | 1596/4096 [00:19<00:44, 56.74it/s, est. speed input: 82045.96 toks/s, output: 80.12 toks/s]
Processed prompts:  40%|███▉      | 1628/4096 [00:20<00:44, 55.68it/s, est. speed input: 81212.67 toks/s, output: 79.31 toks/s]
Processed prompts:  41%|████      | 1660/4096 [00:21<00:44, 54.95it/s, est. speed input: 80441.02 toks/s, output: 78.56 toks/s]
Processed prompts:  41%|████▏     | 1692/4096 [00:21<00:44, 54.35it/s, est. speed input: 79705.87 toks/s, output: 77.84 toks/s]
Processed prompts:  42%|████▏     | 1724/4096 [00:22<00:43, 53.93it/s, est. speed input: 79013.73 toks/s, output: 77.16 toks/s]
Processed prompts:  43%|████▎     | 1756/4096 [00:22<00:43, 53.64it/s, est. speed input: 78359.71 toks/s, output: 76.52 toks/s]
Processed prompts:  44%|████▎     | 1788/4096 [00:23<00:43, 53.37it/s, est. speed input: 77731.75 toks/s, output: 75.91 toks/s]
Processed prompts:  44%|████▍     | 1820/4096 [00:24<00:42, 53.22it/s, est. speed input: 77140.04 toks/s, output: 75.33 toks/s]
Processed prompts:  45%|████▌     | 1852/4096 [00:24<00:41, 53.64it/s, est. speed input: 76639.22 toks/s, output: 74.84 toks/s]
Processed prompts:  46%|████▌     | 1884/4096 [00:25<00:41, 53.39it/s, est. speed input: 76100.33 toks/s, output: 74.32 toks/s]
Processed prompts:  47%|████▋     | 1916/4096 [00:25<00:40, 53.24it/s, est. speed input: 75588.17 toks/s, output: 73.82 toks/s]
Processed prompts:  48%|████▊     | 1948/4096 [00:26<00:40, 53.65it/s, est. speed input: 75154.81 toks/s, output: 73.39 toks/s]
Processed prompts:  48%|████▊     | 1980/4096 [00:27<00:39, 53.40it/s, est. speed input: 74685.42 toks/s, output: 72.93 toks/s]
Processed prompts:  49%|████▉     | 2012/4096 [00:27<00:39, 53.26it/s, est. speed input: 74239.47 toks/s, output: 72.50 toks/s]
Processed prompts:  50%|████▉     | 2044/4096 [00:28<00:38, 53.14it/s, est. speed input: 73809.90 toks/s, output: 72.08 toks/s]
Processed prompts:  51%|█████     | 2076/4096 [00:28<00:38, 53.09it/s, est. speed input: 73402.17 toks/s, output: 71.68 toks/s]
Processed prompts:  51%|█████▏    | 2108/4096 [00:29<00:37, 52.99it/s, est. speed input: 73004.76 toks/s, output: 71.29 toks/s]
Processed prompts:  52%|█████▏    | 2140/4096 [00:30<00:36, 52.96it/s, est. speed input: 72626.45 toks/s, output: 70.92 toks/s]
Processed prompts:  53%|█████▎    | 2172/4096 [00:30<00:36, 52.97it/s, est. speed input: 72266.00 toks/s, output: 70.57 toks/s]
Processed prompts:  54%|█████▍    | 2204/4096 [00:31<00:35, 52.94it/s, est. speed input: 71916.89 toks/s, output: 70.23 toks/s]
Processed prompts:  55%|█████▍    | 2236/4096 [00:31<00:34, 53.90it/s, est. speed input: 71662.24 toks/s, output: 69.98 toks/s]
Processed prompts:  55%|█████▌    | 2268/4096 [00:32<00:34, 53.68it/s, est. speed input: 71343.96 toks/s, output: 69.67 toks/s]
Processed prompts:  56%|█████▌    | 2300/4096 [00:33<00:33, 53.92it/s, est. speed input: 71068.52 toks/s, output: 69.40 toks/s]
Processed prompts:  57%|█████▋    | 2332/4096 [00:33<00:32, 54.10it/s, est. speed input: 70803.65 toks/s, output: 69.14 toks/s]
Processed prompts:  58%|█████▊    | 2364/4096 [00:34<00:31, 54.71it/s, est. speed input: 70583.44 toks/s, output: 68.93 toks/s]
Processed prompts:  58%|█████▊    | 2396/4096 [00:34<00:31, 54.26it/s, est. speed input: 70306.62 toks/s, output: 68.66 toks/s]
Processed prompts:  59%|█████▉    | 2428/4096 [00:35<00:30, 53.90it/s, est. speed input: 70035.27 toks/s, output: 68.39 toks/s]
Processed prompts:  60%|██████    | 2460/4096 [00:36<00:30, 53.69it/s, est. speed input: 69776.74 toks/s, output: 68.14 toks/s]
Processed prompts:  61%|██████    | 2492/4096 [00:36<00:29, 53.92it/s, est. speed input: 69552.16 toks/s, output: 67.92 toks/s]
Processed prompts:  62%|██████▏   | 2524/4096 [00:37<00:29, 53.70it/s, est. speed input: 69308.82 toks/s, output: 67.68 toks/s]
Processed prompts:  62%|██████▏   | 2556/4096 [00:37<00:28, 53.47it/s, est. speed input: 69068.19 toks/s, output: 67.45 toks/s]
Processed prompts:  63%|██████▎   | 2588/4096 [00:38<00:28, 53.75it/s, est. speed input: 68863.94 toks/s, output: 67.25 toks/s]
Processed prompts:  64%|██████▍   | 2620/4096 [00:39<00:27, 53.59it/s, est. speed input: 68643.28 toks/s, output: 67.03 toks/s]
Processed prompts:  65%|██████▍   | 2652/4096 [00:39<00:27, 53.37it/s, est. speed input: 68421.91 toks/s, output: 66.82 toks/s]
Processed prompts:  66%|██████▌   | 2684/4096 [00:40<00:26, 53.26it/s, est. speed input: 68210.10 toks/s, output: 66.61 toks/s]
Processed prompts:  66%|██████▋   | 2716/4096 [00:40<00:25, 53.17it/s, est. speed input: 68003.53 toks/s, output: 66.41 toks/s]
Processed prompts:  67%|██████▋   | 2748/4096 [00:41<00:25, 53.10it/s, est. speed input: 67802.30 toks/s, output: 66.21 toks/s]
Processed prompts:  68%|██████▊   | 2780/4096 [00:42<00:24, 52.98it/s, est. speed input: 67603.28 toks/s, output: 66.02 toks/s]
Processed prompts:  69%|██████▊   | 2812/4096 [00:42<00:24, 52.97it/s, est. speed input: 67413.51 toks/s, output: 65.83 toks/s]
Processed prompts:  69%|██████▉   | 2844/4096 [00:43<00:23, 52.95it/s, est. speed input: 67228.47 toks/s, output: 65.65 toks/s]
Processed prompts:  70%|███████   | 2876/4096 [00:43<00:23, 52.92it/s, est. speed input: 67047.66 toks/s, output: 65.48 toks/s]
Processed prompts:  71%|███████   | 2908/4096 [00:44<00:22, 52.92it/s, est. speed input: 66873.38 toks/s, output: 65.31 toks/s]
Processed prompts:  72%|███████▏  | 2940/4096 [00:45<00:21, 52.90it/s, est. speed input: 66702.30 toks/s, output: 65.14 toks/s]
Processed prompts:  73%|███████▎  | 2972/4096 [00:45<00:21, 52.87it/s, est. speed input: 66535.04 toks/s, output: 64.98 toks/s]
Processed prompts:  73%|███████▎  | 3004/4096 [00:46<00:20, 52.87it/s, est. speed input: 66373.09 toks/s, output: 64.82 toks/s]
Processed prompts:  74%|███████▍  | 3036/4096 [00:46<00:20, 52.90it/s, est. speed input: 66216.80 toks/s, output: 64.66 toks/s]
Processed prompts:  75%|███████▍  | 3068/4096 [00:47<00:19, 52.88it/s, est. speed input: 66062.48 toks/s, output: 64.51 toks/s]
Processed prompts:  76%|███████▌  | 3100/4096 [00:48<00:18, 52.84it/s, est. speed input: 65910.92 toks/s, output: 64.37 toks/s]
Processed prompts:  76%|███████▋  | 3132/4096 [00:48<00:18, 53.29it/s, est. speed input: 65787.39 toks/s, output: 64.25 toks/s]
Processed prompts:  77%|███████▋  | 3164/4096 [00:49<00:17, 53.25it/s, est. speed input: 65648.76 toks/s, output: 64.11 toks/s]
Processed prompts:  78%|███████▊  | 3196/4096 [00:49<00:16, 53.14it/s, est. speed input: 65509.57 toks/s, output: 63.97 toks/s]
Processed prompts:  79%|███████▉  | 3228/4096 [00:50<00:16, 53.11it/s, est. speed input: 65375.76 toks/s, output: 63.84 toks/s]
Processed prompts:  80%|███████▉  | 3260/4096 [00:51<00:15, 53.08it/s, est. speed input: 65244.94 toks/s, output: 63.72 toks/s]
Processed prompts:  80%|████████  | 3292/4096 [00:51<00:15, 52.97it/s, est. speed input: 65113.03 toks/s, output: 63.59 toks/s]
Processed prompts:  81%|████████  | 3324/4096 [00:52<00:14, 52.98it/s, est. speed input: 64988.11 toks/s, output: 63.46 toks/s]
Processed prompts:  82%|████████▏ | 3356/4096 [00:52<00:13, 52.96it/s, est. speed input: 64864.66 toks/s, output: 63.34 toks/s]
Processed prompts:  83%|████████▎ | 3388/4096 [00:53<00:13, 52.97it/s, est. speed input: 64745.18 toks/s, output: 63.23 toks/s]
Processed prompts:  83%|████████▎ | 3420/4096 [00:54<00:12, 52.91it/s, est. speed input: 64625.21 toks/s, output: 63.11 toks/s]
Processed prompts:  84%|████████▍ | 3452/4096 [00:54<00:12, 52.91it/s, est. speed input: 64510.02 toks/s, output: 63.00 toks/s]
Processed prompts:  85%|████████▌ | 3484/4096 [00:55<00:11, 53.79it/s, est. speed input: 64435.46 toks/s, output: 62.93 toks/s]
Processed prompts:  86%|████████▌ | 3516/4096 [00:55<00:10, 53.55it/s, est. speed input: 64326.01 toks/s, output: 62.82 toks/s]
Processed prompts:  87%|████████▋ | 3548/4096 [00:56<00:10, 53.28it/s, est. speed input: 64214.43 toks/s, output: 62.71 toks/s]
Processed prompts:  87%|████████▋ | 3580/4096 [00:57<00:09, 53.14it/s, est. speed input: 64106.80 toks/s, output: 62.60 toks/s]
Processed prompts:  88%|████████▊ | 3612/4096 [00:57<00:09, 53.02it/s, est. speed input: 64000.74 toks/s, output: 62.50 toks/s]
Processed prompts:  89%|████████▉ | 3644/4096 [00:58<00:08, 52.93it/s, est. speed input: 63896.93 toks/s, output: 62.40 toks/s]
Processed prompts:  90%|████████▉ | 3676/4096 [00:59<00:07, 52.75it/s, est. speed input: 63789.86 toks/s, output: 62.29 toks/s]
Processed prompts:  91%|█████████ | 3708/4096 [00:59<00:07, 52.66it/s, est. speed input: 63686.89 toks/s, output: 62.19 toks/s]
Processed prompts:  91%|█████████▏| 3740/4096 [01:00<00:06, 53.21it/s, est. speed input: 63610.31 toks/s, output: 62.12 toks/s]
Processed prompts:  92%|█████████▏| 3772/4096 [01:00<00:06, 53.08it/s, est. speed input: 63515.01 toks/s, output: 62.03 toks/s]
Processed prompts:  93%|█████████▎| 3804/4096 [01:01<00:05, 53.00it/s, est. speed input: 63422.10 toks/s, output: 61.94 toks/s]
Processed prompts:  94%|█████████▎| 3836/4096 [01:02<00:04, 53.30it/s, est. speed input: 63344.64 toks/s, output: 61.86 toks/s]
Processed prompts:  94%|█████████▍| 3868/4096 [01:02<00:04, 53.19it/s, est. speed input: 63256.24 toks/s, output: 61.77 toks/s]
Processed prompts:  95%|█████████▌| 3900/4096 [01:03<00:03, 53.06it/s, est. speed input: 63167.80 toks/s, output: 61.69 toks/s]
Processed prompts:  96%|█████████▌| 3932/4096 [01:03<00:03, 53.03it/s, est. speed input: 63083.10 toks/s, output: 61.60 toks/s]
Processed prompts:  97%|█████████▋| 3964/4096 [01:04<00:02, 52.93it/s, est. speed input: 62997.27 toks/s, output: 61.52 toks/s]
Processed prompts:  98%|█████████▊| 3996/4096 [01:05<00:01, 52.91it/s, est. speed input: 62914.80 toks/s, output: 61.44 toks/s]
Processed prompts:  98%|█████████▊| 4028/4096 [01:05<00:01, 53.25it/s, est. speed input: 62846.44 toks/s, output: 61.37 toks/s]
Processed prompts:  99%|█████████▉| 4060/4096 [01:06<00:00, 53.02it/s, est. speed input: 62762.78 toks/s, output: 61.29 toks/s]
Processed prompts: 100%|█████████▉| 4092/4096 [01:06<00:00, 70.64it/s, est. speed input: 63160.58 toks/s, output: 61.68 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:06<00:00, 70.64it/s, est. speed input: 63222.14 toks/s, output: 61.74 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:06<00:00, 61.74it/s, est. speed input: 63222.14 toks/s, output: 61.74 toks/s]
[rank0]:[W128 00:03:41.258659924 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 150.5s

测试结果:
  Requests/s:   53.24
  Tokens/s:     54567.94
  Total Reqs:   4096
  Elapsed:      76.94s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     54514.70


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cublaslt/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,18.0100,9239.1295,7.1072
1024,1024,1,128,128,17.8784,18325.3421,7.1595
2048,1024,2,256,128,36.3516,37260.3413,7.0423
4096,1024,4,512,128,49.6417,50882.7419,10.3139
8192,1024,8,1024,128,51.9948,53294.6844,19.6943
16384,1024,16,2048,128,52.9705,54294.7217,38.6631
32768,1024,32,4096,128,53.2370,54567.9417,76.9389

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:03:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2215329) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2215329) WARNING 01-28 00:04:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 18.38 requests/s, 9428.83 total tokens/s, 18.38 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:03:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:03:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:03:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:03:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:03:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:03:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:03:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:03:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:03:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:03:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:03:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:03:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:03:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:03:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:03:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:03:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:03:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:03:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:03:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:03:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:03:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:03:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:03:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:03:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:03:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:03:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:03:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:03:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2215329) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2215329) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.95it/s]
(EngineCore_DP0 pid=2215329) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.95it/s]
(EngineCore_DP0 pid=2215329) 
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=2215329) [2026-01-28 00:04:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=2215329) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.72it/s]
(EngineCore_DP0 pid=2215329) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.85it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 624.66it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 637.91it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 635.41it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:45,  2.81it/s, est. speed input: 1440.22 toks/s, output: 2.81 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:16,  7.67it/s, est. speed input: 3348.05 toks/s, output: 6.54 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:10, 11.19it/s, est. speed input: 4572.29 toks/s, output: 8.93 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:08, 13.60it/s, est. speed input: 5398.42 toks/s, output: 10.54 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:07, 15.41it/s, est. speed input: 6023.25 toks/s, output: 11.76 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 16.55it/s, est. speed input: 6478.46 toks/s, output: 12.65 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:06, 17.51it/s, est. speed input: 6859.87 toks/s, output: 13.40 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:06, 18.19it/s, est. speed input: 7169.16 toks/s, output: 14.00 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:05, 18.97it/s, est. speed input: 7557.12 toks/s, output: 14.76 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:05, 19.41it/s, est. speed input: 7859.20 toks/s, output: 15.35 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:05, 19.82it/s, est. speed input: 8119.34 toks/s, output: 15.86 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 19.52it/s, est. speed input: 8269.45 toks/s, output: 16.15 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:04, 19.76it/s, est. speed input: 8441.66 toks/s, output: 16.49 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:04, 19.75it/s, est. speed input: 8528.82 toks/s, output: 16.66 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:04, 19.94it/s, est. speed input: 8663.23 toks/s, output: 16.92 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:04, 19.87it/s, est. speed input: 8728.37 toks/s, output: 17.05 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:04, 19.74it/s, est. speed input: 8782.56 toks/s, output: 17.15 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:04, 19.75it/s, est. speed input: 8839.71 toks/s, output: 17.26 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:04, 19.85it/s, est. speed input: 8923.40 toks/s, output: 17.43 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 19.37it/s, est. speed input: 8938.40 toks/s, output: 17.46 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 19.44it/s, est. speed input: 8979.49 toks/s, output: 17.54 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:03, 19.75it/s, est. speed input: 9052.29 toks/s, output: 17.68 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:03, 19.83it/s, est. speed input: 9110.81 toks/s, output: 17.79 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:03, 19.49it/s, est. speed input: 9123.50 toks/s, output: 17.82 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:03, 19.80it/s, est. speed input: 9183.06 toks/s, output: 17.94 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 20.04it/s, est. speed input: 9239.97 toks/s, output: 18.05 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:03, 19.98it/s, est. speed input: 9265.12 toks/s, output: 18.10 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 19.89it/s, est. speed input: 9287.25 toks/s, output: 18.14 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:03, 19.88it/s, est. speed input: 9310.64 toks/s, output: 18.18 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:03<00:02, 20.06it/s, est. speed input: 9353.59 toks/s, output: 18.27 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:02, 19.95it/s, est. speed input: 9381.68 toks/s, output: 18.32 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 20.08it/s, est. speed input: 9418.28 toks/s, output: 18.39 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:02, 20.03it/s, est. speed input: 9445.41 toks/s, output: 18.45 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 19.81it/s, est. speed input: 9461.29 toks/s, output: 18.48 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:04<00:02, 19.89it/s, est. speed input: 9487.57 toks/s, output: 18.53 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:04<00:01, 20.01it/s, est. speed input: 9515.32 toks/s, output: 18.58 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:04<00:01, 20.00it/s, est. speed input: 9537.24 toks/s, output: 18.63 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 20.04it/s, est. speed input: 9559.88 toks/s, output: 18.67 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 20.11it/s, est. speed input: 9582.78 toks/s, output: 18.72 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 20.10it/s, est. speed input: 9602.27 toks/s, output: 18.75 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:05<00:01, 20.24it/s, est. speed input: 9626.87 toks/s, output: 18.80 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:05<00:01, 19.75it/s, est. speed input: 9625.42 toks/s, output: 18.80 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:05<00:00, 19.88it/s, est. speed input: 9643.39 toks/s, output: 18.83 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:05<00:00, 19.87it/s, est. speed input: 9652.25 toks/s, output: 18.85 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 20.09it/s, est. speed input: 9673.20 toks/s, output: 18.89 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:06<00:00, 19.62it/s, est. speed input: 9670.37 toks/s, output: 18.89 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:06<00:00, 19.60it/s, est. speed input: 9675.63 toks/s, output: 18.90 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:06<00:00, 19.58it/s, est. speed input: 9680.76 toks/s, output: 18.91 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:06<00:00, 19.39it/s, est. speed input: 9680.30 toks/s, output: 18.91 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:06<00:00, 19.53it/s, est. speed input: 9688.05 toks/s, output: 18.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 19.52it/s, est. speed input: 9692.68 toks/s, output: 18.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 19.52it/s, est. speed input: 9692.68 toks/s, output: 18.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 18.93it/s, est. speed input: 9692.68 toks/s, output: 18.93 toks/s]
[rank0]:[W128 00:04:37.691777253 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 55.2s

测试结果:
  Requests/s:   18.38
  Tokens/s:     9428.83
  Total Reqs:   128
  Elapsed:      6.96s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     9410.45

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:04:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2216463) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2216463) WARNING 01-28 00:05:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 18.43 requests/s, 18888.14 total tokens/s, 18.43 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:04:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:04:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:04:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:04:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:04:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:04:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:04:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:04:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:04:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:04:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:04:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:04:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:04:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:04:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:04:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:04:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:04:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:04:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:04:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:04:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:04:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:04:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:04:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:04:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:04:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:04:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:04:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:04:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2216463) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2216463) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]
(EngineCore_DP0 pid=2216463) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]
(EngineCore_DP0 pid=2216463) 
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=2216463) [2026-01-28 00:04:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=2216463) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.82it/s]
(EngineCore_DP0 pid=2216463) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.88it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  26%|██▌       | 33/128 [00:00<00:00, 326.84it/s]
Adding requests:  54%|█████▍    | 69/128 [00:00<00:00, 342.02it/s]
Adding requests:  82%|████████▏ | 105/128 [00:00<00:00, 347.02it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 347.32it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:08, 14.33it/s, est. speed input: 14677.05 toks/s, output: 14.33 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:07, 16.48it/s, est. speed input: 16506.88 toks/s, output: 16.12 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:07, 17.30it/s, est. speed input: 17211.40 toks/s, output: 16.81 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:06, 17.76it/s, est. speed input: 17620.91 toks/s, output: 17.21 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:06, 18.03it/s, est. speed input: 17875.59 toks/s, output: 17.46 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:06, 18.23it/s, est. speed input: 18062.64 toks/s, output: 17.64 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:06, 18.34it/s, est. speed input: 18193.31 toks/s, output: 17.77 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:06, 18.31it/s, est. speed input: 18255.21 toks/s, output: 17.83 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:06, 18.32it/s, est. speed input: 18312.29 toks/s, output: 17.88 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:05, 18.38it/s, est. speed input: 18373.95 toks/s, output: 17.94 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:05, 18.44it/s, est. speed input: 18430.05 toks/s, output: 18.00 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:05, 18.52it/s, est. speed input: 18489.33 toks/s, output: 18.06 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:05, 18.54it/s, est. speed input: 18529.24 toks/s, output: 18.09 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 18.60it/s, est. speed input: 18574.72 toks/s, output: 18.14 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 18.67it/s, est. speed input: 18621.08 toks/s, output: 18.18 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:05, 18.68it/s, est. speed input: 18653.86 toks/s, output: 18.22 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:05, 18.69it/s, est. speed input: 18682.13 toks/s, output: 18.24 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:04, 18.68it/s, est. speed input: 18706.10 toks/s, output: 18.27 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:04, 18.68it/s, est. speed input: 18727.91 toks/s, output: 18.29 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:04, 18.98it/s, est. speed input: 18796.02 toks/s, output: 18.36 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 19.70it/s, est. speed input: 18958.12 toks/s, output: 18.51 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 19.82it/s, est. speed input: 19053.67 toks/s, output: 18.61 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 19.70it/s, est. speed input: 19083.36 toks/s, output: 18.64 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:04, 19.45it/s, est. speed input: 19090.20 toks/s, output: 18.64 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:03, 19.46it/s, est. speed input: 19122.83 toks/s, output: 18.67 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:02<00:03, 19.38it/s, est. speed input: 19141.28 toks/s, output: 18.69 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:02<00:03, 19.45it/s, est. speed input: 19173.86 toks/s, output: 18.72 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:03, 19.91it/s, est. speed input: 19266.85 toks/s, output: 18.82 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 20.15it/s, est. speed input: 19348.24 toks/s, output: 18.89 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 20.36it/s, est. speed input: 19430.64 toks/s, output: 18.98 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:03, 19.93it/s, est. speed input: 19435.27 toks/s, output: 18.98 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:03<00:02, 19.74it/s, est. speed input: 19439.87 toks/s, output: 18.98 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:03<00:02, 20.07it/s, est. speed input: 19508.68 toks/s, output: 19.05 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:03<00:02, 20.33it/s, est. speed input: 19576.88 toks/s, output: 19.12 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 20.13it/s, est. speed input: 19598.89 toks/s, output: 19.14 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 19.82it/s, est. speed input: 19600.30 toks/s, output: 19.14 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:04<00:02, 20.19it/s, est. speed input: 19664.21 toks/s, output: 19.20 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:04<00:01, 20.04it/s, est. speed input: 19680.80 toks/s, output: 19.22 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:04<00:01, 20.18it/s, est. speed input: 19722.03 toks/s, output: 19.26 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:04<00:01, 19.81it/s, est. speed input: 19713.69 toks/s, output: 19.25 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:04<00:01, 19.52it/s, est. speed input: 19698.73 toks/s, output: 19.24 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 19.49it/s, est. speed input: 19702.31 toks/s, output: 19.24 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 19.88it/s, est. speed input: 19743.20 toks/s, output: 19.28 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:05<00:01, 20.11it/s, est. speed input: 19779.17 toks/s, output: 19.32 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:05<00:01, 19.79it/s, est. speed input: 19774.33 toks/s, output: 19.31 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:05<00:00, 20.04it/s, est. speed input: 19808.77 toks/s, output: 19.34 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:05<00:00, 20.09it/s, est. speed input: 19831.07 toks/s, output: 19.37 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:05<00:00, 20.11it/s, est. speed input: 19850.78 toks/s, output: 19.39 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:06<00:00, 20.18it/s, est. speed input: 19874.64 toks/s, output: 19.41 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:06<00:00, 20.02it/s, est. speed input: 19881.15 toks/s, output: 19.42 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:06<00:00, 20.11it/s, est. speed input: 19902.16 toks/s, output: 19.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.29it/s, est. speed input: 19931.05 toks/s, output: 19.46 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.29it/s, est. speed input: 19931.05 toks/s, output: 19.46 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 19.46it/s, est. speed input: 19931.05 toks/s, output: 19.46 toks/s]
[rank0]:[W128 00:05:32.640029745 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.9s

测试结果:
  Requests/s:   18.43
  Tokens/s:     18888.14
  Total Reqs:   128
  Elapsed:      6.95s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     18869.71

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:05:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2217490) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2217490) WARNING 01-28 00:06:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.40 requests/s, 38336.80 total tokens/s, 37.40 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 00:05:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:05:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:05:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:05:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:05:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:05:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:05:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:05:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:05:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:05:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:05:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:05:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:05:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:05:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:05:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:05:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:05:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:05:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:05:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:05:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:05:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:05:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:05:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:05:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:05:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:05:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:05:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:05:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:51] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2217490) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2217490) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.87it/s]
(EngineCore_DP0 pid=2217490) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.87it/s]
(EngineCore_DP0 pid=2217490) 
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=2217490) [2026-01-28 00:05:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=2217490) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.56it/s]
(EngineCore_DP0 pid=2217490) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  8.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.10it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▎        | 35/256 [00:00<00:00, 348.88it/s]
Adding requests:  29%|██▉       | 74/256 [00:00<00:00, 369.91it/s]
Adding requests:  43%|████▎     | 111/256 [00:00<00:00, 369.48it/s]
Adding requests:  58%|█████▊    | 148/256 [00:00<00:00, 369.33it/s]
Adding requests:  72%|███████▏  | 185/256 [00:00<00:00, 368.58it/s]
Adding requests:  87%|████████▋ | 223/256 [00:00<00:00, 371.69it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 372.04it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:00<00:01, 187.88it/s, est. speed input: 192409.97 toks/s, output: 187.88 toks/s]
Processed prompts:  17%|█▋        | 43/256 [00:00<00:03, 65.16it/s, est. speed input: 74916.91 toks/s, output: 73.16 toks/s]   
Processed prompts:  21%|██        | 54/256 [00:00<00:03, 51.56it/s, est. speed input: 61272.48 toks/s, output: 59.84 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:04, 47.73it/s, est. speed input: 57224.39 toks/s, output: 55.88 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:01<00:04, 45.03it/s, est. speed input: 54744.55 toks/s, output: 53.46 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:04, 43.82it/s, est. speed input: 53310.63 toks/s, output: 52.06 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:04, 42.93it/s, est. speed input: 52182.17 toks/s, output: 50.96 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:01<00:04, 42.04it/s, est. speed input: 51167.22 toks/s, output: 49.97 toks/s]
Processed prompts:  36%|███▌      | 91/256 [00:01<00:03, 43.10it/s, est. speed input: 50985.90 toks/s, output: 49.79 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:01<00:04, 39.46it/s, est. speed input: 49495.39 toks/s, output: 48.33 toks/s]
Processed prompts:  39%|███▉      | 101/256 [00:02<00:03, 41.28it/s, est. speed input: 49444.69 toks/s, output: 48.29 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:03, 38.77it/s, est. speed input: 48425.72 toks/s, output: 47.29 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:03, 39.11it/s, est. speed input: 47946.58 toks/s, output: 46.82 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:02<00:03, 39.07it/s, est. speed input: 47614.53 toks/s, output: 46.50 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:02<00:03, 39.12it/s, est. speed input: 47323.05 toks/s, output: 46.21 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:02<00:03, 38.37it/s, est. speed input: 46916.74 toks/s, output: 45.82 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:02<00:03, 37.83it/s, est. speed input: 46543.32 toks/s, output: 45.45 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:02<00:03, 37.47it/s, est. speed input: 46203.11 toks/s, output: 45.12 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:03<00:03, 37.68it/s, est. speed input: 45959.75 toks/s, output: 44.88 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:02, 38.67it/s, est. speed input: 45745.39 toks/s, output: 44.67 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:02, 38.65it/s, est. speed input: 45548.48 toks/s, output: 44.48 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:02, 38.70it/s, est. speed input: 45373.10 toks/s, output: 44.31 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 38.84it/s, est. speed input: 45219.38 toks/s, output: 44.16 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:03<00:02, 38.81it/s, est. speed input: 45059.43 toks/s, output: 44.00 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:03<00:02, 38.88it/s, est. speed input: 44919.46 toks/s, output: 43.87 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:03<00:02, 39.08it/s, est. speed input: 44801.44 toks/s, output: 43.75 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:03<00:02, 39.05it/s, est. speed input: 44672.77 toks/s, output: 43.63 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 38.54it/s, est. speed input: 44502.65 toks/s, output: 43.46 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:02, 38.72it/s, est. speed input: 44392.35 toks/s, output: 43.35 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:01, 38.47it/s, est. speed input: 44252.22 toks/s, output: 43.21 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 38.53it/s, est. speed input: 44140.95 toks/s, output: 43.11 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 38.91it/s, est. speed input: 44064.44 toks/s, output: 43.03 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:04<00:01, 39.21it/s, est. speed input: 43994.20 toks/s, output: 42.96 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:04<00:01, 38.96it/s, est. speed input: 43888.07 toks/s, output: 42.86 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:04<00:01, 39.11it/s, est. speed input: 43813.38 toks/s, output: 42.79 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:04<00:01, 39.00it/s, est. speed input: 43724.78 toks/s, output: 42.70 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:04<00:01, 38.77it/s, est. speed input: 43628.29 toks/s, output: 42.61 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:05<00:01, 38.32it/s, est. speed input: 43512.39 toks/s, output: 42.49 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:05<00:01, 37.81it/s, est. speed input: 43386.47 toks/s, output: 42.37 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:05<00:00, 37.46it/s, est. speed input: 43265.38 toks/s, output: 42.25 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:05<00:00, 37.36it/s, est. speed input: 43159.49 toks/s, output: 42.15 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:05<00:00, 37.40it/s, est. speed input: 43066.93 toks/s, output: 42.06 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:05<00:00, 38.13it/s, est. speed input: 43027.78 toks/s, output: 42.02 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:05<00:00, 38.53it/s, est. speed input: 42981.95 toks/s, output: 41.97 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:05<00:00, 37.98it/s, est. speed input: 42880.84 toks/s, output: 41.88 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:05<00:00, 37.59it/s, est. speed input: 42783.14 toks/s, output: 41.78 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:05<00:00, 37.27it/s, est. speed input: 42685.62 toks/s, output: 41.69 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:06<00:00, 37.13it/s, est. speed input: 42596.60 toks/s, output: 41.60 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 37.13it/s, est. speed input: 42590.08 toks/s, output: 41.59 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 41.59it/s, est. speed input: 42590.08 toks/s, output: 41.59 toks/s]
[rank0]:[W128 00:06:27.889127616 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 55.2s

测试结果:
  Requests/s:   37.40
  Tokens/s:     38336.80
  Total Reqs:   256
  Elapsed:      6.84s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     38299.39

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:06:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2218507) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2218507) WARNING 01-28 00:06:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 73.35 requests/s, 75178.75 total tokens/s, 73.35 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 00:06:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:06:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:06:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:06:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:06:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:06:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:06:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:06:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:06:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:06:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:06:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:06:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:06:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:06:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:06:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:06:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:06:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:06:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:06:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:06:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:06:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:06:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:06:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:06:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:06:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:06:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:06:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:06:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2218507) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2218507) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=2218507) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=2218507) 
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=2218507) [2026-01-28 00:06:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=2218507) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  9.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 10.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.68it/s]
(EngineCore_DP0 pid=2218507) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  8.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 10.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  9.97it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 35/512 [00:00<00:01, 349.77it/s]
Adding requests:  14%|█▍        | 74/512 [00:00<00:01, 368.56it/s]
Adding requests:  22%|██▏       | 111/512 [00:00<00:01, 364.56it/s]
Adding requests:  29%|██▉       | 148/512 [00:00<00:00, 365.89it/s]
Adding requests:  36%|███▌      | 185/512 [00:00<00:00, 362.58it/s]
Adding requests:  44%|████▍     | 224/512 [00:00<00:00, 371.00it/s]
Adding requests:  51%|█████     | 262/512 [00:00<00:00, 373.61it/s]
Adding requests:  59%|█████▉    | 302/512 [00:00<00:00, 381.02it/s]
Adding requests:  67%|██████▋   | 342/512 [00:00<00:00, 386.28it/s]
Adding requests:  75%|███████▍  | 383/512 [00:01<00:00, 392.18it/s]
Adding requests:  83%|████████▎ | 425/512 [00:01<00:00, 398.38it/s]
Adding requests:  91%|█████████ | 466/512 [00:01<00:00, 400.22it/s]
Adding requests:  99%|█████████▉| 507/512 [00:01<00:00, 398.71it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 384.39it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:00<00:00, 625.80it/s, est. speed input: 640895.39 toks/s, output: 625.82 toks/s]
Processed prompts:  31%|███       | 157/512 [00:00<00:02, 142.33it/s, est. speed input: 169227.68 toks/s, output: 165.26 toks/s]
Processed prompts:  37%|███▋      | 188/512 [00:01<00:02, 114.90it/s, est. speed input: 140411.98 toks/s, output: 137.12 toks/s]
Processed prompts:  41%|████      | 208/512 [00:01<00:02, 104.64it/s, est. speed input: 130309.79 toks/s, output: 127.26 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:01<00:02, 98.11it/s, est. speed input: 124372.45 toks/s, output: 121.46 toks/s] 
Processed prompts:  46%|████▋     | 237/512 [00:02<00:02, 95.06it/s, est. speed input: 121251.78 toks/s, output: 118.41 toks/s]
Processed prompts:  48%|████▊     | 248/512 [00:02<00:02, 89.14it/s, est. speed input: 117469.74 toks/s, output: 114.72 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:02<00:03, 83.02it/s, est. speed input: 113912.11 toks/s, output: 111.24 toks/s]
Processed prompts:  52%|█████▏    | 267/512 [00:02<00:02, 83.08it/s, est. speed input: 112638.99 toks/s, output: 110.00 toks/s]
Processed prompts:  54%|█████▍    | 276/512 [00:02<00:02, 83.61it/s, est. speed input: 111601.85 toks/s, output: 108.99 toks/s]
Processed prompts:  56%|█████▌    | 285/512 [00:02<00:02, 83.80it/s, est. speed input: 110587.81 toks/s, output: 108.00 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:02<00:02, 74.61it/s, est. speed input: 107549.53 toks/s, output: 105.03 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:02<00:02, 75.06it/s, est. speed input: 106497.88 toks/s, output: 104.00 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:03<00:02, 75.35it/s, est. speed input: 105504.65 toks/s, output: 103.03 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:03<00:02, 75.60it/s, est. speed input: 104582.64 toks/s, output: 102.13 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:03<00:02, 75.64it/s, est. speed input: 103695.42 toks/s, output: 101.26 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:03<00:02, 75.50it/s, est. speed input: 102839.20 toks/s, output: 100.43 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:03<00:02, 74.59it/s, est. speed input: 101917.92 toks/s, output: 99.53 toks/s] 
Processed prompts:  68%|██████▊   | 350/512 [00:03<00:02, 75.10it/s, est. speed input: 101216.85 toks/s, output: 98.84 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:03<00:02, 74.97it/s, est. speed input: 100488.65 toks/s, output: 98.13 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:03<00:01, 75.08it/s, est. speed input: 99829.35 toks/s, output: 97.49 toks/s] 
Processed prompts:  73%|███████▎  | 374/512 [00:03<00:01, 75.44it/s, est. speed input: 99240.04 toks/s, output: 96.91 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:03<00:01, 75.62it/s, est. speed input: 98673.25 toks/s, output: 96.36 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:04<00:01, 75.86it/s, est. speed input: 98147.45 toks/s, output: 95.85 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:04<00:01, 75.99it/s, est. speed input: 97645.37 toks/s, output: 95.36 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:04<00:01, 76.03it/s, est. speed input: 97161.19 toks/s, output: 94.88 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:04<00:01, 75.95it/s, est. speed input: 96690.12 toks/s, output: 94.42 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:04<00:01, 75.90it/s, est. speed input: 96241.08 toks/s, output: 93.98 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:04<00:01, 75.90it/s, est. speed input: 95815.83 toks/s, output: 93.57 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:04<00:00, 75.97it/s, est. speed input: 95417.01 toks/s, output: 93.18 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:04<00:00, 75.95it/s, est. speed input: 95028.51 toks/s, output: 92.80 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:04<00:00, 75.64it/s, est. speed input: 94630.71 toks/s, output: 92.41 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:05<00:00, 75.72it/s, est. speed input: 94276.41 toks/s, output: 92.07 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:05<00:00, 75.78it/s, est. speed input: 93936.06 toks/s, output: 91.73 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:05<00:00, 75.84it/s, est. speed input: 93611.44 toks/s, output: 91.42 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:05<00:00, 75.78it/s, est. speed input: 93290.94 toks/s, output: 91.10 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:05<00:00, 75.84it/s, est. speed input: 92990.80 toks/s, output: 90.81 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:05<00:00, 75.99it/s, est. speed input: 92710.99 toks/s, output: 90.54 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:05<00:00, 76.62it/s, est. speed input: 92480.50 toks/s, output: 90.31 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 76.62it/s, est. speed input: 92840.09 toks/s, output: 90.66 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 90.66it/s, est. speed input: 92840.09 toks/s, output: 90.66 toks/s]
[rank0]:[W128 00:07:24.854504278 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 57.0s

测试结果:
  Requests/s:   73.35
  Tokens/s:     75178.75
  Total Reqs:   512
  Elapsed:      6.98s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     75105.40

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:07:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2219557) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2219557) WARNING 01-28 00:07:57 [backends.py:609] Failed to read file <frozen os>
Throughput: 79.11 requests/s, 81082.71 total tokens/s, 79.11 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 00:07:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:07:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:07:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:07:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:07:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:07:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:07:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:07:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:07:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:07:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:07:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:07:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:07:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:07:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:07:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:07:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:07:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:07:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:07:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:07:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:07:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:07:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:07:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:07:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:07:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:07:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:07:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:07:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2219557) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2219557) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.85it/s]
(EngineCore_DP0 pid=2219557) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.85it/s]
(EngineCore_DP0 pid=2219557) 
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=2219557) [2026-01-28 00:07:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=2219557) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  5.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  6.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  8.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.24it/s]
(EngineCore_DP0 pid=2219557) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  9.16it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 10.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.94it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 37/1024 [00:00<00:02, 365.25it/s]
Adding requests:   7%|▋         | 76/1024 [00:00<00:02, 378.69it/s]
Adding requests:  11%|█         | 115/1024 [00:00<00:02, 380.14it/s]
Adding requests:  15%|█▌        | 154/1024 [00:00<00:02, 370.22it/s]
Adding requests:  19%|█▉        | 192/1024 [00:00<00:02, 365.53it/s]
Adding requests:  23%|██▎       | 232/1024 [00:00<00:02, 375.62it/s]
Adding requests:  26%|██▋       | 270/1024 [00:00<00:02, 367.41it/s]
Adding requests:  30%|██▉       | 307/1024 [00:00<00:02, 353.22it/s]
Adding requests:  33%|███▎      | 343/1024 [00:00<00:02, 316.40it/s]
Adding requests:  37%|███▋      | 381/1024 [00:01<00:01, 333.41it/s]
Adding requests:  41%|████      | 420/1024 [00:01<00:01, 347.75it/s]
Adding requests:  45%|████▍     | 458/1024 [00:01<00:01, 355.03it/s]
Adding requests:  48%|████▊     | 495/1024 [00:01<00:01, 359.31it/s]
Adding requests:  52%|█████▏    | 532/1024 [00:01<00:01, 351.68it/s]
Adding requests:  56%|█████▌    | 571/1024 [00:01<00:01, 360.38it/s]
Adding requests:  59%|█████▉    | 608/1024 [00:01<00:01, 360.64it/s]
Adding requests:  63%|██████▎   | 648/1024 [00:01<00:01, 371.22it/s]
Adding requests:  67%|██████▋   | 690/1024 [00:01<00:00, 383.79it/s]
Adding requests:  71%|███████   | 729/1024 [00:02<00:00, 382.68it/s]
Adding requests:  75%|███████▌  | 768/1024 [00:02<00:00, 381.80it/s]
Adding requests:  79%|███████▉  | 807/1024 [00:02<00:00, 376.57it/s]
Adding requests:  83%|████████▎ | 845/1024 [00:02<00:00, 375.94it/s]
Adding requests:  87%|████████▋ | 888/1024 [00:02<00:00, 391.08it/s]
Adding requests:  91%|█████████ | 928/1024 [00:02<00:00, 390.97it/s]
Adding requests:  95%|█████████▍| 969/1024 [00:02<00:00, 392.84it/s]
Adding requests:  99%|█████████▊| 1011/1024 [00:02<00:00, 399.39it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 370.93it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:00<00:00, 1513.38it/s, est. speed input: 1550011.21 toks/s, output: 1513.53 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:02<00:04, 152.00it/s, est. speed input: 185074.73 toks/s, output: 180.74 toks/s]   
Processed prompts:  43%|████▎     | 438/1024 [00:02<00:04, 127.46it/s, est. speed input: 157330.59 toks/s, output: 153.64 toks/s]
Processed prompts:  47%|████▋     | 479/1024 [00:03<00:04, 116.61it/s, est. speed input: 146593.18 toks/s, output: 143.16 toks/s]
Processed prompts:  50%|████▉     | 507/1024 [00:03<00:04, 106.17it/s, est. speed input: 138436.89 toks/s, output: 135.19 toks/s]
Processed prompts:  52%|█████▏    | 528/1024 [00:03<00:04, 105.88it/s, est. speed input: 136811.20 toks/s, output: 133.60 toks/s]
Processed prompts:  53%|█████▎    | 545/1024 [00:04<00:04, 102.35it/s, est. speed input: 134348.62 toks/s, output: 131.20 toks/s]
Processed prompts:  55%|█████▍    | 560/1024 [00:04<00:04, 97.04it/s, est. speed input: 131641.98 toks/s, output: 128.56 toks/s] 
Processed prompts:  56%|█████▌    | 572/1024 [00:04<00:05, 89.09it/s, est. speed input: 128515.22 toks/s, output: 125.50 toks/s]
Processed prompts:  57%|█████▋    | 583/1024 [00:04<00:04, 91.71it/s, est. speed input: 128148.36 toks/s, output: 125.14 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:04<00:05, 81.53it/s, est. speed input: 125144.13 toks/s, output: 122.21 toks/s]
Processed prompts:  59%|█████▉    | 603/1024 [00:04<00:05, 82.80it/s, est. speed input: 124457.95 toks/s, output: 121.54 toks/s]
Processed prompts:  60%|█████▉    | 612/1024 [00:05<00:04, 84.04it/s, est. speed input: 123803.35 toks/s, output: 120.90 toks/s]
Processed prompts:  61%|██████    | 621/1024 [00:05<00:04, 85.16it/s, est. speed input: 123172.04 toks/s, output: 120.29 toks/s]
Processed prompts:  62%|██████▏   | 630/1024 [00:05<00:04, 86.06it/s, est. speed input: 122556.26 toks/s, output: 119.68 toks/s]
Processed prompts:  62%|██████▏   | 639/1024 [00:05<00:04, 86.82it/s, est. speed input: 121967.00 toks/s, output: 119.11 toks/s]
Processed prompts:  63%|██████▎   | 648/1024 [00:05<00:04, 87.49it/s, est. speed input: 121405.51 toks/s, output: 118.56 toks/s]
Processed prompts:  64%|██████▍   | 657/1024 [00:05<00:04, 87.96it/s, est. speed input: 120861.03 toks/s, output: 118.03 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:05<00:05, 69.00it/s, est. speed input: 118237.79 toks/s, output: 115.47 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:05<00:04, 71.37it/s, est. speed input: 117590.84 toks/s, output: 114.83 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:05<00:04, 73.36it/s, est. speed input: 116973.28 toks/s, output: 114.23 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:06<00:04, 74.87it/s, est. speed input: 116372.22 toks/s, output: 113.64 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:06<00:04, 76.07it/s, est. speed input: 115795.89 toks/s, output: 113.08 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:06<00:04, 77.05it/s, est. speed input: 115244.78 toks/s, output: 112.54 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:06<00:03, 77.64it/s, est. speed input: 114701.35 toks/s, output: 112.01 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:06<00:03, 78.00it/s, est. speed input: 114169.81 toks/s, output: 111.49 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:06<00:03, 78.30it/s, est. speed input: 113658.41 toks/s, output: 110.99 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:06<00:03, 78.53it/s, est. speed input: 113163.28 toks/s, output: 110.51 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:06<00:03, 78.72it/s, est. speed input: 112684.52 toks/s, output: 110.04 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:06<00:03, 78.94it/s, est. speed input: 112226.06 toks/s, output: 109.60 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:06<00:03, 79.06it/s, est. speed input: 111779.02 toks/s, output: 109.16 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:07<00:03, 79.16it/s, est. speed input: 111345.82 toks/s, output: 108.74 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:07<00:03, 79.21it/s, est. speed input: 110922.67 toks/s, output: 108.32 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:07<00:03, 79.24it/s, est. speed input: 110511.15 toks/s, output: 107.92 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:07<00:02, 79.44it/s, est. speed input: 110122.32 toks/s, output: 107.54 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:07<00:02, 79.37it/s, est. speed input: 109730.80 toks/s, output: 107.16 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:07<00:02, 79.42it/s, est. speed input: 109355.76 toks/s, output: 106.79 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:07<00:02, 79.35it/s, est. speed input: 108983.77 toks/s, output: 106.43 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:07<00:02, 79.36it/s, est. speed input: 108625.40 toks/s, output: 106.08 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:07<00:02, 79.32it/s, est. speed input: 108273.67 toks/s, output: 105.74 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:07<00:02, 79.33it/s, est. speed input: 107932.42 toks/s, output: 105.40 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:08<00:02, 79.31it/s, est. speed input: 107598.71 toks/s, output: 105.08 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:08<00:02, 79.30it/s, est. speed input: 107273.30 toks/s, output: 104.76 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:08<00:01, 79.25it/s, est. speed input: 106953.12 toks/s, output: 104.45 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:08<00:01, 79.24it/s, est. speed input: 106642.26 toks/s, output: 104.14 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:08<00:01, 79.08it/s, est. speed input: 106330.38 toks/s, output: 103.84 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:08<00:01, 79.12it/s, est. speed input: 106034.33 toks/s, output: 103.55 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:08<00:01, 79.16it/s, est. speed input: 105745.26 toks/s, output: 103.27 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:08<00:01, 79.18it/s, est. speed input: 105462.63 toks/s, output: 102.99 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:08<00:01, 79.23it/s, est. speed input: 105188.63 toks/s, output: 102.72 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:08<00:01, 79.18it/s, est. speed input: 104916.20 toks/s, output: 102.46 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:09<00:01, 79.18it/s, est. speed input: 104651.46 toks/s, output: 102.20 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:09<00:00, 80.69it/s, est. speed input: 104231.30 toks/s, output: 101.79 toks/s]
Processed prompts:  94%|█████████▎| 958/1024 [00:09<00:00, 90.50it/s, est. speed input: 104429.56 toks/s, output: 101.98 toks/s]
Processed prompts:  95%|█████████▍| 968/1024 [00:09<00:00, 92.63it/s, est. speed input: 104391.55 toks/s, output: 101.94 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:09<00:00, 74.16it/s, est. speed input: 103279.15 toks/s, output: 100.86 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:09<00:00, 77.50it/s, est. speed input: 102927.64 toks/s, output: 100.52 toks/s]
Processed prompts:  98%|█████████▊| 1003/1024 [00:09<00:00, 80.00it/s, est. speed input: 102808.31 toks/s, output: 100.40 toks/s]
Processed prompts:  99%|█████████▉| 1012/1024 [00:10<00:00, 82.19it/s, est. speed input: 102694.26 toks/s, output: 100.29 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:10<00:00, 82.19it/s, est. speed input: 102983.45 toks/s, output: 100.57 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:10<00:00, 100.57it/s, est. speed input: 102983.45 toks/s, output: 100.57 toks/s]
[rank0]:[W128 00:08:30.099037619 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 66.3s

测试结果:
  Requests/s:   79.11
  Tokens/s:     81082.71
  Total Reqs:   1024
  Elapsed:      12.94s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     81003.61

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:08:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2220812) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2220812) WARNING 01-28 00:09:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 81.30 requests/s, 83330.12 total tokens/s, 81.30 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 00:08:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:08:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:08:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:08:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:08:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:08:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:08:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:08:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:08:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:09:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:09:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:09:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:09:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:09:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:09:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:09:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:09:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:09:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2220812) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2220812) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.80it/s]
(EngineCore_DP0 pid=2220812) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.80it/s]
(EngineCore_DP0 pid=2220812) 
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=2220812) [2026-01-28 00:09:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=2220812) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  9.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.82it/s]
(EngineCore_DP0 pid=2220812) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.87it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.81it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.83it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 34/2048 [00:00<00:05, 338.90it/s]
Adding requests:   3%|▎         | 71/2048 [00:00<00:05, 353.16it/s]
Adding requests:   5%|▌         | 107/2048 [00:00<00:05, 354.72it/s]
Adding requests:   7%|▋         | 143/2048 [00:00<00:05, 356.51it/s]
Adding requests:   9%|▊         | 179/2048 [00:00<00:05, 349.03it/s]
Adding requests:  10%|█         | 215/2048 [00:00<00:05, 350.79it/s]
Adding requests:  12%|█▏        | 253/2048 [00:00<00:05, 357.02it/s]
Adding requests:  14%|█▍        | 289/2048 [00:00<00:04, 356.43it/s]
Adding requests:  16%|█▌        | 325/2048 [00:00<00:04, 354.13it/s]
Adding requests:  18%|█▊        | 362/2048 [00:01<00:04, 357.28it/s]
Adding requests:  19%|█▉        | 399/2048 [00:01<00:04, 359.53it/s]
Adding requests:  21%|██▏       | 436/2048 [00:01<00:04, 359.77it/s]
Adding requests:  23%|██▎       | 473/2048 [00:01<00:04, 361.89it/s]
Adding requests:  25%|██▍       | 510/2048 [00:01<00:04, 359.54it/s]
Adding requests:  27%|██▋       | 546/2048 [00:01<00:04, 357.83it/s]
Adding requests:  29%|██▊       | 584/2048 [00:01<00:04, 363.30it/s]
Adding requests:  30%|███       | 622/2048 [00:01<00:03, 366.49it/s]
Adding requests:  32%|███▏      | 660/2048 [00:01<00:03, 368.94it/s]
Adding requests:  34%|███▍      | 699/2048 [00:01<00:03, 374.95it/s]
Adding requests:  36%|███▌      | 737/2048 [00:02<00:03, 371.42it/s]
Adding requests:  38%|███▊      | 775/2048 [00:02<00:03, 365.19it/s]
Adding requests:  40%|███▉      | 812/2048 [00:02<00:03, 364.36it/s]
Adding requests:  42%|████▏     | 850/2048 [00:02<00:03, 367.97it/s]
Adding requests:  44%|████▎     | 891/2048 [00:02<00:03, 379.87it/s]
Adding requests:  45%|████▌     | 930/2048 [00:02<00:02, 378.12it/s]
Adding requests:  47%|████▋     | 968/2048 [00:02<00:02, 374.83it/s]
Adding requests:  49%|████▉     | 1006/2048 [00:02<00:02, 373.05it/s]
Adding requests:  51%|█████     | 1045/2048 [00:02<00:02, 375.69it/s]
Adding requests:  53%|█████▎    | 1083/2048 [00:02<00:02, 372.97it/s]
Adding requests:  55%|█████▍    | 1121/2048 [00:03<00:02, 370.62it/s]
Adding requests:  57%|█████▋    | 1160/2048 [00:03<00:02, 375.23it/s]
Adding requests:  59%|█████▊    | 1200/2048 [00:03<00:02, 380.91it/s]
Adding requests:  60%|██████    | 1239/2048 [00:03<00:02, 379.88it/s]
Adding requests:  62%|██████▏   | 1277/2048 [00:03<00:02, 376.15it/s]
Adding requests:  64%|██████▍   | 1315/2048 [00:03<00:01, 376.75it/s]
Adding requests:  66%|██████▌   | 1353/2048 [00:03<00:01, 377.43it/s]
Adding requests:  68%|██████▊   | 1392/2048 [00:03<00:01, 380.19it/s]
Adding requests:  70%|██████▉   | 1431/2048 [00:03<00:01, 378.52it/s]
Adding requests:  72%|███████▏  | 1471/2048 [00:03<00:01, 383.21it/s]
Adding requests:  74%|███████▍  | 1511/2048 [00:04<00:01, 386.73it/s]
Adding requests:  76%|███████▌  | 1550/2048 [00:04<00:01, 387.16it/s]
Adding requests:  78%|███████▊  | 1590/2048 [00:04<00:01, 389.25it/s]
Adding requests:  80%|███████▉  | 1629/2048 [00:04<00:01, 389.40it/s]
Adding requests:  81%|████████▏ | 1668/2048 [00:04<00:00, 383.84it/s]
Adding requests:  83%|████████▎ | 1707/2048 [00:04<00:00, 383.05it/s]
Adding requests:  85%|████████▌ | 1746/2048 [00:04<00:00, 379.17it/s]
Adding requests:  87%|████████▋ | 1784/2048 [00:04<00:00, 374.32it/s]
Adding requests:  89%|████████▉ | 1822/2048 [00:04<00:00, 374.67it/s]
Adding requests:  91%|█████████ | 1860/2048 [00:05<00:00, 372.83it/s]
Adding requests:  93%|█████████▎| 1898/2048 [00:05<00:00, 361.83it/s]
Adding requests:  94%|█████████▍| 1935/2048 [00:05<00:00, 363.72it/s]
Adding requests:  96%|█████████▋| 1973/2048 [00:05<00:00, 366.04it/s]
Adding requests:  98%|█████████▊| 2012/2048 [00:05<00:00, 372.33it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 370.60it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:00<00:00, 1747.88it/s, est. speed input: 1790032.13 toks/s, output: 1747.93 toks/s]
Processed prompts:  31%|███       | 625/2048 [00:02<00:06, 228.48it/s, est. speed input: 288049.56 toks/s, output: 281.30 toks/s]   
Processed prompts:  34%|███▍      | 703/2048 [00:03<00:07, 169.36it/s, est. speed input: 224511.38 toks/s, output: 219.25 toks/s]
Processed prompts:  37%|███▋      | 749/2048 [00:03<00:08, 146.27it/s, est. speed input: 202071.14 toks/s, output: 197.33 toks/s]
Processed prompts:  38%|███▊      | 780/2048 [00:04<00:09, 133.21it/s, est. speed input: 190705.60 toks/s, output: 186.24 toks/s]
Processed prompts:  39%|███▉      | 803/2048 [00:04<00:10, 117.02it/s, est. speed input: 179508.74 toks/s, output: 175.30 toks/s]
Processed prompts:  40%|████      | 820/2048 [00:04<00:10, 112.88it/s, est. speed input: 175752.24 toks/s, output: 171.63 toks/s]
Processed prompts:  41%|████      | 835/2048 [00:04<00:11, 107.02it/s, est. speed input: 171905.80 toks/s, output: 167.88 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:05<00:11, 101.31it/s, est. speed input: 168353.44 toks/s, output: 164.41 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:05<00:12, 97.12it/s, est. speed input: 165242.15 toks/s, output: 161.37 toks/s] 
Processed prompts:  43%|████▎     | 882/2048 [00:05<00:12, 93.45it/s, est. speed input: 162342.19 toks/s, output: 158.54 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:05<00:12, 90.43it/s, est. speed input: 159638.58 toks/s, output: 155.90 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:05<00:12, 88.07it/s, est. speed input: 157124.24 toks/s, output: 153.44 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:06<00:12, 87.50it/s, est. speed input: 155026.19 toks/s, output: 151.39 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:06<00:12, 85.91it/s, est. speed input: 152830.19 toks/s, output: 149.25 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:06<00:12, 84.59it/s, est. speed input: 150737.11 toks/s, output: 147.20 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:06<00:12, 84.76it/s, est. speed input: 148962.56 toks/s, output: 145.47 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:06<00:12, 83.76it/s, est. speed input: 147099.17 toks/s, output: 143.65 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:07<00:12, 82.97it/s, est. speed input: 145323.70 toks/s, output: 141.92 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:07<00:12, 82.53it/s, est. speed input: 143663.35 toks/s, output: 140.30 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:07<00:12, 82.29it/s, est. speed input: 142099.63 toks/s, output: 138.77 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:07<00:12, 82.05it/s, est. speed input: 140604.69 toks/s, output: 137.31 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:07<00:11, 81.75it/s, est. speed input: 139165.06 toks/s, output: 135.90 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:08<00:11, 81.66it/s, est. speed input: 137812.47 toks/s, output: 134.58 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:08<00:11, 81.64it/s, est. speed input: 136529.63 toks/s, output: 133.33 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:08<00:11, 81.64it/s, est. speed input: 135308.18 toks/s, output: 132.14 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:08<00:11, 81.47it/s, est. speed input: 134120.30 toks/s, output: 130.98 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:08<00:10, 82.72it/s, est. speed input: 133148.79 toks/s, output: 130.03 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:09<00:10, 82.21it/s, est. speed input: 132056.75 toks/s, output: 128.96 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:09<00:10, 82.01it/s, est. speed input: 131028.80 toks/s, output: 127.96 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:09<00:10, 81.91it/s, est. speed input: 130047.99 toks/s, output: 127.00 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:09<00:10, 81.49it/s, est. speed input: 129070.13 toks/s, output: 126.04 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:09<00:10, 81.34it/s, est. speed input: 128144.89 toks/s, output: 125.14 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:10<00:09, 81.31it/s, est. speed input: 127265.29 toks/s, output: 124.28 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:10<00:09, 82.56it/s, est. speed input: 126543.29 toks/s, output: 123.58 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:10<00:09, 82.13it/s, est. speed input: 125722.73 toks/s, output: 122.78 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:10<00:09, 81.75it/s, est. speed input: 124924.95 toks/s, output: 122.00 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:10<00:09, 81.52it/s, est. speed input: 124159.26 toks/s, output: 121.25 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:11<00:08, 81.35it/s, est. speed input: 123420.11 toks/s, output: 120.53 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:11<00:08, 81.14it/s, est. speed input: 122699.48 toks/s, output: 119.82 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:11<00:08, 81.25it/s, est. speed input: 122025.01 toks/s, output: 119.16 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:11<00:08, 81.27it/s, est. speed input: 121369.65 toks/s, output: 118.52 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:11<00:08, 81.23it/s, est. speed input: 120731.31 toks/s, output: 117.90 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:12<00:07, 81.21it/s, est. speed input: 120113.87 toks/s, output: 117.30 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:12<00:07, 81.34it/s, est. speed input: 119528.38 toks/s, output: 116.73 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:12<00:07, 81.30it/s, est. speed input: 118950.98 toks/s, output: 116.16 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:12<00:07, 81.26it/s, est. speed input: 118390.60 toks/s, output: 115.62 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:12<00:07, 81.26it/s, est. speed input: 117850.22 toks/s, output: 115.09 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:13<00:06, 81.28it/s, est. speed input: 117326.92 toks/s, output: 114.58 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:13<00:06, 81.31it/s, est. speed input: 116820.81 toks/s, output: 114.08 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:13<00:06, 81.26it/s, est. speed input: 116324.07 toks/s, output: 113.60 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:13<00:06, 81.23it/s, est. speed input: 115842.88 toks/s, output: 113.13 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:13<00:06, 81.19it/s, est. speed input: 115373.26 toks/s, output: 112.67 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:13<00:05, 81.25it/s, est. speed input: 114923.54 toks/s, output: 112.23 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:14<00:05, 82.67it/s, est. speed input: 114573.79 toks/s, output: 111.89 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:14<00:05, 82.25it/s, est. speed input: 114144.87 toks/s, output: 111.47 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:14<00:05, 82.00it/s, est. speed input: 113729.52 toks/s, output: 111.06 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:14<00:05, 81.72it/s, est. speed input: 113319.23 toks/s, output: 110.66 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:14<00:04, 81.59it/s, est. speed input: 112923.91 toks/s, output: 110.28 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:15<00:04, 81.45it/s, est. speed input: 112534.96 toks/s, output: 109.90 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:15<00:04, 81.43it/s, est. speed input: 112161.52 toks/s, output: 109.53 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:15<00:04, 81.40it/s, est. speed input: 111795.97 toks/s, output: 109.18 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:15<00:04, 81.28it/s, est. speed input: 111434.25 toks/s, output: 108.82 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:15<00:03, 81.34it/s, est. speed input: 111089.26 toks/s, output: 108.49 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:16<00:03, 81.22it/s, est. speed input: 110744.25 toks/s, output: 108.15 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:16<00:03, 81.23it/s, est. speed input: 110412.30 toks/s, output: 107.82 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:16<00:03, 81.18it/s, est. speed input: 110085.39 toks/s, output: 107.51 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:16<00:03, 81.24it/s, est. speed input: 109771.52 toks/s, output: 107.20 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:16<00:02, 81.23it/s, est. speed input: 109461.28 toks/s, output: 106.90 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:17<00:02, 81.33it/s, est. speed input: 109164.01 toks/s, output: 106.61 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:17<00:02, 81.15it/s, est. speed input: 108861.11 toks/s, output: 106.31 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:17<00:02, 81.15it/s, est. speed input: 108571.10 toks/s, output: 106.03 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:17<00:02, 82.43it/s, est. speed input: 108350.08 toks/s, output: 105.81 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:17<00:01, 82.08it/s, est. speed input: 108073.74 toks/s, output: 105.54 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:18<00:01, 81.72it/s, est. speed input: 107797.91 toks/s, output: 105.27 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:18<00:01, 81.53it/s, est. speed input: 107531.10 toks/s, output: 105.01 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:18<00:01, 81.35it/s, est. speed input: 107267.61 toks/s, output: 104.75 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:18<00:01, 82.40it/s, est. speed input: 107063.29 toks/s, output: 104.55 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:18<00:00, 81.92it/s, est. speed input: 106808.73 toks/s, output: 104.31 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:19<00:00, 81.70it/s, est. speed input: 106564.42 toks/s, output: 104.07 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:19<00:00, 81.55it/s, est. speed input: 106325.09 toks/s, output: 103.83 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:19<00:00, 81.43it/s, est. speed input: 106089.88 toks/s, output: 103.60 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:19<00:00, 82.92it/s, est. speed input: 105926.38 toks/s, output: 103.44 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:19<00:00, 82.92it/s, est. speed input: 106653.32 toks/s, output: 104.15 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:19<00:00, 104.15it/s, est. speed input: 106653.32 toks/s, output: 104.15 toks/s]
[rank0]:[W128 00:09:58.441995996 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 88.4s

测试结果:
  Requests/s:   81.30
  Tokens/s:     83330.12
  Total Reqs:   2048
  Elapsed:      25.19s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     83248.82

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:10:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2223003) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2223003) WARNING 01-28 00:10:50 [backends.py:609] Failed to read file <frozen os>
Throughput: 82.22 requests/s, 84273.00 total tokens/s, 82.22 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 00:10:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:10:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:10:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:10:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:10:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:10:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:10:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:10:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:10:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:10:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:10:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:10:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:10:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:10:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:10:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:10:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:10:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:10:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2223003) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2223003) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]
(EngineCore_DP0 pid=2223003) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]
(EngineCore_DP0 pid=2223003) 
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=2223003) [2026-01-28 00:10:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=2223003) [rank0]:W0128 00:11:00.148000 2223003 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2223003) [rank0]:W0128 00:11:00.274000 2223003 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2223003) [rank0]:W0128 00:11:01.776000 2223003 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2223003) [rank0]:W0128 00:11:01.977000 2223003 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2223003) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  9.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00,  9.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  9.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  9.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  9.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  9.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  9.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00,  9.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  9.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.25it/s]
(EngineCore_DP0 pid=2223003) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  8.14it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  9.36it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  9.53it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  9.59it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.69it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.62it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 34/4096 [00:00<00:12, 336.97it/s]
Adding requests:   2%|▏         | 70/4096 [00:00<00:11, 348.11it/s]
Adding requests:   3%|▎         | 105/4096 [00:00<00:11, 348.86it/s]
Adding requests:   3%|▎         | 141/4096 [00:00<00:11, 348.66it/s]
Adding requests:   4%|▍         | 177/4096 [00:00<00:11, 350.75it/s]
Adding requests:   5%|▌         | 213/4096 [00:00<00:10, 353.50it/s]
Adding requests:   6%|▌         | 252/4096 [00:00<00:10, 363.47it/s]
Adding requests:   7%|▋         | 289/4096 [00:00<00:10, 360.12it/s]
Adding requests:   8%|▊         | 327/4096 [00:00<00:10, 363.88it/s]
Adding requests:   9%|▉         | 366/4096 [00:01<00:10, 369.79it/s]
Adding requests:  10%|▉         | 405/4096 [00:01<00:09, 375.33it/s]
Adding requests:  11%|█         | 443/4096 [00:01<00:09, 376.73it/s]
Adding requests:  12%|█▏        | 481/4096 [00:01<00:09, 375.81it/s]
Adding requests:  13%|█▎        | 519/4096 [00:01<00:09, 363.40it/s]
Adding requests:  14%|█▎        | 556/4096 [00:01<00:09, 359.49it/s]
Adding requests:  14%|█▍        | 593/4096 [00:01<00:09, 359.88it/s]
Adding requests:  15%|█▌        | 630/4096 [00:01<00:09, 359.82it/s]
Adding requests:  16%|█▋        | 667/4096 [00:01<00:09, 361.89it/s]
Adding requests:  17%|█▋        | 706/4096 [00:01<00:09, 367.24it/s]
Adding requests:  18%|█▊        | 743/4096 [00:02<00:09, 365.29it/s]
Adding requests:  19%|█▉        | 782/4096 [00:02<00:08, 372.25it/s]
Adding requests:  20%|██        | 820/4096 [00:02<00:08, 368.44it/s]
Adding requests:  21%|██        | 858/4096 [00:02<00:08, 369.39it/s]
Adding requests:  22%|██▏       | 898/4096 [00:02<00:08, 376.66it/s]
Adding requests:  23%|██▎       | 936/4096 [00:02<00:08, 372.17it/s]
Adding requests:  24%|██▍       | 974/4096 [00:02<00:08, 371.79it/s]
Adding requests:  25%|██▍       | 1012/4096 [00:02<00:08, 371.43it/s]
Adding requests:  26%|██▌       | 1050/4096 [00:02<00:08, 373.56it/s]
Adding requests:  27%|██▋       | 1088/4096 [00:02<00:08, 358.46it/s]
Adding requests:  28%|██▊       | 1127/4096 [00:03<00:08, 367.43it/s]
Adding requests:  28%|██▊       | 1165/4096 [00:03<00:07, 371.07it/s]
Adding requests:  29%|██▉       | 1206/4096 [00:03<00:07, 380.86it/s]
Adding requests:  30%|███       | 1245/4096 [00:03<00:07, 372.47it/s]
Adding requests:  31%|███▏      | 1283/4096 [00:03<00:07, 367.98it/s]
Adding requests:  32%|███▏      | 1321/4096 [00:03<00:07, 371.25it/s]
Adding requests:  33%|███▎      | 1359/4096 [00:03<00:07, 371.73it/s]
Adding requests:  34%|███▍      | 1398/4096 [00:03<00:07, 375.13it/s]
Adding requests:  35%|███▌      | 1436/4096 [00:03<00:07, 371.97it/s]
Adding requests:  36%|███▌      | 1475/4096 [00:04<00:06, 374.49it/s]
Adding requests:  37%|███▋      | 1514/4096 [00:04<00:06, 378.10it/s]
Adding requests:  38%|███▊      | 1554/4096 [00:04<00:06, 383.76it/s]
Adding requests:  39%|███▉      | 1594/4096 [00:04<00:06, 386.56it/s]
Adding requests:  40%|███▉      | 1633/4096 [00:04<00:06, 386.51it/s]
Adding requests:  41%|████      | 1672/4096 [00:04<00:06, 379.03it/s]
Adding requests:  42%|████▏     | 1710/4096 [00:04<00:06, 375.44it/s]
Adding requests:  43%|████▎     | 1748/4096 [00:04<00:06, 372.78it/s]
Adding requests:  44%|████▎     | 1786/4096 [00:04<00:06, 371.79it/s]
Adding requests:  45%|████▍     | 1824/4096 [00:04<00:06, 373.30it/s]
Adding requests:  45%|████▌     | 1862/4096 [00:05<00:06, 372.21it/s]
Adding requests:  46%|████▋     | 1900/4096 [00:05<00:05, 373.36it/s]
Adding requests:  47%|████▋     | 1938/4096 [00:05<00:05, 372.63it/s]
Adding requests:  48%|████▊     | 1977/4096 [00:05<00:05, 375.08it/s]
Adding requests:  49%|████▉     | 2015/4096 [00:05<00:05, 374.54it/s]
Adding requests:  50%|█████     | 2054/4096 [00:05<00:05, 378.81it/s]
Adding requests:  51%|█████     | 2092/4096 [00:05<00:05, 377.86it/s]
Adding requests:  52%|█████▏    | 2130/4096 [00:05<00:05, 368.03it/s]
Adding requests:  53%|█████▎    | 2167/4096 [00:05<00:05, 363.09it/s]
Adding requests:  54%|█████▍    | 2204/4096 [00:05<00:05, 359.62it/s]
Adding requests:  55%|█████▍    | 2242/4096 [00:06<00:05, 365.26it/s]
Adding requests:  56%|█████▌    | 2279/4096 [00:06<00:05, 353.32it/s]
Adding requests:  57%|█████▋    | 2317/4096 [00:06<00:04, 358.52it/s]
Adding requests:  57%|█████▋    | 2353/4096 [00:06<00:04, 357.46it/s]
Adding requests:  58%|█████▊    | 2389/4096 [00:06<00:04, 357.99it/s]
Adding requests:  59%|█████▉    | 2426/4096 [00:06<00:04, 361.18it/s]
Adding requests:  60%|██████    | 2463/4096 [00:06<00:04, 360.22it/s]
Adding requests:  61%|██████    | 2500/4096 [00:06<00:04, 362.36it/s]
Adding requests:  62%|██████▏   | 2539/4096 [00:06<00:04, 370.15it/s]
Adding requests:  63%|██████▎   | 2579/4096 [00:06<00:04, 378.08it/s]
Adding requests:  64%|██████▍   | 2619/4096 [00:07<00:03, 382.40it/s]
Adding requests:  65%|██████▍   | 2659/4096 [00:07<00:03, 386.76it/s]
Adding requests:  66%|██████▌   | 2698/4096 [00:07<00:03, 383.71it/s]
Adding requests:  67%|██████▋   | 2737/4096 [00:07<00:03, 380.55it/s]
Adding requests:  68%|██████▊   | 2776/4096 [00:07<00:03, 374.94it/s]
Adding requests:  69%|██████▊   | 2814/4096 [00:07<00:03, 369.57it/s]
Adding requests:  70%|██████▉   | 2851/4096 [00:07<00:03, 369.27it/s]
Adding requests:  71%|███████   | 2889/4096 [00:07<00:03, 370.31it/s]
Adding requests:  71%|███████▏  | 2927/4096 [00:07<00:03, 364.46it/s]
Adding requests:  72%|███████▏  | 2964/4096 [00:08<00:03, 362.23it/s]
Adding requests:  73%|███████▎  | 3002/4096 [00:08<00:02, 365.70it/s]
Adding requests:  74%|███████▍  | 3039/4096 [00:08<00:02, 360.30it/s]
Adding requests:  75%|███████▌  | 3076/4096 [00:08<00:02, 359.73it/s]
Adding requests:  76%|███████▌  | 3113/4096 [00:08<00:02, 362.38it/s]
Adding requests:  77%|███████▋  | 3150/4096 [00:08<00:02, 359.62it/s]
Adding requests:  78%|███████▊  | 3188/4096 [00:08<00:02, 362.62it/s]
Adding requests:  79%|███████▊  | 3225/4096 [00:08<00:02, 360.24it/s]
Adding requests:  80%|███████▉  | 3262/4096 [00:08<00:02, 361.43it/s]
Adding requests:  81%|████████  | 3299/4096 [00:08<00:02, 363.47it/s]
Adding requests:  81%|████████▏ | 3336/4096 [00:09<00:02, 362.98it/s]
Adding requests:  82%|████████▏ | 3376/4096 [00:09<00:01, 372.53it/s]
Adding requests:  83%|████████▎ | 3414/4096 [00:09<00:01, 373.31it/s]
Adding requests:  84%|████████▍ | 3452/4096 [00:09<00:01, 369.77it/s]
Adding requests:  85%|████████▌ | 3489/4096 [00:09<00:01, 363.54it/s]
Adding requests:  86%|████████▌ | 3526/4096 [00:09<00:01, 363.45it/s]
Adding requests:  87%|████████▋ | 3563/4096 [00:09<00:01, 359.39it/s]
Adding requests:  88%|████████▊ | 3599/4096 [00:09<00:01, 346.39it/s]
Adding requests:  89%|████████▊ | 3634/4096 [00:09<00:01, 344.32it/s]
Adding requests:  90%|████████▉ | 3669/4096 [00:09<00:01, 345.68it/s]
Adding requests:  90%|█████████ | 3705/4096 [00:10<00:01, 346.91it/s]
Adding requests:  91%|█████████▏| 3740/4096 [00:10<00:01, 344.38it/s]
Adding requests:  92%|█████████▏| 3778/4096 [00:10<00:00, 353.53it/s]
Adding requests:  93%|█████████▎| 3814/4096 [00:10<00:00, 353.15it/s]
Adding requests:  94%|█████████▍| 3850/4096 [00:10<00:00, 354.75it/s]
Adding requests:  95%|█████████▍| 3886/4096 [00:10<00:00, 356.03it/s]
Adding requests:  96%|█████████▌| 3924/4096 [00:10<00:00, 361.75it/s]
Adding requests:  97%|█████████▋| 3961/4096 [00:10<00:00, 356.18it/s]
Adding requests:  98%|█████████▊| 3997/4096 [00:10<00:00, 350.85it/s]
Adding requests:  98%|█████████▊| 4033/4096 [00:11<00:00, 349.76it/s]
Adding requests:  99%|█████████▉| 4068/4096 [00:11<00:00, 348.56it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 365.74it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  23%|██▎       | 923/4096 [00:00<00:01, 2124.01it/s, est. speed input: 2175120.40 toks/s, output: 2124.05 toks/s]
Processed prompts:  28%|██▊       | 1136/4096 [00:02<00:09, 327.15it/s, est. speed input: 422025.43 toks/s, output: 412.13 toks/s]  
Processed prompts:  30%|███       | 1229/4096 [00:03<00:12, 231.86it/s, est. speed input: 321625.13 toks/s, output: 314.09 toks/s]
Processed prompts:  31%|███▏      | 1284/4096 [00:04<00:14, 189.00it/s, est. speed input: 280985.24 toks/s, output: 274.40 toks/s]
Processed prompts:  32%|███▏      | 1320/4096 [00:05<00:16, 173.22it/s, est. speed input: 266619.23 toks/s, output: 260.37 toks/s]
Processed prompts:  33%|███▎      | 1347/4096 [00:05<00:17, 153.67it/s, est. speed input: 252686.65 toks/s, output: 246.76 toks/s]
Processed prompts:  33%|███▎      | 1371/4096 [00:05<00:20, 134.26it/s, est. speed input: 240136.05 toks/s, output: 234.51 toks/s]
Processed prompts:  34%|███▍      | 1403/4096 [00:06<00:22, 122.12it/s, est. speed input: 230386.21 toks/s, output: 224.99 toks/s]
Processed prompts:  35%|███▌      | 1435/4096 [00:06<00:23, 112.15it/s, est. speed input: 221789.18 toks/s, output: 216.59 toks/s]
Processed prompts:  36%|███▌      | 1467/4096 [00:07<00:25, 104.36it/s, est. speed input: 214185.24 toks/s, output: 209.16 toks/s]
Processed prompts:  37%|███▋      | 1499/4096 [00:07<00:26, 98.35it/s, est. speed input: 207365.08 toks/s, output: 202.50 toks/s] 
Processed prompts:  37%|███▋      | 1531/4096 [00:07<00:27, 93.79it/s, est. speed input: 201200.00 toks/s, output: 196.48 toks/s]
Processed prompts:  38%|███▊      | 1563/4096 [00:08<00:28, 90.40it/s, est. speed input: 195604.69 toks/s, output: 191.02 toks/s]
Processed prompts:  39%|███▉      | 1595/4096 [00:08<00:28, 88.71it/s, est. speed input: 190776.40 toks/s, output: 186.30 toks/s]
Processed prompts:  40%|███▉      | 1627/4096 [00:08<00:28, 86.74it/s, est. speed input: 186122.49 toks/s, output: 181.76 toks/s]
Processed prompts:  41%|████      | 1659/4096 [00:09<00:28, 85.35it/s, est. speed input: 181861.46 toks/s, output: 177.60 toks/s]
Processed prompts:  41%|████▏     | 1691/4096 [00:09<00:28, 84.40it/s, est. speed input: 177948.11 toks/s, output: 173.78 toks/s]
Processed prompts:  42%|████▏     | 1723/4096 [00:10<00:28, 83.71it/s, est. speed input: 174330.54 toks/s, output: 170.24 toks/s]
Processed prompts:  43%|████▎     | 1755/4096 [00:10<00:28, 83.26it/s, est. speed input: 170992.10 toks/s, output: 166.98 toks/s]
Processed prompts:  44%|████▎     | 1787/4096 [00:10<00:27, 82.96it/s, est. speed input: 167894.40 toks/s, output: 163.96 toks/s]
Processed prompts:  44%|████▍     | 1819/4096 [00:11<00:27, 82.62it/s, est. speed input: 164981.24 toks/s, output: 161.11 toks/s]
Processed prompts:  45%|████▌     | 1851/4096 [00:11<00:27, 83.13it/s, est. speed input: 162427.19 toks/s, output: 158.62 toks/s]
Processed prompts:  46%|████▌     | 1883/4096 [00:12<00:26, 82.82it/s, est. speed input: 159894.35 toks/s, output: 156.15 toks/s]
Processed prompts:  47%|████▋     | 1915/4096 [00:12<00:26, 82.52it/s, est. speed input: 157503.37 toks/s, output: 153.81 toks/s]
Processed prompts:  48%|████▊     | 1947/4096 [00:12<00:25, 83.06it/s, est. speed input: 155400.35 toks/s, output: 151.76 toks/s]
Processed prompts:  48%|████▊     | 1979/4096 [00:13<00:25, 82.71it/s, est. speed input: 153286.95 toks/s, output: 149.69 toks/s]
Processed prompts:  49%|████▉     | 2011/4096 [00:13<00:25, 82.47it/s, est. speed input: 151294.93 toks/s, output: 147.75 toks/s]
Processed prompts:  50%|████▉     | 2043/4096 [00:14<00:24, 82.26it/s, est. speed input: 149406.69 toks/s, output: 145.90 toks/s]
Processed prompts:  51%|█████     | 2075/4096 [00:14<00:24, 82.20it/s, est. speed input: 147634.68 toks/s, output: 144.17 toks/s]
Processed prompts:  51%|█████▏    | 2107/4096 [00:14<00:24, 82.06it/s, est. speed input: 145942.41 toks/s, output: 142.52 toks/s]
Processed prompts:  52%|█████▏    | 2139/4096 [00:15<00:23, 81.99it/s, est. speed input: 144339.88 toks/s, output: 140.96 toks/s]
Processed prompts:  53%|█████▎    | 2171/4096 [00:15<00:23, 81.93it/s, est. speed input: 142816.85 toks/s, output: 139.47 toks/s]
Processed prompts:  54%|█████▍    | 2203/4096 [00:15<00:23, 81.91it/s, est. speed input: 141372.72 toks/s, output: 138.06 toks/s]
Processed prompts:  55%|█████▍    | 2235/4096 [00:16<00:22, 83.36it/s, est. speed input: 140193.14 toks/s, output: 136.91 toks/s]
Processed prompts:  55%|█████▌    | 2267/4096 [00:16<00:22, 82.99it/s, est. speed input: 138884.93 toks/s, output: 135.63 toks/s]
Processed prompts:  56%|█████▌    | 2299/4096 [00:17<00:21, 83.28it/s, est. speed input: 137705.66 toks/s, output: 134.48 toks/s]
Processed prompts:  57%|█████▋    | 2331/4096 [00:17<00:21, 83.47it/s, est. speed input: 136576.00 toks/s, output: 133.37 toks/s]
Processed prompts:  58%|█████▊    | 2363/4096 [00:17<00:20, 84.38it/s, est. speed input: 135583.96 toks/s, output: 132.41 toks/s]
Processed prompts:  58%|█████▊    | 2395/4096 [00:18<00:20, 83.60it/s, est. speed input: 134473.83 toks/s, output: 131.32 toks/s]
Processed prompts:  59%|█████▉    | 2427/4096 [00:18<00:20, 83.07it/s, est. speed input: 133410.70 toks/s, output: 130.28 toks/s]
Processed prompts:  60%|██████    | 2459/4096 [00:19<00:19, 82.68it/s, est. speed input: 132388.84 toks/s, output: 129.29 toks/s]
Processed prompts:  61%|██████    | 2491/4096 [00:19<00:19, 83.04it/s, est. speed input: 131475.35 toks/s, output: 128.39 toks/s]
Processed prompts:  62%|██████▏   | 2523/4096 [00:19<00:19, 82.65it/s, est. speed input: 130530.63 toks/s, output: 127.47 toks/s]
Processed prompts:  62%|██████▏   | 2555/4096 [00:20<00:18, 82.42it/s, est. speed input: 129627.13 toks/s, output: 126.59 toks/s]
Processed prompts:  63%|██████▎   | 2587/4096 [00:20<00:18, 82.82it/s, est. speed input: 128812.45 toks/s, output: 125.79 toks/s]
Processed prompts:  64%|██████▍   | 2619/4096 [00:20<00:17, 82.51it/s, est. speed input: 127971.84 toks/s, output: 124.97 toks/s]
Processed prompts:  65%|██████▍   | 2651/4096 [00:21<00:17, 82.33it/s, est. speed input: 127164.53 toks/s, output: 124.18 toks/s]
Processed prompts:  66%|██████▌   | 2683/4096 [00:21<00:17, 82.16it/s, est. speed input: 126382.60 toks/s, output: 123.42 toks/s]
Processed prompts:  66%|██████▋   | 2715/4096 [00:22<00:16, 82.04it/s, est. speed input: 125627.87 toks/s, output: 122.68 toks/s]
Processed prompts:  67%|██████▋   | 2747/4096 [00:22<00:16, 81.91it/s, est. speed input: 124895.42 toks/s, output: 121.97 toks/s]
Processed prompts:  68%|██████▊   | 2779/4096 [00:22<00:16, 81.93it/s, est. speed input: 124197.78 toks/s, output: 121.29 toks/s]
Processed prompts:  69%|██████▊   | 2811/4096 [00:23<00:15, 81.90it/s, est. speed input: 123520.17 toks/s, output: 120.63 toks/s]
Processed prompts:  69%|██████▉   | 2843/4096 [00:23<00:15, 81.82it/s, est. speed input: 122859.27 toks/s, output: 119.98 toks/s]
Processed prompts:  70%|███████   | 2875/4096 [00:24<00:14, 81.81it/s, est. speed input: 122224.33 toks/s, output: 119.36 toks/s]
Processed prompts:  71%|███████   | 2907/4096 [00:24<00:14, 81.74it/s, est. speed input: 121604.54 toks/s, output: 118.75 toks/s]
Processed prompts:  72%|███████▏  | 2939/4096 [00:24<00:14, 81.77it/s, est. speed input: 121010.29 toks/s, output: 118.17 toks/s]
Processed prompts:  73%|███████▎  | 2971/4096 [00:25<00:13, 81.74it/s, est. speed input: 120430.14 toks/s, output: 117.61 toks/s]
Processed prompts:  73%|███████▎  | 3003/4096 [00:25<00:13, 81.76it/s, est. speed input: 119870.99 toks/s, output: 117.06 toks/s]
Processed prompts:  74%|███████▍  | 3035/4096 [00:26<00:12, 81.75it/s, est. speed input: 119327.01 toks/s, output: 116.53 toks/s]
Processed prompts:  75%|███████▍  | 3067/4096 [00:26<00:12, 81.66it/s, est. speed input: 118793.35 toks/s, output: 116.01 toks/s]
Processed prompts:  76%|███████▌  | 3099/4096 [00:26<00:12, 81.61it/s, est. speed input: 118276.28 toks/s, output: 115.50 toks/s]
Processed prompts:  76%|███████▋  | 3131/4096 [00:27<00:11, 82.31it/s, est. speed input: 117824.12 toks/s, output: 115.06 toks/s]
Processed prompts:  77%|███████▋  | 3163/4096 [00:27<00:11, 82.12it/s, est. speed input: 117338.81 toks/s, output: 114.59 toks/s]
Processed prompts:  78%|███████▊  | 3195/4096 [00:27<00:10, 82.01it/s, est. speed input: 116869.13 toks/s, output: 114.13 toks/s]
Processed prompts:  79%|███████▉  | 3227/4096 [00:28<00:10, 81.90it/s, est. speed input: 116409.70 toks/s, output: 113.68 toks/s]
Processed prompts:  80%|███████▉  | 3259/4096 [00:28<00:10, 81.72it/s, est. speed input: 115956.13 toks/s, output: 113.24 toks/s]
Processed prompts:  80%|████████  | 3291/4096 [00:29<00:09, 81.76it/s, est. speed input: 115525.19 toks/s, output: 112.82 toks/s]
Processed prompts:  81%|████████  | 3323/4096 [00:29<00:09, 81.75it/s, est. speed input: 115103.82 toks/s, output: 112.41 toks/s]
Processed prompts:  82%|████████▏ | 3355/4096 [00:29<00:09, 81.68it/s, est. speed input: 114689.55 toks/s, output: 112.00 toks/s]
Processed prompts:  83%|████████▎ | 3387/4096 [00:30<00:08, 81.63it/s, est. speed input: 114285.36 toks/s, output: 111.61 toks/s]
Processed prompts:  83%|████████▎ | 3419/4096 [00:30<00:08, 81.63it/s, est. speed input: 113894.22 toks/s, output: 111.22 toks/s]
Processed prompts:  84%|████████▍ | 3451/4096 [00:31<00:07, 81.57it/s, est. speed input: 113509.03 toks/s, output: 110.85 toks/s]
Processed prompts:  85%|████████▌ | 3483/4096 [00:31<00:07, 83.02it/s, est. speed input: 113217.65 toks/s, output: 110.56 toks/s]
Processed prompts:  86%|████████▌ | 3515/4096 [00:31<00:07, 82.54it/s, est. speed input: 112850.69 toks/s, output: 110.21 toks/s]
Processed prompts:  87%|████████▋ | 3547/4096 [00:32<00:06, 82.25it/s, est. speed input: 112494.45 toks/s, output: 109.86 toks/s]
Processed prompts:  87%|████████▋ | 3579/4096 [00:32<00:06, 82.04it/s, est. speed input: 112146.38 toks/s, output: 109.52 toks/s]
Processed prompts:  88%|████████▊ | 3611/4096 [00:33<00:05, 81.82it/s, est. speed input: 111802.74 toks/s, output: 109.18 toks/s]
Processed prompts:  89%|████████▉ | 3643/4096 [00:33<00:05, 81.74it/s, est. speed input: 111471.24 toks/s, output: 108.86 toks/s]
Processed prompts:  90%|████████▉ | 3675/4096 [00:33<00:05, 81.63it/s, est. speed input: 111144.39 toks/s, output: 108.54 toks/s]
Processed prompts:  91%|█████████ | 3707/4096 [00:34<00:04, 81.60it/s, est. speed input: 110827.50 toks/s, output: 108.23 toks/s]
Processed prompts:  91%|█████████▏| 3739/4096 [00:34<00:04, 82.19it/s, est. speed input: 110548.77 toks/s, output: 107.96 toks/s]
Processed prompts:  92%|█████████▏| 3771/4096 [00:35<00:03, 81.91it/s, est. speed input: 110241.48 toks/s, output: 107.66 toks/s]
Processed prompts:  93%|█████████▎| 3803/4096 [00:35<00:03, 81.75it/s, est. speed input: 109942.82 toks/s, output: 107.37 toks/s]
Processed prompts:  94%|█████████▎| 3835/4096 [00:35<00:03, 82.32it/s, est. speed input: 109683.82 toks/s, output: 107.11 toks/s]
Processed prompts:  94%|█████████▍| 3867/4096 [00:36<00:02, 82.00it/s, est. speed input: 109395.93 toks/s, output: 106.83 toks/s]
Processed prompts:  95%|█████████▌| 3899/4096 [00:36<00:02, 81.92it/s, est. speed input: 109120.58 toks/s, output: 106.56 toks/s]
Processed prompts:  96%|█████████▌| 3931/4096 [00:36<00:02, 81.82it/s, est. speed input: 108849.72 toks/s, output: 106.30 toks/s]
Processed prompts:  97%|█████████▋| 3963/4096 [00:37<00:01, 81.74it/s, est. speed input: 108583.47 toks/s, output: 106.04 toks/s]
Processed prompts:  98%|█████████▊| 3995/4096 [00:37<00:01, 81.70it/s, est. speed input: 108323.86 toks/s, output: 105.78 toks/s]
Processed prompts:  98%|█████████▊| 4027/4096 [00:38<00:00, 82.27it/s, est. speed input: 108096.11 toks/s, output: 105.56 toks/s]
Processed prompts:  99%|█████████▉| 4059/4096 [00:38<00:00, 82.25it/s, est. speed input: 107854.66 toks/s, output: 105.33 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:38<00:00, 82.25it/s, est. speed input: 108612.22 toks/s, output: 106.07 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:38<00:00, 106.07it/s, est. speed input: 108612.22 toks/s, output: 106.07 toks/s]
[rank0]:[W128 00:12:03.038173791 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 124.6s

测试结果:
  Requests/s:   82.22
  Tokens/s:     84273.00
  Total Reqs:   4096
  Elapsed:      49.82s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     84190.78


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,18.3798,9428.8300,6.9642
1024,1024,1,128,128,18.4275,18888.1411,6.9462
2048,1024,2,256,128,37.4018,38336.7959,6.8446
4096,1024,4,512,128,73.3451,75178.7494,6.9807
8192,1024,8,1024,128,79.1051,81082.7119,12.9448
16384,1024,16,2048,128,81.2977,83330.1153,25.1914
32768,1024,32,4096,128,82.2176,84272.9991,49.8190

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:12:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2224666) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2224666) WARNING 01-28 00:12:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 19.00 requests/s, 9747.00 total tokens/s, 19.00 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:12:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:12:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:12:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:12:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:12:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:12:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:12:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:12:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:12:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:12:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:12:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:12:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:12:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:12:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:12:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:12:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:12:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:12:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2224666) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2224666) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=2224666) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=2224666) 
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=2224666) [2026-01-28 00:12:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2224666) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=2224666) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.54it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  48%|████▊     | 62/128 [00:00<00:00, 619.08it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 640.92it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:28,  4.47it/s, est. speed input: 2288.01 toks/s, output: 4.47 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:10, 11.90it/s, est. speed input: 5417.72 toks/s, output: 10.58 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:07, 15.33it/s, est. speed input: 6827.27 toks/s, output: 13.33 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:07, 16.48it/s, est. speed input: 7331.46 toks/s, output: 14.32 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:06, 17.80it/s, est. speed input: 7892.06 toks/s, output: 15.41 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:06, 17.91it/s, est. speed input: 8067.53 toks/s, output: 15.76 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:06, 18.44it/s, est. speed input: 8281.89 toks/s, output: 16.18 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:05, 19.19it/s, est. speed input: 8569.29 toks/s, output: 16.74 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:05, 19.26it/s, est. speed input: 8685.47 toks/s, output: 16.96 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 19.31it/s, est. speed input: 8782.18 toks/s, output: 17.15 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 19.02it/s, est. speed input: 8827.87 toks/s, output: 17.24 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 18.87it/s, est. speed input: 8872.97 toks/s, output: 17.33 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 19.03it/s, est. speed input: 8939.86 toks/s, output: 17.46 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:04, 19.60it/s, est. speed input: 9068.17 toks/s, output: 17.71 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:04, 19.88it/s, est. speed input: 9172.61 toks/s, output: 17.92 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:04, 20.14it/s, est. speed input: 9269.88 toks/s, output: 18.11 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:04, 20.31it/s, est. speed input: 9354.74 toks/s, output: 18.27 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:04, 20.12it/s, est. speed input: 9402.52 toks/s, output: 18.36 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 20.18it/s, est. speed input: 9460.59 toks/s, output: 18.48 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:03, 20.37it/s, est. speed input: 9524.23 toks/s, output: 18.60 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:03, 20.46it/s, est. speed input: 9578.41 toks/s, output: 18.71 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:02<00:03, 20.56it/s, est. speed input: 9630.16 toks/s, output: 18.81 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:03, 20.09it/s, est. speed input: 9637.04 toks/s, output: 18.82 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 19.90it/s, est. speed input: 9652.56 toks/s, output: 18.85 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 20.11it/s, est. speed input: 9690.91 toks/s, output: 18.93 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:02, 20.03it/s, est. speed input: 9710.57 toks/s, output: 18.97 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:03<00:02, 20.08it/s, est. speed input: 9735.82 toks/s, output: 19.02 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:03<00:02, 20.06it/s, est. speed input: 9755.45 toks/s, output: 19.05 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 19.73it/s, est. speed input: 9754.75 toks/s, output: 19.05 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:02, 19.87it/s, est. speed input: 9775.53 toks/s, output: 19.09 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 20.09it/s, est. speed input: 9802.02 toks/s, output: 19.14 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:04<00:02, 20.28it/s, est. speed input: 9828.35 toks/s, output: 19.20 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:04<00:01, 20.41it/s, est. speed input: 9852.58 toks/s, output: 19.24 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:04<00:01, 20.55it/s, est. speed input: 9877.90 toks/s, output: 19.29 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:04<00:01, 20.47it/s, est. speed input: 9893.10 toks/s, output: 19.32 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 20.51it/s, est. speed input: 9912.00 toks/s, output: 19.36 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 20.55it/s, est. speed input: 9930.48 toks/s, output: 19.40 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:05<00:01, 20.49it/s, est. speed input: 9943.81 toks/s, output: 19.42 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:05<00:01, 20.56it/s, est. speed input: 9961.24 toks/s, output: 19.46 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:05<00:00, 20.23it/s, est. speed input: 9961.78 toks/s, output: 19.46 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:05<00:00, 20.11it/s, est. speed input: 9966.97 toks/s, output: 19.47 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:05<00:00, 20.33it/s, est. speed input: 9984.23 toks/s, output: 19.50 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:06<00:00, 20.44it/s, est. speed input: 9998.99 toks/s, output: 19.53 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:06<00:00, 20.49it/s, est. speed input: 10011.71 toks/s, output: 19.55 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:06<00:00, 20.23it/s, est. speed input: 10012.86 toks/s, output: 19.56 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.41it/s, est. speed input: 10027.52 toks/s, output: 19.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.41it/s, est. speed input: 10027.52 toks/s, output: 19.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 19.58it/s, est. speed input: 10027.52 toks/s, output: 19.58 toks/s]
[rank0]:[W128 00:12:57.763860036 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.6s

测试结果:
  Requests/s:   19.00
  Tokens/s:     9747.00
  Total Reqs:   128
  Elapsed:      6.74s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     9728.00

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:13:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2225755) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2225755) WARNING 01-28 00:13:25 [backends.py:609] Failed to read file <frozen os>
Throughput: 19.01 requests/s, 19487.90 total tokens/s, 19.01 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:13:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:13:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:13:07] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:13:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:13:07] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:13:07] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:13:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:13:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:13:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:13:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:13:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:13:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:13:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:13:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:13:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:13:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:13:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:13:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:13:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:13:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:13:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:13:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:13:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:13:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:13:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:13:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:13:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:13:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:16] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:16] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:16] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:16] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:16] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2225755) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2225755) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]
(EngineCore_DP0 pid=2225755) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]
(EngineCore_DP0 pid=2225755) 
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=2225755) [2026-01-28 00:13:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2225755) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.78it/s]
(EngineCore_DP0 pid=2225755) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.50it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  28%|██▊       | 36/128 [00:00<00:00, 354.33it/s]
Adding requests:  58%|█████▊    | 74/128 [00:00<00:00, 368.05it/s]
Adding requests:  87%|████████▋ | 111/128 [00:00<00:00, 362.13it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 363.43it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:08, 15.35it/s, est. speed input: 15722.52 toks/s, output: 15.35 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:07, 17.07it/s, est. speed input: 17189.98 toks/s, output: 16.79 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:06, 18.27it/s, est. speed input: 18146.13 toks/s, output: 17.72 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:06, 18.85it/s, est. speed input: 18641.10 toks/s, output: 18.20 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:05, 19.63it/s, est. speed input: 19258.57 toks/s, output: 18.81 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:05, 19.62it/s, est. speed input: 19378.36 toks/s, output: 18.92 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:05, 20.21it/s, est. speed input: 19773.32 toks/s, output: 19.31 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:05, 19.89it/s, est. speed input: 19780.22 toks/s, output: 19.32 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:05, 20.23it/s, est. speed input: 19988.31 toks/s, output: 19.52 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 20.55it/s, est. speed input: 20185.30 toks/s, output: 19.71 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:04, 20.80it/s, est. speed input: 20352.76 toks/s, output: 19.88 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:04, 21.01it/s, est. speed input: 20503.27 toks/s, output: 20.02 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:04, 21.09it/s, est. speed input: 20609.23 toks/s, output: 20.13 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:04, 21.02it/s, est. speed input: 20669.29 toks/s, output: 20.18 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:04, 20.88it/s, est. speed input: 20697.29 toks/s, output: 20.21 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 20.92it/s, est. speed input: 20751.99 toks/s, output: 20.27 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:03, 20.93it/s, est. speed input: 20797.40 toks/s, output: 20.31 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:03, 20.66it/s, est. speed input: 20781.87 toks/s, output: 20.29 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:03, 20.58it/s, est. speed input: 20787.63 toks/s, output: 20.30 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:02<00:03, 20.22it/s, est. speed input: 20736.25 toks/s, output: 20.25 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:02<00:03, 20.02it/s, est. speed input: 20698.60 toks/s, output: 20.21 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 19.95it/s, est. speed input: 20676.89 toks/s, output: 20.19 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:03, 19.83it/s, est. speed input: 20646.06 toks/s, output: 20.16 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 19.87it/s, est. speed input: 20640.84 toks/s, output: 20.16 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:03, 19.87it/s, est. speed input: 20631.61 toks/s, output: 20.15 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:03<00:02, 19.86it/s, est. speed input: 20622.92 toks/s, output: 20.14 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:03<00:02, 19.89it/s, est. speed input: 20614.70 toks/s, output: 20.13 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:03<00:02, 19.74it/s, est. speed input: 20591.65 toks/s, output: 20.11 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:03<00:02, 20.04it/s, est. speed input: 20610.33 toks/s, output: 20.13 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:03<00:02, 19.82it/s, est. speed input: 20585.89 toks/s, output: 20.10 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 19.55it/s, est. speed input: 20551.79 toks/s, output: 20.07 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:04<00:02, 19.17it/s, est. speed input: 20502.10 toks/s, output: 20.02 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:04<00:02, 19.39it/s, est. speed input: 20501.17 toks/s, output: 20.02 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:04<00:01, 19.75it/s, est. speed input: 20513.12 toks/s, output: 20.03 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:04<00:01, 20.15it/s, est. speed input: 20542.47 toks/s, output: 20.06 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:04<00:01, 20.22it/s, est. speed input: 20552.20 toks/s, output: 20.07 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:04<00:01, 20.00it/s, est. speed input: 20535.34 toks/s, output: 20.05 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 20.21it/s, est. speed input: 20554.10 toks/s, output: 20.07 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:05<00:01, 19.84it/s, est. speed input: 20523.08 toks/s, output: 20.04 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:05<00:01, 19.73it/s, est. speed input: 20509.43 toks/s, output: 20.03 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:05<00:00, 19.96it/s, est. speed input: 20520.92 toks/s, output: 20.04 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:05<00:00, 20.22it/s, est. speed input: 20541.28 toks/s, output: 20.06 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:05<00:00, 20.27it/s, est. speed input: 20549.80 toks/s, output: 20.07 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:05<00:00, 20.33it/s, est. speed input: 20560.03 toks/s, output: 20.08 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:06<00:00, 20.39it/s, est. speed input: 20570.56 toks/s, output: 20.09 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:06<00:00, 20.42it/s, est. speed input: 20580.72 toks/s, output: 20.10 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:06<00:00, 20.11it/s, est. speed input: 20564.22 toks/s, output: 20.08 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.11it/s, est. speed input: 20548.02 toks/s, output: 20.07 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.07it/s, est. speed input: 20548.02 toks/s, output: 20.07 toks/s]
[rank0]:[W128 00:13:50.295779541 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.6s

测试结果:
  Requests/s:   19.01
  Tokens/s:     19487.90
  Total Reqs:   128
  Elapsed:      6.73s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     19468.89

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:14:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2227166) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2227166) WARNING 01-28 00:14:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.39 requests/s, 40377.64 total tokens/s, 39.39 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 00:14:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:14:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:14:01] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:14:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:01] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:01] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:14:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:14:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:14:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:14:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:14:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:14:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:14:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:14:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:14:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:14:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:14:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:14:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:14:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:14:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2227166) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2227166) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.45it/s]
(EngineCore_DP0 pid=2227166) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.45it/s]
(EngineCore_DP0 pid=2227166) 
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=2227166) [2026-01-28 00:14:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2227166) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.08it/s]
(EngineCore_DP0 pid=2227166) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  9.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.60it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.51it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  15%|█▌        | 39/256 [00:00<00:00, 375.70it/s]
Adding requests:  31%|███       | 79/256 [00:00<00:00, 386.16it/s]
Adding requests:  46%|████▌     | 118/256 [00:00<00:00, 360.92it/s]
Adding requests:  61%|██████    | 155/256 [00:00<00:00, 355.54it/s]
Adding requests:  75%|███████▌  | 193/256 [00:00<00:00, 360.31it/s]
Adding requests:  91%|█████████ | 233/256 [00:00<00:00, 372.75it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 371.87it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 26/256 [00:00<00:01, 199.44it/s, est. speed input: 204258.89 toks/s, output: 199.44 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:00<00:03, 65.88it/s, est. speed input: 76109.22 toks/s, output: 74.32 toks/s]   
Processed prompts:  22%|██▏       | 57/256 [00:00<00:03, 57.82it/s, est. speed input: 67420.59 toks/s, output: 65.84 toks/s]
Processed prompts:  25%|██▌       | 65/256 [00:01<00:03, 53.31it/s, est. speed input: 63120.20 toks/s, output: 61.64 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:03, 48.59it/s, est. speed input: 59340.01 toks/s, output: 57.95 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:03, 47.12it/s, est. speed input: 57721.57 toks/s, output: 56.37 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:03, 45.92it/s, est. speed input: 56396.31 toks/s, output: 55.07 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:03, 43.64it/s, est. speed input: 54760.98 toks/s, output: 53.48 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:01<00:03, 44.62it/s, est. speed input: 54448.09 toks/s, output: 53.17 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:01<00:03, 40.96it/s, est. speed input: 52810.37 toks/s, output: 51.57 toks/s]
Processed prompts:  41%|████      | 105/256 [00:02<00:03, 42.96it/s, est. speed input: 52722.14 toks/s, output: 51.49 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:03, 39.98it/s, est. speed input: 51488.01 toks/s, output: 50.28 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:02<00:03, 40.31it/s, est. speed input: 50895.20 toks/s, output: 49.70 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:02<00:03, 40.52it/s, est. speed input: 50369.75 toks/s, output: 49.19 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:02<00:03, 40.04it/s, est. speed input: 49761.21 toks/s, output: 48.59 toks/s]
Processed prompts:  52%|█████▏    | 133/256 [00:02<00:02, 41.55it/s, est. speed input: 49668.35 toks/s, output: 48.50 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:02<00:03, 38.72it/s, est. speed input: 48828.09 toks/s, output: 47.68 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:02<00:02, 38.94it/s, est. speed input: 48551.11 toks/s, output: 47.41 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:03<00:02, 39.74it/s, est. speed input: 48266.75 toks/s, output: 47.14 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 40.23it/s, est. speed input: 48000.77 toks/s, output: 46.88 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:03<00:02, 42.19it/s, est. speed input: 48042.37 toks/s, output: 46.92 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:03<00:02, 39.59it/s, est. speed input: 47510.30 toks/s, output: 46.40 toks/s]
Processed prompts:  66%|██████▌   | 169/256 [00:03<00:02, 42.05it/s, est. speed input: 47600.81 toks/s, output: 46.49 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:03<00:02, 38.84it/s, est. speed input: 47037.02 toks/s, output: 45.93 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:03<00:01, 39.62it/s, est. speed input: 46860.35 toks/s, output: 45.76 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 40.15it/s, est. speed input: 46698.10 toks/s, output: 45.60 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:04<00:01, 40.53it/s, est. speed input: 46549.37 toks/s, output: 45.46 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:04<00:01, 40.61it/s, est. speed input: 46388.53 toks/s, output: 45.30 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:04<00:01, 40.70it/s, est. speed input: 46242.31 toks/s, output: 45.16 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:04<00:01, 40.75it/s, est. speed input: 46103.70 toks/s, output: 45.02 toks/s]
Processed prompts:  84%|████████▍ | 215/256 [00:04<00:00, 42.05it/s, est. speed input: 46128.20 toks/s, output: 45.05 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:04<00:00, 39.24it/s, est. speed input: 45765.25 toks/s, output: 44.69 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:05<00:00, 39.83it/s, est. speed input: 45660.89 toks/s, output: 44.59 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:05<00:00, 40.32it/s, est. speed input: 45571.09 toks/s, output: 44.50 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:05<00:00, 40.31it/s, est. speed input: 45450.85 toks/s, output: 44.39 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:05<00:00, 40.45it/s, est. speed input: 45352.04 toks/s, output: 44.29 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:05<00:00, 40.44it/s, est. speed input: 45248.09 toks/s, output: 44.19 toks/s]
Processed prompts: 100%|█████████▉| 255/256 [00:05<00:00, 42.61it/s, est. speed input: 45351.10 toks/s, output: 44.29 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 42.61it/s, est. speed input: 45132.67 toks/s, output: 44.07 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 44.07it/s, est. speed input: 45132.67 toks/s, output: 44.07 toks/s]
[rank0]:[W128 00:14:45.729559006 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.3s

测试结果:
  Requests/s:   39.39
  Tokens/s:     40377.64
  Total Reqs:   256
  Elapsed:      6.50s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     40338.24

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:14:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2228245) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2228245) WARNING 01-28 00:15:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 62.60 requests/s, 64160.47 total tokens/s, 62.60 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 00:14:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:14:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:14:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:14:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:14:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:14:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:14:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:14:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:14:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:15:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:15:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:15:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:15:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:15:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:15:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:15:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:15:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:15:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:06] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:06] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:06] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:06] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:06] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2228245) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2228245) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.44it/s]
(EngineCore_DP0 pid=2228245) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.44it/s]
(EngineCore_DP0 pid=2228245) 
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=2228245) [2026-01-28 00:15:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2228245) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  9.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.85it/s]
(EngineCore_DP0 pid=2228245) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  9.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  9.39it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 38/512 [00:00<00:01, 373.12it/s]
Adding requests:  15%|█▌        | 78/512 [00:00<00:01, 387.01it/s]
Adding requests:  23%|██▎       | 117/512 [00:00<00:01, 375.09it/s]
Adding requests:  30%|███       | 155/512 [00:00<00:00, 368.93it/s]
Adding requests:  38%|███▊      | 192/512 [00:00<00:00, 368.99it/s]
Adding requests:  45%|████▌     | 231/512 [00:00<00:00, 375.48it/s]
Adding requests:  53%|█████▎    | 269/512 [00:00<00:00, 376.73it/s]
Adding requests:  60%|██████    | 308/512 [00:00<00:00, 378.61it/s]
Adding requests:  68%|██████▊   | 349/512 [00:00<00:00, 385.80it/s]
Adding requests:  76%|███████▋  | 391/512 [00:01<00:00, 394.37it/s]
Adding requests:  84%|████████▍ | 431/512 [00:01<00:00, 394.98it/s]
Adding requests:  92%|█████████▏| 471/512 [00:01<00:00, 387.49it/s]
Adding requests: 100%|█████████▉| 510/512 [00:01<00:00, 383.32it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 382.19it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:00<00:00, 553.25it/s, est. speed input: 566584.48 toks/s, output: 553.26 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:01<00:03, 117.08it/s, est. speed input: 139936.20 toks/s, output: 136.65 toks/s]
Processed prompts:  33%|███▎      | 169/512 [00:01<00:03, 100.72it/s, est. speed input: 121867.94 toks/s, output: 119.01 toks/s]
Processed prompts:  37%|███▋      | 187/512 [00:01<00:03, 87.87it/s, est. speed input: 110370.72 toks/s, output: 107.78 toks/s] 
Processed prompts:  39%|███▉      | 200/512 [00:01<00:03, 83.96it/s, est. speed input: 106407.44 toks/s, output: 103.91 toks/s]
Processed prompts:  41%|████      | 211/512 [00:02<00:03, 78.02it/s, est. speed input: 102131.63 toks/s, output: 99.74 toks/s] 
Processed prompts:  43%|████▎     | 221/512 [00:02<00:03, 78.27it/s, est. speed input: 100962.91 toks/s, output: 98.60 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:02<00:04, 69.82it/s, est. speed input: 96872.52 toks/s, output: 94.60 toks/s] 
Processed prompts:  46%|████▋     | 238/512 [00:02<00:03, 68.51it/s, est. speed input: 95303.03 toks/s, output: 93.07 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:02<00:03, 67.32it/s, est. speed input: 93865.21 toks/s, output: 91.66 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:02<00:03, 66.37it/s, est. speed input: 92568.73 toks/s, output: 90.40 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:02<00:03, 65.52it/s, est. speed input: 91363.00 toks/s, output: 89.22 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:03<00:03, 64.85it/s, est. speed input: 90253.66 toks/s, output: 88.14 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:03<00:03, 64.33it/s, est. speed input: 89228.41 toks/s, output: 87.14 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:03<00:03, 63.95it/s, est. speed input: 88282.51 toks/s, output: 86.21 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:03<00:03, 63.74it/s, est. speed input: 87417.46 toks/s, output: 85.37 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:03<00:03, 63.60it/s, est. speed input: 86614.58 toks/s, output: 84.58 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:03<00:03, 63.56it/s, est. speed input: 85878.06 toks/s, output: 83.86 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:03<00:03, 63.39it/s, est. speed input: 85167.97 toks/s, output: 83.17 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:03<00:02, 63.35it/s, est. speed input: 84514.83 toks/s, output: 82.53 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:04<00:02, 63.33it/s, est. speed input: 83903.28 toks/s, output: 81.94 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:04<00:02, 64.11it/s, est. speed input: 83432.74 toks/s, output: 81.48 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:04<00:02, 63.90it/s, est. speed input: 82893.57 toks/s, output: 80.95 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:04<00:02, 63.55it/s, est. speed input: 82358.71 toks/s, output: 80.43 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:04<00:02, 63.53it/s, est. speed input: 81880.91 toks/s, output: 79.96 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:04<00:02, 63.49it/s, est. speed input: 81426.10 toks/s, output: 79.52 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:04<00:02, 63.44it/s, est. speed input: 80992.42 toks/s, output: 79.09 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:04<00:01, 63.34it/s, est. speed input: 80573.74 toks/s, output: 78.68 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:05<00:01, 63.20it/s, est. speed input: 80168.30 toks/s, output: 78.29 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:05<00:01, 63.10it/s, est. speed input: 79782.66 toks/s, output: 77.91 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:05<00:01, 63.01it/s, est. speed input: 79413.22 toks/s, output: 77.55 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:05<00:01, 63.09it/s, est. speed input: 79075.00 toks/s, output: 77.22 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:05<00:01, 63.17it/s, est. speed input: 78754.82 toks/s, output: 76.91 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:05<00:01, 63.27it/s, est. speed input: 78451.85 toks/s, output: 76.61 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:05<00:01, 63.21it/s, est. speed input: 78151.41 toks/s, output: 76.32 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:05<00:00, 64.72it/s, est. speed input: 77995.07 toks/s, output: 76.17 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:06<00:00, 64.09it/s, est. speed input: 77705.70 toks/s, output: 75.88 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:06<00:00, 63.75it/s, est. speed input: 77434.71 toks/s, output: 75.62 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:06<00:00, 63.68it/s, est. speed input: 77188.85 toks/s, output: 75.38 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:06<00:00, 63.48it/s, est. speed input: 76940.47 toks/s, output: 75.14 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:06<00:00, 63.36it/s, est. speed input: 76702.83 toks/s, output: 74.90 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:06<00:00, 63.08it/s, est. speed input: 76460.27 toks/s, output: 74.67 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:06<00:00, 64.89it/s, est. speed input: 76371.39 toks/s, output: 74.58 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 64.89it/s, est. speed input: 76668.81 toks/s, output: 74.87 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 74.87it/s, est. speed input: 76668.81 toks/s, output: 74.87 toks/s]
[rank0]:[W128 00:15:42.341872793 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 57.6s

测试结果:
  Requests/s:   62.60
  Tokens/s:     64160.47
  Total Reqs:   512
  Elapsed:      8.18s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     64097.88

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:15:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2229299) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2229299) WARNING 01-28 00:16:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 65.56 requests/s, 67194.70 total tokens/s, 65.56 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 00:15:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:15:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:15:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:15:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:15:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:15:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:15:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:15:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:15:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:16:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:16:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:16:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:16:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:16:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:16:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:16:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:16:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:16:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:06] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:06] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:06] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:06] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:06] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2229299) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2229299) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.42it/s]
(EngineCore_DP0 pid=2229299) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.42it/s]
(EngineCore_DP0 pid=2229299) 
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=2229299) [2026-01-28 00:16:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2229299) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  8.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  9.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.46it/s]
(EngineCore_DP0 pid=2229299) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  8.60it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  9.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.79it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 36/1024 [00:00<00:02, 352.85it/s]
Adding requests:   7%|▋         | 75/1024 [00:00<00:02, 371.50it/s]
Adding requests:  11%|█         | 113/1024 [00:00<00:02, 372.29it/s]
Adding requests:  15%|█▍        | 151/1024 [00:00<00:02, 362.67it/s]
Adding requests:  18%|█▊        | 189/1024 [00:00<00:02, 367.86it/s]
Adding requests:  22%|██▏       | 230/1024 [00:00<00:02, 381.04it/s]
Adding requests:  26%|██▋       | 271/1024 [00:00<00:01, 388.15it/s]
Adding requests:  30%|███       | 310/1024 [00:00<00:01, 373.45it/s]
Adding requests:  34%|███▍      | 348/1024 [00:00<00:01, 372.26it/s]
Adding requests:  38%|███▊      | 386/1024 [00:01<00:01, 370.85it/s]
Adding requests:  42%|████▏     | 427/1024 [00:01<00:01, 380.12it/s]
Adding requests:  46%|████▌     | 468/1024 [00:01<00:01, 386.86it/s]
Adding requests:  50%|████▉     | 508/1024 [00:01<00:01, 386.78it/s]
Adding requests:  53%|█████▎    | 547/1024 [00:01<00:01, 381.71it/s]
Adding requests:  58%|█████▊    | 589/1024 [00:01<00:01, 392.92it/s]
Adding requests:  61%|██████▏   | 629/1024 [00:01<00:01, 393.16it/s]
Adding requests:  65%|██████▌   | 669/1024 [00:01<00:00, 394.61it/s]
Adding requests:  70%|██████▉   | 712/1024 [00:01<00:00, 402.43it/s]
Adding requests:  74%|███████▎  | 753/1024 [00:01<00:00, 401.12it/s]
Adding requests:  78%|███████▊  | 794/1024 [00:02<00:00, 391.68it/s]
Adding requests:  81%|████████▏ | 834/1024 [00:02<00:00, 389.25it/s]
Adding requests:  86%|████████▌ | 877/1024 [00:02<00:00, 398.77it/s]
Adding requests:  90%|████████▉ | 920/1024 [00:02<00:00, 405.90it/s]
Adding requests:  94%|█████████▍| 961/1024 [00:02<00:00, 405.77it/s]
Adding requests:  98%|█████████▊| 1002/1024 [00:02<00:00, 404.62it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 388.58it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:00<00:00, 1340.54it/s, est. speed input: 1372867.43 toks/s, output: 1340.58 toks/s]
Processed prompts:  30%|██▉       | 305/1024 [00:02<00:05, 124.88it/s, est. speed input: 150734.80 toks/s, output: 147.20 toks/s]   
Processed prompts:  36%|███▌      | 365/1024 [00:03<00:06, 99.59it/s, est. speed input: 123113.09 toks/s, output: 120.23 toks/s] 
Processed prompts:  39%|███▉      | 401/1024 [00:03<00:06, 93.84it/s, est. speed input: 116588.29 toks/s, output: 113.86 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:04<00:07, 83.50it/s, est. speed input: 108827.41 toks/s, output: 106.28 toks/s]
Processed prompts:  43%|████▎     | 444/1024 [00:04<00:07, 82.08it/s, est. speed input: 106929.34 toks/s, output: 104.42 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:04<00:07, 78.39it/s, est. speed input: 104576.90 toks/s, output: 102.13 toks/s]
Processed prompts:  46%|████▌     | 470/1024 [00:04<00:06, 80.64it/s, est. speed input: 104481.28 toks/s, output: 102.03 toks/s]
Processed prompts:  47%|████▋     | 481/1024 [00:04<00:06, 81.97it/s, est. speed input: 104171.36 toks/s, output: 101.73 toks/s]
Processed prompts:  48%|████▊     | 492/1024 [00:04<00:07, 71.53it/s, est. speed input: 101321.68 toks/s, output: 98.95 toks/s] 
Processed prompts:  49%|████▉     | 501/1024 [00:05<00:07, 71.92it/s, est. speed input: 100706.60 toks/s, output: 98.35 toks/s]
Processed prompts:  50%|████▉     | 510/1024 [00:05<00:07, 72.24it/s, est. speed input: 100112.92 toks/s, output: 97.77 toks/s]
Processed prompts:  51%|█████     | 518/1024 [00:05<00:07, 70.84it/s, est. speed input: 99365.96 toks/s, output: 97.04 toks/s] 
Processed prompts:  51%|█████▏    | 526/1024 [00:05<00:07, 69.69it/s, est. speed input: 98657.92 toks/s, output: 96.35 toks/s]
Processed prompts:  52%|█████▏    | 534/1024 [00:05<00:07, 68.65it/s, est. speed input: 97969.06 toks/s, output: 95.67 toks/s]
Processed prompts:  53%|█████▎    | 542/1024 [00:05<00:07, 67.91it/s, est. speed input: 97318.69 toks/s, output: 95.04 toks/s]
Processed prompts:  54%|█████▎    | 549/1024 [00:05<00:07, 65.09it/s, est. speed input: 96519.64 toks/s, output: 94.26 toks/s]
Processed prompts:  54%|█████▍    | 556/1024 [00:05<00:07, 62.85it/s, est. speed input: 95737.83 toks/s, output: 93.49 toks/s]
Processed prompts:  55%|█████▍    | 563/1024 [00:06<00:07, 61.41it/s, est. speed input: 95009.00 toks/s, output: 92.78 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:06<00:07, 60.30it/s, est. speed input: 94301.74 toks/s, output: 92.09 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:06<00:07, 61.84it/s, est. speed input: 93775.66 toks/s, output: 91.58 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:06<00:06, 63.03it/s, est. speed input: 93278.47 toks/s, output: 91.09 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:06<00:06, 63.73it/s, est. speed input: 92787.65 toks/s, output: 90.61 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:06<00:06, 64.28it/s, est. speed input: 92318.45 toks/s, output: 90.15 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:06<00:06, 64.73it/s, est. speed input: 91872.78 toks/s, output: 89.72 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:06<00:06, 65.02it/s, est. speed input: 91440.05 toks/s, output: 89.30 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:07<00:06, 65.29it/s, est. speed input: 91027.42 toks/s, output: 88.89 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:07<00:05, 65.38it/s, est. speed input: 90621.49 toks/s, output: 88.50 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:07<00:05, 65.62it/s, est. speed input: 90242.46 toks/s, output: 88.13 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:07<00:05, 65.77it/s, est. speed input: 89874.23 toks/s, output: 87.77 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:07<00:05, 65.70it/s, est. speed input: 89505.20 toks/s, output: 87.41 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:07<00:05, 65.68it/s, est. speed input: 89149.95 toks/s, output: 87.06 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:07<00:05, 65.65it/s, est. speed input: 88804.38 toks/s, output: 86.72 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:07<00:05, 65.70it/s, est. speed input: 88474.79 toks/s, output: 86.40 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:08<00:05, 65.58it/s, est. speed input: 88144.55 toks/s, output: 86.08 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:08<00:04, 65.57it/s, est. speed input: 87829.57 toks/s, output: 85.77 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:08<00:04, 65.54it/s, est. speed input: 87521.52 toks/s, output: 85.47 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:08<00:04, 65.63it/s, est. speed input: 87230.60 toks/s, output: 85.19 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:08<00:04, 65.68it/s, est. speed input: 86946.78 toks/s, output: 84.91 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:08<00:04, 65.57it/s, est. speed input: 86661.67 toks/s, output: 84.63 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:08<00:04, 65.51it/s, est. speed input: 86385.72 toks/s, output: 84.36 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:08<00:04, 65.59it/s, est. speed input: 86124.72 toks/s, output: 84.11 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:08<00:04, 65.64it/s, est. speed input: 85870.35 toks/s, output: 83.86 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:09<00:03, 65.57it/s, est. speed input: 85617.20 toks/s, output: 83.61 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:09<00:03, 65.53it/s, est. speed input: 85370.87 toks/s, output: 83.37 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:09<00:03, 65.65it/s, est. speed input: 85139.29 toks/s, output: 83.14 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:09<00:03, 65.58it/s, est. speed input: 84905.23 toks/s, output: 82.92 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:09<00:03, 65.64it/s, est. speed input: 84682.63 toks/s, output: 82.70 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:09<00:03, 65.57it/s, est. speed input: 84459.86 toks/s, output: 82.48 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:09<00:03, 65.70it/s, est. speed input: 84252.41 toks/s, output: 82.28 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:09<00:03, 65.70it/s, est. speed input: 84044.70 toks/s, output: 82.07 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:10<00:03, 65.66it/s, est. speed input: 83840.29 toks/s, output: 81.88 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:10<00:02, 65.74it/s, est. speed input: 83646.24 toks/s, output: 81.69 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:10<00:02, 65.57it/s, est. speed input: 83445.23 toks/s, output: 81.49 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:10<00:02, 65.50it/s, est. speed input: 83251.49 toks/s, output: 81.30 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:10<00:02, 65.69it/s, est. speed input: 83073.93 toks/s, output: 81.13 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:10<00:02, 65.59it/s, est. speed input: 82889.41 toks/s, output: 80.95 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:10<00:02, 65.58it/s, est. speed input: 82711.76 toks/s, output: 80.77 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:10<00:02, 65.56it/s, est. speed input: 82537.13 toks/s, output: 80.60 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:11<00:02, 65.61it/s, est. speed input: 82369.49 toks/s, output: 80.44 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:11<00:01, 65.58it/s, est. speed input: 82202.63 toks/s, output: 80.28 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:11<00:01, 65.32it/s, est. speed input: 82028.65 toks/s, output: 80.11 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:11<00:01, 65.42it/s, est. speed input: 81870.94 toks/s, output: 79.95 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:11<00:01, 65.50it/s, est. speed input: 81716.85 toks/s, output: 79.80 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:11<00:01, 65.41it/s, est. speed input: 81559.60 toks/s, output: 79.65 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:11<00:01, 67.49it/s, est. speed input: 81495.29 toks/s, output: 79.59 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:11<00:01, 66.82it/s, est. speed input: 81344.80 toks/s, output: 79.44 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:12<00:01, 66.39it/s, est. speed input: 81198.75 toks/s, output: 79.30 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:12<00:00, 66.14it/s, est. speed input: 81057.43 toks/s, output: 79.16 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:12<00:00, 65.90it/s, est. speed input: 80916.38 toks/s, output: 79.02 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:12<00:00, 65.84it/s, est. speed input: 80782.38 toks/s, output: 78.89 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:12<00:00, 67.81it/s, est. speed input: 80728.52 toks/s, output: 78.84 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:12<00:00, 67.14it/s, est. speed input: 80597.99 toks/s, output: 78.71 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:12<00:00, 66.51it/s, est. speed input: 80463.68 toks/s, output: 78.58 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:12<00:00, 66.25it/s, est. speed input: 80338.37 toks/s, output: 78.46 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:12<00:00, 68.18it/s, est. speed input: 80292.67 toks/s, output: 78.41 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 68.18it/s, est. speed input: 80764.63 toks/s, output: 78.87 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 78.87it/s, est. speed input: 80764.63 toks/s, output: 78.87 toks/s]
[rank0]:[W128 00:16:51.484024618 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 68.2s

测试结果:
  Requests/s:   65.56
  Tokens/s:     67194.70
  Total Reqs:   1024
  Elapsed:      15.62s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     67129.15

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:17:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2230553) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2230553) WARNING 01-28 00:17:29 [backends.py:609] Failed to read file <frozen os>
Throughput: 66.93 requests/s, 68604.00 total tokens/s, 66.93 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 00:17:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:17:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:17:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:17:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:17:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:17:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:17:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:17:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:17:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:17:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:17:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:17:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:17:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:17:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:17:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:17:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:17:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:17:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2230553) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2230553) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.41it/s]
(EngineCore_DP0 pid=2230553) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.41it/s]
(EngineCore_DP0 pid=2230553) 
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=2230553) [2026-01-28 00:17:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2230553) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  9.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 10.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 10.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.91it/s]
(EngineCore_DP0 pid=2230553) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  9.22it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  9.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.79it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 35/2048 [00:00<00:05, 341.99it/s]
Adding requests:   4%|▎         | 74/2048 [00:00<00:05, 367.61it/s]
Adding requests:   5%|▌         | 111/2048 [00:00<00:05, 363.68it/s]
Adding requests:   7%|▋         | 148/2048 [00:00<00:05, 356.62it/s]
Adding requests:   9%|▉         | 187/2048 [00:00<00:05, 365.15it/s]
Adding requests:  11%|█         | 225/2048 [00:00<00:04, 369.41it/s]
Adding requests:  13%|█▎        | 266/2048 [00:00<00:04, 380.06it/s]
Adding requests:  15%|█▍        | 305/2048 [00:00<00:04, 373.33it/s]
Adding requests:  17%|█▋        | 343/2048 [00:00<00:04, 364.44it/s]
Adding requests:  19%|█▊        | 380/2048 [00:01<00:04, 361.49it/s]
Adding requests:  20%|██        | 417/2048 [00:01<00:04, 363.96it/s]
Adding requests:  22%|██▏       | 455/2048 [00:01<00:04, 366.37it/s]
Adding requests:  24%|██▍       | 492/2048 [00:01<00:04, 363.82it/s]
Adding requests:  26%|██▌       | 529/2048 [00:01<00:04, 353.15it/s]
Adding requests:  28%|██▊       | 570/2048 [00:01<00:04, 369.46it/s]
Adding requests:  30%|██▉       | 610/2048 [00:01<00:03, 376.37it/s]
Adding requests:  32%|███▏      | 651/2048 [00:01<00:03, 386.06it/s]
Adding requests:  34%|███▍      | 693/2048 [00:01<00:03, 395.58it/s]
Adding requests:  36%|███▌      | 734/2048 [00:01<00:03, 398.11it/s]
Adding requests:  38%|███▊      | 774/2048 [00:02<00:03, 377.31it/s]
Adding requests:  40%|███▉      | 812/2048 [00:02<00:03, 377.12it/s]
Adding requests:  42%|████▏     | 850/2048 [00:02<00:03, 374.08it/s]
Adding requests:  44%|████▎     | 893/2048 [00:02<00:02, 389.31it/s]
Adding requests:  46%|████▌     | 934/2048 [00:02<00:02, 392.81it/s]
Adding requests:  48%|████▊     | 975/2048 [00:02<00:02, 395.73it/s]
Adding requests:  50%|████▉     | 1017/2048 [00:02<00:02, 400.61it/s]
Adding requests:  52%|█████▏    | 1058/2048 [00:02<00:02, 401.04it/s]
Adding requests:  54%|█████▎    | 1099/2048 [00:02<00:02, 402.35it/s]
Adding requests:  56%|█████▌    | 1140/2048 [00:03<00:02, 389.44it/s]
Adding requests:  58%|█████▊    | 1180/2048 [00:03<00:02, 388.89it/s]
Adding requests:  60%|█████▉    | 1222/2048 [00:03<00:02, 395.55it/s]
Adding requests:  62%|██████▏   | 1262/2048 [00:03<00:02, 390.99it/s]
Adding requests:  64%|██████▎   | 1302/2048 [00:03<00:01, 387.22it/s]
Adding requests:  65%|██████▌   | 1341/2048 [00:03<00:01, 382.78it/s]
Adding requests:  67%|██████▋   | 1380/2048 [00:03<00:01, 382.33it/s]
Adding requests:  69%|██████▉   | 1421/2048 [00:03<00:01, 389.47it/s]
Adding requests:  71%|███████▏  | 1462/2048 [00:03<00:01, 395.10it/s]
Adding requests:  73%|███████▎  | 1504/2048 [00:03<00:01, 399.81it/s]
Adding requests:  75%|███████▌  | 1545/2048 [00:04<00:01, 396.35it/s]
Adding requests:  77%|███████▋  | 1587/2048 [00:04<00:01, 401.17it/s]
Adding requests:  80%|███████▉  | 1629/2048 [00:04<00:01, 405.69it/s]
Adding requests:  82%|████████▏ | 1670/2048 [00:04<00:00, 404.58it/s]
Adding requests:  84%|████████▎ | 1711/2048 [00:04<00:00, 405.34it/s]
Adding requests:  86%|████████▌ | 1753/2048 [00:04<00:00, 408.51it/s]
Adding requests:  88%|████████▊ | 1794/2048 [00:04<00:00, 406.61it/s]
Adding requests:  90%|████████▉ | 1835/2048 [00:04<00:00, 404.95it/s]
Adding requests:  92%|█████████▏| 1876/2048 [00:04<00:00, 405.81it/s]
Adding requests:  94%|█████████▎| 1917/2048 [00:04<00:00, 395.44it/s]
Adding requests:  96%|█████████▌| 1957/2048 [00:05<00:00, 391.48it/s]
Adding requests:  98%|█████████▊| 1997/2048 [00:05<00:00, 385.43it/s]
Adding requests:  99%|█████████▉| 2036/2048 [00:05<00:00, 382.84it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 385.65it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:00<00:01, 1390.43it/s, est. speed input: 1423895.40 toks/s, output: 1390.46 toks/s]
Processed prompts:  24%|██▍       | 494/2048 [00:02<00:08, 186.78it/s, est. speed input: 234994.65 toks/s, output: 229.49 toks/s]   
Processed prompts:  27%|██▋       | 556/2048 [00:03<00:10, 138.39it/s, est. speed input: 183138.55 toks/s, output: 178.85 toks/s]
Processed prompts:  29%|██▉       | 593/2048 [00:03<00:11, 125.02it/s, est. speed input: 169374.49 toks/s, output: 165.40 toks/s]
Processed prompts:  30%|███       | 619/2048 [00:04<00:13, 108.05it/s, est. speed input: 155986.82 toks/s, output: 152.33 toks/s]
Processed prompts:  31%|███       | 638/2048 [00:04<00:13, 103.90it/s, est. speed input: 151859.97 toks/s, output: 148.30 toks/s]
Processed prompts:  32%|███▏      | 653/2048 [00:04<00:14, 96.76it/s, est. speed input: 147233.33 toks/s, output: 143.78 toks/s] 
Processed prompts:  32%|███▏      | 665/2048 [00:04<00:15, 87.60it/s, est. speed input: 142468.25 toks/s, output: 139.13 toks/s]
Processed prompts:  33%|███▎      | 675/2048 [00:05<00:17, 77.49it/s, est. speed input: 137718.28 toks/s, output: 134.49 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:05<00:18, 73.94it/s, est. speed input: 134371.75 toks/s, output: 131.22 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:05<00:18, 72.16it/s, est. speed input: 131513.26 toks/s, output: 128.43 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:05<00:18, 70.84it/s, est. speed input: 128910.28 toks/s, output: 125.89 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:05<00:18, 69.81it/s, est. speed input: 126507.35 toks/s, output: 123.54 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:06<00:18, 69.06it/s, est. speed input: 124292.06 toks/s, output: 121.38 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:06<00:18, 68.43it/s, est. speed input: 122221.62 toks/s, output: 119.36 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:06<00:18, 67.96it/s, est. speed input: 120297.72 toks/s, output: 117.48 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:06<00:18, 67.68it/s, est. speed input: 118515.17 toks/s, output: 115.74 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:07<00:18, 67.54it/s, est. speed input: 116865.29 toks/s, output: 114.13 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:07<00:18, 67.40it/s, est. speed input: 115312.27 toks/s, output: 112.61 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:07<00:17, 67.24it/s, est. speed input: 113847.15 toks/s, output: 111.18 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:07<00:17, 67.10it/s, est. speed input: 112465.24 toks/s, output: 109.83 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:08<00:17, 67.14it/s, est. speed input: 111186.67 toks/s, output: 108.58 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:08<00:17, 67.15it/s, est. speed input: 109978.69 toks/s, output: 107.40 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:08<00:16, 67.05it/s, est. speed input: 108821.83 toks/s, output: 106.27 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:08<00:16, 67.97it/s, est. speed input: 107869.06 toks/s, output: 105.34 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:09<00:16, 67.71it/s, est. speed input: 106839.45 toks/s, output: 104.34 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:09<00:16, 67.52it/s, est. speed input: 105862.48 toks/s, output: 103.38 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:09<00:15, 68.46it/s, est. speed input: 105069.85 toks/s, output: 102.61 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:09<00:15, 67.96it/s, est. speed input: 104171.28 toks/s, output: 101.73 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:10<00:15, 67.55it/s, est. speed input: 103308.41 toks/s, output: 100.89 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:10<00:15, 67.32it/s, est. speed input: 102492.32 toks/s, output: 100.09 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:10<00:14, 67.27it/s, est. speed input: 101726.14 toks/s, output: 99.34 toks/s] 
Processed prompts:  52%|█████▏    | 1058/2048 [00:10<00:14, 67.23it/s, est. speed input: 100994.13 toks/s, output: 98.63 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:10<00:14, 67.05it/s, est. speed input: 100276.00 toks/s, output: 97.93 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:11<00:14, 67.01it/s, est. speed input: 99599.13 toks/s, output: 97.26 toks/s] 
Processed prompts:  54%|█████▍    | 1106/2048 [00:11<00:14, 66.97it/s, est. speed input: 98948.25 toks/s, output: 96.63 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:11<00:13, 67.02it/s, est. speed input: 98332.16 toks/s, output: 96.03 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:11<00:13, 66.95it/s, est. speed input: 97730.47 toks/s, output: 95.44 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:12<00:13, 68.02it/s, est. speed input: 97257.17 toks/s, output: 94.98 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:12<00:12, 67.68it/s, est. speed input: 96702.55 toks/s, output: 94.44 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:12<00:12, 67.49it/s, est. speed input: 96172.82 toks/s, output: 93.92 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:12<00:12, 67.31it/s, est. speed input: 95657.83 toks/s, output: 93.42 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:13<00:12, 67.12it/s, est. speed input: 95156.84 toks/s, output: 92.93 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:13<00:12, 67.03it/s, est. speed input: 94676.96 toks/s, output: 92.46 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:13<00:11, 67.01it/s, est. speed input: 94218.36 toks/s, output: 92.01 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:13<00:11, 68.03it/s, est. speed input: 93857.03 toks/s, output: 91.66 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:14<00:11, 67.63it/s, est. speed input: 93420.96 toks/s, output: 91.23 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:14<00:11, 67.32it/s, est. speed input: 92996.98 toks/s, output: 90.82 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:14<00:10, 67.27it/s, est. speed input: 92599.57 toks/s, output: 90.43 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:14<00:10, 67.18it/s, est. speed input: 92211.49 toks/s, output: 90.05 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:15<00:10, 66.98it/s, est. speed input: 91825.33 toks/s, output: 89.67 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:15<00:10, 66.75it/s, est. speed input: 91444.86 toks/s, output: 89.30 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:15<00:10, 66.81it/s, est. speed input: 91091.57 toks/s, output: 88.96 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:15<00:09, 66.79it/s, est. speed input: 90744.86 toks/s, output: 88.62 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:15<00:09, 66.84it/s, est. speed input: 90413.31 toks/s, output: 88.29 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:16<00:09, 66.81it/s, est. speed input: 90086.41 toks/s, output: 87.97 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:16<00:09, 66.85it/s, est. speed input: 89773.44 toks/s, output: 87.67 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:16<00:08, 66.81it/s, est. speed input: 89465.00 toks/s, output: 87.37 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:16<00:08, 66.75it/s, est. speed input: 89163.66 toks/s, output: 87.07 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:17<00:08, 66.61it/s, est. speed input: 88864.43 toks/s, output: 86.78 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:17<00:08, 66.71it/s, est. speed input: 88585.72 toks/s, output: 86.51 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:17<00:07, 66.70it/s, est. speed input: 88309.42 toks/s, output: 86.24 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:17<00:07, 66.80it/s, est. speed input: 88047.03 toks/s, output: 85.98 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:18<00:07, 66.76it/s, est. speed input: 87784.83 toks/s, output: 85.73 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:18<00:07, 66.78it/s, est. speed input: 87532.64 toks/s, output: 85.48 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:18<00:06, 67.74it/s, est. speed input: 87339.34 toks/s, output: 85.29 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:18<00:06, 67.50it/s, est. speed input: 87100.84 toks/s, output: 85.06 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:19<00:06, 67.37it/s, est. speed input: 86870.21 toks/s, output: 84.83 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:19<00:06, 67.09it/s, est. speed input: 86635.54 toks/s, output: 84.60 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:19<00:05, 66.97it/s, est. speed input: 86410.62 toks/s, output: 84.39 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:19<00:05, 66.93it/s, est. speed input: 86192.97 toks/s, output: 84.17 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:20<00:05, 66.81it/s, est. speed input: 85976.25 toks/s, output: 83.96 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:20<00:05, 66.72it/s, est. speed input: 85764.37 toks/s, output: 83.75 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:20<00:05, 66.69it/s, est. speed input: 85558.97 toks/s, output: 83.55 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:20<00:04, 66.77it/s, est. speed input: 85363.09 toks/s, output: 83.36 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:20<00:04, 66.76it/s, est. speed input: 85168.56 toks/s, output: 83.17 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:21<00:04, 66.74it/s, est. speed input: 84977.95 toks/s, output: 82.99 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:21<00:04, 66.59it/s, est. speed input: 84785.09 toks/s, output: 82.80 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:21<00:03, 66.62it/s, est. speed input: 84602.74 toks/s, output: 82.62 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:21<00:03, 66.64it/s, est. speed input: 84424.50 toks/s, output: 82.45 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:22<00:03, 66.63it/s, est. speed input: 84248.97 toks/s, output: 82.27 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:22<00:03, 66.49it/s, est. speed input: 84071.37 toks/s, output: 82.10 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:22<00:02, 66.43it/s, est. speed input: 83898.91 toks/s, output: 81.93 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:22<00:02, 67.52it/s, est. speed input: 83779.43 toks/s, output: 81.82 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:23<00:02, 67.17it/s, est. speed input: 83614.47 toks/s, output: 81.65 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:23<00:02, 67.00it/s, est. speed input: 83456.39 toks/s, output: 81.50 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:23<00:01, 66.78it/s, est. speed input: 83297.08 toks/s, output: 81.34 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:23<00:01, 66.69it/s, est. speed input: 83143.38 toks/s, output: 81.19 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:24<00:01, 67.58it/s, est. speed input: 83031.87 toks/s, output: 81.09 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:24<00:01, 67.28it/s, est. speed input: 82885.40 toks/s, output: 80.94 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:24<00:00, 67.08it/s, est. speed input: 82742.08 toks/s, output: 80.80 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:24<00:00, 66.87it/s, est. speed input: 82598.85 toks/s, output: 80.66 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:25<00:00, 66.72it/s, est. speed input: 82458.16 toks/s, output: 80.53 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:25<00:00, 67.91it/s, est. speed input: 82369.65 toks/s, output: 80.44 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:25<00:00, 67.91it/s, est. speed input: 82935.44 toks/s, output: 80.99 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:25<00:00, 80.99it/s, est. speed input: 82935.44 toks/s, output: 80.99 toks/s]
[rank0]:[W128 00:18:21.144648819 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 90.7s

测试结果:
  Requests/s:   66.93
  Tokens/s:     68604.00
  Total Reqs:   2048
  Elapsed:      30.60s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     68537.07

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:18:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2232193) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2232193) WARNING 01-28 00:19:12 [backends.py:609] Failed to read file <frozen os>
Throughput: 67.49 requests/s, 69172.75 total tokens/s, 67.49 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 00:18:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:18:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:18:53] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:18:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:53] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:53] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:18:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:18:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:18:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:18:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:18:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:19:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:19:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:19:01] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:19:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:01] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:01] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:19:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:19:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:19:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:19:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:19:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2232193) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2232193) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.39it/s]
(EngineCore_DP0 pid=2232193) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.39it/s]
(EngineCore_DP0 pid=2232193) 
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=2232193) [2026-01-28 00:19:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2232193) [rank0]:W0128 00:19:21.052000 2232193 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2232193) [rank0]:W0128 00:19:21.168000 2232193 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2232193) [rank0]:W0128 00:19:22.599000 2232193 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2232193) [rank0]:W0128 00:19:22.802000 2232193 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2232193) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  8.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  9.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  9.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  9.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00,  9.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  9.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.55it/s]
(EngineCore_DP0 pid=2232193) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.70it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.84it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  9.31it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  9.41it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.57it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 37/4096 [00:00<00:11, 363.12it/s]
Adding requests:   2%|▏         | 76/4096 [00:00<00:10, 375.81it/s]
Adding requests:   3%|▎         | 115/4096 [00:00<00:10, 380.98it/s]
Adding requests:   4%|▍         | 154/4096 [00:00<00:10, 377.80it/s]
Adding requests:   5%|▍         | 193/4096 [00:00<00:10, 379.31it/s]
Adding requests:   6%|▌         | 234/4096 [00:00<00:09, 387.73it/s]
Adding requests:   7%|▋         | 273/4096 [00:00<00:09, 388.00it/s]
Adding requests:   8%|▊         | 312/4096 [00:00<00:09, 382.80it/s]
Adding requests:   9%|▊         | 351/4096 [00:00<00:09, 377.20it/s]
Adding requests:   9%|▉         | 389/4096 [00:01<00:09, 371.00it/s]
Adding requests:  10%|█         | 427/4096 [00:01<00:10, 364.99it/s]
Adding requests:  11%|█▏        | 464/4096 [00:01<00:10, 362.64it/s]
Adding requests:  12%|█▏        | 503/4096 [00:01<00:09, 368.81it/s]
Adding requests:  13%|█▎        | 540/4096 [00:01<00:09, 363.62it/s]
Adding requests:  14%|█▍        | 582/4096 [00:01<00:09, 377.79it/s]
Adding requests:  15%|█▌        | 623/4096 [00:01<00:08, 387.15it/s]
Adding requests:  16%|█▌        | 663/4096 [00:01<00:08, 390.12it/s]
Adding requests:  17%|█▋        | 703/4096 [00:01<00:08, 391.63it/s]
Adding requests:  18%|█▊        | 743/4096 [00:01<00:08, 390.32it/s]
Adding requests:  19%|█▉        | 783/4096 [00:02<00:08, 390.47it/s]
Adding requests:  20%|██        | 823/4096 [00:02<00:08, 386.90it/s]
Adding requests:  21%|██        | 862/4096 [00:02<00:08, 385.50it/s]
Adding requests:  22%|██▏       | 905/4096 [00:02<00:08, 397.07it/s]
Adding requests:  23%|██▎       | 945/4096 [00:02<00:08, 393.46it/s]
Adding requests:  24%|██▍       | 987/4096 [00:02<00:07, 400.07it/s]
Adding requests:  25%|██▌       | 1030/4096 [00:02<00:07, 407.16it/s]
Adding requests:  26%|██▌       | 1071/4096 [00:02<00:07, 403.19it/s]
Adding requests:  27%|██▋       | 1112/4096 [00:02<00:07, 385.83it/s]
Adding requests:  28%|██▊       | 1151/4096 [00:03<00:07, 378.71it/s]
Adding requests:  29%|██▉       | 1190/4096 [00:03<00:07, 380.47it/s]
Adding requests:  30%|███       | 1229/4096 [00:03<00:07, 382.86it/s]
Adding requests:  31%|███       | 1268/4096 [00:03<00:07, 380.90it/s]
Adding requests:  32%|███▏      | 1307/4096 [00:03<00:07, 381.58it/s]
Adding requests:  33%|███▎      | 1351/4096 [00:03<00:06, 394.75it/s]
Adding requests:  34%|███▍      | 1395/4096 [00:03<00:06, 405.36it/s]
Adding requests:  35%|███▌      | 1437/4096 [00:03<00:06, 408.86it/s]
Adding requests:  36%|███▌      | 1479/4096 [00:03<00:06, 411.48it/s]
Adding requests:  37%|███▋      | 1521/4096 [00:03<00:06, 404.60it/s]
Adding requests:  38%|███▊      | 1563/4096 [00:04<00:06, 407.65it/s]
Adding requests:  39%|███▉      | 1605/4096 [00:04<00:06, 408.53it/s]
Adding requests:  40%|████      | 1646/4096 [00:04<00:06, 398.70it/s]
Adding requests:  41%|████      | 1686/4096 [00:04<00:06, 393.30it/s]
Adding requests:  42%|████▏     | 1728/4096 [00:04<00:05, 400.54it/s]
Adding requests:  43%|████▎     | 1769/4096 [00:04<00:05, 402.84it/s]
Adding requests:  44%|████▍     | 1812/4096 [00:04<00:05, 408.26it/s]
Adding requests:  45%|████▌     | 1855/4096 [00:04<00:05, 413.30it/s]
Adding requests:  46%|████▋     | 1897/4096 [00:04<00:05, 415.18it/s]
Adding requests:  47%|████▋     | 1939/4096 [00:04<00:05, 407.52it/s]
Adding requests:  48%|████▊     | 1980/4096 [00:05<00:05, 403.66it/s]
Adding requests:  49%|████▉     | 2021/4096 [00:05<00:05, 397.48it/s]
Adding requests:  50%|█████     | 2061/4096 [00:05<00:05, 397.15it/s]
Adding requests:  51%|█████▏    | 2103/4096 [00:05<00:04, 402.72it/s]
Adding requests:  52%|█████▏    | 2144/4096 [00:05<00:04, 402.31it/s]
Adding requests:  53%|█████▎    | 2185/4096 [00:05<00:04, 401.22it/s]
Adding requests:  54%|█████▍    | 2226/4096 [00:05<00:04, 401.53it/s]
Adding requests:  55%|█████▌    | 2267/4096 [00:05<00:04, 403.34it/s]
Adding requests:  56%|█████▋    | 2308/4096 [00:05<00:04, 400.56it/s]
Adding requests:  57%|█████▋    | 2349/4096 [00:05<00:04, 392.95it/s]
Adding requests:  58%|█████▊    | 2389/4096 [00:06<00:04, 394.81it/s]
Adding requests:  59%|█████▉    | 2429/4096 [00:06<00:04, 396.02it/s]
Adding requests:  60%|██████    | 2469/4096 [00:06<00:04, 392.72it/s]
Adding requests:  61%|██████▏   | 2509/4096 [00:06<00:04, 394.20it/s]
Adding requests:  62%|██████▏   | 2552/4096 [00:06<00:03, 402.32it/s]
Adding requests:  63%|██████▎   | 2594/4096 [00:06<00:03, 405.77it/s]
Adding requests:  64%|██████▍   | 2636/4096 [00:06<00:03, 407.85it/s]
Adding requests:  65%|██████▌   | 2677/4096 [00:06<00:03, 408.10it/s]
Adding requests:  66%|██████▋   | 2718/4096 [00:06<00:03, 399.85it/s]
Adding requests:  67%|██████▋   | 2759/4096 [00:07<00:03, 387.67it/s]
Adding requests:  68%|██████▊   | 2798/4096 [00:07<00:03, 378.03it/s]
Adding requests:  69%|██████▉   | 2836/4096 [00:07<00:03, 372.95it/s]
Adding requests:  70%|███████   | 2874/4096 [00:07<00:03, 370.14it/s]
Adding requests:  71%|███████   | 2912/4096 [00:07<00:03, 370.97it/s]
Adding requests:  72%|███████▏  | 2950/4096 [00:07<00:03, 371.89it/s]
Adding requests:  73%|███████▎  | 2991/4096 [00:07<00:02, 381.55it/s]
Adding requests:  74%|███████▍  | 3030/4096 [00:07<00:02, 379.38it/s]
Adding requests:  75%|███████▍  | 3070/4096 [00:07<00:02, 383.91it/s]
Adding requests:  76%|███████▌  | 3109/4096 [00:07<00:02, 379.50it/s]
Adding requests:  77%|███████▋  | 3147/4096 [00:08<00:02, 373.28it/s]
Adding requests:  78%|███████▊  | 3187/4096 [00:08<00:02, 380.19it/s]
Adding requests:  79%|███████▉  | 3226/4096 [00:08<00:02, 379.73it/s]
Adding requests:  80%|███████▉  | 3267/4096 [00:08<00:02, 387.49it/s]
Adding requests:  81%|████████  | 3308/4096 [00:08<00:02, 392.40it/s]
Adding requests:  82%|████████▏ | 3349/4096 [00:08<00:01, 395.52it/s]
Adding requests:  83%|████████▎ | 3389/4096 [00:08<00:01, 394.94it/s]
Adding requests:  84%|████████▎ | 3430/4096 [00:08<00:01, 397.05it/s]
Adding requests:  85%|████████▍ | 3470/4096 [00:08<00:01, 383.25it/s]
Adding requests:  86%|████████▌ | 3509/4096 [00:08<00:01, 380.15it/s]
Adding requests:  87%|████████▋ | 3548/4096 [00:09<00:01, 378.31it/s]
Adding requests:  88%|████████▊ | 3588/4096 [00:09<00:01, 381.76it/s]
Adding requests:  89%|████████▊ | 3627/4096 [00:09<00:01, 372.26it/s]
Adding requests:  89%|████████▉ | 3665/4096 [00:09<00:01, 369.63it/s]
Adding requests:  90%|█████████ | 3706/4096 [00:09<00:01, 380.33it/s]
Adding requests:  91%|█████████▏| 3745/4096 [00:09<00:00, 383.14it/s]
Adding requests:  92%|█████████▏| 3788/4096 [00:09<00:00, 394.91it/s]
Adding requests:  93%|█████████▎| 3829/4096 [00:09<00:00, 397.41it/s]
Adding requests:  95%|█████████▍| 3871/4096 [00:09<00:00, 402.10it/s]
Adding requests:  96%|█████████▌| 3912/4096 [00:10<00:00, 398.48it/s]
Adding requests:  96%|█████████▋| 3952/4096 [00:10<00:00, 383.63it/s]
Adding requests:  97%|█████████▋| 3991/4096 [00:10<00:00, 378.85it/s]
Adding requests:  98%|█████████▊| 4031/4096 [00:10<00:00, 383.32it/s]
Adding requests:  99%|█████████▉| 4070/4096 [00:10<00:00, 375.75it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 389.21it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 699/4096 [00:00<00:01, 2237.51it/s, est. speed input: 2291387.01 toks/s, output: 2237.55 toks/s]
Processed prompts:  23%|██▎       | 923/4096 [00:03<00:15, 202.88it/s, est. speed input: 261847.93 toks/s, output: 255.71 toks/s]   
Processed prompts:  25%|██▍       | 1020/4096 [00:05<00:19, 154.13it/s, est. speed input: 208169.83 toks/s, output: 203.29 toks/s]
Processed prompts:  26%|██▋       | 1076/4096 [00:05<00:20, 148.77it/s, est. speed input: 200645.42 toks/s, output: 195.94 toks/s]
Processed prompts:  27%|██▋       | 1115/4096 [00:06<00:25, 116.70it/s, est. speed input: 177354.39 toks/s, output: 173.20 toks/s]
Processed prompts:  28%|██▊       | 1147/4096 [00:06<00:27, 108.64it/s, est. speed input: 170291.63 toks/s, output: 166.30 toks/s]
Processed prompts:  29%|██▉       | 1179/4096 [00:07<00:29, 100.33it/s, est. speed input: 163777.88 toks/s, output: 159.94 toks/s]
Processed prompts:  30%|██▉       | 1211/4096 [00:07<00:31, 93.03it/s, est. speed input: 158077.96 toks/s, output: 154.37 toks/s] 
Processed prompts:  30%|███       | 1243/4096 [00:08<00:32, 87.44it/s, est. speed input: 153252.87 toks/s, output: 149.66 toks/s]
Processed prompts:  31%|███       | 1275/4096 [00:08<00:34, 82.32it/s, est. speed input: 148723.26 toks/s, output: 145.24 toks/s]
Processed prompts:  32%|███▏      | 1307/4096 [00:09<00:35, 78.32it/s, est. speed input: 144642.18 toks/s, output: 141.25 toks/s]
Processed prompts:  33%|███▎      | 1339/4096 [00:09<00:36, 75.29it/s, est. speed input: 140953.40 toks/s, output: 137.65 toks/s]
Processed prompts:  33%|███▎      | 1371/4096 [00:10<00:37, 73.11it/s, est. speed input: 137625.56 toks/s, output: 134.40 toks/s]
Processed prompts:  34%|███▍      | 1403/4096 [00:10<00:37, 71.47it/s, est. speed input: 134578.75 toks/s, output: 131.42 toks/s]
Processed prompts:  35%|███▌      | 1435/4096 [00:11<00:37, 70.32it/s, est. speed input: 131797.26 toks/s, output: 128.71 toks/s]
Processed prompts:  36%|███▌      | 1467/4096 [00:11<00:37, 69.44it/s, est. speed input: 129229.58 toks/s, output: 126.20 toks/s]
Processed prompts:  37%|███▋      | 1499/4096 [00:12<00:37, 68.86it/s, est. speed input: 126871.44 toks/s, output: 123.90 toks/s]
Processed prompts:  37%|███▋      | 1531/4096 [00:12<00:37, 68.42it/s, est. speed input: 124686.74 toks/s, output: 121.76 toks/s]
Processed prompts:  38%|███▊      | 1563/4096 [00:13<00:37, 68.08it/s, est. speed input: 122653.76 toks/s, output: 119.78 toks/s]
Processed prompts:  39%|███▉      | 1595/4096 [00:13<00:36, 68.36it/s, est. speed input: 120870.36 toks/s, output: 118.04 toks/s]
Processed prompts:  40%|███▉      | 1627/4096 [00:13<00:36, 68.03it/s, est. speed input: 119101.41 toks/s, output: 116.31 toks/s]
Processed prompts:  41%|████      | 1659/4096 [00:14<00:35, 67.88it/s, est. speed input: 117465.47 toks/s, output: 114.71 toks/s]
Processed prompts:  41%|████▏     | 1691/4096 [00:14<00:35, 67.66it/s, est. speed input: 115912.51 toks/s, output: 113.20 toks/s]
Processed prompts:  42%|████▏     | 1723/4096 [00:15<00:35, 67.60it/s, est. speed input: 114470.33 toks/s, output: 111.79 toks/s]
Processed prompts:  43%|████▎     | 1755/4096 [00:15<00:34, 67.50it/s, est. speed input: 113104.98 toks/s, output: 110.45 toks/s]
Processed prompts:  44%|████▎     | 1787/4096 [00:16<00:34, 67.48it/s, est. speed input: 111828.51 toks/s, output: 109.21 toks/s]
Processed prompts:  44%|████▍     | 1819/4096 [00:16<00:33, 67.34it/s, est. speed input: 110602.45 toks/s, output: 108.01 toks/s]
Processed prompts:  45%|████▌     | 1851/4096 [00:17<00:33, 67.86it/s, est. speed input: 109537.52 toks/s, output: 106.97 toks/s]
Processed prompts:  46%|████▌     | 1883/4096 [00:17<00:32, 67.72it/s, est. speed input: 108454.54 toks/s, output: 105.91 toks/s]
Processed prompts:  47%|████▋     | 1915/4096 [00:18<00:32, 67.56it/s, est. speed input: 107419.56 toks/s, output: 104.90 toks/s]
Processed prompts:  48%|████▊     | 1947/4096 [00:18<00:31, 68.02it/s, est. speed input: 106513.60 toks/s, output: 104.02 toks/s]
Processed prompts:  48%|████▊     | 1979/4096 [00:19<00:31, 67.84it/s, est. speed input: 105586.80 toks/s, output: 103.11 toks/s]
Processed prompts:  49%|████▉     | 2011/4096 [00:19<00:30, 67.66it/s, est. speed input: 104698.14 toks/s, output: 102.24 toks/s]
Processed prompts:  50%|████▉     | 2043/4096 [00:20<00:30, 67.54it/s, est. speed input: 103852.51 toks/s, output: 101.42 toks/s]
Processed prompts:  51%|█████     | 2075/4096 [00:20<00:29, 67.43it/s, est. speed input: 103041.92 toks/s, output: 100.63 toks/s]
Processed prompts:  51%|█████▏    | 2107/4096 [00:21<00:29, 67.42it/s, est. speed input: 102275.73 toks/s, output: 99.88 toks/s] 
Processed prompts:  52%|█████▏    | 2139/4096 [00:21<00:29, 67.44it/s, est. speed input: 101547.48 toks/s, output: 99.17 toks/s]
Processed prompts:  53%|█████▎    | 2171/4096 [00:22<00:28, 67.30it/s, est. speed input: 100832.93 toks/s, output: 98.47 toks/s]
Processed prompts:  54%|█████▍    | 2203/4096 [00:22<00:28, 67.38it/s, est. speed input: 100167.57 toks/s, output: 97.82 toks/s]
Processed prompts:  55%|█████▍    | 2235/4096 [00:22<00:27, 68.46it/s, est. speed input: 99631.94 toks/s, output: 97.30 toks/s] 
Processed prompts:  55%|█████▌    | 2267/4096 [00:23<00:26, 68.08it/s, est. speed input: 99006.43 toks/s, output: 96.69 toks/s]
Processed prompts:  56%|█████▌    | 2299/4096 [00:23<00:26, 68.29it/s, est. speed input: 98450.04 toks/s, output: 96.14 toks/s]
Processed prompts:  57%|█████▋    | 2331/4096 [00:24<00:25, 68.47it/s, est. speed input: 97918.42 toks/s, output: 95.62 toks/s]
Processed prompts:  58%|█████▊    | 2363/4096 [00:24<00:25, 69.31it/s, est. speed input: 97469.66 toks/s, output: 95.19 toks/s]
Processed prompts:  58%|█████▊    | 2395/4096 [00:25<00:24, 68.63it/s, est. speed input: 96927.30 toks/s, output: 94.66 toks/s]
Processed prompts:  59%|█████▉    | 2427/4096 [00:25<00:24, 68.17it/s, est. speed input: 96405.87 toks/s, output: 94.15 toks/s]
Processed prompts:  60%|██████    | 2459/4096 [00:26<00:24, 67.84it/s, est. speed input: 95902.28 toks/s, output: 93.65 toks/s]
Processed prompts:  61%|██████    | 2491/4096 [00:26<00:23, 68.12it/s, est. speed input: 95459.23 toks/s, output: 93.22 toks/s]
Processed prompts:  62%|██████▏   | 2523/4096 [00:27<00:23, 67.81it/s, est. speed input: 94990.20 toks/s, output: 92.76 toks/s]
Processed prompts:  62%|██████▏   | 2555/4096 [00:27<00:22, 67.56it/s, est. speed input: 94534.45 toks/s, output: 92.32 toks/s]
Processed prompts:  63%|██████▎   | 2587/4096 [00:28<00:22, 67.96it/s, est. speed input: 94138.56 toks/s, output: 91.93 toks/s]
Processed prompts:  64%|██████▍   | 2619/4096 [00:28<00:21, 67.70it/s, est. speed input: 93714.75 toks/s, output: 91.52 toks/s]
Processed prompts:  65%|██████▍   | 2651/4096 [00:29<00:21, 67.48it/s, est. speed input: 93302.44 toks/s, output: 91.12 toks/s]
Processed prompts:  66%|██████▌   | 2683/4096 [00:29<00:20, 67.35it/s, est. speed input: 92904.38 toks/s, output: 90.73 toks/s]
Processed prompts:  66%|██████▋   | 2715/4096 [00:30<00:20, 67.32it/s, est. speed input: 92523.85 toks/s, output: 90.36 toks/s]
Processed prompts:  67%|██████▋   | 2747/4096 [00:30<00:20, 67.19it/s, est. speed input: 92147.28 toks/s, output: 89.99 toks/s]
Processed prompts:  68%|██████▊   | 2779/4096 [00:31<00:19, 67.14it/s, est. speed input: 91784.95 toks/s, output: 89.63 toks/s]
Processed prompts:  69%|██████▊   | 2811/4096 [00:31<00:19, 67.13it/s, est. speed input: 91435.52 toks/s, output: 89.29 toks/s]
Processed prompts:  69%|██████▉   | 2843/4096 [00:31<00:18, 67.09it/s, est. speed input: 91094.50 toks/s, output: 88.96 toks/s]
Processed prompts:  70%|███████   | 2875/4096 [00:32<00:18, 67.02it/s, est. speed input: 90760.46 toks/s, output: 88.63 toks/s]
Processed prompts:  71%|███████   | 2907/4096 [00:32<00:17, 67.01it/s, est. speed input: 90438.51 toks/s, output: 88.32 toks/s]
Processed prompts:  72%|███████▏  | 2939/4096 [00:33<00:17, 67.00it/s, est. speed input: 90126.25 toks/s, output: 88.01 toks/s]
Processed prompts:  73%|███████▎  | 2971/4096 [00:33<00:16, 66.95it/s, est. speed input: 89819.30 toks/s, output: 87.71 toks/s]
Processed prompts:  73%|███████▎  | 3003/4096 [00:34<00:16, 66.96it/s, est. speed input: 89524.32 toks/s, output: 87.43 toks/s]
Processed prompts:  74%|███████▍  | 3035/4096 [00:34<00:15, 66.94it/s, est. speed input: 89235.28 toks/s, output: 87.14 toks/s]
Processed prompts:  75%|███████▍  | 3067/4096 [00:35<00:15, 66.93it/s, est. speed input: 88954.40 toks/s, output: 86.87 toks/s]
Processed prompts:  76%|███████▌  | 3099/4096 [00:35<00:14, 66.95it/s, est. speed input: 88682.94 toks/s, output: 86.60 toks/s]
Processed prompts:  76%|███████▋  | 3131/4096 [00:36<00:14, 67.49it/s, est. speed input: 88448.53 toks/s, output: 86.38 toks/s]
Processed prompts:  77%|███████▋  | 3163/4096 [00:36<00:13, 67.27it/s, est. speed input: 88186.52 toks/s, output: 86.12 toks/s]
Processed prompts:  78%|███████▊  | 3195/4096 [00:37<00:13, 67.15it/s, est. speed input: 87933.00 toks/s, output: 85.87 toks/s]
Processed prompts:  79%|███████▉  | 3227/4096 [00:37<00:12, 67.04it/s, est. speed input: 87684.50 toks/s, output: 85.63 toks/s]
Processed prompts:  80%|███████▉  | 3259/4096 [00:38<00:12, 66.97it/s, est. speed input: 87442.77 toks/s, output: 85.39 toks/s]
Processed prompts:  80%|████████  | 3291/4096 [00:38<00:12, 66.93it/s, est. speed input: 87207.18 toks/s, output: 85.16 toks/s]
Processed prompts:  81%|████████  | 3323/4096 [00:39<00:11, 66.90it/s, est. speed input: 86977.63 toks/s, output: 84.94 toks/s]
Processed prompts:  82%|████████▏ | 3355/4096 [00:39<00:11, 66.95it/s, est. speed input: 86757.12 toks/s, output: 84.72 toks/s]
Processed prompts:  83%|████████▎ | 3387/4096 [00:40<00:10, 66.92it/s, est. speed input: 86538.64 toks/s, output: 84.51 toks/s]
Processed prompts:  83%|████████▎ | 3419/4096 [00:40<00:10, 66.93it/s, est. speed input: 86326.71 toks/s, output: 84.30 toks/s]
Processed prompts:  84%|████████▍ | 3451/4096 [00:41<00:09, 66.90it/s, est. speed input: 86118.13 toks/s, output: 84.10 toks/s]
Processed prompts:  85%|████████▌ | 3483/4096 [00:41<00:09, 68.06it/s, est. speed input: 85971.28 toks/s, output: 83.96 toks/s]
Processed prompts:  86%|████████▌ | 3515/4096 [00:41<00:08, 67.70it/s, est. speed input: 85771.74 toks/s, output: 83.76 toks/s]
Processed prompts:  87%|████████▋ | 3547/4096 [00:42<00:08, 67.42it/s, est. speed input: 85575.20 toks/s, output: 83.57 toks/s]
Processed prompts:  87%|████████▋ | 3579/4096 [00:42<00:07, 67.21it/s, est. speed input: 85382.47 toks/s, output: 83.38 toks/s]
Processed prompts:  88%|████████▊ | 3611/4096 [00:43<00:07, 67.13it/s, est. speed input: 85197.20 toks/s, output: 83.20 toks/s]
Processed prompts:  89%|████████▉ | 3643/4096 [00:43<00:06, 66.97it/s, est. speed input: 85011.26 toks/s, output: 83.02 toks/s]
Processed prompts:  90%|████████▉ | 3675/4096 [00:44<00:06, 66.94it/s, est. speed input: 84832.92 toks/s, output: 82.84 toks/s]
Processed prompts:  91%|█████████ | 3707/4096 [00:44<00:05, 66.92it/s, est. speed input: 84658.32 toks/s, output: 82.67 toks/s]
Processed prompts:  91%|█████████▏| 3739/4096 [00:45<00:05, 67.40it/s, est. speed input: 84509.38 toks/s, output: 82.53 toks/s]
Processed prompts:  92%|█████████▏| 3771/4096 [00:45<00:04, 67.18it/s, est. speed input: 84339.25 toks/s, output: 82.36 toks/s]
Processed prompts:  93%|█████████▎| 3803/4096 [00:46<00:04, 67.09it/s, est. speed input: 84175.12 toks/s, output: 82.20 toks/s]
Processed prompts:  94%|█████████▎| 3835/4096 [00:46<00:03, 67.56it/s, est. speed input: 84037.28 toks/s, output: 82.07 toks/s]
Processed prompts:  94%|█████████▍| 3867/4096 [00:47<00:03, 67.26it/s, est. speed input: 83875.59 toks/s, output: 81.91 toks/s]
Processed prompts:  95%|█████████▌| 3899/4096 [00:47<00:02, 67.16it/s, est. speed input: 83721.81 toks/s, output: 81.76 toks/s]
Processed prompts:  96%|█████████▌| 3931/4096 [00:48<00:02, 67.04it/s, est. speed input: 83568.84 toks/s, output: 81.61 toks/s]
Processed prompts:  97%|█████████▋| 3963/4096 [00:48<00:01, 66.93it/s, est. speed input: 83418.12 toks/s, output: 81.46 toks/s]
Processed prompts:  98%|█████████▊| 3995/4096 [00:49<00:01, 66.82it/s, est. speed input: 83268.89 toks/s, output: 81.32 toks/s]
Processed prompts:  98%|█████████▊| 4027/4096 [00:49<00:01, 67.35it/s, est. speed input: 83146.49 toks/s, output: 81.20 toks/s]
Processed prompts:  99%|█████████▉| 4059/4096 [00:50<00:00, 67.32it/s, est. speed input: 83010.60 toks/s, output: 81.07 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:50<00:00, 67.32it/s, est. speed input: 83604.70 toks/s, output: 81.65 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:50<00:00, 81.64it/s, est. speed input: 83604.70 toks/s, output: 81.65 toks/s]
[rank0]:[W128 00:20:34.409028681 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 133.3s

测试结果:
  Requests/s:   67.49
  Tokens/s:     69172.75
  Total Reqs:   4096
  Elapsed:      60.69s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     69105.26


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,19.0000,9746.9979,6.7368
1024,1024,1,128,128,19.0126,19487.9028,6.7324
2048,1024,2,256,128,39.3928,40377.6375,6.4986
4096,1024,4,512,128,62.5956,64160.4733,8.1795
8192,1024,8,1024,128,65.5558,67194.7008,15.6203
16384,1024,16,2048,128,66.9307,68604.0001,30.5988
32768,1024,32,4096,128,67.4856,69172.7485,60.6944

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:20:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2234016) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2234016) WARNING 01-28 00:21:03 [backends.py:609] Failed to read file <frozen os>
Throughput: 19.44 requests/s, 9970.17 total tokens/s, 19.44 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:20:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:20:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:20:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:20:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:20:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:20:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:20:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:20:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:20:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:20:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:20:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:20:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:20:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:20:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:20:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:20:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:20:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:20:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:53] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2234016) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2234016) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]
(EngineCore_DP0 pid=2234016) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=2234016) 
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=2234016) [2026-01-28 00:20:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2234016) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.55it/s]
(EngineCore_DP0 pid=2234016) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.36it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 636.41it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 660.14it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:08, 15.16it/s, est. speed input: 7760.93 toks/s, output: 15.16 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:06, 18.34it/s, est. speed input: 9159.63 toks/s, output: 17.89 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:06, 19.35it/s, est. speed input: 9619.44 toks/s, output: 18.79 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:06, 19.39it/s, est. speed input: 9688.11 toks/s, output: 18.92 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:06, 19.13it/s, est. speed input: 9658.13 toks/s, output: 18.86 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:05, 19.07it/s, est. speed input: 9664.41 toks/s, output: 18.88 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:05, 19.03it/s, est. speed input: 9669.20 toks/s, output: 18.88 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:05, 19.25it/s, est. speed input: 9717.24 toks/s, output: 18.98 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:05, 19.77it/s, est. speed input: 9828.43 toks/s, output: 19.20 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:05, 20.06it/s, est. speed input: 9913.77 toks/s, output: 19.36 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:05, 19.63it/s, est. speed input: 9877.23 toks/s, output: 19.29 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 19.72it/s, est. speed input: 9908.01 toks/s, output: 19.35 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:04, 19.86it/s, est. speed input: 9945.03 toks/s, output: 19.42 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:04, 19.98it/s, est. speed input: 9979.33 toks/s, output: 19.49 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:04, 20.14it/s, est. speed input: 10018.19 toks/s, output: 19.57 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:04, 20.27it/s, est. speed input: 10053.97 toks/s, output: 19.64 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:04, 20.35it/s, est. speed input: 10083.95 toks/s, output: 19.70 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 20.23it/s, est. speed input: 10092.44 toks/s, output: 19.71 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:03, 20.27it/s, est. speed input: 10111.57 toks/s, output: 19.75 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:03, 20.29it/s, est. speed input: 10128.61 toks/s, output: 19.78 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:02<00:03, 20.35it/s, est. speed input: 10147.57 toks/s, output: 19.82 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:02<00:03, 20.02it/s, est. speed input: 10132.92 toks/s, output: 19.79 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 20.20it/s, est. speed input: 10153.46 toks/s, output: 19.83 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 20.35it/s, est. speed input: 10173.04 toks/s, output: 19.87 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:02, 20.43it/s, est. speed input: 10189.36 toks/s, output: 19.90 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:03<00:02, 20.29it/s, est. speed input: 10191.20 toks/s, output: 19.90 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:03<00:02, 20.26it/s, est. speed input: 10196.78 toks/s, output: 19.92 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:03<00:02, 19.69it/s, est. speed input: 10165.73 toks/s, output: 19.85 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:02, 19.92it/s, est. speed input: 10177.49 toks/s, output: 19.88 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 20.19it/s, est. speed input: 10194.67 toks/s, output: 19.91 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:04<00:02, 20.12it/s, est. speed input: 10195.41 toks/s, output: 19.91 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:04<00:01, 20.28it/s, est. speed input: 10208.13 toks/s, output: 19.94 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:04<00:01, 20.23it/s, est. speed input: 10210.80 toks/s, output: 19.94 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:04<00:01, 20.26it/s, est. speed input: 10217.02 toks/s, output: 19.96 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:04<00:01, 19.96it/s, est. speed input: 10206.38 toks/s, output: 19.93 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:05<00:01, 19.91it/s, est. speed input: 10204.18 toks/s, output: 19.93 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:05<00:01, 19.73it/s, est. speed input: 10196.41 toks/s, output: 19.91 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:05<00:01, 20.00it/s, est. speed input: 10205.29 toks/s, output: 19.93 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:05<00:00, 20.24it/s, est. speed input: 10216.40 toks/s, output: 19.95 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:05<00:00, 20.11it/s, est. speed input: 10214.74 toks/s, output: 19.95 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:05<00:00, 20.32it/s, est. speed input: 10225.81 toks/s, output: 19.97 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:05<00:00, 20.12it/s, est. speed input: 10222.10 toks/s, output: 19.96 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:06<00:00, 20.23it/s, est. speed input: 10228.46 toks/s, output: 19.98 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:06<00:00, 20.37it/s, est. speed input: 10237.32 toks/s, output: 19.99 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:06<00:00, 20.55it/s, est. speed input: 10248.52 toks/s, output: 20.02 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.55it/s, est. speed input: 10254.67 toks/s, output: 20.03 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.03it/s, est. speed input: 10254.67 toks/s, output: 20.03 toks/s]
[rank0]:[W128 00:21:27.429137794 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.9s

测试结果:
  Requests/s:   19.44
  Tokens/s:     9970.17
  Total Reqs:   128
  Elapsed:      6.59s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     9950.73

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:21:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2235038) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2235038) WARNING 01-28 00:21:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 20.18 requests/s, 20683.97 total tokens/s, 20.18 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:21:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:21:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:21:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:21:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:21:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:21:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:21:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:21:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:21:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:21:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:21:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:21:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:21:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:21:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:21:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:21:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:21:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:21:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2235038) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2235038) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.22it/s]
(EngineCore_DP0 pid=2235038) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.22it/s]
(EngineCore_DP0 pid=2235038) 
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=2235038) [2026-01-28 00:21:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2235038) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.30it/s]
(EngineCore_DP0 pid=2235038) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  29%|██▉       | 37/128 [00:00<00:00, 367.37it/s]
Adding requests:  59%|█████▉    | 76/128 [00:00<00:00, 377.56it/s]
Adding requests:  91%|█████████▏| 117/128 [00:00<00:00, 387.77it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 384.31it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:01, 62.64it/s, est. speed input: 64149.26 toks/s, output: 62.64 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:03, 28.53it/s, est. speed input: 31813.04 toks/s, output: 31.07 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:04, 24.86it/s, est. speed input: 28152.99 toks/s, output: 27.49 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:04, 23.40it/s, est. speed input: 26729.03 toks/s, output: 26.10 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:04, 22.59it/s, est. speed input: 25874.15 toks/s, output: 25.27 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:04, 22.01it/s, est. speed input: 25244.00 toks/s, output: 24.65 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:04, 21.59it/s, est. speed input: 24757.39 toks/s, output: 24.18 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:04, 20.65it/s, est. speed input: 24107.31 toks/s, output: 23.54 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:04, 20.55it/s, est. speed input: 23792.77 toks/s, output: 23.23 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:04, 20.43it/s, est. speed input: 23514.13 toks/s, output: 22.96 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:04, 20.24it/s, est. speed input: 23251.07 toks/s, output: 22.71 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:04, 20.40it/s, est. speed input: 23109.28 toks/s, output: 22.57 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:03, 20.52it/s, est. speed input: 22987.61 toks/s, output: 22.45 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:03, 20.64it/s, est. speed input: 22890.22 toks/s, output: 22.35 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:02<00:03, 20.74it/s, est. speed input: 22807.55 toks/s, output: 22.27 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:02<00:03, 20.28it/s, est. speed input: 22622.91 toks/s, output: 22.09 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:02<00:03, 20.09it/s, est. speed input: 22482.45 toks/s, output: 21.96 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:02<00:03, 19.79it/s, est. speed input: 22325.43 toks/s, output: 21.80 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 19.94it/s, est. speed input: 22250.61 toks/s, output: 21.73 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:03<00:02, 20.29it/s, est. speed input: 22224.91 toks/s, output: 21.70 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:03<00:02, 20.53it/s, est. speed input: 22198.39 toks/s, output: 21.68 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:03<00:02, 20.70it/s, est. speed input: 22174.82 toks/s, output: 21.65 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:03<00:02, 20.70it/s, est. speed input: 22134.74 toks/s, output: 21.62 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:03<00:02, 20.43it/s, est. speed input: 22060.90 toks/s, output: 21.54 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:03<00:02, 20.70it/s, est. speed input: 22055.02 toks/s, output: 21.54 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:04<00:01, 20.88it/s, est. speed input: 22047.06 toks/s, output: 21.53 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:04<00:01, 21.06it/s, est. speed input: 22045.12 toks/s, output: 21.53 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:04<00:01, 20.98it/s, est. speed input: 22020.85 toks/s, output: 21.50 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:04<00:01, 20.57it/s, est. speed input: 21956.36 toks/s, output: 21.44 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:04<00:01, 20.68it/s, est. speed input: 21941.04 toks/s, output: 21.43 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:04<00:01, 20.79it/s, est. speed input: 21928.80 toks/s, output: 21.41 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:04<00:01, 20.46it/s, est. speed input: 21875.81 toks/s, output: 21.36 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:05<00:00, 20.73it/s, est. speed input: 21876.34 toks/s, output: 21.36 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:05<00:00, 20.92it/s, est. speed input: 21876.99 toks/s, output: 21.36 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:05<00:00, 21.09it/s, est. speed input: 21880.16 toks/s, output: 21.37 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:05<00:00, 20.98it/s, est. speed input: 21863.31 toks/s, output: 21.35 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:05<00:00, 20.68it/s, est. speed input: 21826.39 toks/s, output: 21.31 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:05<00:00, 20.69it/s, est. speed input: 21810.81 toks/s, output: 21.30 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:05<00:00, 20.87it/s, est. speed input: 21811.01 toks/s, output: 21.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.87it/s, est. speed input: 21813.91 toks/s, output: 21.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 21.30it/s, est. speed input: 21813.91 toks/s, output: 21.30 toks/s]
[rank0]:[W128 00:22:20.036763090 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.6s

测试结果:
  Requests/s:   20.18
  Tokens/s:     20683.97
  Total Reqs:   128
  Elapsed:      6.34s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     20663.79

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:22:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2236015) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2236015) WARNING 01-28 00:22:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.46 requests/s, 40450.71 total tokens/s, 39.46 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 00:22:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:22:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:22:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:22:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:22:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:22:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:22:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:22:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:22:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:22:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:22:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:22:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:22:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:22:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:22:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:22:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:22:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:22:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2236015) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2236015) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=2236015) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=2236015) 
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=2236015) [2026-01-28 00:22:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2236015) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  9.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.29it/s]
(EngineCore_DP0 pid=2236015) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  8.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.32it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▎        | 35/256 [00:00<00:00, 348.23it/s]
Adding requests:  29%|██▊       | 73/256 [00:00<00:00, 365.13it/s]
Adding requests:  44%|████▍     | 112/256 [00:00<00:00, 373.58it/s]
Adding requests:  59%|█████▊    | 150/256 [00:00<00:00, 370.24it/s]
Adding requests:  73%|███████▎  | 188/256 [00:00<00:00, 363.18it/s]
Adding requests:  88%|████████▊ | 226/256 [00:00<00:00, 366.26it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 366.74it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 26/256 [00:00<00:01, 177.17it/s, est. speed input: 181443.97 toks/s, output: 177.17 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:00<00:03, 67.78it/s, est. speed input: 77944.96 toks/s, output: 76.11 toks/s]   
Processed prompts:  21%|██        | 54/256 [00:00<00:03, 57.80it/s, est. speed input: 67791.23 toks/s, output: 66.20 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:03, 51.24it/s, est. speed input: 61841.45 toks/s, output: 60.39 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:01<00:03, 48.77it/s, est. speed input: 59397.98 toks/s, output: 58.00 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:03, 47.03it/s, est. speed input: 57616.06 toks/s, output: 56.26 toks/s]
Processed prompts:  31%|███       | 79/256 [00:01<00:03, 47.62it/s, est. speed input: 57153.94 toks/s, output: 55.81 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:03, 43.53it/s, est. speed input: 55050.13 toks/s, output: 53.76 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:03, 43.08it/s, est. speed input: 54041.11 toks/s, output: 52.77 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:01<00:03, 42.69it/s, est. speed input: 53166.17 toks/s, output: 51.92 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:01<00:03, 42.42it/s, est. speed input: 52417.34 toks/s, output: 51.19 toks/s]
Processed prompts:  42%|████▏     | 107/256 [00:02<00:03, 43.96it/s, est. speed input: 52312.10 toks/s, output: 51.09 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:03, 40.91it/s, est. speed input: 51207.95 toks/s, output: 50.01 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 41.01it/s, est. speed input: 50659.77 toks/s, output: 49.47 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:02<00:03, 41.29it/s, est. speed input: 50221.66 toks/s, output: 49.04 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:02<00:03, 41.17it/s, est. speed input: 49764.63 toks/s, output: 48.60 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:02<00:02, 40.99it/s, est. speed input: 49335.56 toks/s, output: 48.18 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:02<00:02, 40.98it/s, est. speed input: 48969.96 toks/s, output: 47.82 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:03<00:02, 40.73it/s, est. speed input: 48594.36 toks/s, output: 47.46 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 40.76it/s, est. speed input: 48289.44 toks/s, output: 47.16 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:03<00:02, 40.64it/s, est. speed input: 47986.59 toks/s, output: 46.86 toks/s]
Processed prompts:  64%|██████▍   | 165/256 [00:03<00:02, 42.56it/s, est. speed input: 48050.37 toks/s, output: 46.92 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:03<00:02, 39.23it/s, est. speed input: 47431.33 toks/s, output: 46.32 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:03<00:02, 39.68it/s, est. speed input: 47206.70 toks/s, output: 46.10 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:03<00:01, 40.41it/s, est. speed input: 47059.50 toks/s, output: 45.96 toks/s]
Processed prompts:  73%|███████▎  | 187/256 [00:04<00:01, 41.85it/s, est. speed input: 47067.91 toks/s, output: 45.96 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:04<00:01, 38.22it/s, est. speed input: 46497.53 toks/s, output: 45.41 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:04<00:01, 39.12it/s, est. speed input: 46352.45 toks/s, output: 45.27 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:04<00:01, 38.97it/s, est. speed input: 46191.22 toks/s, output: 45.11 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:04<00:01, 39.90it/s, est. speed input: 46088.53 toks/s, output: 45.01 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:04<00:01, 40.65it/s, est. speed input: 46006.13 toks/s, output: 44.93 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:04<00:00, 41.02it/s, est. speed input: 45913.62 toks/s, output: 44.84 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:05<00:00, 40.99it/s, est. speed input: 45797.71 toks/s, output: 44.72 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:05<00:00, 40.74it/s, est. speed input: 45664.36 toks/s, output: 44.59 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:05<00:00, 40.87it/s, est. speed input: 45568.60 toks/s, output: 44.50 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:05<00:00, 41.19it/s, est. speed input: 45500.53 toks/s, output: 44.43 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:05<00:00, 40.64it/s, est. speed input: 45362.47 toks/s, output: 44.30 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 40.96it/s, est. speed input: 45296.54 toks/s, output: 44.23 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 40.96it/s, est. speed input: 45296.54 toks/s, output: 44.23 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 44.23it/s, est. speed input: 45296.54 toks/s, output: 44.23 toks/s]
[rank0]:[W128 00:23:14.926190132 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.0s

测试结果:
  Requests/s:   39.46
  Tokens/s:     40450.71
  Total Reqs:   256
  Elapsed:      6.49s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     40411.25

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:23:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2237017) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2237017) WARNING 01-28 00:23:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 62.64 requests/s, 64203.60 total tokens/s, 62.64 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 00:23:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:23:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:23:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:23:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:23:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:23:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:23:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:23:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:23:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:23:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:23:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:23:34] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:23:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:34] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:34] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:23:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:23:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:23:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:23:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:23:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:35] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2237017) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2237017) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]
(EngineCore_DP0 pid=2237017) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]
(EngineCore_DP0 pid=2237017) 
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=2237017) [2026-01-28 00:23:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2237017) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  9.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.43it/s]
(EngineCore_DP0 pid=2237017) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  8.24it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  9.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.98it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 37/512 [00:00<00:01, 366.35it/s]
Adding requests:  15%|█▍        | 75/512 [00:00<00:01, 369.95it/s]
Adding requests:  22%|██▏       | 114/512 [00:00<00:01, 378.31it/s]
Adding requests:  30%|██▉       | 153/512 [00:00<00:00, 380.54it/s]
Adding requests:  38%|███▊      | 193/512 [00:00<00:00, 383.93it/s]
Adding requests:  45%|████▌     | 232/512 [00:00<00:00, 375.70it/s]
Adding requests:  53%|█████▎    | 271/512 [00:00<00:00, 378.66it/s]
Adding requests:  60%|██████    | 309/512 [00:00<00:00, 373.75it/s]
Adding requests:  68%|██████▊   | 349/512 [00:00<00:00, 380.87it/s]
Adding requests:  76%|███████▌  | 388/512 [00:01<00:00, 383.61it/s]
Adding requests:  83%|████████▎ | 427/512 [00:01<00:00, 383.95it/s]
Adding requests:  91%|█████████ | 466/512 [00:01<00:00, 379.81it/s]
Adding requests:  99%|█████████▊| 505/512 [00:01<00:00, 370.84it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 376.62it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:00<00:00, 687.42it/s, est. speed input: 703982.96 toks/s, output: 687.43 toks/s]
Processed prompts:  30%|███       | 155/512 [00:01<00:03, 111.09it/s, est. speed input: 132210.29 toks/s, output: 129.11 toks/s]
Processed prompts:  37%|███▋      | 188/512 [00:01<00:03, 93.42it/s, est. speed input: 112723.86 toks/s, output: 110.08 toks/s] 
Processed prompts:  41%|████      | 209/512 [00:02<00:03, 86.54it/s, est. speed input: 105738.65 toks/s, output: 103.26 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:02<00:03, 80.37it/s, est. speed input: 100719.13 toks/s, output: 98.36 toks/s] 
Processed prompts:  46%|████▌     | 236/512 [00:02<00:03, 77.01it/s, est. speed input: 97941.39 toks/s, output: 95.65 toks/s] 
Processed prompts:  48%|████▊     | 246/512 [00:02<00:03, 71.70it/s, est. speed input: 94792.05 toks/s, output: 92.57 toks/s]
Processed prompts:  50%|████▉     | 255/512 [00:02<00:03, 71.58it/s, est. speed input: 93784.23 toks/s, output: 91.59 toks/s]
Processed prompts:  51%|█████▏    | 263/512 [00:02<00:03, 69.94it/s, est. speed input: 92522.39 toks/s, output: 90.35 toks/s]
Processed prompts:  53%|█████▎    | 271/512 [00:03<00:03, 68.32it/s, est. speed input: 91328.71 toks/s, output: 89.19 toks/s]
Processed prompts:  54%|█████▍    | 279/512 [00:03<00:03, 67.14it/s, est. speed input: 90274.57 toks/s, output: 88.16 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:03<00:03, 64.08it/s, est. speed input: 88969.60 toks/s, output: 86.88 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:03<00:03, 63.82it/s, est. speed input: 88066.10 toks/s, output: 86.00 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:03<00:03, 63.66it/s, est. speed input: 87234.44 toks/s, output: 85.19 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:03<00:03, 63.59it/s, est. speed input: 86468.28 toks/s, output: 84.44 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:03<00:03, 63.58it/s, est. speed input: 85759.46 toks/s, output: 83.75 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:03<00:02, 63.46it/s, est. speed input: 85078.48 toks/s, output: 83.08 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:04<00:02, 63.25it/s, est. speed input: 84424.03 toks/s, output: 82.45 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:04<00:02, 64.30it/s, est. speed input: 83969.32 toks/s, output: 82.00 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:04<00:02, 63.99it/s, est. speed input: 83404.33 toks/s, output: 81.45 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:04<00:02, 63.77it/s, est. speed input: 82872.29 toks/s, output: 80.93 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:04<00:02, 63.49it/s, est. speed input: 82353.51 toks/s, output: 80.42 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:04<00:02, 63.41it/s, est. speed input: 81876.65 toks/s, output: 79.96 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:04<00:02, 63.37it/s, est. speed input: 81427.97 toks/s, output: 79.52 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:04<00:01, 63.33it/s, est. speed input: 81000.69 toks/s, output: 79.10 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:05<00:01, 63.29it/s, est. speed input: 80592.59 toks/s, output: 78.70 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:05<00:01, 63.10it/s, est. speed input: 80188.07 toks/s, output: 78.31 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:05<00:01, 63.14it/s, est. speed input: 79820.63 toks/s, output: 77.95 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:05<00:01, 63.16it/s, est. speed input: 79469.05 toks/s, output: 77.61 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:05<00:01, 63.20it/s, est. speed input: 79136.34 toks/s, output: 77.28 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:05<00:01, 63.18it/s, est. speed input: 78813.10 toks/s, output: 76.97 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:05<00:01, 63.25it/s, est. speed input: 78511.94 toks/s, output: 76.67 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:05<00:00, 64.71it/s, est. speed input: 78345.15 toks/s, output: 76.51 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:06<00:00, 64.27it/s, est. speed input: 78062.03 toks/s, output: 76.23 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:06<00:00, 63.94it/s, est. speed input: 77788.69 toks/s, output: 75.97 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:06<00:00, 63.47it/s, est. speed input: 77506.32 toks/s, output: 75.69 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:06<00:00, 63.29it/s, est. speed input: 77247.68 toks/s, output: 75.44 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:06<00:00, 63.32it/s, est. speed input: 77010.44 toks/s, output: 75.21 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:06<00:00, 63.24it/s, est. speed input: 76774.53 toks/s, output: 74.97 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:06<00:00, 64.73it/s, est. speed input: 76661.05 toks/s, output: 74.86 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 64.73it/s, est. speed input: 76959.22 toks/s, output: 75.16 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 75.15it/s, est. speed input: 76959.22 toks/s, output: 75.16 toks/s]
[rank0]:[W128 00:24:11.355441579 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 57.4s

测试结果:
  Requests/s:   62.64
  Tokens/s:     64203.60
  Total Reqs:   512
  Elapsed:      8.17s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     64140.96

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:24:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2238075) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2238075) WARNING 01-28 00:24:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 65.41 requests/s, 67048.78 total tokens/s, 65.41 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 00:24:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:24:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:24:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:24:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:24:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:24:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:24:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:24:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:24:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:24:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:24:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:24:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:24:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:24:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:24:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:24:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:24:34] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:24:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:24:34] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:24:34] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:24:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:24:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:24:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:24:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:24:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:24:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:24:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:24:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:35] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2238075) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2238075) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.30it/s]
(EngineCore_DP0 pid=2238075) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
(EngineCore_DP0 pid=2238075) 
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=2238075) [2026-01-28 00:24:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2238075) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  8.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  9.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  9.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.61it/s]
(EngineCore_DP0 pid=2238075) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  9.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.64it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 36/1024 [00:00<00:02, 356.83it/s]
Adding requests:   7%|▋         | 76/1024 [00:00<00:02, 379.52it/s]
Adding requests:  11%|█▏        | 116/1024 [00:00<00:02, 388.04it/s]
Adding requests:  15%|█▌        | 155/1024 [00:00<00:02, 382.46it/s]
Adding requests:  19%|█▉        | 194/1024 [00:00<00:02, 369.16it/s]
Adding requests:  23%|██▎       | 233/1024 [00:00<00:02, 373.50it/s]
Adding requests:  26%|██▋       | 271/1024 [00:00<00:02, 375.30it/s]
Adding requests:  30%|███       | 309/1024 [00:00<00:01, 374.88it/s]
Adding requests:  34%|███▍      | 349/1024 [00:00<00:01, 382.34it/s]
Adding requests:  38%|███▊      | 390/1024 [00:01<00:01, 388.92it/s]
Adding requests:  42%|████▏     | 431/1024 [00:01<00:01, 393.83it/s]
Adding requests:  46%|████▌     | 471/1024 [00:01<00:01, 395.60it/s]
Adding requests:  50%|████▉     | 511/1024 [00:01<00:01, 394.09it/s]
Adding requests:  54%|█████▍    | 551/1024 [00:01<00:01, 384.59it/s]
Adding requests:  58%|█████▊    | 591/1024 [00:01<00:01, 386.39it/s]
Adding requests:  62%|██████▏   | 630/1024 [00:01<00:01, 382.04it/s]
Adding requests:  66%|██████▌   | 671/1024 [00:01<00:00, 388.09it/s]
Adding requests:  69%|██████▉   | 710/1024 [00:01<00:00, 385.32it/s]
Adding requests:  73%|███████▎  | 749/1024 [00:01<00:00, 377.04it/s]
Adding requests:  77%|███████▋  | 787/1024 [00:02<00:00, 376.75it/s]
Adding requests:  81%|████████  | 825/1024 [00:02<00:00, 371.54it/s]
Adding requests:  84%|████████▍ | 865/1024 [00:02<00:00, 379.12it/s]
Adding requests:  88%|████████▊ | 906/1024 [00:02<00:00, 386.94it/s]
Adding requests:  92%|█████████▏| 945/1024 [00:02<00:00, 381.79it/s]
Adding requests:  96%|█████████▌| 985/1024 [00:02<00:00, 384.82it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 382.87it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:00<00:01, 827.91it/s, est. speed input: 847847.60 toks/s, output: 827.93 toks/s]
Processed prompts:  25%|██▌       | 261/1024 [00:01<00:05, 151.57it/s, est. speed input: 186356.80 toks/s, output: 181.99 toks/s]
Processed prompts:  29%|██▉       | 300/1024 [00:02<00:06, 117.61it/s, est. speed input: 150400.93 toks/s, output: 146.88 toks/s]
Processed prompts:  32%|███▏      | 324/1024 [00:02<00:06, 104.68it/s, est. speed input: 137818.39 toks/s, output: 134.59 toks/s]
Processed prompts:  33%|███▎      | 341/1024 [00:02<00:06, 98.63it/s, est. speed input: 132139.15 toks/s, output: 129.04 toks/s] 
Processed prompts:  35%|███▍      | 355/1024 [00:02<00:07, 89.67it/s, est. speed input: 125929.49 toks/s, output: 122.98 toks/s]
Processed prompts:  36%|███▌      | 366/1024 [00:03<00:07, 89.71it/s, est. speed input: 124556.05 toks/s, output: 121.64 toks/s]
Processed prompts:  37%|███▋      | 377/1024 [00:03<00:07, 89.84it/s, est. speed input: 123318.94 toks/s, output: 120.43 toks/s]
Processed prompts:  38%|███▊      | 387/1024 [00:03<00:08, 74.49it/s, est. speed input: 117450.51 toks/s, output: 114.70 toks/s]
Processed prompts:  39%|███▊      | 396/1024 [00:03<00:08, 74.31it/s, est. speed input: 115972.20 toks/s, output: 113.25 toks/s]
Processed prompts:  39%|███▉      | 404/1024 [00:03<00:08, 72.52it/s, est. speed input: 114332.68 toks/s, output: 111.65 toks/s]
Processed prompts:  40%|████      | 412/1024 [00:03<00:08, 71.08it/s, est. speed input: 112827.21 toks/s, output: 110.18 toks/s]
Processed prompts:  41%|████      | 420/1024 [00:03<00:08, 69.78it/s, est. speed input: 111395.96 toks/s, output: 108.78 toks/s]
Processed prompts:  42%|████▏     | 428/1024 [00:03<00:08, 68.79it/s, est. speed input: 110061.38 toks/s, output: 107.48 toks/s]
Processed prompts:  42%|████▏     | 435/1024 [00:04<00:08, 65.72it/s, est. speed input: 108542.40 toks/s, output: 106.00 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:04<00:09, 63.45it/s, est. speed input: 107115.35 toks/s, output: 104.60 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:04<00:08, 65.71it/s, est. speed input: 106257.31 toks/s, output: 103.77 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:04<00:08, 65.56it/s, est. speed input: 105169.99 toks/s, output: 102.70 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:04<00:08, 65.47it/s, est. speed input: 104144.83 toks/s, output: 101.70 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:04<00:08, 65.57it/s, est. speed input: 103194.00 toks/s, output: 100.78 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:04<00:08, 65.60it/s, est. speed input: 102287.20 toks/s, output: 99.89 toks/s] 
Processed prompts:  48%|████▊     | 490/1024 [00:04<00:08, 65.64it/s, est. speed input: 101426.17 toks/s, output: 99.05 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:05<00:08, 65.57it/s, est. speed input: 100595.48 toks/s, output: 98.24 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:05<00:07, 65.54it/s, est. speed input: 99804.80 toks/s, output: 97.47 toks/s] 
Processed prompts:  50%|█████     | 514/1024 [00:05<00:07, 65.51it/s, est. speed input: 99050.32 toks/s, output: 96.73 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:05<00:07, 65.48it/s, est. speed input: 98329.19 toks/s, output: 96.02 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:05<00:07, 65.39it/s, est. speed input: 97631.31 toks/s, output: 95.34 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:05<00:07, 65.44it/s, est. speed input: 96976.61 toks/s, output: 94.70 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:05<00:07, 65.48it/s, est. speed input: 96349.81 toks/s, output: 94.09 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:05<00:07, 65.54it/s, est. speed input: 95751.70 toks/s, output: 93.51 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:06<00:07, 65.51it/s, est. speed input: 95170.54 toks/s, output: 92.94 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:06<00:06, 65.60it/s, est. speed input: 94623.08 toks/s, output: 92.41 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:06<00:06, 65.79it/s, est. speed input: 94108.43 toks/s, output: 91.90 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:06<00:06, 65.77it/s, est. speed input: 93599.67 toks/s, output: 91.41 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:06<00:06, 65.64it/s, est. speed input: 93099.32 toks/s, output: 90.92 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:06<00:06, 65.65it/s, est. speed input: 92625.91 toks/s, output: 90.45 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:06<00:06, 65.60it/s, est. speed input: 92164.88 toks/s, output: 90.00 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:06<00:06, 65.71it/s, est. speed input: 91732.55 toks/s, output: 89.58 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:07<00:06, 65.63it/s, est. speed input: 91302.24 toks/s, output: 89.16 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:07<00:05, 65.61it/s, est. speed input: 90889.12 toks/s, output: 88.76 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:07<00:05, 65.54it/s, est. speed input: 90486.20 toks/s, output: 88.37 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:07<00:05, 65.49it/s, est. speed input: 90096.30 toks/s, output: 87.98 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:07<00:05, 65.38it/s, est. speed input: 89713.50 toks/s, output: 87.61 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:07<00:05, 65.36it/s, est. speed input: 89347.01 toks/s, output: 87.25 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:07<00:05, 65.19it/s, est. speed input: 88980.83 toks/s, output: 86.90 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:07<00:05, 65.34it/s, est. speed input: 88645.51 toks/s, output: 86.57 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:08<00:05, 65.38it/s, est. speed input: 88315.09 toks/s, output: 86.25 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:08<00:04, 65.37it/s, est. speed input: 87992.56 toks/s, output: 85.93 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:08<00:04, 65.52it/s, est. speed input: 87689.90 toks/s, output: 85.63 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:08<00:04, 65.58it/s, est. speed input: 87393.32 toks/s, output: 85.34 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:08<00:04, 65.59it/s, est. speed input: 87103.02 toks/s, output: 85.06 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:08<00:04, 65.53it/s, est. speed input: 86816.70 toks/s, output: 84.78 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:08<00:04, 65.41it/s, est. speed input: 86533.99 toks/s, output: 84.51 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:08<00:04, 65.51it/s, est. speed input: 86269.90 toks/s, output: 84.25 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:08<00:04, 65.49it/s, est. speed input: 86007.66 toks/s, output: 83.99 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:09<00:03, 65.55it/s, est. speed input: 85756.90 toks/s, output: 83.75 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:09<00:03, 65.69it/s, est. speed input: 85518.25 toks/s, output: 83.51 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:09<00:03, 65.68it/s, est. speed input: 85279.63 toks/s, output: 83.28 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:09<00:03, 65.69it/s, est. speed input: 85048.19 toks/s, output: 83.05 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:09<00:03, 65.69it/s, est. speed input: 84822.07 toks/s, output: 82.83 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:09<00:03, 65.52it/s, est. speed input: 84593.00 toks/s, output: 82.61 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:09<00:03, 65.59it/s, est. speed input: 84379.19 toks/s, output: 82.40 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:09<00:03, 65.60it/s, est. speed input: 84168.58 toks/s, output: 82.20 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:10<00:03, 65.52it/s, est. speed input: 83958.89 toks/s, output: 81.99 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:10<00:02, 65.37it/s, est. speed input: 83749.24 toks/s, output: 81.79 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:10<00:02, 65.28it/s, est. speed input: 83545.20 toks/s, output: 81.59 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:10<00:02, 65.39it/s, est. speed input: 83354.84 toks/s, output: 81.40 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:10<00:02, 65.41it/s, est. speed input: 83165.71 toks/s, output: 81.22 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:10<00:02, 65.44it/s, est. speed input: 82982.25 toks/s, output: 81.04 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:10<00:02, 65.46it/s, est. speed input: 82802.48 toks/s, output: 80.86 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:10<00:02, 65.42it/s, est. speed input: 82624.42 toks/s, output: 80.69 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:11<00:02, 65.24it/s, est. speed input: 82442.74 toks/s, output: 80.51 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:11<00:01, 65.23it/s, est. speed input: 82270.64 toks/s, output: 80.34 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:11<00:01, 65.27it/s, est. speed input: 82104.72 toks/s, output: 80.18 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:11<00:01, 65.31it/s, est. speed input: 81942.66 toks/s, output: 80.02 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:11<00:01, 65.40it/s, est. speed input: 81786.85 toks/s, output: 79.87 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:11<00:01, 65.36it/s, est. speed input: 81629.45 toks/s, output: 79.72 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:11<00:01, 67.28it/s, est. speed input: 81557.63 toks/s, output: 79.65 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:11<00:01, 66.63it/s, est. speed input: 81404.59 toks/s, output: 79.50 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:12<00:01, 66.17it/s, est. speed input: 81254.40 toks/s, output: 79.35 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:12<00:00, 66.02it/s, est. speed input: 81113.75 toks/s, output: 79.21 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:12<00:00, 65.77it/s, est. speed input: 80970.16 toks/s, output: 79.07 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:12<00:00, 65.58it/s, est. speed input: 80828.55 toks/s, output: 78.93 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:12<00:00, 67.44it/s, est. speed input: 80767.87 toks/s, output: 78.87 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:12<00:00, 66.70it/s, est. speed input: 80629.58 toks/s, output: 78.74 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:12<00:00, 66.26it/s, est. speed input: 80496.74 toks/s, output: 78.61 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:12<00:00, 66.09it/s, est. speed input: 80371.60 toks/s, output: 78.49 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:12<00:00, 68.01it/s, est. speed input: 80323.90 toks/s, output: 78.44 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 68.01it/s, est. speed input: 80795.73 toks/s, output: 78.90 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 78.90it/s, est. speed input: 80795.73 toks/s, output: 78.90 toks/s]
[rank0]:[W128 00:25:20.523834085 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 68.2s

测试结果:
  Requests/s:   65.41
  Tokens/s:     67048.78
  Total Reqs:   1024
  Elapsed:      15.65s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     66983.36

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:25:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2239306) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2239306) WARNING 01-28 00:25:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 66.73 requests/s, 68402.88 total tokens/s, 66.73 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 00:25:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:25:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:25:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:25:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:25:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:25:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:25:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:25:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:25:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:25:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:25:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:25:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:25:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:25:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:25:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:25:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:25:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:25:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:49] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:49] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:49] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:49] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:49] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2239306) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2239306) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.30it/s]
(EngineCore_DP0 pid=2239306) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.30it/s]
(EngineCore_DP0 pid=2239306) 
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=2239306) [2026-01-28 00:25:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2239306) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  9.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  9.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 10.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.75it/s]
(EngineCore_DP0 pid=2239306) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  8.17it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  9.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.64it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 36/2048 [00:00<00:05, 354.35it/s]
Adding requests:   4%|▎         | 74/2048 [00:00<00:05, 364.93it/s]
Adding requests:   5%|▌         | 111/2048 [00:00<00:05, 362.46it/s]
Adding requests:   7%|▋         | 149/2048 [00:00<00:05, 367.16it/s]
Adding requests:   9%|▉         | 187/2048 [00:00<00:05, 368.51it/s]
Adding requests:  11%|█         | 225/2048 [00:00<00:04, 368.92it/s]
Adding requests:  13%|█▎        | 262/2048 [00:00<00:04, 369.20it/s]
Adding requests:  15%|█▍        | 300/2048 [00:00<00:04, 369.77it/s]
Adding requests:  17%|█▋        | 339/2048 [00:00<00:04, 374.42it/s]
Adding requests:  18%|█▊        | 377/2048 [00:01<00:04, 373.95it/s]
Adding requests:  20%|██        | 415/2048 [00:01<00:04, 369.93it/s]
Adding requests:  22%|██▏       | 454/2048 [00:01<00:04, 375.80it/s]
Adding requests:  24%|██▍       | 492/2048 [00:01<00:04, 374.05it/s]
Adding requests:  26%|██▌       | 530/2048 [00:01<00:04, 367.19it/s]
Adding requests:  28%|██▊       | 571/2048 [00:01<00:03, 379.57it/s]
Adding requests:  30%|██▉       | 611/2048 [00:01<00:03, 384.99it/s]
Adding requests:  32%|███▏      | 650/2048 [00:01<00:03, 378.41it/s]
Adding requests:  34%|███▍      | 692/2048 [00:01<00:03, 389.28it/s]
Adding requests:  36%|███▌      | 734/2048 [00:01<00:03, 395.57it/s]
Adding requests:  38%|███▊      | 774/2048 [00:02<00:03, 384.12it/s]
Adding requests:  40%|███▉      | 813/2048 [00:02<00:03, 381.06it/s]
Adding requests:  42%|████▏     | 852/2048 [00:02<00:03, 372.54it/s]
Adding requests:  43%|████▎     | 890/2048 [00:02<00:03, 372.31it/s]
Adding requests:  45%|████▌     | 929/2048 [00:02<00:02, 376.73it/s]
Adding requests:  47%|████▋     | 971/2048 [00:02<00:02, 386.83it/s]
Adding requests:  49%|████▉     | 1013/2048 [00:02<00:02, 394.24it/s]
Adding requests:  51%|█████▏    | 1053/2048 [00:02<00:02, 386.17it/s]
Adding requests:  53%|█████▎    | 1093/2048 [00:02<00:02, 389.10it/s]
Adding requests:  55%|█████▌    | 1133/2048 [00:02<00:02, 390.84it/s]
Adding requests:  57%|█████▋    | 1176/2048 [00:03<00:02, 402.10it/s]
Adding requests:  60%|█████▉    | 1220/2048 [00:03<00:02, 411.43it/s]
Adding requests:  62%|██████▏   | 1262/2048 [00:03<00:01, 405.11it/s]
Adding requests:  64%|██████▎   | 1303/2048 [00:03<00:01, 403.61it/s]
Adding requests:  66%|██████▌   | 1345/2048 [00:03<00:01, 406.76it/s]
Adding requests:  68%|██████▊   | 1386/2048 [00:03<00:01, 401.71it/s]
Adding requests:  70%|██████▉   | 1427/2048 [00:03<00:01, 403.55it/s]
Adding requests:  72%|███████▏  | 1468/2048 [00:03<00:01, 396.03it/s]
Adding requests:  74%|███████▎  | 1510/2048 [00:03<00:01, 402.02it/s]
Adding requests:  76%|███████▌  | 1551/2048 [00:04<00:01, 391.30it/s]
Adding requests:  78%|███████▊  | 1591/2048 [00:04<00:01, 388.66it/s]
Adding requests:  80%|███████▉  | 1632/2048 [00:04<00:01, 393.43it/s]
Adding requests:  82%|████████▏ | 1672/2048 [00:04<00:00, 385.71it/s]
Adding requests:  84%|████████▎ | 1711/2048 [00:04<00:00, 382.22it/s]
Adding requests:  85%|████████▌ | 1750/2048 [00:04<00:00, 379.24it/s]
Adding requests:  87%|████████▋ | 1788/2048 [00:04<00:00, 372.75it/s]
Adding requests:  89%|████████▉ | 1827/2048 [00:04<00:00, 375.38it/s]
Adding requests:  91%|█████████ | 1865/2048 [00:04<00:00, 375.07it/s]
Adding requests:  93%|█████████▎| 1903/2048 [00:04<00:00, 374.55it/s]
Adding requests:  95%|█████████▍| 1944/2048 [00:05<00:00, 383.79it/s]
Adding requests:  97%|█████████▋| 1987/2048 [00:05<00:00, 394.75it/s]
Adding requests:  99%|█████████▉| 2029/2048 [00:05<00:00, 401.21it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 384.99it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:00<00:01, 1336.84it/s, est. speed input: 1369060.56 toks/s, output: 1336.87 toks/s]
Processed prompts:  24%|██▍       | 488/2048 [00:02<00:08, 182.68it/s, est. speed input: 230340.94 toks/s, output: 224.94 toks/s]   
Processed prompts:  27%|██▋       | 548/2048 [00:03<00:11, 135.28it/s, est. speed input: 179534.34 toks/s, output: 175.33 toks/s]
Processed prompts:  29%|██▊       | 584/2048 [00:03<00:12, 121.82it/s, est. speed input: 165739.88 toks/s, output: 161.86 toks/s]
Processed prompts:  30%|██▉       | 609/2048 [00:03<00:12, 119.41it/s, est. speed input: 162050.90 toks/s, output: 158.25 toks/s]
Processed prompts:  31%|███       | 629/2048 [00:04<00:14, 97.66it/s, est. speed input: 148823.77 toks/s, output: 145.34 toks/s] 
Processed prompts:  31%|███▏      | 644/2048 [00:04<00:15, 91.96it/s, est. speed input: 144444.40 toks/s, output: 141.06 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:04<00:16, 85.48it/s, est. speed input: 140212.39 toks/s, output: 136.93 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:05<00:16, 81.47it/s, est. speed input: 136809.56 toks/s, output: 133.60 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:05<00:17, 78.00it/s, est. speed input: 133704.40 toks/s, output: 130.57 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:05<00:17, 75.15it/s, est. speed input: 130861.14 toks/s, output: 127.79 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:05<00:18, 72.95it/s, est. speed input: 128268.45 toks/s, output: 125.26 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:06<00:18, 71.32it/s, est. speed input: 125893.05 toks/s, output: 122.94 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:06<00:18, 70.03it/s, est. speed input: 123684.47 toks/s, output: 120.79 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:06<00:18, 69.09it/s, est. speed input: 121638.57 toks/s, output: 118.79 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:06<00:18, 68.45it/s, est. speed input: 119747.10 toks/s, output: 116.94 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:06<00:18, 67.93it/s, est. speed input: 117972.39 toks/s, output: 115.21 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:07<00:18, 67.54it/s, est. speed input: 116313.03 toks/s, output: 113.59 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:07<00:18, 67.26it/s, est. speed input: 114759.05 toks/s, output: 112.07 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:07<00:17, 67.22it/s, est. speed input: 113330.87 toks/s, output: 110.67 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:07<00:17, 67.13it/s, est. speed input: 111978.26 toks/s, output: 109.35 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:08<00:17, 67.07it/s, est. speed input: 110705.23 toks/s, output: 108.11 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:08<00:17, 66.96it/s, est. speed input: 109494.60 toks/s, output: 106.93 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:08<00:16, 66.96it/s, est. speed input: 108361.83 toks/s, output: 105.82 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:08<00:16, 67.93it/s, est. speed input: 107429.12 toks/s, output: 104.91 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:09<00:16, 67.58it/s, est. speed input: 106400.89 toks/s, output: 103.91 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:09<00:16, 67.31it/s, est. speed input: 105422.10 toks/s, output: 102.95 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:09<00:15, 68.04it/s, est. speed input: 104610.22 toks/s, output: 102.16 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:09<00:15, 67.78it/s, est. speed input: 103740.25 toks/s, output: 101.31 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:10<00:15, 67.47it/s, est. speed input: 102896.31 toks/s, output: 100.48 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:10<00:15, 67.17it/s, est. speed input: 102082.65 toks/s, output: 99.69 toks/s] 
Processed prompts:  51%|█████     | 1042/2048 [00:10<00:15, 67.03it/s, est. speed input: 101312.96 toks/s, output: 98.94 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:10<00:14, 66.88it/s, est. speed input: 100572.31 toks/s, output: 98.21 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:11<00:14, 66.84it/s, est. speed input: 99870.37 toks/s, output: 97.53 toks/s] 
Processed prompts:  53%|█████▎    | 1090/2048 [00:11<00:14, 66.89it/s, est. speed input: 99207.00 toks/s, output: 96.88 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:11<00:14, 66.86it/s, est. speed input: 98564.47 toks/s, output: 96.25 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:11<00:13, 66.79it/s, est. speed input: 97943.44 toks/s, output: 95.65 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:11<00:13, 66.75it/s, est. speed input: 97348.07 toks/s, output: 95.07 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:12<00:13, 67.73it/s, est. speed input: 96870.69 toks/s, output: 94.60 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:12<00:13, 67.50it/s, est. speed input: 96326.82 toks/s, output: 94.07 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:12<00:12, 67.29it/s, est. speed input: 95799.81 toks/s, output: 93.55 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:12<00:12, 67.04it/s, est. speed input: 95282.31 toks/s, output: 93.05 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:13<00:12, 66.94it/s, est. speed input: 94791.19 toks/s, output: 92.57 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:13<00:12, 66.86it/s, est. speed input: 94316.01 toks/s, output: 92.11 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:13<00:11, 66.86it/s, est. speed input: 93861.98 toks/s, output: 91.66 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:13<00:11, 67.71it/s, est. speed input: 93492.04 toks/s, output: 91.30 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:14<00:11, 67.35it/s, est. speed input: 93059.40 toks/s, output: 90.88 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:14<00:11, 67.21it/s, est. speed input: 92649.64 toks/s, output: 90.48 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:14<00:10, 67.04it/s, est. speed input: 92247.77 toks/s, output: 90.09 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:14<00:10, 66.90it/s, est. speed input: 91857.51 toks/s, output: 89.70 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:15<00:10, 66.72it/s, est. speed input: 91474.00 toks/s, output: 89.33 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:15<00:10, 66.73it/s, est. speed input: 91112.08 toks/s, output: 88.98 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:15<00:10, 66.67it/s, est. speed input: 90756.47 toks/s, output: 88.63 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:15<00:09, 66.63it/s, est. speed input: 90411.71 toks/s, output: 88.29 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:16<00:09, 66.70it/s, est. speed input: 90084.14 toks/s, output: 87.97 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:16<00:09, 66.67it/s, est. speed input: 89760.99 toks/s, output: 87.66 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:16<00:09, 66.66it/s, est. speed input: 89447.63 toks/s, output: 87.35 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:16<00:08, 66.66it/s, est. speed input: 89143.91 toks/s, output: 87.05 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:16<00:08, 66.60it/s, est. speed input: 88845.36 toks/s, output: 86.76 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:17<00:08, 66.58it/s, est. speed input: 88556.39 toks/s, output: 86.48 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:17<00:08, 66.64it/s, est. speed input: 88279.36 toks/s, output: 86.21 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:17<00:07, 66.60it/s, est. speed input: 88005.45 toks/s, output: 85.94 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:17<00:07, 66.50it/s, est. speed input: 87734.21 toks/s, output: 85.68 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:18<00:07, 66.53it/s, est. speed input: 87476.22 toks/s, output: 85.43 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:18<00:07, 66.56it/s, est. speed input: 87225.51 toks/s, output: 85.18 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:18<00:06, 67.65it/s, est. speed input: 87040.10 toks/s, output: 85.00 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:18<00:06, 67.31it/s, est. speed input: 86799.29 toks/s, output: 84.76 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:19<00:06, 67.00it/s, est. speed input: 86561.03 toks/s, output: 84.53 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:19<00:06, 66.85it/s, est. speed input: 86331.96 toks/s, output: 84.31 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:19<00:05, 66.80it/s, est. speed input: 86111.14 toks/s, output: 84.09 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:19<00:05, 66.77it/s, est. speed input: 85895.82 toks/s, output: 83.88 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:20<00:05, 66.61it/s, est. speed input: 85678.86 toks/s, output: 83.67 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:20<00:05, 66.71it/s, est. speed input: 85477.41 toks/s, output: 83.47 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:20<00:05, 66.59it/s, est. speed input: 85271.29 toks/s, output: 83.27 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:20<00:04, 66.66it/s, est. speed input: 85077.80 toks/s, output: 83.08 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:21<00:04, 66.60it/s, est. speed input: 84883.20 toks/s, output: 82.89 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:21<00:04, 66.51it/s, est. speed input: 84690.86 toks/s, output: 82.71 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:21<00:04, 66.53it/s, est. speed input: 84506.76 toks/s, output: 82.53 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:21<00:03, 66.51it/s, est. speed input: 84324.74 toks/s, output: 82.35 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:22<00:03, 66.43it/s, est. speed input: 84144.16 toks/s, output: 82.17 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:22<00:03, 66.38it/s, est. speed input: 83967.39 toks/s, output: 82.00 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:22<00:03, 66.39it/s, est. speed input: 83796.55 toks/s, output: 81.83 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:22<00:02, 66.37it/s, est. speed input: 83628.05 toks/s, output: 81.67 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:22<00:02, 67.30it/s, est. speed input: 83504.08 toks/s, output: 81.55 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:23<00:02, 67.12it/s, est. speed input: 83347.32 toks/s, output: 81.39 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:23<00:02, 66.93it/s, est. speed input: 83190.62 toks/s, output: 81.24 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:23<00:01, 66.73it/s, est. speed input: 83034.33 toks/s, output: 81.09 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:23<00:01, 66.53it/s, est. speed input: 82878.92 toks/s, output: 80.94 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:24<00:01, 67.57it/s, est. speed input: 82774.53 toks/s, output: 80.83 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:24<00:01, 67.13it/s, est. speed input: 82625.36 toks/s, output: 80.69 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:24<00:00, 66.89it/s, est. speed input: 82481.46 toks/s, output: 80.55 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:24<00:00, 66.67it/s, est. speed input: 82338.20 toks/s, output: 80.41 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:25<00:00, 66.49it/s, est. speed input: 82196.80 toks/s, output: 80.27 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:25<00:00, 67.66it/s, est. speed input: 82107.89 toks/s, output: 80.18 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:25<00:00, 67.66it/s, est. speed input: 82671.97 toks/s, output: 80.73 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:25<00:00, 80.73it/s, est. speed input: 82671.97 toks/s, output: 80.73 toks/s]
[rank0]:[W128 00:26:50.910777330 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 90.4s

测试结果:
  Requests/s:   66.73
  Tokens/s:     68402.88
  Total Reqs:   2048
  Elapsed:      30.69s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     68336.15

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:27:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2241009) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2241009) WARNING 01-28 00:27:40 [backends.py:609] Failed to read file <frozen os>
Throughput: 67.18 requests/s, 68855.75 total tokens/s, 67.18 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 00:27:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:27:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:27:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:27:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:27:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:27:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:27:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:27:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:27:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:27:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:27:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:27:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:27:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:27:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:27:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:27:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:27:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:27:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2241009) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2241009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=2241009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=2241009) 
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=2241009) [2026-01-28 00:27:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2241009) [rank0]:W0128 00:27:49.885000 2241009 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2241009) [rank0]:W0128 00:27:50.005000 2241009 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2241009) [rank0]:W0128 00:27:51.423000 2241009 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2241009) [rank0]:W0128 00:27:51.610000 2241009 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2241009) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  9.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  9.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  9.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 10.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.80it/s]
(EngineCore_DP0 pid=2241009) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 10.06it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  9.92it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  9.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 10.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.94it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 38/4096 [00:00<00:10, 372.90it/s]
Adding requests:   2%|▏         | 79/4096 [00:00<00:10, 389.89it/s]
Adding requests:   3%|▎         | 119/4096 [00:00<00:10, 390.93it/s]
Adding requests:   4%|▍         | 159/4096 [00:00<00:10, 381.41it/s]
Adding requests:   5%|▍         | 198/4096 [00:00<00:10, 371.97it/s]
Adding requests:   6%|▌         | 237/4096 [00:00<00:10, 375.62it/s]
Adding requests:   7%|▋         | 276/4096 [00:00<00:10, 379.95it/s]
Adding requests:   8%|▊         | 315/4096 [00:00<00:09, 380.48it/s]
Adding requests:   9%|▊         | 356/4096 [00:00<00:09, 388.05it/s]
Adding requests:  10%|▉         | 397/4096 [00:01<00:09, 392.06it/s]
Adding requests:  11%|█         | 437/4096 [00:01<00:09, 385.89it/s]
Adding requests:  12%|█▏        | 477/4096 [00:01<00:09, 388.82it/s]
Adding requests:  13%|█▎        | 516/4096 [00:01<00:09, 385.18it/s]
Adding requests:  14%|█▎        | 555/4096 [00:01<00:09, 381.98it/s]
Adding requests:  15%|█▍        | 595/4096 [00:01<00:09, 386.93it/s]
Adding requests:  15%|█▌        | 634/4096 [00:01<00:09, 383.44it/s]
Adding requests:  16%|█▋        | 675/4096 [00:01<00:08, 390.83it/s]
Adding requests:  18%|█▊        | 718/4096 [00:01<00:08, 401.35it/s]
Adding requests:  19%|█▊        | 759/4096 [00:01<00:08, 403.73it/s]
Adding requests:  20%|█▉        | 800/4096 [00:02<00:08, 399.85it/s]
Adding requests:  21%|██        | 841/4096 [00:02<00:08, 393.30it/s]
Adding requests:  22%|██▏       | 882/4096 [00:02<00:08, 397.07it/s]
Adding requests:  23%|██▎       | 923/4096 [00:02<00:07, 400.60it/s]
Adding requests:  24%|██▎       | 964/4096 [00:02<00:07, 394.03it/s]
Adding requests:  25%|██▍       | 1004/4096 [00:02<00:07, 390.34it/s]
Adding requests:  26%|██▌       | 1046/4096 [00:02<00:07, 396.73it/s]
Adding requests:  27%|██▋       | 1086/4096 [00:02<00:07, 389.11it/s]
Adding requests:  27%|██▋       | 1125/4096 [00:02<00:07, 382.71it/s]
Adding requests:  28%|██▊       | 1164/4096 [00:02<00:07, 383.90it/s]
Adding requests:  29%|██▉       | 1203/4096 [00:03<00:07, 385.27it/s]
Adding requests:  30%|███       | 1242/4096 [00:03<00:07, 382.41it/s]
Adding requests:  31%|███▏      | 1281/4096 [00:03<00:07, 378.47it/s]
Adding requests:  32%|███▏      | 1321/4096 [00:03<00:07, 384.51it/s]
Adding requests:  33%|███▎      | 1361/4096 [00:03<00:07, 388.28it/s]
Adding requests:  34%|███▍      | 1404/4096 [00:03<00:06, 399.72it/s]
Adding requests:  35%|███▌      | 1445/4096 [00:03<00:06, 393.68it/s]
Adding requests:  36%|███▋      | 1486/4096 [00:03<00:06, 396.55it/s]
Adding requests:  37%|███▋      | 1528/4096 [00:03<00:06, 401.32it/s]
Adding requests:  38%|███▊      | 1570/4096 [00:04<00:06, 406.74it/s]
Adding requests:  39%|███▉      | 1614/4096 [00:04<00:05, 415.03it/s]
Adding requests:  40%|████      | 1657/4096 [00:04<00:05, 417.12it/s]
Adding requests:  41%|████▏     | 1699/4096 [00:04<00:05, 415.12it/s]
Adding requests:  43%|████▎     | 1741/4096 [00:04<00:05, 409.68it/s]
Adding requests:  44%|████▎     | 1782/4096 [00:04<00:05, 401.24it/s]
Adding requests:  45%|████▍     | 1823/4096 [00:04<00:05, 396.50it/s]
Adding requests:  45%|████▌     | 1863/4096 [00:04<00:05, 386.62it/s]
Adding requests:  46%|████▋     | 1902/4096 [00:04<00:05, 386.38it/s]
Adding requests:  47%|████▋     | 1941/4096 [00:04<00:05, 384.01it/s]
Adding requests:  48%|████▊     | 1980/4096 [00:05<00:05, 382.61it/s]
Adding requests:  49%|████▉     | 2021/4096 [00:05<00:05, 390.39it/s]
Adding requests:  50%|█████     | 2062/4096 [00:05<00:05, 395.05it/s]
Adding requests:  51%|█████▏    | 2103/4096 [00:05<00:05, 396.95it/s]
Adding requests:  52%|█████▏    | 2143/4096 [00:05<00:05, 379.73it/s]
Adding requests:  53%|█████▎    | 2182/4096 [00:05<00:05, 378.03it/s]
Adding requests:  54%|█████▍    | 2224/4096 [00:05<00:04, 387.68it/s]
Adding requests:  55%|█████▌    | 2263/4096 [00:05<00:04, 381.92it/s]
Adding requests:  56%|█████▌    | 2302/4096 [00:05<00:04, 376.21it/s]
Adding requests:  57%|█████▋    | 2342/4096 [00:06<00:04, 381.47it/s]
Adding requests:  58%|█████▊    | 2383/4096 [00:06<00:04, 389.69it/s]
Adding requests:  59%|█████▉    | 2423/4096 [00:06<00:04, 383.99it/s]
Adding requests:  60%|██████    | 2462/4096 [00:06<00:04, 384.55it/s]
Adding requests:  61%|██████    | 2501/4096 [00:06<00:04, 383.24it/s]
Adding requests:  62%|██████▏   | 2541/4096 [00:06<00:04, 385.36it/s]
Adding requests:  63%|██████▎   | 2580/4096 [00:06<00:03, 386.31it/s]
Adding requests:  64%|██████▍   | 2619/4096 [00:06<00:03, 379.28it/s]
Adding requests:  65%|██████▍   | 2659/4096 [00:06<00:03, 383.92it/s]
Adding requests:  66%|██████▌   | 2698/4096 [00:06<00:03, 383.09it/s]
Adding requests:  67%|██████▋   | 2739/4096 [00:07<00:03, 389.04it/s]
Adding requests:  68%|██████▊   | 2780/4096 [00:07<00:03, 393.23it/s]
Adding requests:  69%|██████▉   | 2820/4096 [00:07<00:03, 389.62it/s]
Adding requests:  70%|██████▉   | 2860/4096 [00:07<00:03, 391.60it/s]
Adding requests:  71%|███████   | 2900/4096 [00:07<00:03, 393.94it/s]
Adding requests:  72%|███████▏  | 2940/4096 [00:07<00:02, 391.48it/s]
Adding requests:  73%|███████▎  | 2982/4096 [00:07<00:02, 397.34it/s]
Adding requests:  74%|███████▍  | 3024/4096 [00:07<00:02, 403.21it/s]
Adding requests:  75%|███████▍  | 3065/4096 [00:07<00:02, 394.03it/s]
Adding requests:  76%|███████▌  | 3105/4096 [00:07<00:02, 388.63it/s]
Adding requests:  77%|███████▋  | 3146/4096 [00:08<00:02, 392.41it/s]
Adding requests:  78%|███████▊  | 3187/4096 [00:08<00:02, 396.99it/s]
Adding requests:  79%|███████▉  | 3227/4096 [00:08<00:02, 397.29it/s]
Adding requests:  80%|███████▉  | 3269/4096 [00:08<00:02, 401.09it/s]
Adding requests:  81%|████████  | 3310/4096 [00:08<00:01, 399.94it/s]
Adding requests:  82%|████████▏ | 3351/4096 [00:08<00:01, 397.82it/s]
Adding requests:  83%|████████▎ | 3391/4096 [00:08<00:01, 391.11it/s]
Adding requests:  84%|████████▍ | 3431/4096 [00:08<00:01, 387.07it/s]
Adding requests:  85%|████████▍ | 3470/4096 [00:08<00:01, 372.80it/s]
Adding requests:  86%|████████▌ | 3508/4096 [00:09<00:01, 372.20it/s]
Adding requests:  87%|████████▋ | 3547/4096 [00:09<00:01, 376.75it/s]
Adding requests:  88%|████████▊ | 3587/4096 [00:09<00:01, 381.41it/s]
Adding requests:  89%|████████▊ | 3626/4096 [00:09<00:01, 366.44it/s]
Adding requests:  89%|████████▉ | 3663/4096 [00:09<00:01, 362.57it/s]
Adding requests:  90%|█████████ | 3702/4096 [00:09<00:01, 370.41it/s]
Adding requests:  91%|█████████▏| 3741/4096 [00:09<00:00, 375.40it/s]
Adding requests:  92%|█████████▏| 3782/4096 [00:09<00:00, 383.86it/s]
Adding requests:  93%|█████████▎| 3822/4096 [00:09<00:00, 388.24it/s]
Adding requests:  94%|█████████▍| 3862/4096 [00:09<00:00, 391.22it/s]
Adding requests:  95%|█████████▌| 3902/4096 [00:10<00:00, 381.87it/s]
Adding requests:  96%|█████████▌| 3941/4096 [00:10<00:00, 382.77it/s]
Adding requests:  97%|█████████▋| 3980/4096 [00:10<00:00, 379.35it/s]
Adding requests:  98%|█████████▊| 4022/4096 [00:10<00:00, 388.43it/s]
Adding requests:  99%|█████████▉| 4061/4096 [00:10<00:00, 374.19it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 388.29it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 699/4096 [00:00<00:01, 2034.22it/s, est. speed input: 2083199.51 toks/s, output: 2034.26 toks/s]
Processed prompts:  22%|██▏       | 903/4096 [00:03<00:14, 224.28it/s, est. speed input: 289478.46 toks/s, output: 282.69 toks/s]   
Processed prompts:  24%|██▍       | 991/4096 [00:04<00:19, 160.51it/s, est. speed input: 220497.03 toks/s, output: 215.33 toks/s]
Processed prompts:  25%|██▌       | 1042/4096 [00:05<00:20, 151.97it/s, est. speed input: 210113.42 toks/s, output: 205.19 toks/s]
Processed prompts:  26%|██▋       | 1077/4096 [00:05<00:21, 137.45it/s, est. speed input: 198600.28 toks/s, output: 193.95 toks/s]
Processed prompts:  27%|██▋       | 1102/4096 [00:06<00:25, 119.66it/s, est. speed input: 187192.26 toks/s, output: 182.80 toks/s]
Processed prompts:  27%|██▋       | 1121/4096 [00:06<00:29, 101.28it/s, est. speed input: 176498.17 toks/s, output: 172.36 toks/s]
Processed prompts:  28%|██▊       | 1147/4096 [00:06<00:32, 90.23it/s, est. speed input: 168546.45 toks/s, output: 164.60 toks/s] 
Processed prompts:  29%|██▉       | 1179/4096 [00:07<00:34, 84.17it/s, est. speed input: 162162.56 toks/s, output: 158.36 toks/s]
Processed prompts:  30%|██▉       | 1211/4096 [00:07<00:36, 79.53it/s, est. speed input: 156546.50 toks/s, output: 152.88 toks/s]
Processed prompts:  30%|███       | 1243/4096 [00:08<00:37, 76.58it/s, est. speed input: 151774.80 toks/s, output: 148.22 toks/s]
Processed prompts:  31%|███       | 1275/4096 [00:08<00:38, 73.86it/s, est. speed input: 147310.03 toks/s, output: 143.86 toks/s]
Processed prompts:  32%|███▏      | 1307/4096 [00:09<00:38, 71.98it/s, est. speed input: 143328.28 toks/s, output: 139.97 toks/s]
Processed prompts:  33%|███▎      | 1339/4096 [00:09<00:39, 70.51it/s, est. speed input: 139692.17 toks/s, output: 136.42 toks/s]
Processed prompts:  33%|███▎      | 1371/4096 [00:10<00:39, 69.51it/s, est. speed input: 136404.70 toks/s, output: 133.21 toks/s]
Processed prompts:  34%|███▍      | 1403/4096 [00:10<00:39, 68.80it/s, est. speed input: 133410.22 toks/s, output: 130.28 toks/s]
Processed prompts:  35%|███▌      | 1435/4096 [00:11<00:38, 68.29it/s, est. speed input: 130664.93 toks/s, output: 127.60 toks/s]
Processed prompts:  36%|███▌      | 1467/4096 [00:11<00:38, 67.93it/s, est. speed input: 128142.62 toks/s, output: 125.14 toks/s]
Processed prompts:  37%|███▋      | 1499/4096 [00:12<00:38, 67.72it/s, est. speed input: 125829.87 toks/s, output: 122.88 toks/s]
Processed prompts:  37%|███▋      | 1531/4096 [00:12<00:38, 67.50it/s, est. speed input: 123672.07 toks/s, output: 120.77 toks/s]
Processed prompts:  38%|███▊      | 1563/4096 [00:13<00:37, 67.47it/s, est. speed input: 121698.24 toks/s, output: 118.85 toks/s]
Processed prompts:  39%|███▉      | 1595/4096 [00:13<00:36, 67.88it/s, est. speed input: 119952.27 toks/s, output: 117.14 toks/s]
Processed prompts:  40%|███▉      | 1627/4096 [00:14<00:36, 67.66it/s, est. speed input: 118220.52 toks/s, output: 115.45 toks/s]
Processed prompts:  41%|████      | 1659/4096 [00:14<00:36, 67.42it/s, est. speed input: 116587.54 toks/s, output: 113.85 toks/s]
Processed prompts:  41%|████▏     | 1691/4096 [00:15<00:35, 67.35it/s, est. speed input: 115074.78 toks/s, output: 112.38 toks/s]
Processed prompts:  42%|████▏     | 1723/4096 [00:15<00:35, 67.26it/s, est. speed input: 113647.88 toks/s, output: 110.98 toks/s]
Processed prompts:  43%|████▎     | 1755/4096 [00:16<00:34, 67.15it/s, est. speed input: 112298.57 toks/s, output: 109.67 toks/s]
Processed prompts:  44%|████▎     | 1787/4096 [00:16<00:34, 67.14it/s, est. speed input: 111037.40 toks/s, output: 108.43 toks/s]
Processed prompts:  44%|████▍     | 1819/4096 [00:16<00:33, 67.03it/s, est. speed input: 109831.59 toks/s, output: 107.26 toks/s]
Processed prompts:  45%|████▌     | 1851/4096 [00:17<00:33, 67.54it/s, est. speed input: 108779.46 toks/s, output: 106.23 toks/s]
Processed prompts:  46%|████▌     | 1883/4096 [00:17<00:32, 67.30it/s, est. speed input: 107697.16 toks/s, output: 105.17 toks/s]
Processed prompts:  47%|████▋     | 1915/4096 [00:18<00:32, 67.21it/s, est. speed input: 106680.63 toks/s, output: 104.18 toks/s]
Processed prompts:  48%|████▊     | 1947/4096 [00:18<00:31, 67.63it/s, est. speed input: 105780.79 toks/s, output: 103.30 toks/s]
Processed prompts:  48%|████▊     | 1979/4096 [00:19<00:31, 67.42it/s, est. speed input: 104858.89 toks/s, output: 102.40 toks/s]
Processed prompts:  49%|████▉     | 2011/4096 [00:19<00:31, 67.25it/s, est. speed input: 103979.33 toks/s, output: 101.54 toks/s]
Processed prompts:  50%|████▉     | 2043/4096 [00:20<00:30, 67.09it/s, est. speed input: 103136.71 toks/s, output: 100.72 toks/s]
Processed prompts:  51%|█████     | 2075/4096 [00:20<00:30, 67.01it/s, est. speed input: 102336.10 toks/s, output: 99.94 toks/s] 
Processed prompts:  51%|█████▏    | 2107/4096 [00:21<00:29, 67.03it/s, est. speed input: 101579.85 toks/s, output: 99.20 toks/s]
Processed prompts:  52%|█████▏    | 2139/4096 [00:21<00:29, 66.97it/s, est. speed input: 100849.43 toks/s, output: 98.49 toks/s]
Processed prompts:  53%|█████▎    | 2171/4096 [00:22<00:28, 66.95it/s, est. speed input: 100152.96 toks/s, output: 97.81 toks/s]
Processed prompts:  54%|█████▍    | 2203/4096 [00:22<00:28, 66.94it/s, est. speed input: 99485.84 toks/s, output: 97.15 toks/s] 
Processed prompts:  55%|█████▍    | 2235/4096 [00:23<00:27, 68.07it/s, est. speed input: 98960.26 toks/s, output: 96.64 toks/s]
Processed prompts:  55%|█████▌    | 2267/4096 [00:23<00:27, 67.67it/s, est. speed input: 98338.99 toks/s, output: 96.03 toks/s]
Processed prompts:  56%|█████▌    | 2299/4096 [00:24<00:26, 67.99it/s, est. speed input: 97798.48 toks/s, output: 95.51 toks/s]
Processed prompts:  57%|█████▋    | 2331/4096 [00:24<00:25, 68.19it/s, est. speed input: 97276.11 toks/s, output: 95.00 toks/s]
Processed prompts:  58%|█████▊    | 2363/4096 [00:24<00:25, 68.97it/s, est. speed input: 96829.85 toks/s, output: 94.56 toks/s]
Processed prompts:  58%|█████▊    | 2395/4096 [00:25<00:24, 68.23it/s, est. speed input: 96288.40 toks/s, output: 94.03 toks/s]
Processed prompts:  59%|█████▉    | 2427/4096 [00:25<00:24, 67.77it/s, est. speed input: 95771.96 toks/s, output: 93.53 toks/s]
Processed prompts:  60%|██████    | 2459/4096 [00:26<00:24, 67.50it/s, est. speed input: 95277.32 toks/s, output: 93.04 toks/s]
Processed prompts:  61%|██████    | 2491/4096 [00:26<00:23, 67.75it/s, est. speed input: 94837.33 toks/s, output: 92.61 toks/s]
Processed prompts:  62%|██████▏   | 2523/4096 [00:27<00:23, 67.48it/s, est. speed input: 94375.82 toks/s, output: 92.16 toks/s]
Processed prompts:  62%|██████▏   | 2555/4096 [00:27<00:22, 67.23it/s, est. speed input: 93925.48 toks/s, output: 91.72 toks/s]
Processed prompts:  63%|██████▎   | 2587/4096 [00:28<00:22, 67.58it/s, est. speed input: 93531.55 toks/s, output: 91.34 toks/s]
Processed prompts:  64%|██████▍   | 2619/4096 [00:28<00:21, 67.38it/s, est. speed input: 93115.97 toks/s, output: 90.93 toks/s]
Processed prompts:  65%|██████▍   | 2651/4096 [00:29<00:21, 67.20it/s, est. speed input: 92711.48 toks/s, output: 90.54 toks/s]
Processed prompts:  66%|██████▌   | 2683/4096 [00:29<00:21, 67.08it/s, est. speed input: 92319.90 toks/s, output: 90.16 toks/s]
Processed prompts:  66%|██████▋   | 2715/4096 [00:30<00:20, 66.97it/s, est. speed input: 91939.50 toks/s, output: 89.78 toks/s]
Processed prompts:  67%|██████▋   | 2747/4096 [00:30<00:20, 66.91it/s, est. speed input: 91571.75 toks/s, output: 89.43 toks/s]
Processed prompts:  68%|██████▊   | 2779/4096 [00:31<00:19, 66.88it/s, est. speed input: 91216.14 toks/s, output: 89.08 toks/s]
Processed prompts:  69%|██████▊   | 2811/4096 [00:31<00:19, 66.82it/s, est. speed input: 90868.43 toks/s, output: 88.74 toks/s]
Processed prompts:  69%|██████▉   | 2843/4096 [00:32<00:18, 66.84it/s, est. speed input: 90535.62 toks/s, output: 88.41 toks/s]
Processed prompts:  70%|███████   | 2875/4096 [00:32<00:18, 66.84it/s, est. speed input: 90211.72 toks/s, output: 88.10 toks/s]
Processed prompts:  71%|███████   | 2907/4096 [00:33<00:17, 66.81it/s, est. speed input: 89895.18 toks/s, output: 87.79 toks/s]
Processed prompts:  72%|███████▏  | 2939/4096 [00:33<00:17, 66.74it/s, est. speed input: 89584.47 toks/s, output: 87.48 toks/s]
Processed prompts:  73%|███████▎  | 2971/4096 [00:34<00:16, 66.72it/s, est. speed input: 89284.47 toks/s, output: 87.19 toks/s]
Processed prompts:  73%|███████▎  | 3003/4096 [00:34<00:16, 66.77it/s, est. speed input: 88996.23 toks/s, output: 86.91 toks/s]
Processed prompts:  74%|███████▍  | 3035/4096 [00:35<00:15, 66.79it/s, est. speed input: 88715.55 toks/s, output: 86.64 toks/s]
Processed prompts:  75%|███████▍  | 3067/4096 [00:35<00:15, 66.82it/s, est. speed input: 88443.06 toks/s, output: 86.37 toks/s]
Processed prompts:  76%|███████▌  | 3099/4096 [00:35<00:14, 66.73it/s, est. speed input: 88171.62 toks/s, output: 86.11 toks/s]
Processed prompts:  76%|███████▋  | 3131/4096 [00:36<00:14, 67.19it/s, est. speed input: 87937.12 toks/s, output: 85.88 toks/s]
Processed prompts:  77%|███████▋  | 3163/4096 [00:36<00:13, 67.06it/s, est. speed input: 87683.09 toks/s, output: 85.63 toks/s]
Processed prompts:  78%|███████▊  | 3195/4096 [00:37<00:13, 66.93it/s, est. speed input: 87433.54 toks/s, output: 85.38 toks/s]
Processed prompts:  79%|███████▉  | 3227/4096 [00:37<00:12, 66.87it/s, est. speed input: 87191.69 toks/s, output: 85.15 toks/s]
Processed prompts:  80%|███████▉  | 3259/4096 [00:38<00:12, 66.78it/s, est. speed input: 86953.61 toks/s, output: 84.92 toks/s]
Processed prompts:  80%|████████  | 3291/4096 [00:38<00:12, 66.74it/s, est. speed input: 86722.47 toks/s, output: 84.69 toks/s]
Processed prompts:  81%|████████  | 3323/4096 [00:39<00:11, 66.75it/s, est. speed input: 86498.64 toks/s, output: 84.47 toks/s]
Processed prompts:  82%|████████▏ | 3355/4096 [00:39<00:11, 66.64it/s, est. speed input: 86274.50 toks/s, output: 84.25 toks/s]
Processed prompts:  83%|████████▎ | 3387/4096 [00:40<00:10, 66.68it/s, est. speed input: 86061.48 toks/s, output: 84.04 toks/s]
Processed prompts:  83%|████████▎ | 3419/4096 [00:40<00:10, 66.65it/s, est. speed input: 85850.81 toks/s, output: 83.84 toks/s]
Processed prompts:  84%|████████▍ | 3451/4096 [00:41<00:09, 66.69it/s, est. speed input: 85648.14 toks/s, output: 83.64 toks/s]
Processed prompts:  85%|████████▌ | 3483/4096 [00:41<00:09, 67.82it/s, est. speed input: 85503.14 toks/s, output: 83.50 toks/s]
Processed prompts:  86%|████████▌ | 3515/4096 [00:42<00:08, 67.44it/s, est. speed input: 85305.63 toks/s, output: 83.31 toks/s]
Processed prompts:  87%|████████▋ | 3547/4096 [00:42<00:08, 67.19it/s, est. speed input: 85113.08 toks/s, output: 83.12 toks/s]
Processed prompts:  87%|████████▋ | 3579/4096 [00:43<00:07, 66.99it/s, est. speed input: 84923.94 toks/s, output: 82.93 toks/s]
Processed prompts:  88%|████████▊ | 3611/4096 [00:43<00:07, 66.94it/s, est. speed input: 84742.64 toks/s, output: 82.76 toks/s]
Processed prompts:  89%|████████▉ | 3643/4096 [00:44<00:06, 66.85it/s, est. speed input: 84562.91 toks/s, output: 82.58 toks/s]
Processed prompts:  90%|████████▉ | 3675/4096 [00:44<00:06, 66.79it/s, est. speed input: 84387.21 toks/s, output: 82.41 toks/s]
Processed prompts:  91%|█████████ | 3707/4096 [00:45<00:05, 66.72it/s, est. speed input: 84214.12 toks/s, output: 82.24 toks/s]
Processed prompts:  91%|█████████▏| 3739/4096 [00:45<00:05, 67.26it/s, est. speed input: 84070.53 toks/s, output: 82.10 toks/s]
Processed prompts:  92%|█████████▏| 3771/4096 [00:46<00:04, 66.97it/s, est. speed input: 83900.94 toks/s, output: 81.93 toks/s]
Processed prompts:  93%|█████████▎| 3803/4096 [00:46<00:04, 66.88it/s, est. speed input: 83739.57 toks/s, output: 81.78 toks/s]
Processed prompts:  94%|█████████▎| 3835/4096 [00:46<00:03, 67.33it/s, est. speed input: 83603.13 toks/s, output: 81.64 toks/s]
Processed prompts:  94%|█████████▍| 3867/4096 [00:47<00:03, 67.10it/s, est. speed input: 83446.62 toks/s, output: 81.49 toks/s]
Processed prompts:  95%|█████████▌| 3899/4096 [00:47<00:02, 67.01it/s, est. speed input: 83296.53 toks/s, output: 81.34 toks/s]
Processed prompts:  96%|█████████▌| 3931/4096 [00:48<00:02, 66.83it/s, est. speed input: 83144.21 toks/s, output: 81.20 toks/s]
Processed prompts:  97%|█████████▋| 3963/4096 [00:48<00:01, 66.68it/s, est. speed input: 82994.05 toks/s, output: 81.05 toks/s]
Processed prompts:  98%|█████████▊| 3995/4096 [00:49<00:01, 66.62it/s, est. speed input: 82848.46 toks/s, output: 80.91 toks/s]
Processed prompts:  98%|█████████▊| 4027/4096 [00:49<00:01, 67.03it/s, est. speed input: 82724.12 toks/s, output: 80.79 toks/s]
Processed prompts:  99%|█████████▉| 4059/4096 [00:50<00:00, 67.04it/s, est. speed input: 82590.60 toks/s, output: 80.65 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:50<00:00, 67.04it/s, est. speed input: 83183.14 toks/s, output: 81.23 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:50<00:00, 81.23it/s, est. speed input: 83183.14 toks/s, output: 81.23 toks/s]
[rank0]:[W128 00:29:03.233831014 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 133.4s

测试结果:
  Requests/s:   67.18
  Tokens/s:     68855.75
  Total Reqs:   4096
  Elapsed:      60.97s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     68788.57


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,19.4350,9970.1683,6.5860
1024,1024,1,128,128,20.1795,20683.9742,6.3431
2048,1024,2,256,128,39.4641,40450.7122,6.4869
4096,1024,4,512,128,62.6377,64203.5968,8.1740
8192,1024,8,1024,128,65.4134,67048.7777,15.6543
16384,1024,16,2048,128,66.7345,68402.8839,30.6888
32768,1024,32,4096,128,67.1763,68855.7479,60.9738

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:29:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2242752) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2242752) WARNING 01-28 00:29:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 19.64 requests/s, 10074.68 total tokens/s, 19.64 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:29:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:29:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:29:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:29:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:29:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:29:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:29:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:29:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:29:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:29:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:29:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:29:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:29:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:29:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:29:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:29:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:29:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:29:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2242752) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2242752) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.17it/s]
(EngineCore_DP0 pid=2242752) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.17it/s]
(EngineCore_DP0 pid=2242752) 
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=2242752) [2026-01-28 00:29:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=2242752) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.35it/s]
(EngineCore_DP0 pid=2242752) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.05it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  52%|█████▏    | 66/128 [00:00<00:00, 650.44it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 668.73it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 16.29it/s, est. speed input: 8341.83 toks/s, output: 16.29 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:06, 18.01it/s, est. speed input: 9079.38 toks/s, output: 17.73 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:06, 19.38it/s, est. speed input: 9657.69 toks/s, output: 18.86 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:05, 20.01it/s, est. speed input: 9944.13 toks/s, output: 19.42 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:05, 20.31it/s, est. speed input: 10099.60 toks/s, output: 19.73 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:05, 20.48it/s, est. speed input: 10196.72 toks/s, output: 19.91 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:05, 20.53it/s, est. speed input: 10253.17 toks/s, output: 20.03 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:05, 20.48it/s, est. speed input: 10277.23 toks/s, output: 20.07 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 20.20it/s, est. speed input: 10248.73 toks/s, output: 20.02 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:04, 20.26it/s, est. speed input: 10270.03 toks/s, output: 20.06 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:04, 20.42it/s, est. speed input: 10304.17 toks/s, output: 20.13 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:04, 20.53it/s, est. speed input: 10333.42 toks/s, output: 20.18 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:04, 20.31it/s, est. speed input: 10318.07 toks/s, output: 20.15 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:04, 20.43it/s, est. speed input: 10339.23 toks/s, output: 20.19 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 20.24it/s, est. speed input: 10324.83 toks/s, output: 20.17 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 19.79it/s, est. speed input: 10277.36 toks/s, output: 20.07 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 19.56it/s, est. speed input: 10248.39 toks/s, output: 20.02 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:03, 19.50it/s, est. speed input: 10233.67 toks/s, output: 19.99 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:03, 19.94it/s, est. speed input: 10257.63 toks/s, output: 20.03 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:02<00:03, 20.18it/s, est. speed input: 10275.22 toks/s, output: 20.07 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:02<00:03, 20.18it/s, est. speed input: 10278.30 toks/s, output: 20.07 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 20.29it/s, est. speed input: 10289.58 toks/s, output: 20.10 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 20.25it/s, est. speed input: 10291.15 toks/s, output: 20.10 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:02, 20.50it/s, est. speed input: 10312.16 toks/s, output: 20.14 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:03<00:02, 20.62it/s, est. speed input: 10328.03 toks/s, output: 20.17 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:03<00:02, 20.68it/s, est. speed input: 10341.24 toks/s, output: 20.20 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:03<00:02, 20.72it/s, est. speed input: 10353.10 toks/s, output: 20.22 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:03<00:02, 20.35it/s, est. speed input: 10339.40 toks/s, output: 20.19 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 20.46it/s, est. speed input: 10349.10 toks/s, output: 20.21 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:04<00:02, 20.47it/s, est. speed input: 10354.06 toks/s, output: 20.22 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:04<00:01, 20.17it/s, est. speed input: 10341.02 toks/s, output: 20.20 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:04<00:01, 20.38it/s, est. speed input: 10352.41 toks/s, output: 20.22 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:04<00:01, 20.46it/s, est. speed input: 10359.33 toks/s, output: 20.23 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:04<00:01, 20.57it/s, est. speed input: 10368.22 toks/s, output: 20.25 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:04<00:01, 20.27it/s, est. speed input: 10358.01 toks/s, output: 20.23 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:05<00:01, 20.14it/s, est. speed input: 10352.39 toks/s, output: 20.22 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:05<00:01, 20.27it/s, est. speed input: 10357.20 toks/s, output: 20.23 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:05<00:00, 20.31it/s, est. speed input: 10359.70 toks/s, output: 20.23 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:05<00:00, 19.95it/s, est. speed input: 10344.49 toks/s, output: 20.20 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:05<00:00, 20.15it/s, est. speed input: 10350.08 toks/s, output: 20.21 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:05<00:00, 20.31it/s, est. speed input: 10356.15 toks/s, output: 20.23 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:06<00:00, 20.46it/s, est. speed input: 10363.16 toks/s, output: 20.24 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:06<00:00, 20.28it/s, est. speed input: 10358.61 toks/s, output: 20.23 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.34it/s, est. speed input: 10361.61 toks/s, output: 20.24 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.34it/s, est. speed input: 10361.61 toks/s, output: 20.24 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.24it/s, est. speed input: 10361.61 toks/s, output: 20.24 toks/s]
[rank0]:[W128 00:29:56.065816752 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.7s

测试结果:
  Requests/s:   19.64
  Tokens/s:     10074.68
  Total Reqs:   128
  Elapsed:      6.52s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     10055.04

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:30:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2243758) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2243758) WARNING 01-28 00:30:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 19.87 requests/s, 20367.82 total tokens/s, 19.87 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:30:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:30:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:30:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:30:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:30:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:30:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:30:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:30:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:30:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:30:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:30:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:30:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:30:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:30:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:30:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:30:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:30:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:30:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:30:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:30:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:30:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:30:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:30:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:30:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:30:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:30:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:30:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:30:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:15] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2243758) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2243758) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.20it/s]
(EngineCore_DP0 pid=2243758) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.20it/s]
(EngineCore_DP0 pid=2243758) 
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=2243758) [2026-01-28 00:30:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=2243758) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.39it/s]
(EngineCore_DP0 pid=2243758) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.14it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  27%|██▋       | 34/128 [00:00<00:00, 336.73it/s]
Adding requests:  55%|█████▌    | 71/128 [00:00<00:00, 354.16it/s]
Adding requests:  84%|████████▍ | 108/128 [00:00<00:00, 361.30it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 363.06it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:03, 38.58it/s, est. speed input: 39506.16 toks/s, output: 38.58 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 26.48it/s, est. speed input: 28614.19 toks/s, output: 27.94 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:04, 23.88it/s, est. speed input: 26186.27 toks/s, output: 25.57 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:05, 22.55it/s, est. speed input: 24913.75 toks/s, output: 24.33 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:05, 21.98it/s, est. speed input: 24251.67 toks/s, output: 23.68 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:04, 21.42it/s, est. speed input: 23688.37 toks/s, output: 23.13 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:04, 21.34it/s, est. speed input: 23417.60 toks/s, output: 22.87 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:04, 21.28it/s, est. speed input: 23207.50 toks/s, output: 22.66 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:04, 21.29it/s, est. speed input: 23059.34 toks/s, output: 22.52 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:04, 21.23it/s, est. speed input: 22918.41 toks/s, output: 22.38 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:04, 20.98it/s, est. speed input: 22738.17 toks/s, output: 22.20 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:04, 20.52it/s, est. speed input: 22500.18 toks/s, output: 21.97 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:04, 20.56it/s, est. speed input: 22397.87 toks/s, output: 21.87 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:03, 20.87it/s, est. speed input: 22380.21 toks/s, output: 21.86 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:03, 21.04it/s, est. speed input: 22355.38 toks/s, output: 21.83 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:03, 21.16it/s, est. speed input: 22330.29 toks/s, output: 21.81 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:02<00:03, 21.19it/s, est. speed input: 22299.02 toks/s, output: 21.78 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:02<00:03, 20.93it/s, est. speed input: 22216.25 toks/s, output: 21.70 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:02<00:03, 20.62it/s, est. speed input: 22118.36 toks/s, output: 21.60 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:02<00:03, 20.01it/s, est. speed input: 21957.09 toks/s, output: 21.44 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 20.20it/s, est. speed input: 21919.55 toks/s, output: 21.41 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:03<00:02, 20.22it/s, est. speed input: 21865.94 toks/s, output: 21.35 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:03<00:02, 20.58it/s, est. speed input: 21871.71 toks/s, output: 21.36 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:03<00:02, 20.54it/s, est. speed input: 21832.35 toks/s, output: 21.32 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:03<00:02, 20.55it/s, est. speed input: 21801.96 toks/s, output: 21.29 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:03<00:02, 20.39it/s, est. speed input: 21751.70 toks/s, output: 21.24 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:03<00:02, 20.56it/s, est. speed input: 21740.69 toks/s, output: 21.23 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:04<00:01, 20.82it/s, est. speed input: 21748.74 toks/s, output: 21.24 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:04<00:01, 21.02it/s, est. speed input: 21757.37 toks/s, output: 21.25 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:04<00:01, 21.11it/s, est. speed input: 21760.42 toks/s, output: 21.25 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:04<00:01, 20.86it/s, est. speed input: 21728.66 toks/s, output: 21.22 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:04<00:01, 20.72it/s, est. speed input: 21702.27 toks/s, output: 21.19 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:04<00:01, 20.66it/s, est. speed input: 21680.80 toks/s, output: 21.17 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:04<00:01, 20.85it/s, est. speed input: 21685.23 toks/s, output: 21.18 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:05<00:00, 21.01it/s, est. speed input: 21691.05 toks/s, output: 21.18 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:05<00:00, 21.11it/s, est. speed input: 21695.17 toks/s, output: 21.19 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:05<00:00, 21.10it/s, est. speed input: 21692.56 toks/s, output: 21.18 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:05<00:00, 20.77it/s, est. speed input: 21660.94 toks/s, output: 21.15 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:05<00:00, 20.48it/s, est. speed input: 21625.31 toks/s, output: 21.12 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:05<00:00, 20.48it/s, est. speed input: 21608.90 toks/s, output: 21.10 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:05<00:00, 19.88it/s, est. speed input: 21539.87 toks/s, output: 21.03 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 19.88it/s, est. speed input: 21530.85 toks/s, output: 21.03 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 21.03it/s, est. speed input: 21530.85 toks/s, output: 21.03 toks/s]
[rank0]:[W128 00:30:49.139330088 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.1s

测试结果:
  Requests/s:   19.87
  Tokens/s:     20367.82
  Total Reqs:   128
  Elapsed:      6.44s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     20347.95

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:31:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2244722) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2244722) WARNING 01-28 00:31:18 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.60 requests/s, 40587.92 total tokens/s, 39.60 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 00:31:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:31:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:31:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:31:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:31:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:31:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:31:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:31:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:31:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:31:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:31:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:31:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:31:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:31:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:31:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:31:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:31:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:31:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:09] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2244722) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2244722) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.17it/s]
(EngineCore_DP0 pid=2244722) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.17it/s]
(EngineCore_DP0 pid=2244722) 
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=2244722) [2026-01-28 00:31:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=2244722) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  9.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.10it/s]
(EngineCore_DP0 pid=2244722) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  9.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.91it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 36/256 [00:00<00:00, 356.02it/s]
Adding requests:  30%|███       | 77/256 [00:00<00:00, 383.02it/s]
Adding requests:  46%|████▌     | 117/256 [00:00<00:00, 388.74it/s]
Adding requests:  61%|██████    | 156/256 [00:00<00:00, 385.28it/s]
Adding requests:  76%|███████▌  | 195/256 [00:00<00:00, 369.61it/s]
Adding requests:  91%|█████████ | 233/256 [00:00<00:00, 372.01it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 378.24it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 26/256 [00:00<00:01, 178.32it/s, est. speed input: 182624.38 toks/s, output: 178.33 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:00<00:03, 66.22it/s, est. speed input: 76313.26 toks/s, output: 74.52 toks/s]   
Processed prompts:  21%|██        | 54/256 [00:00<00:03, 56.47it/s, est. speed input: 66334.86 toks/s, output: 64.78 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:03, 51.78it/s, est. speed input: 61771.24 toks/s, output: 60.32 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:01<00:03, 49.19it/s, est. speed input: 59359.56 toks/s, output: 57.97 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:03, 46.85it/s, est. speed input: 57336.27 toks/s, output: 55.99 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:03, 44.96it/s, est. speed input: 55666.12 toks/s, output: 54.36 toks/s]
Processed prompts:  33%|███▎      | 85/256 [00:01<00:03, 45.75it/s, est. speed input: 55286.71 toks/s, output: 53.99 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:04, 41.48it/s, est. speed input: 53284.16 toks/s, output: 52.03 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:01<00:03, 43.14it/s, est. speed input: 53094.04 toks/s, output: 51.85 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:01<00:03, 39.23it/s, est. speed input: 51433.61 toks/s, output: 50.23 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:03, 39.94it/s, est. speed input: 50833.80 toks/s, output: 49.64 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:03, 40.50it/s, est. speed input: 50327.83 toks/s, output: 49.15 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 40.69it/s, est. speed input: 49833.67 toks/s, output: 48.67 toks/s]
Processed prompts:  48%|████▊     | 123/256 [00:02<00:03, 42.83it/s, est. speed input: 49886.92 toks/s, output: 48.72 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:02<00:03, 40.15it/s, est. speed input: 49097.83 toks/s, output: 47.95 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:02<00:03, 40.42it/s, est. speed input: 48727.56 toks/s, output: 47.59 toks/s]
Processed prompts:  54%|█████▍    | 139/256 [00:02<00:02, 42.15it/s, est. speed input: 48722.87 toks/s, output: 47.58 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:03<00:02, 38.45it/s, est. speed input: 47872.17 toks/s, output: 46.75 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:03<00:02, 38.40it/s, est. speed input: 47586.65 toks/s, output: 46.47 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 39.20it/s, est. speed input: 47331.08 toks/s, output: 46.22 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:03<00:02, 39.95it/s, est. speed input: 47132.74 toks/s, output: 46.03 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:03<00:02, 40.16it/s, est. speed input: 46906.38 toks/s, output: 45.81 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:03<00:02, 40.32it/s, est. speed input: 46701.47 toks/s, output: 45.61 toks/s]
Processed prompts:  69%|██████▉   | 177/256 [00:03<00:01, 42.41it/s, est. speed input: 46798.22 toks/s, output: 45.70 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:01, 39.03it/s, est. speed input: 46275.96 toks/s, output: 45.19 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:04<00:01, 39.66it/s, est. speed input: 46126.57 toks/s, output: 45.05 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:04<00:01, 40.52it/s, est. speed input: 46042.20 toks/s, output: 44.96 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:04<00:01, 41.25it/s, est. speed input: 45977.38 toks/s, output: 44.90 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:04<00:01, 41.38it/s, est. speed input: 45874.63 toks/s, output: 44.80 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:04<00:01, 41.80it/s, est. speed input: 45813.05 toks/s, output: 44.74 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:04<00:00, 41.62it/s, est. speed input: 45706.13 toks/s, output: 44.63 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:05<00:00, 41.22it/s, est. speed input: 45575.67 toks/s, output: 44.51 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:05<00:00, 41.47it/s, est. speed input: 45506.69 toks/s, output: 44.44 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:05<00:00, 41.72it/s, est. speed input: 45447.91 toks/s, output: 44.38 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:05<00:00, 42.03it/s, est. speed input: 45406.17 toks/s, output: 44.34 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:05<00:00, 42.04it/s, est. speed input: 45346.03 toks/s, output: 44.28 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:05<00:00, 42.26it/s, est. speed input: 45308.50 toks/s, output: 44.25 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 42.26it/s, est. speed input: 45300.68 toks/s, output: 44.24 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 44.24it/s, est. speed input: 45300.68 toks/s, output: 44.24 toks/s]
[rank0]:[W128 00:31:44.478987563 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.4s

测试结果:
  Requests/s:   39.60
  Tokens/s:     40587.92
  Total Reqs:   256
  Elapsed:      6.46s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     40548.32

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:31:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2245746) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2245746) WARNING 01-28 00:32:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 60.29 requests/s, 61801.43 total tokens/s, 60.29 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 00:31:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:31:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:31:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:31:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:31:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:31:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:31:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:31:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:31:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:32:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:32:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:32:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:32:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:32:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:32:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:32:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:32:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:32:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2245746) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2245746) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.13it/s]
(EngineCore_DP0 pid=2245746) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.13it/s]
(EngineCore_DP0 pid=2245746) 
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=2245746) [2026-01-28 00:32:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=2245746) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  9.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.36it/s]
(EngineCore_DP0 pid=2245746) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  9.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 10.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  9.95it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 36/512 [00:00<00:01, 351.95it/s]
Adding requests:  15%|█▍        | 75/512 [00:00<00:01, 370.38it/s]
Adding requests:  22%|██▏       | 113/512 [00:00<00:01, 365.47it/s]
Adding requests:  29%|██▉       | 150/512 [00:00<00:00, 362.69it/s]
Adding requests:  37%|███▋      | 187/512 [00:00<00:00, 352.07it/s]
Adding requests:  44%|████▍     | 224/512 [00:00<00:00, 356.07it/s]
Adding requests:  51%|█████     | 262/512 [00:00<00:00, 360.48it/s]
Adding requests:  59%|█████▉    | 301/512 [00:00<00:00, 366.93it/s]
Adding requests:  66%|██████▋   | 340/512 [00:00<00:00, 373.55it/s]
Adding requests:  74%|███████▍  | 378/512 [00:01<00:00, 372.72it/s]
Adding requests:  81%|████████▏ | 416/512 [00:01<00:00, 365.24it/s]
Adding requests:  88%|████████▊ | 453/512 [00:01<00:00, 364.43it/s]
Adding requests:  96%|█████████▌| 492/512 [00:01<00:00, 371.98it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 366.03it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:00<00:00, 614.92it/s, est. speed input: 629778.85 toks/s, output: 614.95 toks/s]
Processed prompts:  29%|██▉       | 148/512 [00:01<00:03, 112.57it/s, est. speed input: 134410.63 toks/s, output: 131.26 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:01<00:03, 89.88it/s, est. speed input: 110264.33 toks/s, output: 107.68 toks/s] 
Processed prompts:  38%|███▊      | 197/512 [00:01<00:03, 85.96it/s, est. speed input: 105332.17 toks/s, output: 102.86 toks/s]
Processed prompts:  41%|████      | 211/512 [00:02<00:03, 77.86it/s, est. speed input: 99129.39 toks/s, output: 96.81 toks/s]  
Processed prompts:  43%|████▎     | 222/512 [00:02<00:03, 73.29it/s, est. speed input: 95647.84 toks/s, output: 93.41 toks/s]
Processed prompts:  45%|████▌     | 232/512 [00:02<00:03, 73.71it/s, est. speed input: 94701.36 toks/s, output: 92.48 toks/s]
Processed prompts:  47%|████▋     | 241/512 [00:02<00:03, 72.81it/s, est. speed input: 93510.67 toks/s, output: 91.32 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:02<00:04, 64.67it/s, est. speed input: 90174.53 toks/s, output: 88.06 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:02<00:03, 63.86it/s, est. speed input: 88943.55 toks/s, output: 86.86 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:03<00:03, 63.13it/s, est. speed input: 87810.05 toks/s, output: 85.75 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:03<00:03, 62.59it/s, est. speed input: 86781.64 toks/s, output: 84.75 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:03<00:03, 62.15it/s, est. speed input: 85828.70 toks/s, output: 83.82 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:03<00:03, 61.74it/s, est. speed input: 84935.95 toks/s, output: 82.94 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:03<00:03, 61.47it/s, est. speed input: 84111.02 toks/s, output: 82.14 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:03<00:03, 61.29it/s, est. speed input: 83348.24 toks/s, output: 81.39 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:03<00:03, 61.08it/s, est. speed input: 82623.91 toks/s, output: 80.69 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:04<00:03, 61.07it/s, est. speed input: 81969.58 toks/s, output: 80.05 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:04<00:02, 61.03it/s, est. speed input: 81351.46 toks/s, output: 79.44 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:04<00:02, 60.96it/s, est. speed input: 80764.40 toks/s, output: 78.87 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:04<00:02, 61.91it/s, est. speed input: 80342.76 toks/s, output: 78.46 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:04<00:02, 61.50it/s, est. speed input: 79808.92 toks/s, output: 77.94 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:04<00:02, 61.38it/s, est. speed input: 79325.89 toks/s, output: 77.47 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:04<00:02, 61.15it/s, est. speed input: 78852.13 toks/s, output: 77.00 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:04<00:02, 61.00it/s, est. speed input: 78405.10 toks/s, output: 76.57 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:05<00:02, 60.89it/s, est. speed input: 77980.20 toks/s, output: 76.15 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:05<00:01, 60.75it/s, est. speed input: 77570.00 toks/s, output: 75.75 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:05<00:01, 60.71it/s, est. speed input: 77186.97 toks/s, output: 75.38 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:05<00:01, 60.65it/s, est. speed input: 76817.98 toks/s, output: 75.02 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:05<00:01, 60.77it/s, est. speed input: 76483.09 toks/s, output: 74.69 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:05<00:01, 60.65it/s, est. speed input: 76143.65 toks/s, output: 74.36 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:05<00:01, 60.67it/s, est. speed input: 75829.84 toks/s, output: 74.05 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:05<00:01, 60.73it/s, est. speed input: 75534.20 toks/s, output: 73.76 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:06<00:00, 62.10it/s, est. speed input: 75366.28 toks/s, output: 73.60 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:06<00:00, 61.74it/s, est. speed input: 75093.29 toks/s, output: 73.33 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:06<00:00, 61.34it/s, est. speed input: 74818.29 toks/s, output: 73.06 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:06<00:00, 61.20it/s, est. speed input: 74566.96 toks/s, output: 72.82 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:06<00:00, 61.00it/s, est. speed input: 74316.72 toks/s, output: 72.57 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:06<00:00, 60.98it/s, est. speed input: 74085.17 toks/s, output: 72.35 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:06<00:00, 60.77it/s, est. speed input: 73847.96 toks/s, output: 72.12 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:07<00:00, 60.89it/s, est. speed input: 73639.44 toks/s, output: 71.91 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 60.89it/s, est. speed input: 73932.42 toks/s, output: 72.20 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 72.20it/s, est. speed input: 73932.42 toks/s, output: 72.20 toks/s]
[rank0]:[W128 00:32:42.459245295 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 58.0s

测试结果:
  Requests/s:   60.29
  Tokens/s:     61801.43
  Total Reqs:   512
  Elapsed:      8.49s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     61741.13

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:32:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2246886) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2246886) WARNING 01-28 00:33:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 62.89 requests/s, 64460.38 total tokens/s, 62.89 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 00:32:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:32:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:32:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:32:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:32:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:32:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:32:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:32:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:32:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:33:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:33:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:33:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:33:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:33:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:33:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:33:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:33:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:33:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2246886) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2246886) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.08it/s]
(EngineCore_DP0 pid=2246886) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.08it/s]
(EngineCore_DP0 pid=2246886) 
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=2246886) [2026-01-28 00:33:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=2246886) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  9.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  9.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.28it/s]
(EngineCore_DP0 pid=2246886) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  9.00it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  9.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.28it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 36/1024 [00:00<00:02, 357.09it/s]
Adding requests:   7%|▋         | 75/1024 [00:00<00:02, 373.46it/s]
Adding requests:  11%|█         | 113/1024 [00:00<00:02, 370.89it/s]
Adding requests:  15%|█▍        | 151/1024 [00:00<00:02, 369.75it/s]
Adding requests:  18%|█▊        | 189/1024 [00:00<00:02, 371.72it/s]
Adding requests:  22%|██▏       | 230/1024 [00:00<00:02, 382.11it/s]
Adding requests:  26%|██▋       | 269/1024 [00:00<00:01, 380.69it/s]
Adding requests:  30%|███       | 308/1024 [00:00<00:01, 375.59it/s]
Adding requests:  34%|███▍      | 346/1024 [00:00<00:01, 367.90it/s]
Adding requests:  37%|███▋      | 383/1024 [00:01<00:01, 364.95it/s]
Adding requests:  41%|████      | 421/1024 [00:01<00:01, 368.78it/s]
Adding requests:  45%|████▍     | 458/1024 [00:01<00:01, 366.10it/s]
Adding requests:  48%|████▊     | 495/1024 [00:01<00:01, 364.33it/s]
Adding requests:  52%|█████▏    | 532/1024 [00:01<00:01, 356.43it/s]
Adding requests:  56%|█████▌    | 570/1024 [00:01<00:01, 362.85it/s]
Adding requests:  59%|█████▉    | 607/1024 [00:01<00:01, 363.44it/s]
Adding requests:  63%|██████▎   | 646/1024 [00:01<00:01, 369.35it/s]
Adding requests:  67%|██████▋   | 688/1024 [00:01<00:00, 383.45it/s]
Adding requests:  71%|███████   | 729/1024 [00:01<00:00, 390.96it/s]
Adding requests:  75%|███████▌  | 769/1024 [00:02<00:00, 386.68it/s]
Adding requests:  79%|███████▉  | 808/1024 [00:02<00:00, 384.19it/s]
Adding requests:  83%|████████▎ | 847/1024 [00:02<00:00, 384.73it/s]
Adding requests:  87%|████████▋ | 886/1024 [00:02<00:00, 383.59it/s]
Adding requests:  91%|█████████ | 928/1024 [00:02<00:00, 391.58it/s]
Adding requests:  95%|█████████▍| 968/1024 [00:02<00:00, 394.04it/s]
Adding requests:  98%|█████████▊| 1008/1024 [00:02<00:00, 393.07it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 377.84it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:00<00:00, 1018.03it/s, est. speed input: 1042548.17 toks/s, output: 1018.05 toks/s]
Processed prompts:  27%|██▋       | 272/1024 [00:01<00:05, 134.69it/s, est. speed input: 164728.29 toks/s, output: 160.87 toks/s]   
Processed prompts:  31%|███       | 318/1024 [00:02<00:06, 105.07it/s, est. speed input: 132778.36 toks/s, output: 129.67 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:02<00:07, 91.74it/s, est. speed input: 120026.80 toks/s, output: 117.21 toks/s] 
Processed prompts:  36%|███▌      | 365/1024 [00:03<00:07, 89.00it/s, est. speed input: 116617.48 toks/s, output: 113.88 toks/s]
Processed prompts:  37%|███▋      | 380/1024 [00:03<00:07, 83.37it/s, est. speed input: 112537.07 toks/s, output: 109.90 toks/s]
Processed prompts:  38%|███▊      | 392/1024 [00:03<00:07, 84.72it/s, est. speed input: 111984.65 toks/s, output: 109.36 toks/s]
Processed prompts:  39%|███▉      | 404/1024 [00:03<00:08, 75.09it/s, est. speed input: 107755.32 toks/s, output: 105.23 toks/s]
Processed prompts:  40%|████      | 414/1024 [00:03<00:08, 75.73it/s, est. speed input: 106908.40 toks/s, output: 104.40 toks/s]
Processed prompts:  41%|████▏     | 423/1024 [00:04<00:08, 74.78it/s, est. speed input: 105825.54 toks/s, output: 103.35 toks/s]
Processed prompts:  42%|████▏     | 432/1024 [00:04<00:07, 74.08it/s, est. speed input: 104845.95 toks/s, output: 102.39 toks/s]
Processed prompts:  43%|████▎     | 440/1024 [00:04<00:08, 71.61it/s, est. speed input: 103676.16 toks/s, output: 101.25 toks/s]
Processed prompts:  44%|████▍     | 448/1024 [00:04<00:08, 69.61it/s, est. speed input: 102581.40 toks/s, output: 100.18 toks/s]
Processed prompts:  45%|████▍     | 456/1024 [00:04<00:08, 69.41it/s, est. speed input: 101767.80 toks/s, output: 99.38 toks/s] 
Processed prompts:  45%|████▌     | 464/1024 [00:04<00:08, 67.67it/s, est. speed input: 100766.62 toks/s, output: 98.40 toks/s]
Processed prompts:  46%|████▌     | 471/1024 [00:04<00:08, 64.00it/s, est. speed input: 99584.73 toks/s, output: 97.25 toks/s] 
Processed prompts:  47%|████▋     | 478/1024 [00:04<00:08, 61.49it/s, est. speed input: 98487.98 toks/s, output: 96.18 toks/s]
Processed prompts:  47%|████▋     | 485/1024 [00:05<00:09, 59.80it/s, est. speed input: 97462.44 toks/s, output: 95.18 toks/s]
Processed prompts:  48%|████▊     | 491/1024 [00:05<00:09, 56.12it/s, est. speed input: 96270.73 toks/s, output: 94.01 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:05<00:09, 55.80it/s, est. speed input: 95322.61 toks/s, output: 93.09 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:05<00:08, 58.13it/s, est. speed input: 94632.74 toks/s, output: 92.41 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:05<00:08, 59.48it/s, est. speed input: 93936.76 toks/s, output: 91.73 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:05<00:08, 60.76it/s, est. speed input: 93311.54 toks/s, output: 91.12 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:05<00:08, 61.48it/s, est. speed input: 92691.81 toks/s, output: 90.52 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:05<00:07, 61.79it/s, est. speed input: 92078.66 toks/s, output: 89.92 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:06<00:07, 62.23it/s, est. speed input: 91514.09 toks/s, output: 89.37 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:06<00:07, 62.39it/s, est. speed input: 90956.75 toks/s, output: 88.82 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:06<00:07, 62.73it/s, est. speed input: 90444.05 toks/s, output: 88.32 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:06<00:07, 62.88it/s, est. speed input: 89942.90 toks/s, output: 87.83 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:06<00:07, 62.93it/s, est. speed input: 89456.37 toks/s, output: 87.36 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:06<00:06, 62.80it/s, est. speed input: 88973.41 toks/s, output: 86.89 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:06<00:06, 62.77it/s, est. speed input: 88513.26 toks/s, output: 86.44 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:06<00:06, 62.78it/s, est. speed input: 88073.15 toks/s, output: 86.01 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:07<00:06, 62.78it/s, est. speed input: 87647.73 toks/s, output: 85.59 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:07<00:06, 62.93it/s, est. speed input: 87249.68 toks/s, output: 85.20 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:07<00:06, 63.02it/s, est. speed input: 86863.76 toks/s, output: 84.83 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:07<00:06, 63.11it/s, est. speed input: 86493.48 toks/s, output: 84.47 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:07<00:06, 62.89it/s, est. speed input: 86113.40 toks/s, output: 84.09 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:07<00:05, 63.16it/s, est. speed input: 85777.44 toks/s, output: 83.77 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:07<00:05, 63.04it/s, est. speed input: 85429.71 toks/s, output: 83.43 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:08<00:05, 62.99it/s, est. speed input: 85095.30 toks/s, output: 83.10 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:08<00:05, 62.91it/s, est. speed input: 84768.60 toks/s, output: 82.78 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:08<00:05, 62.96it/s, est. speed input: 84459.46 toks/s, output: 82.48 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:08<00:05, 63.02it/s, est. speed input: 84160.87 toks/s, output: 82.19 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:08<00:05, 62.95it/s, est. speed input: 83863.85 toks/s, output: 81.90 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:08<00:05, 62.95it/s, est. speed input: 83578.55 toks/s, output: 81.62 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:08<00:04, 62.89it/s, est. speed input: 83297.70 toks/s, output: 81.35 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:08<00:04, 62.81it/s, est. speed input: 83023.03 toks/s, output: 81.08 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:09<00:04, 62.97it/s, est. speed input: 82768.94 toks/s, output: 80.83 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:09<00:04, 62.93it/s, est. speed input: 82513.10 toks/s, output: 80.58 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:09<00:04, 62.91it/s, est. speed input: 82264.48 toks/s, output: 80.34 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:09<00:04, 62.85it/s, est. speed input: 82020.24 toks/s, output: 80.10 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:09<00:04, 62.97it/s, est. speed input: 81791.14 toks/s, output: 79.87 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:09<00:04, 62.90it/s, est. speed input: 81559.59 toks/s, output: 79.65 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:09<00:03, 62.99it/s, est. speed input: 81341.86 toks/s, output: 79.44 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:09<00:03, 62.91it/s, est. speed input: 81122.03 toks/s, output: 79.22 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:10<00:03, 63.15it/s, est. speed input: 80923.68 toks/s, output: 79.03 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:10<00:03, 63.06it/s, est. speed input: 80716.64 toks/s, output: 78.82 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:10<00:03, 63.01it/s, est. speed input: 80515.03 toks/s, output: 78.63 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:10<00:03, 62.99it/s, est. speed input: 80319.48 toks/s, output: 78.44 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:10<00:03, 62.93it/s, est. speed input: 80125.76 toks/s, output: 78.25 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:10<00:03, 62.99it/s, est. speed input: 79942.07 toks/s, output: 78.07 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:10<00:02, 62.85it/s, est. speed input: 79753.87 toks/s, output: 77.88 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:10<00:02, 63.03it/s, est. speed input: 79583.67 toks/s, output: 77.72 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:11<00:02, 62.98it/s, est. speed input: 79408.74 toks/s, output: 77.55 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:11<00:02, 62.90it/s, est. speed input: 79235.53 toks/s, output: 77.38 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:11<00:02, 62.95it/s, est. speed input: 79071.38 toks/s, output: 77.22 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:11<00:02, 62.81it/s, est. speed input: 78902.40 toks/s, output: 77.05 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:11<00:02, 62.71it/s, est. speed input: 78737.62 toks/s, output: 76.89 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:11<00:02, 62.89it/s, est. speed input: 78587.35 toks/s, output: 76.75 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:11<00:01, 62.82it/s, est. speed input: 78431.66 toks/s, output: 76.59 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:11<00:01, 62.85it/s, est. speed input: 78282.57 toks/s, output: 76.45 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:12<00:01, 63.06it/s, est. speed input: 78145.19 toks/s, output: 76.31 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:12<00:01, 63.09it/s, est. speed input: 78005.59 toks/s, output: 76.18 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:12<00:01, 64.86it/s, est. speed input: 77940.59 toks/s, output: 76.11 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:12<00:01, 64.32it/s, est. speed input: 77804.87 toks/s, output: 75.98 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:12<00:01, 63.83it/s, est. speed input: 77667.25 toks/s, output: 75.85 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:12<00:00, 63.61it/s, est. speed input: 77536.89 toks/s, output: 75.72 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:12<00:00, 63.43it/s, est. speed input: 77407.97 toks/s, output: 75.59 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:12<00:00, 63.13it/s, est. speed input: 77275.14 toks/s, output: 75.46 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:13<00:00, 64.95it/s, est. speed input: 77222.64 toks/s, output: 75.41 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:13<00:00, 64.22it/s, est. speed input: 77095.78 toks/s, output: 75.29 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:13<00:00, 63.78it/s, est. speed input: 76972.87 toks/s, output: 75.17 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:13<00:00, 63.51it/s, est. speed input: 76854.10 toks/s, output: 75.05 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:13<00:00, 65.43it/s, est. speed input: 76814.06 toks/s, output: 75.01 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:13<00:00, 65.43it/s, est. speed input: 77265.20 toks/s, output: 75.45 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:13<00:00, 75.45it/s, est. speed input: 77265.20 toks/s, output: 75.45 toks/s]
[rank0]:[W128 00:33:51.164116316 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.7s

测试结果:
  Requests/s:   62.89
  Tokens/s:     64460.38
  Total Reqs:   1024
  Elapsed:      16.28s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     64397.50

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:34:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2248146) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2248146) WARNING 01-28 00:34:30 [backends.py:609] Failed to read file <frozen os>
Throughput: 64.08 requests/s, 65681.27 total tokens/s, 64.08 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 00:34:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:34:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:34:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:34:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:34:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:34:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:34:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:34:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:34:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:34:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:34:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:34:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:34:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:34:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:34:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:34:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:34:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:34:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2248146) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2248146) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.11it/s]
(EngineCore_DP0 pid=2248146) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.11it/s]
(EngineCore_DP0 pid=2248146) 
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=2248146) [2026-01-28 00:34:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=2248146) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  9.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  9.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  9.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  9.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.60it/s]
(EngineCore_DP0 pid=2248146) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  9.11it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 10.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 10.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.98it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 37/2048 [00:00<00:05, 361.49it/s]
Adding requests:   4%|▎         | 75/2048 [00:00<00:05, 371.95it/s]
Adding requests:   6%|▌         | 113/2048 [00:00<00:05, 371.80it/s]
Adding requests:   7%|▋         | 151/2048 [00:00<00:05, 360.03it/s]
Adding requests:   9%|▉         | 190/2048 [00:00<00:05, 367.93it/s]
Adding requests:  11%|█         | 227/2048 [00:00<00:05, 364.16it/s]
Adding requests:  13%|█▎        | 264/2048 [00:00<00:04, 365.71it/s]
Adding requests:  15%|█▍        | 301/2048 [00:00<00:04, 359.11it/s]
Adding requests:  17%|█▋        | 338/2048 [00:00<00:04, 360.68it/s]
Adding requests:  18%|█▊        | 376/2048 [00:01<00:04, 363.88it/s]
Adding requests:  20%|██        | 413/2048 [00:01<00:04, 365.16it/s]
Adding requests:  22%|██▏       | 452/2048 [00:01<00:04, 370.35it/s]
Adding requests:  24%|██▍       | 491/2048 [00:01<00:04, 373.95it/s]
Adding requests:  26%|██▌       | 529/2048 [00:01<00:04, 372.81it/s]
Adding requests:  28%|██▊       | 572/2048 [00:01<00:03, 387.55it/s]
Adding requests:  30%|██▉       | 611/2048 [00:01<00:03, 385.28it/s]
Adding requests:  32%|███▏      | 654/2048 [00:01<00:03, 395.78it/s]
Adding requests:  34%|███▍      | 694/2048 [00:01<00:03, 392.30it/s]
Adding requests:  36%|███▌      | 734/2048 [00:01<00:03, 388.91it/s]
Adding requests:  38%|███▊      | 773/2048 [00:02<00:03, 368.61it/s]
Adding requests:  40%|███▉      | 811/2048 [00:02<00:03, 371.26it/s]
Adding requests:  41%|████▏     | 849/2048 [00:02<00:03, 372.62it/s]
Adding requests:  43%|████▎     | 890/2048 [00:02<00:03, 382.70it/s]
Adding requests:  45%|████▌     | 931/2048 [00:02<00:02, 389.47it/s]
Adding requests:  47%|████▋     | 972/2048 [00:02<00:02, 394.50it/s]
Adding requests:  49%|████▉     | 1012/2048 [00:02<00:02, 390.49it/s]
Adding requests:  51%|█████▏    | 1052/2048 [00:02<00:02, 385.78it/s]
Adding requests:  53%|█████▎    | 1091/2048 [00:02<00:02, 384.76it/s]
Adding requests:  55%|█████▌    | 1131/2048 [00:02<00:02, 385.69it/s]
Adding requests:  57%|█████▋    | 1172/2048 [00:03<00:02, 392.76it/s]
Adding requests:  59%|█████▉    | 1216/2048 [00:03<00:02, 403.91it/s]
Adding requests:  61%|██████▏   | 1257/2048 [00:03<00:01, 398.53it/s]
Adding requests:  63%|██████▎   | 1297/2048 [00:03<00:01, 391.04it/s]
Adding requests:  65%|██████▌   | 1339/2048 [00:03<00:01, 399.03it/s]
Adding requests:  67%|██████▋   | 1381/2048 [00:03<00:01, 403.71it/s]
Adding requests:  70%|██████▉   | 1424/2048 [00:03<00:01, 409.76it/s]
Adding requests:  72%|███████▏  | 1467/2048 [00:03<00:01, 414.61it/s]
Adding requests:  74%|███████▎  | 1509/2048 [00:03<00:01, 404.92it/s]
Adding requests:  76%|███████▌  | 1551/2048 [00:04<00:01, 407.58it/s]
Adding requests:  78%|███████▊  | 1592/2048 [00:04<00:01, 404.85it/s]
Adding requests:  80%|███████▉  | 1633/2048 [00:04<00:01, 398.04it/s]
Adding requests:  82%|████████▏ | 1673/2048 [00:04<00:00, 395.78it/s]
Adding requests:  84%|████████▎ | 1714/2048 [00:04<00:00, 397.82it/s]
Adding requests:  86%|████████▌ | 1756/2048 [00:04<00:00, 402.05it/s]
Adding requests:  88%|████████▊ | 1797/2048 [00:04<00:00, 400.61it/s]
Adding requests:  90%|████████▉ | 1838/2048 [00:04<00:00, 399.56it/s]
Adding requests:  92%|█████████▏| 1878/2048 [00:04<00:00, 388.95it/s]
Adding requests:  94%|█████████▎| 1917/2048 [00:04<00:00, 381.37it/s]
Adding requests:  96%|█████████▌| 1958/2048 [00:05<00:00, 386.99it/s]
Adding requests:  98%|█████████▊| 1997/2048 [00:05<00:00, 380.89it/s]
Adding requests:  99%|█████████▉| 2036/2048 [00:05<00:00, 380.10it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 385.93it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:00<00:01, 1376.41it/s, est. speed input: 1409571.09 toks/s, output: 1376.44 toks/s]
Processed prompts:  23%|██▎       | 476/2048 [00:02<00:09, 174.22it/s, est. speed input: 219189.32 toks/s, output: 214.05 toks/s]   
Processed prompts:  26%|██▌       | 537/2048 [00:03<00:11, 129.27it/s, est. speed input: 170825.63 toks/s, output: 166.82 toks/s]
Processed prompts:  28%|██▊       | 573/2048 [00:03<00:12, 116.42it/s, est. speed input: 157665.74 toks/s, output: 153.97 toks/s]
Processed prompts:  29%|██▉       | 598/2048 [00:04<00:14, 100.58it/s, est. speed input: 145166.50 toks/s, output: 141.76 toks/s]
Processed prompts:  30%|███       | 616/2048 [00:04<00:14, 96.43it/s, est. speed input: 141196.80 toks/s, output: 137.89 toks/s] 
Processed prompts:  31%|███       | 630/2048 [00:04<00:15, 89.40it/s, est. speed input: 136745.91 toks/s, output: 133.54 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:04<00:17, 81.26it/s, est. speed input: 132372.09 toks/s, output: 129.27 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:05<00:17, 77.37it/s, est. speed input: 129142.06 toks/s, output: 126.11 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:05<00:18, 74.25it/s, est. speed input: 126262.48 toks/s, output: 123.30 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:05<00:18, 71.56it/s, est. speed input: 123585.73 toks/s, output: 120.69 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:05<00:19, 69.62it/s, est. speed input: 121174.54 toks/s, output: 118.33 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:06<00:19, 68.10it/s, est. speed input: 118946.93 toks/s, output: 116.16 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:06<00:19, 67.05it/s, est. speed input: 116908.38 toks/s, output: 114.17 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:06<00:19, 66.21it/s, est. speed input: 115004.31 toks/s, output: 112.31 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:06<00:19, 65.63it/s, est. speed input: 113241.12 toks/s, output: 110.59 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:07<00:19, 65.18it/s, est. speed input: 111594.89 toks/s, output: 108.98 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:07<00:19, 64.94it/s, est. speed input: 110072.25 toks/s, output: 107.49 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:07<00:19, 64.71it/s, est. speed input: 108637.36 toks/s, output: 106.09 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:07<00:18, 64.51it/s, est. speed input: 107287.03 toks/s, output: 104.77 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:08<00:18, 64.43it/s, est. speed input: 106027.47 toks/s, output: 103.54 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:08<00:18, 64.37it/s, est. speed input: 104841.66 toks/s, output: 102.38 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:08<00:18, 64.26it/s, est. speed input: 103712.70 toks/s, output: 101.28 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:08<00:17, 64.16it/s, est. speed input: 102643.78 toks/s, output: 100.24 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:09<00:17, 64.12it/s, est. speed input: 101637.98 toks/s, output: 99.26 toks/s] 
Processed prompts:  45%|████▌     | 930/2048 [00:09<00:17, 65.12it/s, est. speed input: 100825.29 toks/s, output: 98.46 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:09<00:17, 64.77it/s, est. speed input: 99914.10 toks/s, output: 97.57 toks/s] 
Processed prompts:  47%|████▋     | 962/2048 [00:09<00:16, 64.52it/s, est. speed input: 99047.83 toks/s, output: 96.73 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:10<00:16, 65.29it/s, est. speed input: 98339.05 toks/s, output: 96.03 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:10<00:16, 65.01it/s, est. speed input: 97566.42 toks/s, output: 95.28 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:10<00:16, 64.64it/s, est. speed input: 96809.14 toks/s, output: 94.54 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:10<00:15, 64.47it/s, est. speed input: 96096.50 toks/s, output: 93.84 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:11<00:15, 64.32it/s, est. speed input: 95412.81 toks/s, output: 93.18 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:11<00:15, 64.17it/s, est. speed input: 94753.49 toks/s, output: 92.53 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:11<00:15, 64.21it/s, est. speed input: 94137.49 toks/s, output: 91.93 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:11<00:14, 64.12it/s, est. speed input: 93535.34 toks/s, output: 91.34 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:12<00:14, 63.97it/s, est. speed input: 92950.11 toks/s, output: 90.77 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:12<00:14, 63.92it/s, est. speed input: 92393.16 toks/s, output: 90.23 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:12<00:14, 64.00it/s, est. speed input: 91869.09 toks/s, output: 89.72 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:12<00:13, 65.04it/s, est. speed input: 91454.42 toks/s, output: 89.31 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:13<00:13, 64.66it/s, est. speed input: 90956.55 toks/s, output: 88.82 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:13<00:13, 64.43it/s, est. speed input: 90480.57 toks/s, output: 88.36 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:13<00:13, 64.31it/s, est. speed input: 90025.70 toks/s, output: 87.92 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:13<00:12, 64.27it/s, est. speed input: 89589.81 toks/s, output: 87.49 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:14<00:12, 64.05it/s, est. speed input: 89154.62 toks/s, output: 87.06 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:14<00:12, 64.01it/s, est. speed input: 88743.13 toks/s, output: 86.66 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:14<00:12, 64.93it/s, est. speed input: 88418.91 toks/s, output: 86.35 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:14<00:11, 64.74it/s, est. speed input: 88042.14 toks/s, output: 85.98 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:15<00:11, 64.36it/s, est. speed input: 87659.17 toks/s, output: 85.60 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:15<00:11, 64.20it/s, est. speed input: 87296.87 toks/s, output: 85.25 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:15<00:11, 64.17it/s, est. speed input: 86951.29 toks/s, output: 84.91 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:15<00:10, 64.07it/s, est. speed input: 86611.35 toks/s, output: 84.58 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:16<00:10, 64.00it/s, est. speed input: 86282.55 toks/s, output: 84.26 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:16<00:10, 63.88it/s, est. speed input: 85958.10 toks/s, output: 83.94 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:16<00:10, 63.98it/s, est. speed input: 85655.94 toks/s, output: 83.65 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:16<00:09, 64.03it/s, est. speed input: 85361.42 toks/s, output: 83.36 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:17<00:09, 63.92it/s, est. speed input: 85066.14 toks/s, output: 83.07 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:17<00:09, 63.89it/s, est. speed input: 84782.64 toks/s, output: 82.80 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:17<00:09, 63.90it/s, est. speed input: 84508.59 toks/s, output: 82.53 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:17<00:08, 63.86it/s, est. speed input: 84239.57 toks/s, output: 82.27 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:18<00:08, 63.88it/s, est. speed input: 83980.70 toks/s, output: 82.01 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:18<00:08, 63.76it/s, est. speed input: 83721.49 toks/s, output: 81.76 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:18<00:08, 63.90it/s, est. speed input: 83482.01 toks/s, output: 81.53 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:18<00:07, 63.88it/s, est. speed input: 83241.88 toks/s, output: 81.29 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:19<00:07, 63.86it/s, est. speed input: 83007.56 toks/s, output: 81.06 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:19<00:07, 63.84it/s, est. speed input: 82779.19 toks/s, output: 80.84 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:19<00:07, 64.81it/s, est. speed input: 82610.28 toks/s, output: 80.67 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:19<00:06, 64.49it/s, est. speed input: 82391.83 toks/s, output: 80.46 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:20<00:06, 64.24it/s, est. speed input: 82177.16 toks/s, output: 80.25 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:20<00:06, 63.98it/s, est. speed input: 81963.68 toks/s, output: 80.04 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:20<00:06, 63.97it/s, est. speed input: 81763.98 toks/s, output: 79.85 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:20<00:05, 63.87it/s, est. speed input: 81564.35 toks/s, output: 79.65 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:21<00:05, 63.79it/s, est. speed input: 81369.23 toks/s, output: 79.46 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:21<00:05, 63.80it/s, est. speed input: 81181.90 toks/s, output: 79.28 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:21<00:05, 63.83it/s, est. speed input: 80999.87 toks/s, output: 79.10 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:21<00:04, 63.90it/s, est. speed input: 80824.41 toks/s, output: 78.93 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:22<00:04, 63.81it/s, est. speed input: 80646.25 toks/s, output: 78.76 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:22<00:04, 63.77it/s, est. speed input: 80473.00 toks/s, output: 78.59 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:22<00:04, 63.83it/s, est. speed input: 80308.26 toks/s, output: 78.43 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:22<00:03, 63.87it/s, est. speed input: 80146.60 toks/s, output: 78.27 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:23<00:03, 63.85it/s, est. speed input: 79985.97 toks/s, output: 78.11 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:23<00:03, 63.79it/s, est. speed input: 79827.18 toks/s, output: 77.96 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:23<00:03, 63.73it/s, est. speed input: 79670.98 toks/s, output: 77.80 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:23<00:02, 63.78it/s, est. speed input: 79521.64 toks/s, output: 77.66 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:24<00:02, 64.74it/s, est. speed input: 79414.97 toks/s, output: 77.55 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:24<00:02, 64.32it/s, est. speed input: 79264.53 toks/s, output: 77.41 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:24<00:02, 64.13it/s, est. speed input: 79121.63 toks/s, output: 77.27 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:24<00:01, 63.99it/s, est. speed input: 78981.20 toks/s, output: 77.13 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:25<00:01, 63.90it/s, est. speed input: 78843.94 toks/s, output: 77.00 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:25<00:01, 64.74it/s, est. speed input: 78745.32 toks/s, output: 76.90 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:25<00:01, 64.38it/s, est. speed input: 78611.41 toks/s, output: 76.77 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:25<00:00, 64.20it/s, est. speed input: 78482.42 toks/s, output: 76.64 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:26<00:00, 64.08it/s, est. speed input: 78356.62 toks/s, output: 76.52 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:26<00:00, 63.86it/s, est. speed input: 78227.55 toks/s, output: 76.39 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:26<00:00, 65.00it/s, est. speed input: 78149.77 toks/s, output: 76.32 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:26<00:00, 65.00it/s, est. speed input: 78686.65 toks/s, output: 76.84 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:26<00:00, 76.84it/s, est. speed input: 78686.65 toks/s, output: 76.84 toks/s]
[rank0]:[W128 00:35:23.009408265 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 91.9s

测试结果:
  Requests/s:   64.08
  Tokens/s:     65681.27
  Total Reqs:   2048
  Elapsed:      31.96s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     65617.19

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:35:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2249789) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2249789) WARNING 01-28 00:36:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 64.55 requests/s, 66162.15 total tokens/s, 64.55 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 00:35:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:35:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:35:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:35:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:35:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:35:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:35:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:35:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:35:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:36:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:36:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:36:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:36:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:36:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:36:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:36:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:36:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:36:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:36:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:36:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:36:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:36:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:36:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2249789) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2249789) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.04it/s]
(EngineCore_DP0 pid=2249789) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.04it/s]
(EngineCore_DP0 pid=2249789) 
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=2249789) [2026-01-28 00:36:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=2249789) [rank0]:W0128 00:36:24.348000 2249789 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2249789) [rank0]:W0128 00:36:24.472000 2249789 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2249789) [rank0]:W0128 00:36:25.913000 2249789 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2249789) [rank0]:W0128 00:36:26.118000 2249789 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=2249789) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  9.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  9.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  9.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.94it/s]
(EngineCore_DP0 pid=2249789) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.68it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.88it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  9.22it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  9.20it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.30it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.10it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 34/4096 [00:00<00:12, 336.69it/s]
Adding requests:   2%|▏         | 69/4096 [00:00<00:11, 343.35it/s]
Adding requests:   3%|▎         | 105/4096 [00:00<00:11, 349.04it/s]
Adding requests:   3%|▎         | 143/4096 [00:00<00:10, 360.50it/s]
Adding requests:   4%|▍         | 182/4096 [00:00<00:10, 367.74it/s]
Adding requests:   5%|▌         | 220/4096 [00:00<00:10, 370.43it/s]
Adding requests:   6%|▋         | 258/4096 [00:00<00:10, 364.31it/s]
Adding requests:   7%|▋         | 296/4096 [00:00<00:10, 367.60it/s]
Adding requests:   8%|▊         | 333/4096 [00:00<00:10, 360.37it/s]
Adding requests:   9%|▉         | 370/4096 [00:01<00:10, 360.61it/s]
Adding requests:  10%|▉         | 407/4096 [00:01<00:10, 360.10it/s]
Adding requests:  11%|█         | 444/4096 [00:01<00:10, 357.89it/s]
Adding requests:  12%|█▏        | 481/4096 [00:01<00:10, 361.07it/s]
Adding requests:  13%|█▎        | 518/4096 [00:01<00:10, 355.80it/s]
Adding requests:  14%|█▎        | 554/4096 [00:01<00:10, 346.53it/s]
Adding requests:  14%|█▍        | 591/4096 [00:01<00:09, 350.74it/s]
Adding requests:  15%|█▌        | 627/4096 [00:01<00:09, 353.34it/s]
Adding requests:  16%|█▌        | 663/4096 [00:01<00:09, 355.02it/s]
Adding requests:  17%|█▋        | 700/4096 [00:01<00:09, 358.30it/s]
Adding requests:  18%|█▊        | 738/4096 [00:02<00:09, 362.25it/s]
Adding requests:  19%|█▉        | 775/4096 [00:02<00:09, 358.02it/s]
Adding requests:  20%|█▉        | 811/4096 [00:02<00:09, 349.85it/s]
Adding requests:  21%|██        | 847/4096 [00:02<00:09, 349.68it/s]
Adding requests:  22%|██▏       | 885/4096 [00:02<00:08, 357.26it/s]
Adding requests:  23%|██▎       | 923/4096 [00:02<00:08, 362.50it/s]
Adding requests:  23%|██▎       | 961/4096 [00:02<00:08, 365.59it/s]
Adding requests:  24%|██▍       | 998/4096 [00:02<00:08, 364.23it/s]
Adding requests:  25%|██▌       | 1038/4096 [00:02<00:08, 372.51it/s]
Adding requests:  26%|██▋       | 1076/4096 [00:02<00:08, 363.14it/s]
Adding requests:  27%|██▋       | 1114/4096 [00:03<00:08, 363.46it/s]
Adding requests:  28%|██▊       | 1151/4096 [00:03<00:08, 361.16it/s]
Adding requests:  29%|██▉       | 1189/4096 [00:03<00:07, 366.10it/s]
Adding requests:  30%|██▉       | 1226/4096 [00:03<00:07, 366.75it/s]
Adding requests:  31%|███       | 1263/4096 [00:03<00:07, 363.02it/s]
Adding requests:  32%|███▏      | 1300/4096 [00:03<00:07, 363.78it/s]
Adding requests:  33%|███▎      | 1338/4096 [00:03<00:07, 366.10it/s]
Adding requests:  34%|███▎      | 1376/4096 [00:03<00:07, 368.57it/s]
Adding requests:  35%|███▍      | 1414/4096 [00:03<00:07, 371.90it/s]
Adding requests:  35%|███▌      | 1452/4096 [00:04<00:07, 366.45it/s]
Adding requests:  36%|███▋      | 1490/4096 [00:04<00:07, 369.32it/s]
Adding requests:  37%|███▋      | 1528/4096 [00:04<00:06, 372.12it/s]
Adding requests:  38%|███▊      | 1566/4096 [00:04<00:06, 371.60it/s]
Adding requests:  39%|███▉      | 1605/4096 [00:04<00:06, 375.88it/s]
Adding requests:  40%|████      | 1643/4096 [00:04<00:06, 375.13it/s]
Adding requests:  41%|████      | 1681/4096 [00:04<00:06, 369.42it/s]
Adding requests:  42%|████▏     | 1719/4096 [00:04<00:06, 371.66it/s]
Adding requests:  43%|████▎     | 1758/4096 [00:04<00:06, 376.93it/s]
Adding requests:  44%|████▍     | 1796/4096 [00:04<00:06, 375.91it/s]
Adding requests:  45%|████▍     | 1834/4096 [00:05<00:06, 373.51it/s]
Adding requests:  46%|████▌     | 1872/4096 [00:05<00:06, 369.17it/s]
Adding requests:  47%|████▋     | 1909/4096 [00:05<00:05, 367.28it/s]
Adding requests:  48%|████▊     | 1947/4096 [00:05<00:05, 369.74it/s]
Adding requests:  48%|████▊     | 1985/4096 [00:05<00:05, 370.18it/s]
Adding requests:  49%|████▉     | 2023/4096 [00:05<00:05, 371.53it/s]
Adding requests:  50%|█████     | 2061/4096 [00:05<00:05, 372.47it/s]
Adding requests:  51%|█████     | 2099/4096 [00:05<00:05, 372.14it/s]
Adding requests:  52%|█████▏    | 2137/4096 [00:05<00:05, 365.10it/s]
Adding requests:  53%|█████▎    | 2174/4096 [00:05<00:05, 362.64it/s]
Adding requests:  54%|█████▍    | 2211/4096 [00:06<00:05, 363.83it/s]
Adding requests:  55%|█████▍    | 2251/4096 [00:06<00:04, 372.50it/s]
Adding requests:  56%|█████▌    | 2289/4096 [00:06<00:05, 359.12it/s]
Adding requests:  57%|█████▋    | 2326/4096 [00:06<00:04, 360.76it/s]
Adding requests:  58%|█████▊    | 2363/4096 [00:06<00:04, 362.10it/s]
Adding requests:  59%|█████▊    | 2401/4096 [00:06<00:04, 365.65it/s]
Adding requests:  60%|█████▉    | 2438/4096 [00:06<00:04, 365.28it/s]
Adding requests:  60%|██████    | 2475/4096 [00:06<00:04, 363.78it/s]
Adding requests:  61%|██████▏   | 2512/4096 [00:06<00:04, 361.42it/s]
Adding requests:  62%|██████▏   | 2549/4096 [00:07<00:04, 362.98it/s]
Adding requests:  63%|██████▎   | 2586/4096 [00:07<00:04, 364.98it/s]
Adding requests:  64%|██████▍   | 2623/4096 [00:07<00:04, 364.67it/s]
Adding requests:  65%|██████▍   | 2661/4096 [00:07<00:03, 366.53it/s]
Adding requests:  66%|██████▌   | 2698/4096 [00:07<00:03, 362.49it/s]
Adding requests:  67%|██████▋   | 2735/4096 [00:07<00:03, 360.67it/s]
Adding requests:  68%|██████▊   | 2773/4096 [00:07<00:03, 363.62it/s]
Adding requests:  69%|██████▊   | 2810/4096 [00:07<00:03, 360.29it/s]
Adding requests:  70%|██████▉   | 2848/4096 [00:07<00:03, 363.49it/s]
Adding requests:  70%|███████   | 2885/4096 [00:07<00:03, 364.72it/s]
Adding requests:  71%|███████▏  | 2922/4096 [00:08<00:03, 362.19it/s]
Adding requests:  72%|███████▏  | 2959/4096 [00:08<00:03, 360.93it/s]
Adding requests:  73%|███████▎  | 2996/4096 [00:08<00:03, 360.07it/s]
Adding requests:  74%|███████▍  | 3033/4096 [00:08<00:02, 358.90it/s]
Adding requests:  75%|███████▍  | 3069/4096 [00:08<00:02, 357.87it/s]
Adding requests:  76%|███████▌  | 3106/4096 [00:08<00:02, 358.93it/s]
Adding requests:  77%|███████▋  | 3143/4096 [00:08<00:02, 358.17it/s]
Adding requests:  78%|███████▊  | 3182/4096 [00:08<00:02, 366.48it/s]
Adding requests:  79%|███████▊  | 3221/4096 [00:08<00:02, 373.16it/s]
Adding requests:  80%|███████▉  | 3259/4096 [00:08<00:02, 374.71it/s]
Adding requests:  80%|████████  | 3297/4096 [00:09<00:02, 371.77it/s]
Adding requests:  81%|████████▏ | 3336/4096 [00:09<00:02, 376.38it/s]
Adding requests:  82%|████████▏ | 3374/4096 [00:09<00:01, 370.22it/s]
Adding requests:  83%|████████▎ | 3412/4096 [00:09<00:01, 367.15it/s]
Adding requests:  84%|████████▍ | 3450/4096 [00:09<00:01, 368.01it/s]
Adding requests:  85%|████████▌ | 3487/4096 [00:09<00:01, 366.69it/s]
Adding requests:  86%|████████▌ | 3524/4096 [00:09<00:01, 364.55it/s]
Adding requests:  87%|████████▋ | 3562/4096 [00:09<00:01, 366.00it/s]
Adding requests:  88%|████████▊ | 3599/4096 [00:09<00:01, 366.55it/s]
Adding requests:  89%|████████▉ | 3636/4096 [00:09<00:01, 352.21it/s]
Adding requests:  90%|████████▉ | 3672/4096 [00:10<00:01, 353.41it/s]
Adding requests:  91%|█████████ | 3709/4096 [00:10<00:01, 357.92it/s]
Adding requests:  91%|█████████▏| 3746/4096 [00:10<00:00, 359.24it/s]
Adding requests:  92%|█████████▏| 3785/4096 [00:10<00:00, 365.70it/s]
Adding requests:  93%|█████████▎| 3822/4096 [00:10<00:00, 366.60it/s]
Adding requests:  94%|█████████▍| 3860/4096 [00:10<00:00, 369.42it/s]
Adding requests:  95%|█████████▌| 3897/4096 [00:10<00:00, 364.67it/s]
Adding requests:  96%|█████████▌| 3935/4096 [00:10<00:00, 369.16it/s]
Adding requests:  97%|█████████▋| 3976/4096 [00:10<00:00, 380.79it/s]
Adding requests:  98%|█████████▊| 4015/4096 [00:11<00:00, 382.93it/s]
Adding requests:  99%|█████████▉| 4054/4096 [00:11<00:00, 377.83it/s]
Adding requests: 100%|█████████▉| 4093/4096 [00:11<00:00, 379.50it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 365.10it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 699/4096 [00:00<00:00, 6589.11it/s, est. speed input: 6748690.90 toks/s, output: 6589.47 toks/s]
Processed prompts:  33%|███▎      | 1358/4096 [00:09<00:23, 115.58it/s, est. speed input: 139517.21 toks/s, output: 136.25 toks/s]  
Processed prompts:  40%|███▉      | 1634/4096 [00:14<00:26, 94.62it/s, est. speed input: 115996.65 toks/s, output: 113.28 toks/s] 
Processed prompts:  44%|████▎     | 1787/4096 [00:16<00:26, 86.80it/s, est. speed input: 108231.21 toks/s, output: 105.69 toks/s]
Processed prompts:  46%|████▌     | 1882/4096 [00:17<00:25, 87.98it/s, est. speed input: 107733.82 toks/s, output: 105.21 toks/s]
Processed prompts:  48%|████▊     | 1947/4096 [00:19<00:27, 79.01it/s, est. speed input: 102919.17 toks/s, output: 100.51 toks/s]
Processed prompts:  49%|████▊     | 1991/4096 [00:19<00:26, 79.84it/s, est. speed input: 102608.22 toks/s, output: 100.20 toks/s]
Processed prompts:  49%|████▉     | 2024/4096 [00:20<00:26, 78.30it/s, est. speed input: 101755.29 toks/s, output: 99.37 toks/s] 
Processed prompts:  50%|█████     | 2049/4096 [00:20<00:27, 74.42it/s, est. speed input: 100556.43 toks/s, output: 98.20 toks/s]
Processed prompts:  51%|█████     | 2075/4096 [00:21<00:28, 70.77it/s, est. speed input: 99462.27 toks/s, output: 97.13 toks/s] 
Processed prompts:  51%|█████▏    | 2107/4096 [00:21<00:28, 69.53it/s, est. speed input: 98696.12 toks/s, output: 96.38 toks/s]
Processed prompts:  52%|█████▏    | 2139/4096 [00:22<00:28, 68.41it/s, est. speed input: 97965.61 toks/s, output: 95.67 toks/s]
Processed prompts:  53%|█████▎    | 2171/4096 [00:22<00:28, 67.43it/s, est. speed input: 97263.50 toks/s, output: 94.98 toks/s]
Processed prompts:  54%|█████▍    | 2203/4096 [00:23<00:28, 66.63it/s, est. speed input: 96591.61 toks/s, output: 94.33 toks/s]
Processed prompts:  55%|█████▍    | 2235/4096 [00:23<00:27, 66.95it/s, est. speed input: 96056.33 toks/s, output: 93.80 toks/s]
Processed prompts:  55%|█████▌    | 2267/4096 [00:24<00:27, 66.18it/s, est. speed input: 95434.79 toks/s, output: 93.20 toks/s]
Processed prompts:  56%|█████▌    | 2299/4096 [00:24<00:27, 66.10it/s, est. speed input: 94887.72 toks/s, output: 92.66 toks/s]
Processed prompts:  57%|█████▋    | 2331/4096 [00:25<00:26, 66.03it/s, est. speed input: 94359.51 toks/s, output: 92.15 toks/s]
Processed prompts:  58%|█████▊    | 2363/4096 [00:25<00:26, 66.57it/s, est. speed input: 93906.26 toks/s, output: 91.71 toks/s]
Processed prompts:  58%|█████▊    | 2395/4096 [00:26<00:25, 65.86it/s, est. speed input: 93372.04 toks/s, output: 91.18 toks/s]
Processed prompts:  59%|█████▉    | 2427/4096 [00:26<00:25, 65.46it/s, est. speed input: 92865.83 toks/s, output: 90.69 toks/s]
Processed prompts:  60%|██████    | 2459/4096 [00:27<00:25, 65.07it/s, est. speed input: 92368.72 toks/s, output: 90.20 toks/s]
Processed prompts:  61%|██████    | 2491/4096 [00:27<00:24, 65.29it/s, est. speed input: 91931.43 toks/s, output: 89.78 toks/s]
Processed prompts:  62%|██████▏   | 2523/4096 [00:28<00:24, 64.94it/s, est. speed input: 91467.76 toks/s, output: 89.32 toks/s]
Processed prompts:  62%|██████▏   | 2555/4096 [00:28<00:23, 64.73it/s, est. speed input: 91022.25 toks/s, output: 88.89 toks/s]
Processed prompts:  63%|██████▎   | 2587/4096 [00:29<00:23, 65.02it/s, est. speed input: 90627.11 toks/s, output: 88.50 toks/s]
Processed prompts:  64%|██████▍   | 2619/4096 [00:29<00:22, 64.76it/s, est. speed input: 90208.44 toks/s, output: 88.09 toks/s]
Processed prompts:  65%|██████▍   | 2651/4096 [00:30<00:22, 64.51it/s, est. speed input: 89798.71 toks/s, output: 87.69 toks/s]
Processed prompts:  66%|██████▌   | 2683/4096 [00:30<00:21, 64.37it/s, est. speed input: 89405.31 toks/s, output: 87.31 toks/s]
Processed prompts:  66%|██████▋   | 2715/4096 [00:31<00:21, 64.34it/s, est. speed input: 89029.33 toks/s, output: 86.94 toks/s]
Processed prompts:  67%|██████▋   | 2747/4096 [00:31<00:20, 64.34it/s, est. speed input: 88666.03 toks/s, output: 86.59 toks/s]
Processed prompts:  68%|██████▊   | 2779/4096 [00:32<00:20, 64.26it/s, est. speed input: 88308.88 toks/s, output: 86.24 toks/s]
Processed prompts:  69%|██████▊   | 2811/4096 [00:32<00:20, 64.23it/s, est. speed input: 87964.03 toks/s, output: 85.90 toks/s]
Processed prompts:  69%|██████▉   | 2843/4096 [00:33<00:19, 64.22it/s, est. speed input: 87630.94 toks/s, output: 85.58 toks/s]
Processed prompts:  70%|███████   | 2875/4096 [00:33<00:19, 64.20it/s, est. speed input: 87306.33 toks/s, output: 85.26 toks/s]
Processed prompts:  71%|███████   | 2907/4096 [00:34<00:18, 64.16it/s, est. speed input: 86989.60 toks/s, output: 84.95 toks/s]
Processed prompts:  72%|███████▏  | 2939/4096 [00:34<00:18, 64.11it/s, est. speed input: 86680.43 toks/s, output: 84.65 toks/s]
Processed prompts:  73%|███████▎  | 2971/4096 [00:35<00:17, 64.13it/s, est. speed input: 86383.70 toks/s, output: 84.36 toks/s]
Processed prompts:  73%|███████▎  | 3003/4096 [00:35<00:17, 64.15it/s, est. speed input: 86095.70 toks/s, output: 84.08 toks/s]
Processed prompts:  74%|███████▍  | 3035/4096 [00:36<00:16, 64.15it/s, est. speed input: 85814.21 toks/s, output: 83.80 toks/s]
Processed prompts:  75%|███████▍  | 3067/4096 [00:36<00:16, 64.09it/s, est. speed input: 85537.65 toks/s, output: 83.53 toks/s]
Processed prompts:  76%|███████▌  | 3099/4096 [00:37<00:15, 64.09it/s, est. speed input: 85270.34 toks/s, output: 83.27 toks/s]
Processed prompts:  76%|███████▋  | 3131/4096 [00:37<00:14, 64.57it/s, est. speed input: 85038.19 toks/s, output: 83.05 toks/s]
Processed prompts:  77%|███████▋  | 3163/4096 [00:38<00:14, 64.40it/s, est. speed input: 84782.80 toks/s, output: 82.80 toks/s]
Processed prompts:  78%|███████▊  | 3195/4096 [00:38<00:14, 64.30it/s, est. speed input: 84535.70 toks/s, output: 82.55 toks/s]
Processed prompts:  79%|███████▉  | 3227/4096 [00:39<00:13, 64.24it/s, est. speed input: 84294.96 toks/s, output: 82.32 toks/s]
Processed prompts:  80%|███████▉  | 3259/4096 [00:39<00:13, 64.14it/s, est. speed input: 84057.03 toks/s, output: 82.09 toks/s]
Processed prompts:  80%|████████  | 3291/4096 [00:40<00:12, 64.08it/s, est. speed input: 83825.57 toks/s, output: 81.86 toks/s]
Processed prompts:  81%|████████  | 3323/4096 [00:40<00:12, 64.04it/s, est. speed input: 83600.32 toks/s, output: 81.64 toks/s]
Processed prompts:  82%|████████▏ | 3355/4096 [00:41<00:11, 64.02it/s, est. speed input: 83380.42 toks/s, output: 81.43 toks/s]
Processed prompts:  83%|████████▎ | 3387/4096 [00:41<00:11, 64.03it/s, est. speed input: 83167.49 toks/s, output: 81.22 toks/s]
Processed prompts:  83%|████████▎ | 3419/4096 [00:42<00:10, 64.01it/s, est. speed input: 82958.14 toks/s, output: 81.01 toks/s]
Processed prompts:  84%|████████▍ | 3451/4096 [00:42<00:10, 64.03it/s, est. speed input: 82755.33 toks/s, output: 80.82 toks/s]
Processed prompts:  85%|████████▌ | 3483/4096 [00:43<00:09, 65.09it/s, est. speed input: 82608.38 toks/s, output: 80.67 toks/s]
Processed prompts:  86%|████████▌ | 3515/4096 [00:43<00:08, 64.77it/s, est. speed input: 82413.39 toks/s, output: 80.48 toks/s]
Processed prompts:  87%|████████▋ | 3547/4096 [00:44<00:08, 64.49it/s, est. speed input: 82219.92 toks/s, output: 80.29 toks/s]
Processed prompts:  87%|████████▋ | 3579/4096 [00:44<00:08, 64.30it/s, est. speed input: 82031.45 toks/s, output: 80.11 toks/s]
Processed prompts:  88%|████████▊ | 3611/4096 [00:45<00:07, 64.25it/s, est. speed input: 81850.76 toks/s, output: 79.93 toks/s]
Processed prompts:  89%|████████▉ | 3643/4096 [00:45<00:07, 64.25it/s, est. speed input: 81675.48 toks/s, output: 79.76 toks/s]
Processed prompts:  90%|████████▉ | 3675/4096 [00:46<00:06, 64.17it/s, est. speed input: 81500.68 toks/s, output: 79.59 toks/s]
Processed prompts:  91%|█████████ | 3707/4096 [00:46<00:06, 64.10it/s, est. speed input: 81328.82 toks/s, output: 79.42 toks/s]
Processed prompts:  91%|█████████▏| 3739/4096 [00:47<00:05, 64.61it/s, est. speed input: 81185.34 toks/s, output: 79.28 toks/s]
Processed prompts:  92%|█████████▏| 3771/4096 [00:47<00:05, 64.40it/s, est. speed input: 81020.21 toks/s, output: 79.12 toks/s]
Processed prompts:  93%|█████████▎| 3803/4096 [00:48<00:04, 64.33it/s, est. speed input: 80861.38 toks/s, output: 78.97 toks/s]
Processed prompts:  94%|█████████▎| 3835/4096 [00:48<00:04, 64.71it/s, est. speed input: 80724.20 toks/s, output: 78.83 toks/s]
Processed prompts:  94%|█████████▍| 3867/4096 [00:49<00:03, 64.48it/s, est. speed input: 80569.36 toks/s, output: 78.68 toks/s]
Processed prompts:  95%|█████████▌| 3899/4096 [00:49<00:03, 64.29it/s, est. speed input: 80416.09 toks/s, output: 78.53 toks/s]
Processed prompts:  96%|█████████▌| 3931/4096 [00:50<00:02, 64.19it/s, est. speed input: 80267.27 toks/s, output: 78.39 toks/s]
Processed prompts:  97%|█████████▋| 3963/4096 [00:50<00:02, 64.12it/s, est. speed input: 80121.03 toks/s, output: 78.24 toks/s]
Processed prompts:  98%|█████████▊| 3995/4096 [00:51<00:01, 64.11it/s, est. speed input: 79979.81 toks/s, output: 78.11 toks/s]
Processed prompts:  98%|█████████▊| 4027/4096 [00:51<00:01, 64.57it/s, est. speed input: 79859.39 toks/s, output: 77.99 toks/s]
Processed prompts:  99%|█████████▉| 4059/4096 [00:52<00:00, 64.49it/s, est. speed input: 79725.89 toks/s, output: 77.86 toks/s]
Processed prompts: 100%|█████████▉| 4091/4096 [00:52<00:00, 84.74it/s, est. speed input: 80198.90 toks/s, output: 78.32 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:52<00:00, 84.74it/s, est. speed input: 80296.57 toks/s, output: 78.41 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:52<00:00, 78.41it/s, est. speed input: 80296.57 toks/s, output: 78.41 toks/s]
[rank0]:[W128 00:37:41.830900004 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 138.0s

测试结果:
  Requests/s:   64.55
  Tokens/s:     66162.15
  Total Reqs:   4096
  Elapsed:      63.46s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     66097.60


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,19.6388,10074.6819,6.5177
1024,1024,1,128,128,19.8710,20367.8207,6.4415
2048,1024,2,256,128,39.5980,40587.9159,6.4650
4096,1024,4,512,128,60.2941,61801.4264,8.4917
8192,1024,8,1024,128,62.8882,64460.3833,16.2829
16384,1024,16,2048,128,64.0793,65681.2705,31.9604
32768,1024,32,4096,128,64.5484,66162.1467,63.4562

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260127_235452.log
[SUCCESS] bitnet1.58-2b-int8 Prefill 完成 (2578.1s)

------------------------------------------------------------
  Prefill Benchmark: bitnet1.58-2b-fp8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-fp8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA A100 80GB PCIe                     ││
│ GPU (short):      A100                                      │
│ Memory:           79.3 GB                                    │
│ CC:               cc80 (Ampere)                              ││
│ SM Code:          sm_80                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.95.05                                 │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✗                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-fp8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['prefill']
  M_prefill:        [512, 1024, 2048, 4096, 8192, 16384, 32768]
  M_decode:         [512, 1024, 2048, 4096, 8192, 16384, 32768]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_003749.log

[WARNING] 硬件不支持 FP8，跳过: BitNet-2B-FP8
[WARNING]   原因: GPU A100 (CC cc80) does not support native FP8.
FP8 requires Ada (CC 8.9+) or Hopper (CC 9.0+) or newer.


============================================================
  Benchmark 完成!
============================================================


总计: 0 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_003749.log
[SUCCESS] bitnet1.58-2b-fp8 Prefill 完成 (6.3s)

[INFO] Prefill 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 4: 完整 Prefill Benchmark - SUCCESS
Duration: 2584.4 seconds (43.1 minutes)
----------------------------------------------------------------------


======================================================================
TASK 5: 完整 Decode Benchmark
Started: 2026-01-28 00:37:50
======================================================================


------------------------------------------------------------
  Decode Benchmark: bitnet1.58-2b-int8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-int8 --backend cublaslt,cusparselt --stage decode --sparsity 2_4,2_6,2_8,2_10 --M 64,128,256,512


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA A100 80GB PCIe                     ││
│ GPU (short):      A100                                      │
│ Memory:           79.3 GB                                    │
│ CC:               cc80 (Ampere)                              ││
│ SM Code:          sm_80                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.95.05                                 │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✗                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-int8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['decode']
  M_prefill:        [64, 128, 256, 512]
  M_decode:         [64, 128, 256, 512]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_003755.log


============================================================
  BitNet-2B-INT8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/A100_cc80_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:38:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2251891) WARNING 01-28 00:38:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 26.07 requests/s, 7090.62 total tokens/s, 6673.52 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 00:38:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:38:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:38:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:38:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:38:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:38:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:38:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:38:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:38:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:38:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:38:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:38:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:38:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:38:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:38:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:38:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:38:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:38:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2251891) [2026-01-28 00:38:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2251891) [2026-01-28 00:38:12] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2251891) [2026-01-28 00:38:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2251891) [2026-01-28 00:38:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2251891) [2026-01-28 00:38:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2251891) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2251891) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.73it/s]
(EngineCore_DP0 pid=2251891) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.73it/s]
(EngineCore_DP0 pid=2251891) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2251891) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  3.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:03,  5.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:02,  6.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  7.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  8.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  9.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  8.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  8.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00,  9.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00,  9.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  9.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.49it/s]
(EngineCore_DP0 pid=2251891) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  8.25it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.97it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  9.28it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  9.31it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  9.46it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  9.51it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  9.61it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  9.65it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00,  9.61it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  9.34it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.38it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3536.93it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:30,  2.39s/it, est. speed input: 6.69 toks/s, output: 106.97 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.39s/it, est. speed input: 420.41 toks/s, output: 6726.60 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 26.27it/s, est. speed input: 420.41 toks/s, output: 6726.60 toks/s]
[rank0]:[W128 00:38:38.427754009 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.0s

测试结果:
  Requests/s:   26.07
  Tokens/s:     7090.62
  Total Reqs:   64
  Elapsed:      2.46s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      6673.52

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:38:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2252753) WARNING 01-28 00:39:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.23 requests/s, 10942.66 total tokens/s, 10298.98 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 00:38:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:38:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:38:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:38:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:38:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:38:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:38:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:38:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:38:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:38:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:38:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:38:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:38:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:38:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:38:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:38:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:38:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:38:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:38:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2252753) [2026-01-28 00:38:57] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2252753) [2026-01-28 00:38:57] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2252753) [2026-01-28 00:38:57] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2252753) [2026-01-28 00:38:57] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2252753) [2026-01-28 00:38:57] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2252753) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2252753) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=2252753) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=2252753) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2252753) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:03,  9.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:06,  5.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:04,  6.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:04,  7.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:03,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:03,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:03,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:02,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  8.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  9.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  9.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:01<00:02,  9.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:01<00:02,  9.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:01,  9.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:01,  9.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  9.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  9.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  9.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:02<00:01,  9.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:02<00:01,  9.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:02<00:01,  9.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:02<00:00,  9.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  9.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  9.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  9.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  9.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:03<00:00,  9.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:03<00:00,  9.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:03<00:00,  9.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:03<00:00,  9.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:03<00:00,  8.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:03<00:00,  8.86it/s]
(EngineCore_DP0 pid=2252753) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:01,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.82it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  9.19it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  9.30it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  9.25it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  9.34it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:01,  9.17it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:00,  9.12it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  9.20it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  9.41it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  9.54it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00,  9.65it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  9.74it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:01<00:00,  9.60it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00,  9.71it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:01<00:00,  9.72it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  9.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  9.36it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3755.23it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:03<06:26,  3.05s/it, est. speed input: 5.25 toks/s, output: 84.06 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 56.75it/s, est. speed input: 645.87 toks/s, output: 10333.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 56.75it/s, est. speed input: 650.93 toks/s, output: 10414.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.68it/s, est. speed input: 650.93 toks/s, output: 10414.78 toks/s]
[rank0]:[W128 00:39:26.360959533 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.9s

测试结果:
  Requests/s:   40.23
  Tokens/s:     10942.66
  Total Reqs:   128
  Elapsed:      3.18s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      10298.98

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:39:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2253712) WARNING 01-28 00:39:54 [backends.py:609] Failed to read file <frozen os>
Throughput: 58.73 requests/s, 15974.43 total tokens/s, 15034.76 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 00:39:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:39:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:39:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:39:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:39:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:39:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:39:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:39:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:39:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:39:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:39:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:39:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:39:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:39:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:39:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:39:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:39:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:39:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:39:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:39:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:39:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:39:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:39:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:39:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:39:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:39:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:39:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:39:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2253712) [2026-01-28 00:39:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2253712) [2026-01-28 00:39:45] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2253712) [2026-01-28 00:39:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2253712) [2026-01-28 00:39:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2253712) [2026-01-28 00:39:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2253712) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2253712) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.74it/s]
(EngineCore_DP0 pid=2253712) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.74it/s]
(EngineCore_DP0 pid=2253712) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2253712) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:03,  9.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:03,  9.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  8.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  9.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  9.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:02,  9.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:02,  9.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:02,  9.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:02,  9.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  9.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02,  9.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:02,  9.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:02,  9.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:01<00:02,  9.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:01<00:01,  9.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:01<00:01,  9.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  9.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  9.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  9.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  9.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:02<00:01,  9.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:02<00:01,  9.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:02<00:00,  9.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:02<00:00,  9.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:00,  9.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:03<00:00,  9.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  9.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:03<00:00,  9.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:03<00:00,  9.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:03<00:00,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:03<00:00,  9.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:03<00:00,  9.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:03<00:00,  9.49it/s]
(EngineCore_DP0 pid=2253712) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:03,  8.53it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:03,  9.29it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  9.49it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  9.64it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  9.76it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:02,  9.68it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:02,  9.71it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02,  9.63it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:02,  9.64it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  9.64it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  9.70it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  9.68it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  9.66it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  9.71it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  9.71it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  9.34it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:01,  9.35it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01,  9.45it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:01<00:01,  9.55it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  9.65it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  9.73it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  9.79it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  9.79it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  9.77it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  9.77it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:00,  9.61it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:02<00:00,  9.34it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:02<00:00,  9.29it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:03<00:00,  9.35it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00,  9.49it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  9.54it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00,  9.64it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00,  9.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  9.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  9.62it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3812.58it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<16:08,  3.80s/it, est. speed input: 4.21 toks/s, output: 67.37 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:03<00:05, 31.36it/s, est. speed input: 356.48 toks/s, output: 5703.67 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:01, 68.46it/s, est. speed input: 662.38 toks/s, output: 10597.98 toks/s]
Processed prompts:  89%|████████▊ | 227/256 [00:04<00:00, 102.28it/s, est. speed input: 878.57 toks/s, output: 14057.12 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 102.28it/s, est. speed input: 954.69 toks/s, output: 15274.97 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 59.67it/s, est. speed input: 954.69 toks/s, output: 15274.97 toks/s] 
[rank0]:[W128 00:40:17.171221198 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.8s

测试结果:
  Requests/s:   58.73
  Tokens/s:     15974.43
  Total Reqs:   256
  Elapsed:      4.36s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      15034.76

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:40:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2254644) WARNING 01-28 00:40:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 58.55 requests/s, 15926.75 total tokens/s, 14989.88 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 00:40:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:40:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:40:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:40:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:40:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:40:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:40:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:40:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:40:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:40:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:40:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:40:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:40:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:40:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:40:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:40:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:40:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:40:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:40:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:40:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:40:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:40:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:40:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:40:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:40:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:40:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:40:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:40:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2254644) [2026-01-28 00:40:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2254644) [2026-01-28 00:40:36] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2254644) [2026-01-28 00:40:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2254644) [2026-01-28 00:40:36] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2254644) [2026-01-28 00:40:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2254644) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2254644) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.73it/s]
(EngineCore_DP0 pid=2254644) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.73it/s]
(EngineCore_DP0 pid=2254644) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2254644) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  9.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  8.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  8.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  9.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:04,  9.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:04,  9.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:04,  9.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:04,  9.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  9.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  9.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  9.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  9.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:03,  9.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:03,  9.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:03,  9.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:01<00:03,  9.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:01<00:03,  9.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  9.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  9.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  9.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:02,  9.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:02,  9.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:02,  9.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:02<00:02,  9.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:02<00:02,  9.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  9.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  9.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  8.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:02,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:01,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:03<00:01,  8.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:03<00:01,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:04<00:01,  9.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01,  9.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  9.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  9.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:04<00:01,  9.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:00,  9.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:04<00:00,  9.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:04<00:00,  9.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:04<00:00,  9.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:05<00:00,  9.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:05<00:00,  9.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  9.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00,  9.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:05<00:00,  9.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  9.14it/s]
(EngineCore_DP0 pid=2254644) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:06,  7.82it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:05,  8.63it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:05,  8.89it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:05,  9.11it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:05,  9.14it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:04,  9.15it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:04,  9.28it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:00<00:04,  9.30it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:00<00:04,  9.13it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:04,  9.24it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:04,  9.09it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:01<00:04,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:04,  9.15it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:01<00:03,  9.28it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:03,  9.28it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:01<00:03,  9.36it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:01<00:03,  9.44it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:01<00:03,  9.33it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:02<00:03,  9.25it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:02<00:03,  9.23it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:02<00:03,  9.24it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:02<00:03,  9.27it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:02,  9.36it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:02,  9.35it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:02<00:02,  9.31it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:02<00:02,  9.35it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:02<00:02,  9.38it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:03<00:02,  9.41it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:03<00:02,  9.44it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:03<00:02,  9.37it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:02,  9.34it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:03<00:02,  9.33it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:03<00:01,  9.35it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:03<00:01,  9.39it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:03<00:01,  9.42it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:03<00:01,  9.46it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:03<00:01,  9.38it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:04<00:01,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:04<00:01,  8.91it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:04<00:01,  8.93it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:04<00:01,  9.07it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:04<00:00,  9.17it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:04<00:00,  9.25it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:04<00:00,  9.25it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:04<00:00,  9.27it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:04<00:00,  9.28it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:05<00:00,  9.37it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:05<00:00,  9.44it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:05<00:00,  9.24it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:05<00:00,  9.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  9.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  9.25it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  74%|███████▎  | 377/512 [00:00<00:00, 3761.45it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3802.23it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<54:29,  6.40s/it, est. speed input: 2.50 toks/s, output: 40.01 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:06<00:33, 13.57it/s, est. speed input: 154.11 toks/s, output: 2465.71 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:06<00:10, 36.49it/s, est. speed input: 339.42 toks/s, output: 5430.78 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:06<00:03, 76.37it/s, est. speed input: 573.82 toks/s, output: 9181.08 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:06<00:01, 124.91it/s, est. speed input: 789.18 toks/s, output: 12626.90 toks/s]
Processed prompts:  81%|████████  | 415/512 [00:06<00:00, 171.33it/s, est. speed input: 954.25 toks/s, output: 15267.97 toks/s]
Processed prompts:  96%|█████████▌| 491/512 [00:07<00:00, 199.13it/s, est. speed input: 1090.26 toks/s, output: 17444.07 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 199.13it/s, est. speed input: 951.68 toks/s, output: 15226.88 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 59.48it/s, est. speed input: 951.68 toks/s, output: 15226.88 toks/s] 
[rank0]:[W128 00:41:16.849519691 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 58.7s

测试结果:
  Requests/s:   58.55
  Tokens/s:     15926.75
  Total Reqs:   512
  Elapsed:      8.74s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      14989.88


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/A100_cc80_INT8_py312_cu129_x86_64/cublaslt/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,26.0684,7090.6150,2.4551
128,16,128,128,256,256,40.2304,10942.6643,3.1817
256,16,256,256,256,256,58.7295,15974.4339,4.3590
512,16,512,512,256,256,58.5542,15926.7527,8.7440

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:41:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2255670) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2255670) WARNING 01-28 00:41:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.24 requests/s, 8224.19 total tokens/s, 7740.42 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 00:41:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:41:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:41:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:41:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:41:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:41:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:41:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:41:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:41:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:41:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:41:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:41:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:41:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:41:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:41:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:41:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:41:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:41:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:41:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:41:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:41:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:41:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:41:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:41:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:41:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:41:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:41:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:41:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:34] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2255670) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2255670) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.86it/s]
(EngineCore_DP0 pid=2255670) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.86it/s]
(EngineCore_DP0 pid=2255670) 
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=2255670) [2026-01-28 00:41:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=2255670) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  4.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:02,  5.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  8.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  9.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:01,  9.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00,  9.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  9.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  9.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00,  9.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00,  9.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00,  9.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:01<00:00,  9.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.09it/s]
(EngineCore_DP0 pid=2255670) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  8.30it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  9.62it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  9.94it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 10.13it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 10.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.00it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3601.03it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:09,  2.06s/it, est. speed input: 7.76 toks/s, output: 124.10 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.06s/it, est. speed input: 488.18 toks/s, output: 7810.77 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 30.51it/s, est. speed input: 488.18 toks/s, output: 7810.77 toks/s]
[rank0]:[W128 00:42:06.133405860 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.2s

测试结果:
  Requests/s:   30.24
  Tokens/s:     8224.19
  Total Reqs:   64
  Elapsed:      2.12s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      7740.42

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:42:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2256606) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2256606) WARNING 01-28 00:42:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 45.93 requests/s, 12493.08 total tokens/s, 11758.19 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 00:42:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:42:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:42:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:42:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:42:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:42:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:42:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:42:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:42:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:42:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:42:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:42:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:42:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:42:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:42:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:42:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:42:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:42:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:42:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:42:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:42:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:42:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:42:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:42:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:42:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:42:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:42:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:42:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:24] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:24] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:24] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:24] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:24] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2256606) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2256606) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.94it/s]
(EngineCore_DP0 pid=2256606) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.93it/s]
(EngineCore_DP0 pid=2256606) 
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=2256606) [2026-01-28 00:42:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=2256606) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:09,  3.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:12,  2.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:07,  4.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:01<00:04,  6.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  7.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  9.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  9.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  9.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:01,  9.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:01,  9.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  9.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  9.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  9.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:02<00:01, 10.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:00,  9.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00, 10.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  9.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:03<00:00, 10.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:03<00:00,  9.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  9.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.74it/s]
(EngineCore_DP0 pid=2256606) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  8.88it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01, 10.09it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01, 10.20it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01, 10.25it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 10.24it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00, 10.30it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00, 10.36it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00, 10.44it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00, 10.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:01<00:00, 10.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:01<00:00, 10.26it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3922.52it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<05:38,  2.66s/it, est. speed input: 6.00 toks/s, output: 96.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.66s/it, est. speed input: 743.96 toks/s, output: 11903.33 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 46.49it/s, est. speed input: 743.96 toks/s, output: 11903.33 toks/s]
[rank0]:[W128 00:42:55.259904000 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.1s

测试结果:
  Requests/s:   45.93
  Tokens/s:     12493.08
  Total Reqs:   128
  Elapsed:      2.79s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      11758.19

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:43:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2257528) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2257528) WARNING 01-28 00:43:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 66.68 requests/s, 18135.69 total tokens/s, 17068.88 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 00:43:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:43:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:43:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:43:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:43:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:43:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:43:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:43:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:43:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:43:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:43:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:43:12] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:43:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:12] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:12] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:43:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:43:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:43:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:43:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:43:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2257528) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2257528) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=2257528) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=2257528) 
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=2257528) [2026-01-28 00:43:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=2257528) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:03,  9.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:03,  9.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  9.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  9.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  9.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:02,  9.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:02, 10.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02, 10.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02,  9.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:02,  9.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:01<00:01,  9.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:01<00:01,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:01<00:01,  9.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  9.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  9.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  9.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:02<00:01,  9.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:02<00:00,  9.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:02<00:00,  9.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  9.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:03<00:00,  9.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:03<00:00,  9.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:03<00:00,  9.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:03<00:00,  9.78it/s]
(EngineCore_DP0 pid=2257528) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:03,  8.95it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03, 10.00it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  9.85it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:02, 10.00it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02,  9.86it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  9.98it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02, 10.08it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  9.87it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:01, 10.00it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:01,  9.90it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01,  9.76it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  9.96it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01, 10.06it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01, 10.08it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:00,  9.98it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:02<00:00,  9.87it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:02<00:00,  9.70it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:02<00:00,  9.65it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  9.83it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00,  9.85it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00,  9.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  9.93it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3786.63it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<14:14,  3.35s/it, est. speed input: 4.77 toks/s, output: 76.38 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:03<00:03, 39.87it/s, est. speed input: 453.74 toks/s, output: 7259.79 toks/s]
Processed prompts:  71%|███████▏  | 183/256 [00:03<00:00, 84.28it/s, est. speed input: 822.34 toks/s, output: 13157.32 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:03<00:00, 120.24it/s, est. speed input: 1070.44 toks/s, output: 17126.91 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 120.24it/s, est. speed input: 1086.30 toks/s, output: 17380.77 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 67.89it/s, est. speed input: 1086.30 toks/s, output: 17380.77 toks/s] 
[rank0]:[W128 00:43:47.602693734 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.3s

测试结果:
  Requests/s:   66.68
  Tokens/s:     18135.69
  Total Reqs:   256
  Elapsed:      3.84s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      17068.88

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:43:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2258475) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2258475) WARNING 01-28 00:44:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 68.72 requests/s, 18692.27 total tokens/s, 17592.73 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 00:43:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:43:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:43:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:43:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:43:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:43:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:43:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:43:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:43:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:43:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:44:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:44:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:44:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:44:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:44:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:44:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:44:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:44:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:44:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:44:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:44:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:44:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:44:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:44:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2258475) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2258475) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.94it/s]
(EngineCore_DP0 pid=2258475) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.94it/s]
(EngineCore_DP0 pid=2258475) 
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=2258475) [2026-01-28 00:44:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=2258475) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  9.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  9.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:04,  9.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:04, 10.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:04, 10.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:00<00:04, 10.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:03, 10.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:03,  9.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:03,  9.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:03,  9.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:01<00:03,  9.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:01<00:03,  9.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:02, 10.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:02, 10.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:02, 10.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:02<00:02, 10.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:02<00:02, 10.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:01, 10.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:01, 10.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:01,  9.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:03<00:01,  9.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:03<00:01, 10.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:03<00:01, 10.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:00, 10.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:04<00:00, 10.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:04<00:00, 10.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:04<00:00, 10.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:04<00:00, 10.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00, 10.05it/s]
(EngineCore_DP0 pid=2258475) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:05,  9.01it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:05,  9.17it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:04,  9.53it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:04,  9.99it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:00<00:04,  9.95it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:00<00:04,  9.78it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:04,  9.62it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:04,  9.57it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:01<00:04,  9.50it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:03,  9.55it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:03,  9.77it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:01<00:03, 10.01it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:01<00:03,  9.99it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:02<00:03, 10.13it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:02<00:02, 10.20it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:02,  9.99it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:02<00:02,  9.85it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:02<00:02,  9.95it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:02<00:02,  9.93it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:03<00:02,  9.81it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:02,  9.73it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:03<00:01,  9.63it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:03<00:01,  9.54it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:03<00:01,  9.48it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:03<00:01,  9.45it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:03<00:01,  9.43it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:03<00:01,  9.51it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:03<00:01,  9.69it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:04<00:01,  9.60it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:04<00:01,  9.51it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:04<00:00,  9.44it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:04<00:00,  9.60it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:04<00:00,  9.55it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:04<00:00,  9.64it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:04<00:00,  9.97it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:05<00:00,  9.85it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:05<00:00,  9.74it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  9.67it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  9.73it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  73%|███████▎  | 375/512 [00:00<00:00, 3748.15it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3772.74it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:05<46:23,  5.45s/it, est. speed input: 2.94 toks/s, output: 46.99 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:05<00:28, 15.91it/s, est. speed input: 180.78 toks/s, output: 2892.46 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:05<00:06, 50.42it/s, est. speed input: 461.13 toks/s, output: 7377.96 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [00:05<00:02, 106.12it/s, est. speed input: 794.06 toks/s, output: 12704.95 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [00:05<00:00, 159.35it/s, est. speed input: 1039.80 toks/s, output: 16636.63 toks/s]
Processed prompts:  91%|█████████ | 467/512 [00:06<00:00, 207.69it/s, est. speed input: 1235.23 toks/s, output: 19763.73 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 207.69it/s, est. speed input: 1120.19 toks/s, output: 17922.95 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 70.01it/s, est. speed input: 1120.19 toks/s, output: 17922.95 toks/s] 
[rank0]:[W128 00:44:49.224529688 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 62.8s

测试结果:
  Requests/s:   68.72
  Tokens/s:     18692.27
  Total Reqs:   512
  Elapsed:      7.45s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      17592.73


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,30.2360,8224.1925,2.1167
128,16,128,128,256,256,45.9304,12493.0773,2.7868
256,16,256,256,256,256,66.6753,18135.6885,3.8395
512,16,512,512,256,256,68.7216,18692.2704,7.4504

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:44:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2259559) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2259559) WARNING 01-28 00:45:18 [backends.py:609] Failed to read file <frozen os>
Throughput: 29.02 requests/s, 7894.24 total tokens/s, 7429.87 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 00:44:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:44:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:44:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:44:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:44:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:44:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:44:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:44:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:44:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:44:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:44:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:44:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:44:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:44:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:45:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:45:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:45:07] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:45:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:45:07] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:45:07] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:45:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:45:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:45:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:45:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:45:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:45:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:45:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:45:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2259559) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2259559) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]
(EngineCore_DP0 pid=2259559) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]
(EngineCore_DP0 pid=2259559) 
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=2259559) [2026-01-28 00:45:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2259559) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:08,  2.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:07,  2.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:01<00:04,  3.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:01<00:03,  4.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:02,  5.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:02,  6.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  7.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  7.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  9.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:00,  9.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:02<00:00,  9.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  9.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  9.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  9.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  9.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  6.95it/s]
(EngineCore_DP0 pid=2259559) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00,  9.15it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  9.30it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  9.40it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  9.66it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  9.74it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  9.67it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00,  9.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.63it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3608.88it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:15,  2.15s/it, est. speed input: 7.45 toks/s, output: 119.19 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.15s/it, est. speed input: 468.41 toks/s, output: 7494.47 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 29.27it/s, est. speed input: 468.41 toks/s, output: 7494.47 toks/s]
[rank0]:[W128 00:45:42.051631954 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.7s

测试结果:
  Requests/s:   29.02
  Tokens/s:     7894.24
  Total Reqs:   64
  Elapsed:      2.21s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      7429.87

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:45:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2260564) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2260564) WARNING 01-28 00:46:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 43.13 requests/s, 11730.93 total tokens/s, 11040.87 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 00:45:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:45:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:45:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:45:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:45:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:45:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:45:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:45:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:45:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:45:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:45:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:45:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:45:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:45:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:46:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:46:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:46:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:46:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:46:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:46:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:46:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:46:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:46:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2260564) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2260564) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.40it/s]
(EngineCore_DP0 pid=2260564) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.39it/s]
(EngineCore_DP0 pid=2260564) 
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=2260564) [2026-01-28 00:46:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2260564) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:09,  3.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:09,  3.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  5.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:04,  6.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  7.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:02,  9.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  9.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  9.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  9.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:01<00:02,  9.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:01,  9.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01, 10.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  9.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01, 10.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:02<00:01, 10.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:02<00:00, 10.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00, 10.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00, 10.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:03<00:00, 10.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:03<00:00, 10.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:03<00:00,  9.15it/s]
(EngineCore_DP0 pid=2260564) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  8.79it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  9.70it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01, 10.16it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01, 10.42it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 10.45it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00, 10.13it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00, 10.17it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00, 10.30it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00, 10.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:01<00:00, 10.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:01<00:00, 10.22it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3736.16it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<06:00,  2.84s/it, est. speed input: 5.64 toks/s, output: 90.21 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.84s/it, est. speed input: 698.43 toks/s, output: 11174.89 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.65it/s, est. speed input: 698.43 toks/s, output: 11174.89 toks/s]
[rank0]:[W128 00:46:32.702737839 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.7s

测试结果:
  Requests/s:   43.13
  Tokens/s:     11730.93
  Total Reqs:   128
  Elapsed:      2.97s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      11040.87

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:46:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2261484) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2261484) WARNING 01-28 00:46:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 62.33 requests/s, 16953.40 total tokens/s, 15956.14 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 00:46:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:46:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:46:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:46:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:46:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:46:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:46:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:46:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:46:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:46:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:46:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:46:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:46:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:46:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:46:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:46:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:46:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:46:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:46:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2261484) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2261484) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.37it/s]
(EngineCore_DP0 pid=2261484) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.37it/s]
(EngineCore_DP0 pid=2261484) 
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=2261484) [2026-01-28 00:46:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2261484) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:03,  9.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:03,  9.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  9.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  9.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  9.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:02,  9.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:02,  9.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:02,  9.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02,  9.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  9.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:02,  9.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:02,  9.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:01<00:01,  9.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:01<00:01,  9.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  9.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  9.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  9.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:02<00:01,  9.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:02<00:00,  9.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:02<00:00,  9.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:00,  9.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  9.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:03<00:00,  9.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:03<00:00,  9.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:03<00:00,  9.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:03<00:00,  9.66it/s]
(EngineCore_DP0 pid=2261484) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:03,  8.74it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  9.64it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  9.91it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:02, 10.04it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02,  9.98it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  9.97it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  9.80it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  9.87it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  9.91it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:01,  9.99it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01,  9.88it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:01<00:01,  9.89it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  9.90it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  9.84it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  9.55it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  9.55it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  9.66it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:00,  9.75it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:02<00:00,  9.75it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00,  9.80it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  9.62it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:03<00:00,  9.78it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00,  9.78it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  9.82it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3872.17it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<15:13,  3.58s/it, est. speed input: 4.47 toks/s, output: 71.48 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:03<00:05, 33.23it/s, est. speed input: 377.93 toks/s, output: 6046.87 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:03<00:01, 75.30it/s, est. speed input: 725.76 toks/s, output: 11612.08 toks/s]
Processed prompts:  92%|█████████▏| 235/256 [00:03<00:00, 110.95it/s, est. speed input: 958.08 toks/s, output: 15329.17 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 110.95it/s, est. speed input: 1013.86 toks/s, output: 16221.78 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 63.36it/s, est. speed input: 1013.86 toks/s, output: 16221.78 toks/s] 
[rank0]:[W128 00:47:24.769114602 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.0s

测试结果:
  Requests/s:   62.33
  Tokens/s:     16953.40
  Total Reqs:   256
  Elapsed:      4.11s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      15956.14

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:47:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2262429) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2262429) WARNING 01-28 00:47:51 [backends.py:609] Failed to read file <frozen os>
Throughput: 63.94 requests/s, 17391.28 total tokens/s, 16368.26 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 00:47:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:47:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:47:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:47:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:47:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:47:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:47:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:47:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:47:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:47:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:47:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:47:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:47:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:47:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:47:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:47:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:47:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:47:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:47:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:47:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:47:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:47:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:47:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:47:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:47:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:47:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:47:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:47:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2262429) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2262429) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.40it/s]
(EngineCore_DP0 pid=2262429) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.40it/s]
(EngineCore_DP0 pid=2262429) 
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=2262429) [2026-01-28 00:47:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2262429) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:04,  9.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:04,  9.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:04,  9.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:04,  9.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:04, 10.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:04,  9.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  9.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:03,  9.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:03, 10.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:01<00:03, 10.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:01<00:03,  9.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:02, 10.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:02, 10.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:02, 10.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:02<00:02, 10.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:02<00:02, 10.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:01, 10.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:01, 10.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:03<00:01,  9.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:03<00:01,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:03<00:01,  9.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:03<00:01,  9.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:03<00:01,  9.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:03<00:01,  9.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:00,  9.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:04<00:00,  9.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:04<00:00,  9.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:04<00:00,  9.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:04<00:00,  9.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:04<00:00,  9.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:05<00:00,  9.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  9.94it/s]
(EngineCore_DP0 pid=2262429) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:05,  8.41it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:05,  8.86it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:05,  9.14it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:04,  9.72it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:04, 10.11it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:00<00:04, 10.07it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:03, 10.18it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:03, 10.31it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:03, 10.31it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:01<00:03, 10.26it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:01<00:03, 10.18it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:02<00:02, 10.24it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:02, 10.35it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:02<00:02, 10.13it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:02<00:02, 10.18it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:02<00:02, 10.28it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:01, 10.15it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:03<00:01, 10.19it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:03<00:01, 10.15it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:03<00:01, 10.20it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:03<00:01, 10.06it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:04<00:01,  9.92it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:04<00:00,  9.89it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:04<00:00, 10.11it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:04<00:00,  9.89it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:04<00:00,  9.84it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:04<00:00,  9.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  9.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00, 10.06it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  76%|███████▌  | 389/512 [00:00<00:00, 3889.14it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3896.94it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:05<49:39,  5.83s/it, est. speed input: 2.74 toks/s, output: 43.90 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:05<00:30, 14.88it/s, est. speed input: 169.04 toks/s, output: 2704.68 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:06<00:07, 47.20it/s, est. speed input: 431.37 toks/s, output: 6901.93 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:06<00:02, 93.36it/s, est. speed input: 705.86 toks/s, output: 11293.77 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:06<00:01, 143.37it/s, est. speed input: 933.03 toks/s, output: 14928.50 toks/s]
Processed prompts:  87%|████████▋ | 447/512 [00:06<00:00, 190.69it/s, est. speed input: 1113.91 toks/s, output: 17822.51 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 190.69it/s, est. speed input: 1040.26 toks/s, output: 16644.12 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 65.01it/s, est. speed input: 1040.26 toks/s, output: 16644.12 toks/s] 
[rank0]:[W128 00:48:27.905604379 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 63.2s

测试结果:
  Requests/s:   63.94
  Tokens/s:     17391.28
  Total Reqs:   512
  Elapsed:      8.01s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      16368.26


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,29.0229,7894.2358,2.2052
128,16,128,128,256,256,43.1284,11730.9288,2.9679
256,16,256,256,256,256,62.3287,16953.4016,4.1073
512,16,512,512,256,256,63.9385,17391.2764,8.0077

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_8) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:48:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2263539) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2263539) WARNING 01-28 00:48:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 28.71 requests/s, 7808.79 total tokens/s, 7349.45 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 00:48:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:48:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:48:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:48:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:48:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:48:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:48:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:48:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:48:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:48:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:48:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:48:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:48:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:48:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:48:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:48:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:48:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:48:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:48:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:48:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:48:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:48:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:48:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:48:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:48:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:48:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:48:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:48:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2263539) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2263539) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=2263539) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=2263539) 
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=2263539) [2026-01-28 00:48:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2263539) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:02,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:02,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:01,  9.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  9.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  9.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:01,  9.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00,  9.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00, 10.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 10.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00, 10.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:01<00:00, 10.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00,  9.87it/s]
(EngineCore_DP0 pid=2263539) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  9.00it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  9.89it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00, 10.07it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 10.26it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 10.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.23it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3485.81it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:16,  2.17s/it, est. speed input: 7.37 toks/s, output: 117.91 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.17s/it, est. speed input: 463.42 toks/s, output: 7414.67 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 28.96it/s, est. speed input: 463.42 toks/s, output: 7414.67 toks/s]
[rank0]:[W128 00:49:18.467714694 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.5s

测试结果:
  Requests/s:   28.71
  Tokens/s:     7808.79
  Total Reqs:   64
  Elapsed:      2.23s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      7349.45

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:49:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2264466) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2264466) WARNING 01-28 00:49:45 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.05 requests/s, 11437.98 total tokens/s, 10765.16 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 00:49:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:49:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:49:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:49:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:49:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:49:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:49:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:49:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:49:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:49:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:49:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:49:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:49:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:49:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:49:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:49:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:49:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:49:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:49:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:49:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:49:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:49:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:49:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:49:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:49:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:49:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:49:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:49:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:36] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:36] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:36] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2264466) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2264466) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=2264466) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=2264466) 
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=2264466) [2026-01-28 00:49:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2264466) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:09,  3.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:09,  3.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  4.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:05,  6.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  7.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:02,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:02,  8.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  9.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  9.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:01<00:02,  9.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:01,  9.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:01,  9.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  9.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  9.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:02<00:01,  9.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:02<00:01,  9.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  9.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  9.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  9.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:03<00:00,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:03<00:00,  9.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:03<00:00,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:03<00:00,  9.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:03<00:00,  8.87it/s]
(EngineCore_DP0 pid=2264466) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:01,  9.14it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  9.19it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  9.39it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  9.30it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  9.10it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  9.26it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  9.41it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:00,  9.71it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  9.99it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00, 10.10it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  9.99it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:01<00:00,  9.90it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00,  9.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:01<00:00, 10.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:01<00:00,  9.72it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3699.52it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<06:09,  2.91s/it, est. speed input: 5.49 toks/s, output: 87.89 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00,  2.91s/it, est. speed input: 680.87 toks/s, output: 10893.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 42.55it/s, est. speed input: 680.87 toks/s, output: 10893.87 toks/s]
[rank0]:[W128 00:50:07.154359582 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.7s

测试结果:
  Requests/s:   42.05
  Tokens/s:     11437.98
  Total Reqs:   128
  Elapsed:      3.04s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      10765.16

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:50:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2265394) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2265394) WARNING 01-28 00:50:35 [backends.py:609] Failed to read file <frozen os>
Throughput: 61.41 requests/s, 16702.51 total tokens/s, 15720.01 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 00:50:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:50:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:50:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:50:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:50:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:50:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:50:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:50:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:50:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:50:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:50:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:50:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:50:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:50:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:50:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:50:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:50:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:50:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:50:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:50:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:50:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:50:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:50:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:50:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:50:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:50:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:50:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:50:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2265394) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2265394) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
(EngineCore_DP0 pid=2265394) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
(EngineCore_DP0 pid=2265394) 
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=2265394) [2026-01-28 00:50:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2265394) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:03,  9.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:03,  9.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  9.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  9.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  9.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:02,  9.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:02,  9.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:02,  9.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:02,  9.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  9.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:02,  9.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:01<00:01,  9.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:01<00:01,  9.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:01<00:01,  9.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  9.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  9.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  9.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:02<00:01,  9.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:02<00:01,  9.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:02<00:00,  9.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:02<00:00, 10.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  9.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:03<00:00, 10.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:03<00:00, 10.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:03<00:00,  9.83it/s]
(EngineCore_DP0 pid=2265394) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:03,  8.96it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  9.74it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  9.68it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  9.52it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:02,  9.98it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02,  9.89it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02, 10.00it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02, 10.05it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  9.95it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  9.99it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:01, 10.10it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:01<00:01, 10.22it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01, 10.21it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01, 10.29it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:00, 10.18it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:02<00:00, 10.10it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:02<00:00, 10.17it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00, 10.24it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:03<00:00, 10.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00, 10.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00, 10.13it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3662.89it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<15:26,  3.63s/it, est. speed input: 4.41 toks/s, output: 70.48 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:03<00:05, 32.80it/s, est. speed input: 372.92 toks/s, output: 5966.69 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:03<00:01, 74.37it/s, est. speed input: 716.36 toks/s, output: 11461.70 toks/s]
Processed prompts:  92%|█████████▏| 235/256 [00:03<00:00, 109.41it/s, est. speed input: 945.14 toks/s, output: 15122.24 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 109.41it/s, est. speed input: 999.60 toks/s, output: 15993.46 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 62.47it/s, est. speed input: 999.60 toks/s, output: 15993.46 toks/s] 
[rank0]:[W128 00:51:00.526587362 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.4s

测试结果:
  Requests/s:   61.41
  Tokens/s:     16702.51
  Total Reqs:   256
  Elapsed:      4.17s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      15720.01

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:51:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2266337) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2266337) WARNING 01-28 00:51:28 [backends.py:609] Failed to read file <frozen os>
Throughput: 63.50 requests/s, 17272.90 total tokens/s, 16256.84 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 00:51:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:51:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:51:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:51:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:51:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:51:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:51:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:51:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:51:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:51:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:51:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:51:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:51:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:51:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:51:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:51:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:51:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:51:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:51:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:51:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:51:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:51:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:51:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:51:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:51:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:51:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:51:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:51:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:18] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:18] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:18] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:18] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:18] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2266337) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2266337) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
(EngineCore_DP0 pid=2266337) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
(EngineCore_DP0 pid=2266337) 
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=2266337) [2026-01-28 00:51:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2266337) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  9.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  9.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  9.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:04,  9.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:04, 10.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:04,  9.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:04,  9.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  9.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  9.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  9.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:03,  9.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:03,  9.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:03,  9.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:03,  9.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:01<00:03,  9.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:01<00:03,  9.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:01<00:03,  9.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  9.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  9.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:02,  9.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:02<00:02,  9.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:02<00:02,  9.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:02<00:02,  9.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  9.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  9.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  9.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  9.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:01,  9.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:01,  9.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:03<00:01,  9.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:03<00:01,  9.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:03<00:01,  9.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  9.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:04<00:01,  9.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:00,  9.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:04<00:00,  9.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:04<00:00,  9.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:04<00:00,  9.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:04<00:00,  9.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:04<00:00,  9.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  9.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:05<00:00,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  8.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  9.47it/s]
(EngineCore_DP0 pid=2266337) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:06,  8.00it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:05,  8.62it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:05,  8.91it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:05,  9.16it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:04,  9.21it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:04,  9.24it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:04,  9.25it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:00<00:04,  9.28it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:00<00:04,  9.42it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:04,  9.45it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:04,  9.60it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:03,  9.96it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:03,  9.95it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:01<00:03,  9.83it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:01<00:03,  9.81it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:01<00:03, 10.05it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:02<00:03,  9.85it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:02<00:02,  9.90it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:02,  9.79it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:02,  9.81it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:02<00:02,  9.76it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:02<00:02, 10.04it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:02<00:02, 10.14it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:01, 10.12it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:03<00:01, 10.17it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:03<00:01, 10.19it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:03<00:01, 10.18it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:03<00:01, 10.30it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:04<00:01,  9.98it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:04<00:00, 10.11it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:04<00:00, 10.12it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:04<00:00, 10.11it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:04<00:00,  9.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00, 10.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  9.87it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  76%|███████▌  | 387/512 [00:00<00:00, 3862.48it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3854.21it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:05<49:44,  5.84s/it, est. speed input: 2.74 toks/s, output: 43.83 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:05<00:30, 14.86it/s, est. speed input: 168.77 toks/s, output: 2700.32 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:06<00:07, 47.13it/s, est. speed input: 430.67 toks/s, output: 6890.76 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:06<00:02, 93.23it/s, est. speed input: 704.77 toks/s, output: 11276.33 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:06<00:01, 143.22it/s, est. speed input: 931.67 toks/s, output: 14906.62 toks/s]
Processed prompts:  87%|████████▋ | 447/512 [00:06<00:00, 190.57it/s, est. speed input: 1112.39 toks/s, output: 17798.17 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 190.57it/s, est. speed input: 1033.25 toks/s, output: 16531.98 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 64.58it/s, est. speed input: 1033.25 toks/s, output: 16531.98 toks/s] 
[rank0]:[W128 00:52:05.731603558 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 65.1s

测试结果:
  Requests/s:   63.50
  Tokens/s:     17272.90
  Total Reqs:   512
  Elapsed:      8.06s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      16256.84


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,28.7088,7808.7867,2.2293
128,16,128,128,256,256,42.0514,11437.9850,3.0439
256,16,256,256,256,256,61.4063,16702.5115,4.1690
512,16,512,512,256,256,63.5033,17272.8952,8.0626

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:52:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2267461) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2267461) WARNING 01-28 00:52:33 [backends.py:609] Failed to read file <frozen os>
Throughput: 27.90 requests/s, 7589.98 total tokens/s, 7143.51 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 00:52:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:52:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:52:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:52:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:52:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:52:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:52:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:52:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:52:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:52:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:52:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:52:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:52:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:52:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:52:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:52:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:52:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:52:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:52:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:52:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:52:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:52:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:52:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:52:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:52:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:52:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:52:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:52:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2267461) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2267461) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=2267461) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=2267461) 
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=2267461) [2026-01-28 00:52:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=2267461) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:05,  3.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  3.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:03,  4.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  6.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:02,  6.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  8.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00,  9.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  9.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00,  9.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  9.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  9.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.13it/s]
(EngineCore_DP0 pid=2267461) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  8.04it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.74it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  9.48it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  9.45it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  9.51it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  9.94it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  9.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.74it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3591.88it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:20,  2.23s/it, est. speed input: 7.16 toks/s, output: 114.58 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.23s/it, est. speed input: 450.24 toks/s, output: 7203.76 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 28.14it/s, est. speed input: 450.24 toks/s, output: 7203.76 toks/s]
[rank0]:[W128 00:52:57.471927953 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.8s

测试结果:
  Requests/s:   27.90
  Tokens/s:     7589.98
  Total Reqs:   64
  Elapsed:      2.29s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      7143.51

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:53:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2268436) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2268436) WARNING 01-28 00:53:25 [backends.py:609] Failed to read file <frozen os>
Throughput: 41.63 requests/s, 11322.59 total tokens/s, 10656.55 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 00:53:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:53:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:53:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:53:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:53:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:53:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:53:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:53:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:53:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:53:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:53:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:53:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:53:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:53:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:53:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:53:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:53:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:53:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:15] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2268436) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2268436) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=2268436) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=2268436) 
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=2268436) [2026-01-28 00:53:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=2268436) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:09,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:09,  3.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  4.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:05,  5.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  6.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:03,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  8.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:02,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:02,  9.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  9.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  9.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  9.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  9.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:01<00:02,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:02,  9.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:01,  9.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:01,  9.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01,  9.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  9.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  9.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:02<00:01,  9.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:02<00:01,  9.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  9.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:00,  9.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  9.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  9.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  9.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:03<00:00,  9.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:03<00:00,  9.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:03<00:00,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  9.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.59it/s]
(EngineCore_DP0 pid=2268436) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  8.17it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:01,  9.09it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  9.75it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  9.86it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  9.87it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  9.62it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:01,  9.68it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  9.75it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  9.65it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  9.71it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00,  9.67it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  9.63it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00,  9.86it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:01<00:00,  9.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:01<00:00,  9.71it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3645.61it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<06:13,  2.94s/it, est. speed input: 5.44 toks/s, output: 87.03 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00,  2.94s/it, est. speed input: 674.02 toks/s, output: 10784.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 42.12it/s, est. speed input: 674.02 toks/s, output: 10784.26 toks/s]
[rank0]:[W128 00:53:47.117456710 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.6s

测试结果:
  Requests/s:   41.63
  Tokens/s:     11322.59
  Total Reqs:   128
  Elapsed:      3.07s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      10656.55

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:53:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2269374) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2269374) WARNING 01-28 00:54:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 60.61 requests/s, 16486.22 total tokens/s, 15516.44 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 00:53:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:53:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:53:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:53:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:53:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:53:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:53:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:53:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:53:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:53:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:54:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:54:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:54:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:54:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:54:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:54:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:54:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:54:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:54:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2269374) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2269374) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.19it/s]
(EngineCore_DP0 pid=2269374) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.19it/s]
(EngineCore_DP0 pid=2269374) 
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=2269374) [2026-01-28 00:54:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=2269374) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:03,  9.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:03,  9.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  9.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  9.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:02, 10.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:02, 10.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:00<00:02, 10.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02, 10.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02, 10.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:01, 10.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:01<00:01,  9.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:01<00:01,  9.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  9.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01, 10.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:02<00:01, 10.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:02<00:00, 10.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:02<00:00,  9.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:02<00:00,  9.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  9.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:03<00:00,  9.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:03<00:00, 10.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:03<00:00,  9.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:03<00:00,  9.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:03<00:00,  9.89it/s]
(EngineCore_DP0 pid=2269374) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:03,  8.78it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:03,  9.23it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  9.97it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  9.97it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:02,  9.91it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02, 10.14it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:02, 10.26it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02, 10.17it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  9.97it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  9.89it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:01,  9.86it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01, 10.05it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:01<00:01, 10.21it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01, 10.21it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  9.89it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:00, 10.10it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:02<00:00, 10.14it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:02<00:00, 10.26it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00, 10.29it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00, 10.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00, 10.13it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3641.17it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<15:36,  3.67s/it, est. speed input: 4.36 toks/s, output: 69.69 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:03<00:05, 32.41it/s, est. speed input: 368.54 toks/s, output: 5896.62 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:03<00:01, 70.72it/s, est. speed input: 684.57 toks/s, output: 10953.11 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:04<00:00, 105.86it/s, est. speed input: 910.52 toks/s, output: 14568.28 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 105.86it/s, est. speed input: 986.49 toks/s, output: 15783.82 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 61.65it/s, est. speed input: 986.49 toks/s, output: 15783.82 toks/s] 
[rank0]:[W128 00:54:39.406128639 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.4s

测试结果:
  Requests/s:   60.61
  Tokens/s:     16486.22
  Total Reqs:   256
  Elapsed:      4.22s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      15516.44

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:54:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=2270316) [INFO] Loading compress extension: cusparselt_compress_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2270316) WARNING 01-28 00:55:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 62.41 requests/s, 16976.13 total tokens/s, 15977.54 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 00:54:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:54:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:54:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:54:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:54:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:54:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:54:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:54:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:54:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:54:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:54:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:54:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:54:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:54:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:54:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:54:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:54:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:54:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:54:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:58] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:58] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:58] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:58] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:58] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:58] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2270316) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2270316) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]
(EngineCore_DP0 pid=2270316) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]
(EngineCore_DP0 pid=2270316) 
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=2270316) [2026-01-28 00:54:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=2270316) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  9.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:04,  9.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:04,  9.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:04,  9.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  9.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:03,  9.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:03,  9.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:03,  9.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:01<00:03,  9.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:01<00:03,  9.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:01<00:03,  9.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  9.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  9.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:02,  9.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:02,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:02<00:02,  9.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:02<00:02,  9.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:02<00:02,  9.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  9.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  9.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:01,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:01,  9.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:01,  9.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:03<00:01,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:03<00:01,  9.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:03<00:01,  9.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  9.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:00,  9.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:04<00:00,  9.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:04<00:00,  9.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:04<00:00,  9.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:04<00:00,  9.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00, 10.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  9.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  9.75it/s]
(EngineCore_DP0 pid=2270316) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:06,  8.05it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:05,  9.09it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:04,  9.81it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:04, 10.03it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:00<00:04, 10.03it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:04,  9.95it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:04,  9.77it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:01<00:04,  9.60it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:03,  9.64it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:03,  9.88it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:01<00:03, 10.04it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:01<00:03, 10.06it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:02<00:02, 10.04it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:02<00:02,  9.85it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:02,  9.74it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:02,  9.57it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:02<00:02,  9.61it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:02<00:02,  9.81it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:02<00:02,  9.84it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:02<00:02,  9.76it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:03<00:02,  9.59it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:02,  9.58it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:03<00:02,  9.41it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:03<00:01,  9.24it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:03<00:01,  9.13it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:03<00:01,  9.11it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:03<00:01,  9.58it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:04<00:01,  9.64it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:04<00:01,  9.50it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:04<00:01,  9.35it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:04<00:00,  9.65it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:04<00:00,  9.80it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:04<00:00,  9.96it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:04<00:00,  9.93it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:05<00:00,  9.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  9.73it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  79%|███████▊  | 403/512 [00:00<00:00, 4023.21it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3998.38it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:05<50:30,  5.93s/it, est. speed input: 2.70 toks/s, output: 43.17 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:06<00:30, 14.64it/s, est. speed input: 166.24 toks/s, output: 2659.90 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:06<00:09, 39.33it/s, est. speed input: 366.00 toks/s, output: 5855.96 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:06<00:02, 88.59it/s, est. speed input: 657.52 toks/s, output: 10520.23 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:06<00:01, 141.46it/s, est. speed input: 896.74 toks/s, output: 14347.79 toks/s]
Processed prompts:  85%|████████▌ | 437/512 [00:06<00:00, 189.32it/s, est. speed input: 1075.26 toks/s, output: 17204.22 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 189.32it/s, est. speed input: 1014.61 toks/s, output: 16233.66 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 63.41it/s, est. speed input: 1014.61 toks/s, output: 16233.66 toks/s] 
[rank0]:[W128 00:55:43.359190874 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 63.8s

测试结果:
  Requests/s:   62.41
  Tokens/s:     16976.13
  Total Reqs:   512
  Elapsed:      8.20s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      15977.54


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/A100_cc80_INT8_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,27.9043,7589.9806,2.2935
128,16,128,128,256,256,41.6272,11322.5888,3.0749
256,16,256,256,256,256,60.6111,16486.2219,4.2236
512,16,512,512,256,256,62.4123,16976.1331,8.2035

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 20 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_003755.log
[SUCCESS] bitnet1.58-2b-int8 Decode 完成 (1076.0s)

------------------------------------------------------------
  Decode Benchmark: bitnet1.58-2b-fp8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-fp8 --backend cublaslt,cusparselt --stage decode --sparsity 2_4,2_6,2_8,2_10 --M 64,128,256,512


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA A100 80GB PCIe                     ││
│ GPU (short):      A100                                      │
│ Memory:           79.3 GB                                    │
│ CC:               cc80 (Ampere)                              ││
│ SM Code:          sm_80                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.95.05                                 │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✗                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-fp8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['decode']
  M_prefill:        [64, 128, 256, 512]
  M_decode:         [64, 128, 256, 512]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_005552.log

[WARNING] 硬件不支持 FP8，跳过: BitNet-2B-FP8
[WARNING]   原因: GPU A100 (CC cc80) does not support native FP8.
FP8 requires Ada (CC 8.9+) or Hopper (CC 9.0+) or newer.


============================================================
  Benchmark 完成!
============================================================


总计: 0 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_005552.log
[SUCCESS] bitnet1.58-2b-fp8 Decode 完成 (6.3s)

[INFO] Decode 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 5: 完整 Decode Benchmark - SUCCESS
Duration: 1082.3 seconds (18.0 minutes)
----------------------------------------------------------------------


======================================================================
TASK 6: Kernel: cuBLASLt
Started: 2026-01-28 00:55:52
======================================================================


------------------------------------------------------------
  cuBLASLt Kernel: BitNet-2B
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cublaslt --model BitNet-2B

============================================================
cuBLASLt Dense GEMM 算法搜索
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuBLASLt 可用

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 2560)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 2560)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 6912)
      → 算法数: 3, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16
============================================================
============================================================
cuBLASLt Dense GEMM 算法搜索
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuBLASLt 可用

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 8, 有效: 8
    NK 2/4: (2560, 2560)
      → 算法数: 8, 有效: 8
    NK 3/4: (13824, 2560)
      → 算法数: 8, 有效: 8
    NK 4/4: (2560, 6912)
      → 算法数: 8, 有效: 8

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16
============================================================
============================================================
cuBLASLt Dense GEMM 算法搜索
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cublaslt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuBLASLt 可用

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 2560)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 2560)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 6912)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8
============================================================
[INFO] dtype=all, 将测试: ['fp16', 'bf16', 'int8']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['fp16', 'bf16', 'int8']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[SUCCESS] cuBLASLt Kernel 测试完成 (52.9s)

----------------------------------------------------------------------
TASK 6: Kernel: cuBLASLt - SUCCESS
Duration: 52.9 seconds (0.9 minutes)
----------------------------------------------------------------------


======================================================================
TASK 7: Kernel: cuSPARSELt 高稀疏 (2_4~2_10)
Started: 2026-01-28 00:56:45
======================================================================


------------------------------------------------------------
  cuSPARSELt 高稀疏 Kernel: BitNet-2B
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cusparselt --model BitNet-2B --sparsity 2_4,2_6,2_8,2_10

[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3840, 2560), (2560, 2560), (13824, 2560), (2560, 6912)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_4
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 2560)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 2560)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 6912)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_4.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3840, 3424), (2560, 3424), (13824, 3424), (2560, 9216)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_6
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3424)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 3424)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 3424)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 9216)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_6.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3840, 3840), (2560, 3840), (13824, 3840), (2560, 10368)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_8
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3840)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 3840)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 3840)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 10368)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3840, 4096), (2560, 4096), (13824, 4096), (2560, 11072)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_10
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4096)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 4096)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 4096)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 11072)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_10.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3840, 2560), (2560, 2560), (13824, 2560), (2560, 6912)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_4
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 2560)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 2560)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 6912)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_4.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3840, 3424), (2560, 3424), (13824, 3424), (2560, 9216)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_6
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3424)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 3424)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 3424)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 9216)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_6.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3840, 3840), (2560, 3840), (13824, 3840), (2560, 10368)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_8
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3840)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 3840)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 3840)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 10368)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3840, 4096), (2560, 4096), (13824, 4096), (2560, 11072)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_10
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4096)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 4096)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 4096)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 11072)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_10.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3840, 2560), (2560, 2560), (13824, 2560), (2560, 6912)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_4
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 5, 有效: 5
    NK 2/4: (2560, 2560)
      → 算法数: 5, 有效: 5
    NK 3/4: (13824, 2560)
      → 算法数: 5, 有效: 5
    NK 4/4: (2560, 6912)
      → 算法数: 5, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_4.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3840, 3424), (2560, 3424), (13824, 3424), (2560, 9216)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_6
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3424)
      → 算法数: 5, 有效: 5
    NK 2/4: (2560, 3424)
      → 算法数: 5, 有效: 5
    NK 3/4: (13824, 3424)
      → 算法数: 5, 有效: 5
    NK 4/4: (2560, 9216)
      → 算法数: 5, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_6.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3840, 3840), (2560, 3840), (13824, 3840), (2560, 10368)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_8
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3840)
      → 算法数: 5, 有效: 5
    NK 2/4: (2560, 3840)
      → 算法数: 5, 有效: 5
    NK 3/4: (13824, 3840)
      → 算法数: 5, 有效: 5
    NK 4/4: (2560, 10368)
      → 算法数: 5, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3840, 4096), (2560, 4096), (13824, 4096), (2560, 11072)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_10
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4096)
      → 算法数: 5, 有效: 5
    NK 2/4: (2560, 4096)
      → 算法数: 5, 有效: 5
    NK 3/4: (13824, 4096)
      → 算法数: 5, 有效: 5
    NK 4/4: (2560, 11072)
      → 算法数: 5, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_10.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8
============================================================
[INFO] dtype=all, 将测试: ['fp16', 'bf16', 'int8']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['fp16', 'bf16', 'int8']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_4

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_6

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_8

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_10

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_4

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_6

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_8

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_10

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_4

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_6

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_8

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[SUCCESS] cuSPARSELt 高稀疏 Kernel 测试完成 (188.9s)

----------------------------------------------------------------------
TASK 7: Kernel: cuSPARSELt 高稀疏 (2_4~2_10) - SUCCESS
Duration: 188.9 seconds (3.1 minutes)
----------------------------------------------------------------------


======================================================================
TASK 8: Kernel: cuSPARSELt 低稀疏 (2_12~2_inf)
Started: 2026-01-28 00:59:54
======================================================================


------------------------------------------------------------
  cuSPARSELt 低稀疏 Kernel: BitNet-2B
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cusparselt --model BitNet-2B --sparsity 2_12,2_14,2_16,2_inf

[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3840, 4288), (2560, 4288), (13824, 4288), (2560, 11520)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_12
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4288)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 4288)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 4288)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 11520)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_12.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3840, 4416), (2560, 4416), (13824, 4416), (2560, 11872)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_14
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4416)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 4416)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 4416)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 11872)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_14.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3840, 4480), (2560, 4480), (13824, 4480), (2560, 12096)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_16
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4480)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 4480)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 4480)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 12096)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_16.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3840, 5120), (2560, 5120), (13824, 5120), (2560, 13824)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_inf
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 5120)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 5120)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 5120)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 13824)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_inf.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3840, 4288), (2560, 4288), (13824, 4288), (2560, 11520)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_12
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4288)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 4288)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 4288)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 11520)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_12.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3840, 4416), (2560, 4416), (13824, 4416), (2560, 11872)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_14
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4416)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 4416)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 4416)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 11872)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_14.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3840, 4480), (2560, 4480), (13824, 4480), (2560, 12096)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_16
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4480)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 4480)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 4480)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 12096)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_16.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3840, 5120), (2560, 5120), (13824, 5120), (2560, 13824)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_inf
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 5120)
      → 算法数: 4, 有效: 5
    NK 2/4: (2560, 5120)
      → 算法数: 4, 有效: 5
    NK 3/4: (13824, 5120)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 13824)
      → 算法数: 4, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_inf.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3840, 4288), (2560, 4288), (13824, 4288), (2560, 11520)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_12
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4288)
      → 算法数: 5, 有效: 5
    NK 2/4: (2560, 4288)
      → 算法数: 5, 有效: 5
    NK 3/4: (13824, 4288)
      → 算法数: 5, 有效: 5
    NK 4/4: (2560, 11520)
      → 算法数: 5, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_12.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3840, 4416), (2560, 4416), (13824, 4416), (2560, 11872)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_14
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4416)
      → 算法数: 5, 有效: 5
    NK 2/4: (2560, 4416)
      → 算法数: 5, 有效: 5
    NK 3/4: (13824, 4416)
      → 算法数: 5, 有效: 5
    NK 4/4: (2560, 11872)
      → 算法数: 5, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_14.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3840, 4480), (2560, 4480), (13824, 4480), (2560, 12096)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_16
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4480)
      → 算法数: 5, 有效: 5
    NK 2/4: (2560, 4480)
      → 算法数: 5, 有效: 5
    NK 3/4: (13824, 4480)
      → 算法数: 5, 有效: 5
    NK 4/4: (2560, 12096)
      → 算法数: 5, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_16.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3840, 5120), (2560, 5120), (13824, 5120), (2560, 13824)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_inf
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_A100_cc80_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 5120)
      → 算法数: 5, 有效: 5
    NK 2/4: (2560, 5120)
      → 算法数: 5, 有效: 5
    NK 3/4: (13824, 5120)
      → 算法数: 5, 有效: 5
    NK 4/4: (2560, 13824)
      → 算法数: 5, 有效: 5

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_inf.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/A100_cc80_py312_cu129_x86_64/INT8
============================================================
[INFO] dtype=all, 将测试: ['fp16', 'bf16', 'int8']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA A100 80GB PCIe (cc80)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['fp16', 'bf16', 'int8']
Backend: cusparselt
Sparsity: ['2_12', '2_14', '2_16', '2_inf']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_12

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_14

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_16

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_inf

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_12

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_14

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_16

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_inf

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_12

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_14

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_16

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_inf

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[SUCCESS] cuSPARSELt 低稀疏 Kernel 测试完成 (219.0s)

----------------------------------------------------------------------
TASK 8: Kernel: cuSPARSELt 低稀疏 (2_12~2_inf) - SUCCESS
Duration: 219.0 seconds (3.6 minutes)
----------------------------------------------------------------------



============================================================
  最终总结
============================================================


  Task 1: 基础模型准备 (下载 + 量化) - SUCCESS (0.0s)
  Task 2: SlideSparse 转换 (prune + slide) - SUCCESS (0.0s)
  Task 3: 离线调优 (粗调优 + 细调优) - SUCCESS (500.4s)
  Task 4: 完整 Prefill Benchmark - SUCCESS (2584.4s)
  Task 5: 完整 Decode Benchmark - SUCCESS (1082.3s)
  Task 6: Kernel: cuBLASLt - SUCCESS (52.9s)
  Task 7: Kernel: cuSPARSELt 高稀疏 (2_4~2_10) - SUCCESS (188.9s)
  Task 8: Kernel: cuSPARSELt 低稀疏 (2_12~2_inf) - SUCCESS (219.0s)

  总计: 8 成功, 0 失败, 0 跳过
  总耗时: 4627.8 秒 (1.29 小时)

[INFO] 日志文件: /root/vllmbench/slidesparse/tools/bitnet_bench_20260127_234625.log
[INFO] 状态文件: /root/vllmbench/slidesparse/tools/bitnet_bench_20260127_234625_status.json
