# SlideSparse Benchmark 最终报告 - RTX 5080 (16GB)

**硬件**: NVIDIA RTX 5080 (16GB VRAM, Blackwell cc120)  
**测试日期**: 2026-01-25 ~ 2026-01-26  
**日志文件**: 
- `prepare_bench_20260125_134124.log` (第一次运行，被中断)
- `prepare_bench_20260125_161102.log` (第二次运行，完成)

---

## 📊 测试总结

| 类别 | 成功 | 失败 | 成功率 |
|------|------|------|--------|
| **Llama3.2-1B/3B** | 240 | 0 | 100% |
| **Qwen2.5-7B** | 96 | 24 | 80% |
| **Qwen2.5-14B** | 0 | 120 | 0% |
| **总计** | **336** | **144** | **70%** |

### 失败原因分析

| 模型 | Checkpoint 大小 | 16GB 显卡上的限制 |
|------|-----------------|-------------------|
| Llama3.2-1B | 1.9~2.0 GB | ✅ 无限制，全部通过 |
| Llama3.2-3B | 4.2 GB | ✅ 无限制，全部通过 |
| Qwen2.5-7B | 8.2 GB (base) | ⚠️ 大 M 值 OOM (剩余 ~8GB 不足) |
| Qwen2.5-14B | 16 GB | ❌ 无法加载 (超过显存容量) |

**SlideSparse Checkpoint 额外开销**:
- 2_4 稀疏度: 基本无增长 (~8.2 GB)
- 2_6 稀疏度: +2.8 GB (~11 GB)
- 2_8 稀疏度: +3.8 GB (~12 GB)  
- 2_10 稀疏度: +3.8 GB (~12 GB)

---

## 1. 离线调优结果 ✅ 全部成功

### 1.1 GEMM 算法搜索

| 库 | 模型 | 状态 |
|----|------|------|
| **cuBLASLt** | Llama3.2-1B-INT8/FP8 | ✅ 16 NK组合 |
| **cuBLASLt** | Llama3.2-3B-INT8/FP8 | ✅ 16 NK组合 |
| **cuBLASLt** | Qwen2.5-7B-INT8/FP8 | ✅ 16 NK组合 |
| **cuBLASLt** | Qwen2.5-14B-INT8/FP8 | ✅ 16 NK组合 |
| **cuSPARSELt** | Llama3.2-1B-INT8/FP8 | ✅ 16 NK组合 |
| **cuSPARSELt** | Llama3.2-3B-INT8/FP8 | ✅ 16 NK组合 |
| **cuSPARSELt** | Qwen2.5-7B-INT8/FP8 | ✅ 16 NK组合 |
| **cuSPARSELt** | Qwen2.5-14B-INT8/FP8 | ✅ 16 NK组合 |

### 1.2 Triton Kernel 调优

| Kernel | 模型 | 状态 |
|--------|------|------|
| **Quant Only** | Llama3.2-1B/3B, Qwen2.5-7B/14B | ✅ 4 模型 |
| **Dequant+Bias** | Llama3.2-1B/3B, Qwen2.5-7B/14B | ✅ 4 模型 |
| **Quant+Slide** | Llama3.2-1B/3B, Qwen2.5-7B/14B | ✅ 4 模型 |

---

## 2. Prefill Throughput 测试

### 2.1 INT8 Prefill

| 模型 | cuBLASLt | 2_4 | 2_6 | 2_8 | 2_10 |
|------|----------|-----|-----|-----|------|
| **Llama3.2-1B-INT8** | ✅ 8/8 | ✅ 8/8 | ✅ 8/8 | ✅ 8/8 | ✅ 8/8 |
| **Llama3.2-3B-INT8** | ✅ 8/8 | ✅ 8/8 | ✅ 8/8 | ✅ 8/8 | ✅ 8/8 |
| **Qwen2.5-7B-INT8** | ⚠️ 6/8 | ⚠️ 6/8 | ⚠️ 5/8 | ⚠️ 5/8 | ⚠️ 4/8 |
| **Qwen2.5-14B-INT8** | ❌ 0/8 | ❌ 0/8 | ❌ 0/8 | ❌ 0/8 | ❌ 0/8 |

**Qwen2.5-7B-INT8 失败详情**:
| Backend | 成功 M 值 | 失败 M 值 |
|---------|-----------|-----------|
| cuBLASLt | 512~16384 | 32768, 65536 |
| 2_4 | 512~16384 | 32768, 65536 |
| 2_6 | 512~8192 | 16384, 32768, 65536 |
| 2_8 | 512~8192 | 16384, 32768, 65536 |
| 2_10 | 512~4096 | 8192, 16384, 32768, 65536 |

### 2.2 FP8 Prefill

| 模型 | cuBLASLt | 2_4 | 2_6 | 2_8 | 2_10 |
|------|----------|-----|-----|-----|------|
| **Llama3.2-1B-FP8** | ✅ 8/8 | ✅ 8/8 | ✅ 8/8 | ✅ 8/8 | ✅ 8/8 |
| **Llama3.2-3B-FP8** | ✅ 8/8 | ✅ 8/8 | ✅ 8/8 | ✅ 8/8 | ✅ 8/8 |
| **Qwen2.5-7B-FP8** | ⚠️ 6/8 | ⚠️ 7/8 | ⚠️ 6/8 | ⚠️ 6/8 | ⚠️ 6/8 |
| **Qwen2.5-14B-FP8** | ❌ 0/8 | ❌ 0/8 | ❌ 0/8 | ❌ 0/8 | ❌ 0/8 |

**Qwen2.5-7B-FP8 失败详情**:
| Backend | 成功 M 值 | 失败 M 值 |
|---------|-----------|-----------|
| cuBLASLt | 512~16384 | 32768, 65536 |
| 2_4 | 512~32768 | 65536 |
| 2_6 | 512~16384 | 32768, 65536 |
| 2_8 | 512~16384 | 32768, 65536 |
| 2_10 | 512~16384 | 32768, 65536 |

---

## 3. Decode Throughput 测试

### 3.1 INT8 Decode

| 模型 | cuBLASLt | 2_4 | 2_6 | 2_8 | 2_10 |
|------|----------|-----|-----|-----|------|
| **Llama3.2-1B-INT8** | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 |
| **Llama3.2-3B-INT8** | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 |
| **Qwen2.5-7B-INT8** | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 | ⚠️ 3/4 |
| **Qwen2.5-14B-INT8** | ❌ 0/4 | ❌ 0/4 | ❌ 0/4 | ❌ 0/4 | ❌ 0/4 |

**Qwen2.5-7B-INT8 2_10 失败**: M=512 (KV Cache 空间不足，checkpoint 12GB 过大)

### 3.2 FP8 Decode

| 模型 | cuBLASLt | 2_4 | 2_6 | 2_8 | 2_10 |
|------|----------|-----|-----|-----|------|
| **Llama3.2-1B-FP8** | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 |
| **Llama3.2-3B-FP8** | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 |
| **Qwen2.5-7B-FP8** | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 | ✅ 4/4 |
| **Qwen2.5-14B-FP8** | ❌ 0/4 | ❌ 0/4 | ❌ 0/4 | ❌ 0/4 | ❌ 0/4 |

---

## 4. 数据可用性总结

### ✅ 完整数据 (可直接使用)

| 模型 | Prefill M 范围 | Decode M 范围 |
|------|----------------|---------------|
| Llama3.2-1B-INT8/FP8 | 512~65536 (全部) | 64~512 (全部) |
| Llama3.2-3B-INT8/FP8 | 512~65536 (全部) | 64~512 (全部) |

### ⚠️ 部分数据 (大 M 值缺失)

| 模型 | Backend | Prefill 可用 | Decode 可用 |
|------|---------|--------------|-------------|
| Qwen2.5-7B-INT8 | cuBLASLt | 512~16384 | 64~512 |
| Qwen2.5-7B-INT8 | 2_4 | 512~16384 | 64~512 |
| Qwen2.5-7B-INT8 | 2_6 | 512~8192 | 64~512 |
| Qwen2.5-7B-INT8 | 2_8 | 512~8192 | 64~512 |
| Qwen2.5-7B-INT8 | 2_10 | 512~4096 | 64~256 |
| Qwen2.5-7B-FP8 | cuBLASLt | 512~16384 | 64~512 |
| Qwen2.5-7B-FP8 | 2_4 | 512~32768 | 64~512 |
| Qwen2.5-7B-FP8 | 2_6~2_10 | 512~16384 | 64~512 |

### ❌ 无数据 (需要 24GB+ 显卡)

- Qwen2.5-14B-INT8 (所有配置)
- Qwen2.5-14B-FP8 (所有配置)

---

## 5. 结论与建议

### 5.1 RTX 5080 (16GB) 适用场景

 **推荐使用**:
- Llama3.2-1B/3B: 全部 M 值均可测试
- Qwen2.5-7B: 小到中等 M 值 (Prefill ≤16384, Decode ≤512)

 **不适用**:
- Qwen2.5-14B: 需要 24GB+ 显卡 (如 RTX 4090/5090, A100)
- Qwen2.5-7B 超大 M 值 (Prefill >16384)

### 5.2 在其他显卡上运行

```bash
# 24GB+ 显卡 (RTX 4090/5090, A100)
# 可以运行 Qwen2.5-14B 和所有 M 值

# 如果只有 16GB 显卡，建议限制 M 值:
# Qwen2.5-7B Prefill
--m_list 512,1024,2048,4096,8192,16384

# Qwen2.5-7B Decode  
--m_list 64,128,256,512  # FP8 全部可用
--m_list 64,128,256      # INT8 2_10 只能到 256
```

### 5.3 JSON 数据完整性

 JSON 文件，可直接用于分析：
- Prefill: 244 个 JSON 文件
- Decode: 130 个 JSON 文件

---

*报告生成时间: 2026-01-26*
