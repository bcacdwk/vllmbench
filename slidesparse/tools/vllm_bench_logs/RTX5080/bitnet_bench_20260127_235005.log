======================================================================
BitNet Benchmark Log
Started: 2026-01-27 23:50:05
======================================================================

Hardware:
  GPU: NVIDIA GeForce RTX 5080 (cc120)
  Python: py312
  CUDA: cu129
  Arch: x86_64

[INFO] 日志文件: /root/vllmbench/slidesparse/tools/bitnet_bench_20260127_235005.log

======================================================================
TASK 1: 基础模型准备 (下载 + 量化)
Started: 2026-01-27 23:50:05
======================================================================


------------------------------------------------------------
  Step 1: 下载 BitNet BF16 模型
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/model_download.py --model bitnet1.58-2b-bf16


Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]Downloading 'model.safetensors' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/xGOKKLRSlIhH692hSVvI1-gpoa8=.529637ff6dab1f5890767356928693f69ffe61d3b6040a43de9306b37bfd5ae1.incomplete'
Downloading 'generation_config.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.650ab2390d65b8182f2599fe7a0f2014eec3f38b.incomplete'
Downloading 'special_tokens_map.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.d8cd5076496dbe4be2320312abc10adc43097b81.incomplete'
Downloading 'data_summary_card.md' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/rO2cXQjMJqsORRCCpgXS1A8CgMk=.156c5705d7cf1e2f11a27e62f673c4576af7aa19.incomplete'
Downloading 'README.md' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.cfe176334831acea2ecdaef02b5ae25416b14941.incomplete'
Downloading '.gitattributes' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'
Downloading 'config.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.8cb95b54570aababf062b4b8d95e78dc74a7ba51.incomplete'
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/generation_config.json
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/special_tokens_map.json
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/README.md
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/config.json
Downloading 'LICENSE' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/DhCjcNQuMpl4FL346qr3tvNUCgY=.48ea6616b5b8581df3401872996cecf1f8b08a0d.incomplete'
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/LICENSE
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/data_summary_card.md
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/.gitattributes

Fetching 10 files:  10%|█         | 1/10 [00:00<00:06,  1.41it/s]Downloading 'tokenizer_config.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.d54415c89774937967b9baac420d12455ff6e267.incomplete'
Downloading 'tokenizer.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.b197f72effb9d5ed16ee0f5663e11e4cfac2ba62.incomplete'
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/tokenizer_config.json
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/tokenizer.json
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/model.safetensors

Fetching 10 files:  70%|███████   | 7/10 [00:39<00:17,  5.81s/it]
Fetching 10 files: 100%|██████████| 10/10 [00:39<00:00,  3.91s/it]
/root/vllmbench/checkpoints/BitNet-2B-BF16

============================================================
  准备下载 1 个模型 (~4.8 GB)
============================================================

  - BitNet-2B-BF16 (4.8 GB)


============================================================
  下载: BitNet-2B-BF16
============================================================

[INFO] HuggingFace: microsoft/bitnet-b1.58-2B-4T-bf16
[INFO] 本地目录: /root/vllmbench/checkpoints/BitNet-2B-BF16

[INFO] 下载命令: hf download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir /root/vllmbench/checkpoints/BitNet-2B-BF16
[SUCCESS] 下载成功: /root/vllmbench/checkpoints/BitNet-2B-BF16


============================================================
  下载完成
============================================================

成功: 1/1

============================================================
  模型下载状态
============================================================


INT8 模型:
----------------------------------------
  ✗ Qwen2.5-0.5B-INT8 - not downloaded
  ✓ Llama3.2-1B-INT8 - 1.9 GB
  ✗ Qwen2.5-1.5B-INT8 - not downloaded
  ✗ BitNet-2B-INT8 - not downloaded
  ✗ Qwen2.5-3B-INT8 - not downloaded
  ✓ Llama3.2-3B-INT8 - 4.1 GB
  ✓ Qwen2.5-7B-INT8 - 8.1 GB
  ✓ Qwen2.5-14B-INT8 - 15.2 GB

FP8 模型:
----------------------------------------
  ✗ Qwen2.5-0.5B-FP8 - not downloaded
  ✓ Llama3.2-1B-FP8 - 1.9 GB
  ✗ Qwen2.5-1.5B-FP8 - not downloaded
  ✗ BitNet-2B-FP8 - not downloaded
  ✗ Qwen2.5-3B-FP8 - not downloaded
  ✓ Llama3.2-3B-FP8 - 4.1 GB
  ✓ Qwen2.5-7B-FP8 - 8.1 GB
  ✓ Qwen2.5-14B-FP8 - 15.2 GB

BF16 模型:
----------------------------------------
  ✓ BitNet-2B-BF16 - 4.5 GB

----------------------------------------
总计: 9 已下载, 8 缺失
[INFO] Checkpoints 目录大小: 64G
[SUCCESS] 下载完成 (43.3s)

------------------------------------------------------------
  Step 2: 量化为 BitNet-2B-INT8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-BF16 --bitnet --output-dtype int8 --Z 2 --L 2 --skip-slide
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/root/vllmbench/slidesparse/utils.py:2928: UserWarning: L=2 < 4，这是纯量化模式（无稀疏），slide 操作将被跳过
  warnings.warn(
======================================================================
Processing: BitNet-2B-BF16
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=2, N=1, expand=0.000)
[INFO] Mode: magnitude
[INFO] BitNet Mode: enabled (output_dtype=int8)
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2

[INFO] Copying non-weight files...
[INFO]   Copied: tokenizer.json, special_tokens_map.json, tokenizer_config.json, LICENSE, config.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.20%)
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.11%)
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.86%)
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.70%)
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.95%)
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (30.11%)
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.73%)
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (34.84%)
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (48.35%)
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (48.39%)
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.94%)
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.03%)
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.16%)
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.12%)
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.94%)
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.50%)
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.07%)
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (30.23%)
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.13%)
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.83%)
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.66%)
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.92%)
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.62%)
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.08%)
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (24.26%)
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.88%)
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.42%)
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.01%)
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.84%)
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.71%)
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.80%)
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (28.71%)
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.19%)
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.88%)
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.76%)
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.23%)
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.39%)
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.59%)
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.83%)
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.56%)
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.50%)
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.27%)
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.43%)
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.34%)
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.94%)
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (32.82%)
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.05%)
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.77%)
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (11.51%)
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.69%)
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.65%)
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.42%)
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.66%)
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.11%)
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (9.15%)
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.99%)
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.84%)
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.18%)
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.87%)
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.35%)
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.09%)
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (10.66%)
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.45%)
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.72%)
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.49%)
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (31.51%)
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (11.87%)
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (24.63%)
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.37%)
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.68%)
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.87%)
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.57%)
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.39%)
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.39%)
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.62%)
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (9.69%)
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.53%)
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.64%)
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.42%)
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.76%)
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.46%)
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.77%)
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.25%)
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.50%)
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (35.59%)
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (35.75%)
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.00%)
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.57%)
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.80%)
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.10%)
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.53%)
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.29%)
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.71%)
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (31.97%)
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.26%)
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.80%)
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.48%)
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.43%)
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.31%)
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.73%)
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.93%)
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.33%)
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.75%)
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.57%)
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.56%)
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.30%)
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.90%)
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.96%)
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.52%)
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.68%)
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (10.90%)
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.93%)
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.02%)
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.00%)
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (26.12%)
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.42%)
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.67%)
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.05%)
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.28%)
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.27%)
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.51%)
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.09%)
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.74%)
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (26.75%)
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.67%)
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.08%)
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.17%)
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.72%)
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.73%)
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.89%)
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.86%)
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.68%)
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (13.43%)
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.59%)
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.71%)
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.87%)
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.93%)
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.90%)
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (13.77%)
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (13.64%)
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.03%)
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.46%)
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.20%)
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.79%)
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (11.23%)
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.35%)
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.77%)
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.66%)
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.38%)
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.80%)
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.11%)
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.23%)
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.79%)
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.63%)
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.30%)
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.55%)
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (31.95%)
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.35%)
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (24.72%)
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.33%)
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.01%)
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.91%)
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.39%)
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.90%)
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.76%)
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.22%)
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.48%)
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.07%)
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.66%)
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.86%)
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.77%)
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.19%)
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.28%)
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.58%)
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.12%)
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.42%)
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.40%)
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.35%)
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.08%)
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.80%)
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.22%)
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.38%)
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.52%)
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.81%)
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.55%)
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.32%)
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.18%)
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.32%)
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.95%)
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.81%)
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.26%)
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.76%)
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.84%)
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (13.07%)
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.28%)
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.65%)
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.41%)
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.82%)
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.87%)
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.70%)
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.07%)
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.12%)
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.52%)
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.18%)
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.47%)
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.99%)
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.45%)

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 122 layers
[INFO] Time: 9.15s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2/conversion_report.json
[SUCCESS] BitNet-2B-INT8 量化完成并移动到 /root/vllmbench/checkpoints/BitNet-2B-INT8 (12.7s)
[SUCCESS]   ✓ 已修正 BitNet-2B-INT8/config.json (vLLM 兼容)
[SUCCESS]   ✓ 已删除 60 个不兼容权重 (ffn_sub_norm, attn_sub_norm)

------------------------------------------------------------
  Step 2: 量化为 BitNet-2B-FP8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-BF16 --bitnet --output-dtype fp8_e4m3 --Z 2 --L 2 --skip-slide
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/root/vllmbench/slidesparse/utils.py:2928: UserWarning: L=2 < 4，这是纯量化模式（无稀疏），slide 操作将被跳过
  warnings.warn(
======================================================================
Processing: BitNet-2B-BF16
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=2, N=1, expand=0.000)
[INFO] Mode: magnitude
[INFO] BitNet Mode: enabled (output_dtype=fp8_e4m3)
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2

[INFO] Copying non-weight files...
[INFO]   Copied: tokenizer.json, special_tokens_map.json, tokenizer_config.json, LICENSE, config.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.20%)
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.11%)
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.86%)
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.70%)
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.95%)
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (30.11%)
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.73%)
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (34.84%)
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (48.35%)
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (48.39%)
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.94%)
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.03%)
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.16%)
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.12%)
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.94%)
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.50%)
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.07%)
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (30.23%)
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.13%)
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.83%)
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.66%)
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.92%)
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.62%)
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.08%)
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (24.26%)
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.88%)
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.42%)
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.01%)
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.84%)
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.71%)
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.80%)
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (28.71%)
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.19%)
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.88%)
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.76%)
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.23%)
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.39%)
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.59%)
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.83%)
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.56%)
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.50%)
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.27%)
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.43%)
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.34%)
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.94%)
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (32.82%)
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.05%)
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.77%)
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (11.51%)
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.69%)
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.65%)
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.42%)
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.66%)
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.11%)
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (9.15%)
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.99%)
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.84%)
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.18%)
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.87%)
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.35%)
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.09%)
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (10.66%)
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.45%)
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.72%)
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.49%)
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (31.51%)
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (11.87%)
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (24.63%)
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.37%)
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.68%)
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.87%)
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.57%)
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.39%)
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.39%)
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.62%)
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (9.69%)
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.53%)
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.64%)
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.42%)
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.76%)
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.46%)
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.77%)
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.25%)
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.50%)
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (35.59%)
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (35.75%)
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.00%)
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.57%)
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.80%)
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.10%)
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.53%)
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.29%)
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.71%)
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (31.97%)
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.26%)
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.80%)
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.48%)
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.43%)
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.31%)
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.73%)
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.93%)
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.33%)
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.75%)
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.57%)
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.56%)
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.30%)
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.90%)
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.96%)
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.52%)
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.68%)
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (10.90%)
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.93%)
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.02%)
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.00%)
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (26.12%)
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.42%)
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.67%)
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.05%)
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.28%)
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.27%)
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.51%)
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.09%)
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.74%)
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (26.75%)
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.67%)
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.08%)
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.17%)
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.72%)
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.73%)
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.89%)
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.86%)
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.68%)
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (13.43%)
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.59%)
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.71%)
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.87%)
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.93%)
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.90%)
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (13.77%)
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (13.64%)
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.03%)
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.46%)
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.20%)
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.79%)
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (11.23%)
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.35%)
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.77%)
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.66%)
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.38%)
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.80%)
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.11%)
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.23%)
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.79%)
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.63%)
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.30%)
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.55%)
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (31.95%)
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.35%)
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (24.72%)
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.33%)
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.01%)
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.91%)
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.39%)
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.90%)
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.76%)
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.22%)
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.48%)
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.07%)
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.66%)
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.86%)
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.77%)
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.19%)
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.28%)
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.58%)
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.12%)
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.42%)
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.40%)
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.35%)
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.08%)
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.80%)
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.22%)
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.38%)
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.52%)
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.81%)
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.55%)
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.32%)
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.18%)
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.32%)
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.95%)
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.81%)
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.26%)
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.76%)
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.84%)
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (13.07%)
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.28%)
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.65%)
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.41%)
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.82%)
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.87%)
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.70%)
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.07%)
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.12%)
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.52%)
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.18%)
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.47%)
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.99%)
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.45%)

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 122 layers
[INFO] Time: 9.82s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2/conversion_report.json
[SUCCESS] BitNet-2B-FP8 量化完成并移动到 /root/vllmbench/checkpoints/BitNet-2B-FP8 (13.2s)
[SUCCESS]   ✓ 已修正 BitNet-2B-FP8/config.json (vLLM 兼容)
[SUCCESS]   ✓ 已删除 60 个不兼容权重 (ffn_sub_norm, attn_sub_norm)

[INFO] 基础模型准备统计: 成功 2, 失败 0

------------------------------------------------------------
  验证基础模型
------------------------------------------------------------
[SUCCESS]   ✓ BitNet-2B-INT8
[SUCCESS]   ✓ BitNet-2B-FP8

----------------------------------------------------------------------
TASK 1: 基础模型准备 (下载 + 量化) - SUCCESS
Duration: 74.3 seconds (1.2 minutes)
----------------------------------------------------------------------


======================================================================
TASK 2: SlideSparse 转换 (prune + slide)
Started: 2026-01-27 23:51:19
======================================================================


------------------------------------------------------------
  转换: BitNet-2B-INT8 -> SlideSparse-2_4
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-INT8 --Z 2 --L 4
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

======================================================================
Processing: BitNet-2B-INT8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=4, N=2, expand=1.000)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4

[INFO] Copying non-weight files...
[INFO]   Copied: tokenizer.json, conversion_report.json, special_tokens_map.json, tokenizer_config.json, slidesparse_config.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 46.33s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4/conversion_report.json
[SUCCESS] BitNet-2B-INT8-SlideSparse-2_4 转换完成 (49.8s)

------------------------------------------------------------
  转换: BitNet-2B-INT8 -> SlideSparse-2_6
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-INT8 --Z 2 --L 6
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

======================================================================
Processing: BitNet-2B-INT8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=6, N=3, expand=1.333)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6

[INFO] Copying non-weight files...
[INFO]   Copied: tokenizer.json, conversion_report.json, special_tokens_map.json, tokenizer_config.json, slidesparse_config.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 52.68s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6/conversion_report.json
[SUCCESS] BitNet-2B-INT8-SlideSparse-2_6 转换完成 (56.1s)

------------------------------------------------------------
  转换: BitNet-2B-INT8 -> SlideSparse-2_8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-INT8 --Z 2 --L 8
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

======================================================================
Processing: BitNet-2B-INT8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=8, N=4, expand=1.500)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8

[INFO] Copying non-weight files...
[INFO]   Copied: tokenizer.json, conversion_report.json, special_tokens_map.json, tokenizer_config.json, slidesparse_config.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 48.19s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8/conversion_report.json
[SUCCESS] BitNet-2B-INT8-SlideSparse-2_8 转换完成 (51.6s)

------------------------------------------------------------
  转换: BitNet-2B-INT8 -> SlideSparse-2_10
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-INT8 --Z 2 --L 10
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

======================================================================
Processing: BitNet-2B-INT8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=10, N=5, expand=1.600)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10

[INFO] Copying non-weight files...
[INFO]   Copied: tokenizer.json, conversion_report.json, special_tokens_map.json, tokenizer_config.json, slidesparse_config.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 48.27s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10/conversion_report.json
[SUCCESS] BitNet-2B-INT8-SlideSparse-2_10 转换完成 (51.7s)

------------------------------------------------------------
  转换: BitNet-2B-FP8 -> SlideSparse-2_4
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-FP8 --Z 2 --L 4
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

======================================================================
Processing: BitNet-2B-FP8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=4, N=2, expand=1.000)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4

[INFO] Copying non-weight files...
[INFO]   Copied: tokenizer.json, conversion_report.json, special_tokens_map.json, tokenizer_config.json, slidesparse_config.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 46.44s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4/conversion_report.json
[SUCCESS] BitNet-2B-FP8-SlideSparse-2_4 转换完成 (50.0s)

------------------------------------------------------------
  转换: BitNet-2B-FP8 -> SlideSparse-2_6
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-FP8 --Z 2 --L 6
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

======================================================================
Processing: BitNet-2B-FP8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=6, N=3, expand=1.333)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6

[INFO] Copying non-weight files...
[INFO]   Copied: tokenizer.json, conversion_report.json, special_tokens_map.json, tokenizer_config.json, slidesparse_config.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 49.44s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6/conversion_report.json
[SUCCESS] BitNet-2B-FP8-SlideSparse-2_6 转换完成 (52.9s)

------------------------------------------------------------
  转换: BitNet-2B-FP8 -> SlideSparse-2_8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-FP8 --Z 2 --L 8
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

======================================================================
Processing: BitNet-2B-FP8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=8, N=4, expand=1.500)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8

[INFO] Copying non-weight files...
[INFO]   Copied: tokenizer.json, conversion_report.json, special_tokens_map.json, tokenizer_config.json, slidesparse_config.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 47.82s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8/conversion_report.json
[SUCCESS] BitNet-2B-FP8-SlideSparse-2_8 转换完成 (51.2s)

------------------------------------------------------------
  转换: BitNet-2B-FP8 -> SlideSparse-2_10
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-FP8 --Z 2 --L 10
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

======================================================================
Processing: BitNet-2B-FP8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=10, N=5, expand=1.600)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10

[INFO] Copying non-weight files...
[INFO]   Copied: tokenizer.json, conversion_report.json, special_tokens_map.json, tokenizer_config.json, slidesparse_config.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 46.52s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10/conversion_report.json
[SUCCESS] BitNet-2B-FP8-SlideSparse-2_10 转换完成 (50.0s)

[INFO] SlideSparse 转换统计: 成功 8, 跳过 0, 失败 0

------------------------------------------------------------
  验证 SlideSparse 模型
------------------------------------------------------------
[SUCCESS]   ✓ BitNet-2B-INT8-SlideSparse-2_4
[SUCCESS]   ✓ BitNet-2B-INT8-SlideSparse-2_6
[SUCCESS]   ✓ BitNet-2B-INT8-SlideSparse-2_8
[SUCCESS]   ✓ BitNet-2B-INT8-SlideSparse-2_10
[SUCCESS]   ✓ BitNet-2B-FP8-SlideSparse-2_4
[SUCCESS]   ✓ BitNet-2B-FP8-SlideSparse-2_6
[SUCCESS]   ✓ BitNet-2B-FP8-SlideSparse-2_8
[SUCCESS]   ✓ BitNet-2B-FP8-SlideSparse-2_10

----------------------------------------------------------------------
TASK 2: SlideSparse 转换 (prune + slide) - SUCCESS
Duration: 413.3 seconds (6.9 minutes)
----------------------------------------------------------------------


======================================================================
TASK 3: 离线调优 (粗调优 + 细调优)
Started: 2026-01-27 23:58:12
======================================================================


------------------------------------------------------------
  粗调优: cuBLASLt + Triton quant_only
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/offline_autotune_algsearch.py --model BitNet-2B --dtype all --m_list 256,1024,4096,16384,32768 --Lmax 10 --warmup 25 --repeat 50 --kernels 1,0,0,0,1


============================================================
  SlideSparse 统一离线调优
============================================================

  GPU:           NVIDIA GeForce RTX 5080 (cc120)
  Python:        py312
  CUDA:          cu129
  Arch:          x86_64

  数据类型:      ['int8', 'fp8']
  输出类型:      bf16
  高精度累加:    否
  模型 (base):   ['BitNet-2B']
  Lmax:          10
  M-quick:       否
  M 列表:        [256, 1024, 4096, 16384, 32768]
  Warmup/Repeat: 25/50

  Kernel 调优:
    ✓ cuBLASLt GEMM
    ✗ cuSPARSELt GEMM
    ✗ Triton Dequant + Bias
    ✗ Triton Quant + Slide
    ✓ Triton Quant Only

============================================================
  Step 0: 编译 CUDA 扩展
============================================================


------------------------------------------------------------
  编译 cublaslt
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/cublaslt_gemm/build_cublaslt.py build --force
[SUCCESS] cublaslt 编译成功

------------------------------------------------------------
  编译 cusparselt
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/cusparselt_gemm/build_cusparselt.py build --force
[SUCCESS] cusparselt 编译成功

------------------------------------------------------------
  编译 compress
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/build_compress.py build --force
[SUCCESS] compress 编译成功

============================================================
  Step 1: cuBLASLt GEMM
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] dtype=int8, outdtype=int32
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/search/cuBLASLt_AlgSearch/alg_search.py --dtype int8 --outdtype int32 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --compile --Lmax 10 --m_list 256,1024,4096,16384,32768
[SUCCESS] cuBLASLt GEMM (int8) 完成
[INFO] dtype=fp8e4m3, outdtype=bf16
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/search/cuBLASLt_AlgSearch/alg_search.py --dtype fp8e4m3 --outdtype bf16 --model BitNet-2B-FP8 --warmup 25 --repeat 50 --compile --Lmax 10 --m_list 256,1024,4096,16384,32768
[SUCCESS] cuBLASLt GEMM (fp8) 完成

============================================================
  Step 2: cuSPARSELt GEMM [跳过]
============================================================


============================================================
  Step 3: Triton Dequant + Bias [跳过]
============================================================


============================================================
  Step 4: Triton Quant + Slide [跳过]
============================================================


============================================================
  Step 5: Triton Quant Only
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/quant_only_triton/autotune_autogen_quant_only.py --model BitNet-2B-INT8 --warmup 25 --repeat 50 --Lmax 10 --m_list 256,1024,4096,16384,32768
[SUCCESS] Triton Quant Only 完成

============================================================
  调优总结
============================================================

  cuBLASLt GEMM: [全部成功] (2/2)
  cuSPARSELt GEMM: [跳过]
  Triton Dequant + Bias: [跳过]
  Triton Quant + Slide: [跳过]
  Triton Quant Only: [全部成功] (1/1)

总计: 成功 3, 失败 0, 跳过 3
[SUCCESS] 粗调优完成 (208.1s)

------------------------------------------------------------
  细调优: cuSPARSELt + Triton Dequant/QuantSlide
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/offline_autotune_algsearch.py --model BitNet-2B --dtype all --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768 --Lmax 10 --warmup 25 --repeat 50 --kernels 0,1,1,1,0


============================================================
  SlideSparse 统一离线调优
============================================================

  GPU:           NVIDIA GeForce RTX 5080 (cc120)
  Python:        py312
  CUDA:          cu129
  Arch:          x86_64

  数据类型:      ['int8', 'fp8']
  输出类型:      bf16
  高精度累加:    否
  模型 (base):   ['BitNet-2B']
  Lmax:          10
  M-quick:       否
  M 列表:        [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]
  Warmup/Repeat: 25/50

  Kernel 调优:
    ✗ cuBLASLt GEMM
    ✓ cuSPARSELt GEMM
    ✓ Triton Dequant + Bias
    ✓ Triton Quant + Slide
    ✗ Triton Quant Only

============================================================
  Step 0: 编译 CUDA 扩展
============================================================


------------------------------------------------------------
  编译 cublaslt
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/cublaslt_gemm/build_cublaslt.py build --force
[SUCCESS] cublaslt 编译成功

------------------------------------------------------------
  编译 cusparselt
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/cusparselt_gemm/build_cusparselt.py build --force
[SUCCESS] cusparselt 编译成功

------------------------------------------------------------
  编译 compress
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/build_compress.py build --force
[SUCCESS] compress 编译成功

============================================================
  Step 1: cuBLASLt GEMM [跳过]
============================================================


============================================================
  Step 2: cuSPARSELt GEMM
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] dtype=int8, outdtype=bf16
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/search/cuSPARSELt_AlgSearch/alg_search.py --dtype int8 --outdtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --compile --Lmax 10 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768
[SUCCESS] cuSPARSELt GEMM (int8) 完成
[INFO] dtype=fp8e4m3, outdtype=bf16
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/search/cuSPARSELt_AlgSearch/alg_search.py --dtype fp8e4m3 --outdtype bf16 --model BitNet-2B-FP8 --warmup 25 --repeat 50 --compile --Lmax 10 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768
[SUCCESS] cuSPARSELt GEMM (fp8) 完成

============================================================
  Step 3: Triton Dequant + Bias
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/autotune_autogen_dequant_bias.py --model BitNet-2B-INT8 --warmup 25 --repeat 50 --Lmax 10 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768
[SUCCESS] Triton Dequant + Bias 完成

============================================================
  Step 4: Triton Quant + Slide
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/autotune_autogen_quant_slide.py --model BitNet-2B-INT8 --warmup 25 --repeat 50 --Lmax 10 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768
[SUCCESS] Triton Quant + Slide 完成

============================================================
  Step 5: Triton Quant Only [跳过]
============================================================


============================================================
  调优总结
============================================================

  cuBLASLt GEMM: [跳过]
  cuSPARSELt GEMM: [全部成功] (2/2)
  Triton Dequant + Bias: [全部成功] (1/1)
  Triton Quant + Slide: [全部成功] (1/1)
  Triton Quant Only: [跳过]

总计: 成功 4, 失败 0, 跳过 2
[SUCCESS] 细调优完成 (375.7s)

----------------------------------------------------------------------
TASK 3: 离线调优 (粗调优 + 细调优) - SUCCESS
Duration: 583.8 seconds (9.7 minutes)
----------------------------------------------------------------------


======================================================================
TASK 4: 完整 Prefill Benchmark
Started: 2026-01-28 00:07:56
======================================================================


------------------------------------------------------------
  Prefill Benchmark: bitnet1.58-2b-int8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-int8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA GeForce RTX 5080                   ││
│ GPU (short):      RTX5080                                   │
│ Memory:           15.5 GB                                    │
│ CC:               cc120 (Blackwell)                            │
│ SM Code:          sm_120                                    │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.95.05                                 │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-int8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['prefill']
  M_prefill:        [512, 1024, 2048, 4096, 8192, 16384, 32768]
  M_decode:         [512, 1024, 2048, 4096, 8192, 16384, 32768]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_000759.log


============================================================
  BitNet-2B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:08:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3456227) WARNING 01-28 00:08:13 [backends.py:609] Failed to read file <frozen os>
Throughput: 52.29 requests/s, 26822.78 total tokens/s, 52.29 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:08:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:08:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:08:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:08:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:08:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:08:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:08:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:08:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:08:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:08:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:08:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:08:07] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:08:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:07] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:07] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:08:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:08:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:08:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:08:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:08:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3456227) [2026-01-28 00:08:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3456227) [2026-01-28 00:08:08] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3456227) [2026-01-28 00:08:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3456227) [2026-01-28 00:08:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3456227) [2026-01-28 00:08:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3456227) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3456227) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.31it/s]
(EngineCore_DP0 pid=3456227) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.31it/s]
(EngineCore_DP0 pid=3456227) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3456227) 2026-01-28 00:08:21,583 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3456227) 2026-01-28 00:08:21,606 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3456227) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.42it/s]
(EngineCore_DP0 pid=3456227) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.59it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.59it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1282.63it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:16,  7.53it/s, est. speed input: 3853.79 toks/s, output: 7.53 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 33.72it/s, est. speed input: 15023.30 toks/s, output: 29.34 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:02, 43.39it/s, est. speed input: 19246.63 toks/s, output: 37.59 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 48.11it/s, est. speed input: 21444.31 toks/s, output: 41.88 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 50.76it/s, est. speed input: 22793.49 toks/s, output: 44.52 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 52.86it/s, est. speed input: 23809.39 toks/s, output: 46.50 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 54.17it/s, est. speed input: 24538.52 toks/s, output: 47.93 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 55.63it/s, est. speed input: 25193.78 toks/s, output: 49.21 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 55.99it/s, est. speed input: 25613.23 toks/s, output: 50.03 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 56.09it/s, est. speed input: 25928.53 toks/s, output: 50.64 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 56.56it/s, est. speed input: 26242.06 toks/s, output: 51.25 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 56.97it/s, est. speed input: 26515.68 toks/s, output: 51.79 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 57.32it/s, est. speed input: 26755.15 toks/s, output: 52.26 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:00, 57.39it/s, est. speed input: 26944.22 toks/s, output: 52.63 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 57.12it/s, est. speed input: 27075.47 toks/s, output: 52.88 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 57.19it/s, est. speed input: 27214.73 toks/s, output: 53.15 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 57.69it/s, est. speed input: 27379.51 toks/s, output: 53.48 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:01<00:00, 58.02it/s, est. speed input: 27525.30 toks/s, output: 53.76 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 57.84it/s, est. speed input: 27622.10 toks/s, output: 53.95 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 57.84it/s, est. speed input: 27719.57 toks/s, output: 54.14 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 57.80it/s, est. speed input: 27804.89 toks/s, output: 54.31 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 58.05it/s, est. speed input: 27902.16 toks/s, output: 54.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 58.05it/s, est. speed input: 27916.52 toks/s, output: 54.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 54.52it/s, est. speed input: 27916.52 toks/s, output: 54.52 toks/s]
[rank0]:[W128 00:08:25.711834821 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 27.4s

测试结果:
  Requests/s:   52.29
  Tokens/s:     26822.78
  Total Reqs:   128
  Elapsed:      2.45s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     26770.50

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:08:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3457167) WARNING 01-28 00:08:39 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.32 requests/s, 43379.66 total tokens/s, 42.32 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:08:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:08:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:08:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:08:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:08:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:08:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:08:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:08:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:08:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:08:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:08:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:08:34] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:08:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:34] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:34] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:08:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:08:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:08:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:08:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:08:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3457167) [2026-01-28 00:08:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3457167) [2026-01-28 00:08:35] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3457167) [2026-01-28 00:08:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3457167) [2026-01-28 00:08:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3457167) [2026-01-28 00:08:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3457167) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3457167) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.88it/s]
(EngineCore_DP0 pid=3457167) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.87it/s]
(EngineCore_DP0 pid=3457167) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3457167) 2026-01-28 00:08:46,422 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3457167) 2026-01-28 00:08:46,440 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3457167) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 23.01it/s]
(EngineCore_DP0 pid=3457167) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.19it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  51%|█████     | 65/128 [00:00<00:00, 641.74it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 689.07it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:01, 89.57it/s, est. speed input: 91727.33 toks/s, output: 89.57 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:01, 55.95it/s, est. speed input: 60904.65 toks/s, output: 59.48 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 50.34it/s, est. speed input: 55377.27 toks/s, output: 54.08 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 47.80it/s, est. speed input: 52868.42 toks/s, output: 51.63 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 46.43it/s, est. speed input: 51508.26 toks/s, output: 50.30 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 45.49it/s, est. speed input: 50529.71 toks/s, output: 49.34 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 44.85it/s, est. speed input: 49795.20 toks/s, output: 48.63 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 44.40it/s, est. speed input: 49216.86 toks/s, output: 48.06 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 44.09it/s, est. speed input: 48753.97 toks/s, output: 47.61 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 43.90it/s, est. speed input: 48380.85 toks/s, output: 47.25 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 43.74it/s, est. speed input: 48060.63 toks/s, output: 46.93 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 43.64it/s, est. speed input: 47790.40 toks/s, output: 46.67 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 43.50it/s, est. speed input: 47541.35 toks/s, output: 46.43 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 43.46it/s, est. speed input: 47335.71 toks/s, output: 46.23 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 43.38it/s, est. speed input: 47146.32 toks/s, output: 46.04 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 43.35it/s, est. speed input: 46982.42 toks/s, output: 45.88 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 43.35it/s, est. speed input: 46841.62 toks/s, output: 45.74 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 43.31it/s, est. speed input: 46708.97 toks/s, output: 45.61 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 43.32it/s, est. speed input: 46593.92 toks/s, output: 45.50 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 43.30it/s, est. speed input: 46486.75 toks/s, output: 45.40 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 43.26it/s, est. speed input: 46383.82 toks/s, output: 45.30 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 43.26it/s, est. speed input: 46294.02 toks/s, output: 45.21 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 43.21it/s, est. speed input: 46204.86 toks/s, output: 45.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.21it/s, est. speed input: 46192.91 toks/s, output: 45.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.11it/s, est. speed input: 46192.91 toks/s, output: 45.11 toks/s]
[rank0]:[W128 00:08:50.589140216 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.0s

测试结果:
  Requests/s:   42.32
  Tokens/s:     43379.66
  Total Reqs:   128
  Elapsed:      3.02s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     43337.34

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:08:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3457838) WARNING 01-28 00:09:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 44.95 requests/s, 46077.79 total tokens/s, 44.95 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 00:08:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:08:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:08:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:08:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:08:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:08:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:08:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:08:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:08:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:08:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:09:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:09:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:09:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:09:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:09:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:09:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:09:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:09:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:09:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3457838) [2026-01-28 00:09:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3457838) [2026-01-28 00:09:01] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3457838) [2026-01-28 00:09:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3457838) [2026-01-28 00:09:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3457838) [2026-01-28 00:09:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3457838) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3457838) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.91it/s]
(EngineCore_DP0 pid=3457838) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.91it/s]
(EngineCore_DP0 pid=3457838) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3457838) 2026-01-28 00:09:12,272 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3457838) 2026-01-28 00:09:12,292 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3457838) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 21.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 21.26it/s]
(EngineCore_DP0 pid=3457838) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.38it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  26%|██▌       | 66/256 [00:00<00:00, 656.18it/s]
Adding requests:  55%|█████▌    | 141/256 [00:00<00:00, 704.46it/s]
Adding requests:  84%|████████▎ | 214/256 [00:00<00:00, 715.19it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 713.20it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:01, 151.50it/s, est. speed input: 155144.76 toks/s, output: 151.50 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:00<00:03, 65.51it/s, est. speed input: 73731.00 toks/s, output: 72.00 toks/s]   
Processed prompts:  17%|█▋        | 43/256 [00:00<00:03, 60.36it/s, est. speed input: 68011.75 toks/s, output: 66.42 toks/s]
Processed prompts:  20%|█▉        | 51/256 [00:00<00:03, 55.28it/s, est. speed input: 63453.91 toks/s, output: 61.97 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:00<00:03, 50.16it/s, est. speed input: 59442.50 toks/s, output: 58.05 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:01<00:03, 48.97it/s, est. speed input: 57957.52 toks/s, output: 56.60 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:01<00:03, 48.05it/s, est. speed input: 56779.39 toks/s, output: 55.45 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:01<00:03, 47.33it/s, est. speed input: 55814.99 toks/s, output: 54.51 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:03, 46.75it/s, est. speed input: 54994.84 toks/s, output: 53.71 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:01<00:03, 46.37it/s, est. speed input: 54321.94 toks/s, output: 53.05 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:01<00:03, 46.10it/s, est. speed input: 53748.87 toks/s, output: 52.49 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:01<00:03, 45.92it/s, est. speed input: 53255.88 toks/s, output: 52.01 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:03, 45.84it/s, est. speed input: 52840.31 toks/s, output: 51.60 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:03, 45.70it/s, est. speed input: 52453.05 toks/s, output: 51.22 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 45.64it/s, est. speed input: 52119.87 toks/s, output: 50.90 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:02<00:02, 45.52it/s, est. speed input: 51806.43 toks/s, output: 50.59 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:02<00:02, 45.54it/s, est. speed input: 51544.37 toks/s, output: 50.34 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:02<00:02, 45.54it/s, est. speed input: 51307.17 toks/s, output: 50.10 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:02<00:02, 45.54it/s, est. speed input: 51090.71 toks/s, output: 49.89 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:02<00:02, 45.47it/s, est. speed input: 50882.50 toks/s, output: 49.69 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 45.43it/s, est. speed input: 50693.12 toks/s, output: 49.50 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:03<00:02, 45.44it/s, est. speed input: 50523.53 toks/s, output: 49.34 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:03<00:01, 45.49it/s, est. speed input: 50374.97 toks/s, output: 49.19 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:03<00:01, 45.45it/s, est. speed input: 50227.34 toks/s, output: 49.05 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:03<00:01, 45.47it/s, est. speed input: 50095.25 toks/s, output: 48.92 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:03<00:01, 45.49it/s, est. speed input: 49974.44 toks/s, output: 48.80 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:03<00:01, 45.46it/s, est. speed input: 49856.49 toks/s, output: 48.69 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:04<00:01, 45.45it/s, est. speed input: 49746.48 toks/s, output: 48.58 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:04<00:01, 45.46it/s, est. speed input: 49646.56 toks/s, output: 48.48 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:04<00:01, 45.43it/s, est. speed input: 49548.07 toks/s, output: 48.39 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:04<00:00, 45.43it/s, est. speed input: 49458.23 toks/s, output: 48.30 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:04<00:00, 45.35it/s, est. speed input: 49364.89 toks/s, output: 48.21 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:04<00:00, 45.38it/s, est. speed input: 49285.29 toks/s, output: 48.13 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:04<00:00, 45.42it/s, est. speed input: 49212.29 toks/s, output: 48.06 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:04<00:00, 45.45it/s, est. speed input: 49143.08 toks/s, output: 47.99 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:05<00:00, 45.48it/s, est. speed input: 49078.39 toks/s, output: 47.93 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:05<00:00, 45.47it/s, est. speed input: 49014.16 toks/s, output: 47.87 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 47.76it/s, est. speed input: 49147.20 toks/s, output: 48.00 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 47.76it/s, est. speed input: 49147.20 toks/s, output: 48.00 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 47.99it/s, est. speed input: 49147.20 toks/s, output: 48.00 toks/s]
[rank0]:[W128 00:09:19.243638553 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 28.6s

测试结果:
  Requests/s:   44.95
  Tokens/s:     46077.79
  Total Reqs:   256
  Elapsed:      5.69s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     46032.84

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:09:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3458709) WARNING 01-28 00:09:35 [backends.py:609] Failed to read file <frozen os>
Throughput: 43.79 requests/s, 44882.06 total tokens/s, 43.79 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 00:09:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:09:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:09:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:09:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:09:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:09:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:09:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:09:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:09:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:09:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:09:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:09:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:09:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:09:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:09:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:09:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:09:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:09:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:09:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3458709) [2026-01-28 00:09:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3458709) [2026-01-28 00:09:30] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3458709) [2026-01-28 00:09:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3458709) [2026-01-28 00:09:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3458709) [2026-01-28 00:09:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3458709) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3458709) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.95it/s]
(EngineCore_DP0 pid=3458709) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.95it/s]
(EngineCore_DP0 pid=3458709) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3458709) 2026-01-28 00:09:41,726 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3458709) 2026-01-28 00:09:41,743 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3458709) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 17.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 16.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 16.52it/s]
(EngineCore_DP0 pid=3458709) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  6.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 11.46it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 68/512 [00:00<00:00, 670.97it/s]
Adding requests:  28%|██▊       | 144/512 [00:00<00:00, 717.25it/s]
Adding requests:  43%|████▎     | 218/512 [00:00<00:00, 726.96it/s]
Adding requests:  58%|█████▊    | 295/512 [00:00<00:00, 740.90it/s]
Adding requests:  72%|███████▏  | 371/512 [00:00<00:00, 744.44it/s]
Adding requests:  87%|████████▋ | 447/512 [00:00<00:00, 746.51it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 737.26it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 30/512 [00:00<00:01, 259.40it/s, est. speed input: 265658.22 toks/s, output: 259.41 toks/s]
Processed prompts:  11%|█         | 56/512 [00:00<00:06, 75.12it/s, est. speed input: 86835.34 toks/s, output: 84.80 toks/s]   
Processed prompts:  14%|█▎        | 70/512 [00:01<00:07, 58.52it/s, est. speed input: 70015.30 toks/s, output: 68.37 toks/s]
Processed prompts:  16%|█▌        | 80/512 [00:01<00:07, 57.64it/s, est. speed input: 67937.29 toks/s, output: 66.34 toks/s]
Processed prompts:  17%|█▋        | 88/512 [00:01<00:07, 54.16it/s, est. speed input: 64971.19 toks/s, output: 63.45 toks/s]
Processed prompts:  19%|█▊        | 95/512 [00:01<00:08, 49.91it/s, est. speed input: 61999.78 toks/s, output: 60.55 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:01<00:08, 46.75it/s, est. speed input: 59666.41 toks/s, output: 58.27 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:01<00:08, 46.02it/s, est. speed input: 58312.51 toks/s, output: 56.95 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:02<00:08, 45.43it/s, est. speed input: 57171.89 toks/s, output: 55.83 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:02<00:08, 45.00it/s, est. speed input: 56206.89 toks/s, output: 54.89 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:02<00:08, 44.70it/s, est. speed input: 55386.38 toks/s, output: 54.09 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:02<00:08, 44.52it/s, est. speed input: 54688.64 toks/s, output: 53.41 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:02<00:08, 44.41it/s, est. speed input: 54082.83 toks/s, output: 52.81 toks/s]
Processed prompts:  31%|███       | 158/512 [00:03<00:07, 44.27it/s, est. speed input: 53536.42 toks/s, output: 52.28 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:03<00:07, 44.20it/s, est. speed input: 53057.69 toks/s, output: 51.81 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:03<00:07, 44.16it/s, est. speed input: 52631.04 toks/s, output: 51.40 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:03<00:07, 44.11it/s, est. speed input: 52244.08 toks/s, output: 51.02 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:03<00:07, 44.13it/s, est. speed input: 51905.39 toks/s, output: 50.69 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:03<00:07, 44.14it/s, est. speed input: 51597.94 toks/s, output: 50.39 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:06, 44.08it/s, est. speed input: 51306.07 toks/s, output: 50.10 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:04<00:06, 44.05it/s, est. speed input: 51040.03 toks/s, output: 49.84 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:04<00:06, 44.00it/s, est. speed input: 50791.50 toks/s, output: 49.60 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:04<00:06, 43.98it/s, est. speed input: 50564.97 toks/s, output: 49.38 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:04<00:06, 44.00it/s, est. speed input: 50359.14 toks/s, output: 49.18 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:06, 44.01it/s, est. speed input: 50168.86 toks/s, output: 48.99 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:05, 43.99it/s, est. speed input: 49988.53 toks/s, output: 48.82 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:05<00:05, 44.03it/s, est. speed input: 49826.47 toks/s, output: 48.66 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:05<00:05, 44.00it/s, est. speed input: 49667.60 toks/s, output: 48.50 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:05<00:05, 44.04it/s, est. speed input: 49526.94 toks/s, output: 48.37 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:05<00:05, 44.07it/s, est. speed input: 49394.46 toks/s, output: 48.24 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:06<00:04, 44.09it/s, est. speed input: 49269.45 toks/s, output: 48.11 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:06<00:04, 44.05it/s, est. speed input: 49147.09 toks/s, output: 48.00 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:06<00:04, 44.01it/s, est. speed input: 49029.01 toks/s, output: 47.88 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:06<00:04, 43.98it/s, est. speed input: 48918.41 toks/s, output: 47.77 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:06<00:04, 44.00it/s, est. speed input: 48816.83 toks/s, output: 47.67 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:07<00:04, 44.02it/s, est. speed input: 48720.93 toks/s, output: 47.58 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:07<00:03, 44.00it/s, est. speed input: 48626.93 toks/s, output: 47.49 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:07<00:03, 44.01it/s, est. speed input: 48540.18 toks/s, output: 47.40 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:07<00:03, 43.98it/s, est. speed input: 48453.87 toks/s, output: 47.32 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:07<00:03, 43.97it/s, est. speed input: 48373.19 toks/s, output: 47.24 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:07<00:03, 44.01it/s, est. speed input: 48299.51 toks/s, output: 47.17 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:08<00:02, 44.03it/s, est. speed input: 48228.30 toks/s, output: 47.10 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:08<00:02, 44.01it/s, est. speed input: 48158.04 toks/s, output: 47.03 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:08<00:02, 43.98it/s, est. speed input: 48089.81 toks/s, output: 46.96 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:08<00:02, 44.00it/s, est. speed input: 48026.72 toks/s, output: 46.90 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:08<00:02, 44.00it/s, est. speed input: 47965.89 toks/s, output: 46.84 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:09<00:02, 44.03it/s, est. speed input: 47909.38 toks/s, output: 46.79 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:09<00:01, 44.02it/s, est. speed input: 47853.04 toks/s, output: 46.73 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:09<00:01, 44.02it/s, est. speed input: 47798.89 toks/s, output: 46.68 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:09<00:01, 43.99it/s, est. speed input: 47745.01 toks/s, output: 46.63 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:09<00:01, 44.01it/s, est. speed input: 47696.26 toks/s, output: 46.58 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:09<00:01, 44.02it/s, est. speed input: 47648.48 toks/s, output: 46.53 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:10<00:00, 44.00it/s, est. speed input: 47601.39 toks/s, output: 46.49 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:10<00:00, 43.97it/s, est. speed input: 47554.53 toks/s, output: 46.44 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:10<00:00, 43.97it/s, est. speed input: 47510.70 toks/s, output: 46.40 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:10<00:00, 43.94it/s, est. speed input: 47466.30 toks/s, output: 46.35 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:10<00:00, 43.95it/s, est. speed input: 47425.62 toks/s, output: 46.31 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:10<00:00, 45.82it/s, est. speed input: 47492.20 toks/s, output: 46.38 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 45.82it/s, est. speed input: 47677.30 toks/s, output: 46.56 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 46.56it/s, est. speed input: 47677.30 toks/s, output: 46.56 toks/s]
[rank0]:[W128 00:09:55.845952786 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.6s

测试结果:
  Requests/s:   43.79
  Tokens/s:     44882.06
  Total Reqs:   512
  Elapsed:      11.69s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     44838.27

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:10:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3459635) WARNING 01-28 00:10:11 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.46 requests/s, 43522.98 total tokens/s, 42.46 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 00:10:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:10:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:10:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:10:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:10:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:10:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:10:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:10:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:10:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:10:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:10:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:10:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:10:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:10:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:10:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:10:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:10:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:10:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3459635) [2026-01-28 00:10:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3459635) [2026-01-28 00:10:07] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3459635) [2026-01-28 00:10:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3459635) [2026-01-28 00:10:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3459635) [2026-01-28 00:10:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3459635) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3459635) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.02it/s]
(EngineCore_DP0 pid=3459635) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.02it/s]
(EngineCore_DP0 pid=3459635) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3459635) 2026-01-28 00:10:18,699 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3459635) 2026-01-28 00:10:18,716 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3459635) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 14.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 13.31it/s]
(EngineCore_DP0 pid=3459635) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.76it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 72/1024 [00:00<00:01, 712.35it/s]
Adding requests:  15%|█▍        | 149/1024 [00:00<00:01, 745.55it/s]
Adding requests:  22%|██▏       | 226/1024 [00:00<00:01, 752.00it/s]
Adding requests:  29%|██▉       | 302/1024 [00:00<00:00, 741.42it/s]
Adding requests:  37%|███▋      | 377/1024 [00:00<00:00, 743.03it/s]
Adding requests:  44%|████▍     | 452/1024 [00:00<00:00, 743.45it/s]
Adding requests:  51%|█████▏    | 527/1024 [00:00<00:00, 733.57it/s]
Adding requests:  59%|█████▊    | 601/1024 [00:00<00:00, 730.71it/s]
Adding requests:  66%|██████▋   | 679/1024 [00:00<00:00, 744.67it/s]
Adding requests:  74%|███████▍  | 757/1024 [00:01<00:00, 753.38it/s]
Adding requests:  81%|████████▏ | 833/1024 [00:01<00:00, 733.77it/s]
Adding requests:  89%|████████▉ | 911/1024 [00:01<00:00, 747.37it/s]
Adding requests:  97%|█████████▋| 989/1024 [00:01<00:00, 755.97it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 746.28it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:00<00:03, 274.82it/s, est. speed input: 281436.42 toks/s, output: 274.83 toks/s]
Processed prompts:   8%|▊         | 86/1024 [00:00<00:09, 96.59it/s, est. speed input: 113851.33 toks/s, output: 111.18 toks/s] 
Processed prompts:  10%|▉         | 101/1024 [00:01<00:12, 72.26it/s, est. speed input: 90055.67 toks/s, output: 87.94 toks/s] 
Processed prompts:  11%|█         | 111/1024 [00:01<00:13, 67.78it/s, est. speed input: 85056.87 toks/s, output: 83.06 toks/s]
Processed prompts:  12%|█▏        | 119/1024 [00:01<00:14, 61.48it/s, est. speed input: 79974.16 toks/s, output: 78.10 toks/s]
Processed prompts:  12%|█▏        | 126/1024 [00:01<00:16, 55.07it/s, est. speed input: 75382.57 toks/s, output: 73.62 toks/s]
Processed prompts:  13%|█▎        | 132/1024 [00:01<00:18, 48.66it/s, est. speed input: 71125.02 toks/s, output: 69.46 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:02<00:20, 43.98it/s, est. speed input: 67680.97 toks/s, output: 66.09 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:02<00:20, 43.59it/s, est. speed input: 65697.19 toks/s, output: 64.16 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:02<00:20, 43.33it/s, est. speed input: 64025.81 toks/s, output: 62.52 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:02<00:19, 43.12it/s, est. speed input: 62581.66 toks/s, output: 61.11 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:02<00:19, 42.96it/s, est. speed input: 61326.19 toks/s, output: 59.89 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:03<00:19, 42.86it/s, est. speed input: 60229.49 toks/s, output: 58.82 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:03<00:19, 42.79it/s, est. speed input: 59259.53 toks/s, output: 57.87 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:03<00:19, 42.75it/s, est. speed input: 58401.77 toks/s, output: 57.03 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:03<00:19, 42.73it/s, est. speed input: 57633.17 toks/s, output: 56.28 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:03<00:19, 42.67it/s, est. speed input: 56933.31 toks/s, output: 55.60 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:03<00:18, 42.67it/s, est. speed input: 56307.22 toks/s, output: 54.99 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:04<00:18, 42.67it/s, est. speed input: 55736.91 toks/s, output: 54.43 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:04<00:18, 42.67it/s, est. speed input: 55216.34 toks/s, output: 53.92 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:04<00:18, 42.64it/s, est. speed input: 54734.91 toks/s, output: 53.45 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:04<00:18, 42.63it/s, est. speed input: 54291.51 toks/s, output: 53.02 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:04<00:17, 42.62it/s, est. speed input: 53882.79 toks/s, output: 52.62 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:05<00:17, 42.61it/s, est. speed input: 53503.78 toks/s, output: 52.25 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:05<00:17, 42.63it/s, est. speed input: 53156.30 toks/s, output: 51.91 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:05<00:17, 42.62it/s, est. speed input: 52828.90 toks/s, output: 51.59 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:05<00:17, 42.63it/s, est. speed input: 52524.18 toks/s, output: 51.29 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:17, 42.60it/s, est. speed input: 52235.58 toks/s, output: 51.01 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:06<00:16, 42.58it/s, est. speed input: 51964.54 toks/s, output: 50.75 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:06<00:16, 42.59it/s, est. speed input: 51712.97 toks/s, output: 50.50 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:06<00:16, 42.60it/s, est. speed input: 51476.59 toks/s, output: 50.27 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:06<00:16, 42.59it/s, est. speed input: 51252.38 toks/s, output: 50.05 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:06<00:16, 42.57it/s, est. speed input: 51038.46 toks/s, output: 49.84 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:06<00:15, 42.58it/s, est. speed input: 50838.04 toks/s, output: 49.65 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:07<00:15, 42.58it/s, est. speed input: 50647.98 toks/s, output: 49.46 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:07<00:15, 42.57it/s, est. speed input: 50467.32 toks/s, output: 49.28 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:07<00:15, 42.56it/s, est. speed input: 50294.99 toks/s, output: 49.12 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:07<00:15, 42.54it/s, est. speed input: 50128.99 toks/s, output: 48.95 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:07<00:14, 42.55it/s, est. speed input: 49973.88 toks/s, output: 48.80 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:08<00:14, 42.56it/s, est. speed input: 49826.20 toks/s, output: 48.66 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:08<00:14, 42.58it/s, est. speed input: 49685.97 toks/s, output: 48.52 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:08<00:14, 42.58it/s, est. speed input: 49551.68 toks/s, output: 48.39 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:08<00:14, 42.57it/s, est. speed input: 49421.33 toks/s, output: 48.26 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:08<00:14, 42.56it/s, est. speed input: 49296.78 toks/s, output: 48.14 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:09<00:13, 42.56it/s, est. speed input: 49177.87 toks/s, output: 48.03 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:09<00:13, 42.56it/s, est. speed input: 49064.02 toks/s, output: 47.91 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:09<00:13, 42.58it/s, est. speed input: 48956.24 toks/s, output: 47.81 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:09<00:13, 42.58it/s, est. speed input: 48851.07 toks/s, output: 47.71 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:09<00:13, 42.58it/s, est. speed input: 48750.29 toks/s, output: 47.61 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:09<00:12, 42.58it/s, est. speed input: 48653.48 toks/s, output: 47.51 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:10<00:12, 42.59it/s, est. speed input: 48560.77 toks/s, output: 47.42 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:10<00:12, 42.60it/s, est. speed input: 48471.67 toks/s, output: 47.34 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:10<00:12, 42.61it/s, est. speed input: 48385.63 toks/s, output: 47.25 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:10<00:12, 42.58it/s, est. speed input: 48300.57 toks/s, output: 47.17 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:10<00:11, 42.59it/s, est. speed input: 48220.27 toks/s, output: 47.09 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:11<00:11, 42.58it/s, est. speed input: 48141.63 toks/s, output: 47.01 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:11<00:11, 42.58it/s, est. speed input: 48066.01 toks/s, output: 46.94 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:11<00:11, 42.57it/s, est. speed input: 47992.47 toks/s, output: 46.87 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:11<00:11, 42.57it/s, est. speed input: 47921.38 toks/s, output: 46.80 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:11<00:11, 42.55it/s, est. speed input: 47851.83 toks/s, output: 46.73 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:12<00:10, 42.56it/s, est. speed input: 47785.62 toks/s, output: 46.67 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:12<00:10, 42.58it/s, est. speed input: 47721.81 toks/s, output: 46.60 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:12<00:10, 42.57it/s, est. speed input: 47659.16 toks/s, output: 46.54 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:12<00:10, 42.58it/s, est. speed input: 47598.82 toks/s, output: 46.48 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:12<00:10, 42.55it/s, est. speed input: 47538.70 toks/s, output: 46.42 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:12<00:09, 42.55it/s, est. speed input: 47481.38 toks/s, output: 46.37 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:13<00:09, 42.54it/s, est. speed input: 47425.16 toks/s, output: 46.31 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:13<00:09, 42.56it/s, est. speed input: 47371.80 toks/s, output: 46.26 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:13<00:09, 42.56it/s, est. speed input: 47318.99 toks/s, output: 46.21 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:13<00:09, 42.55it/s, est. speed input: 47267.60 toks/s, output: 46.16 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:13<00:08, 42.54it/s, est. speed input: 47217.30 toks/s, output: 46.11 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:14<00:08, 42.57it/s, est. speed input: 47169.85 toks/s, output: 46.06 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:14<00:08, 42.58it/s, est. speed input: 47123.10 toks/s, output: 46.02 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:14<00:08, 42.57it/s, est. speed input: 47076.98 toks/s, output: 45.97 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:14<00:08, 42.54it/s, est. speed input: 47030.86 toks/s, output: 45.93 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:14<00:08, 42.53it/s, est. speed input: 46986.49 toks/s, output: 45.89 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:15<00:07, 42.52it/s, est. speed input: 46942.98 toks/s, output: 45.84 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:15<00:07, 42.53it/s, est. speed input: 46901.38 toks/s, output: 45.80 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:15<00:07, 42.53it/s, est. speed input: 46860.71 toks/s, output: 45.76 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:15<00:07, 42.54it/s, est. speed input: 46821.38 toks/s, output: 45.72 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:15<00:07, 42.51it/s, est. speed input: 46781.20 toks/s, output: 45.68 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:15<00:06, 42.53it/s, est. speed input: 46743.67 toks/s, output: 45.65 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:16<00:06, 42.57it/s, est. speed input: 46708.16 toks/s, output: 45.61 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:16<00:06, 42.58it/s, est. speed input: 46673.04 toks/s, output: 45.58 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:16<00:06, 42.58it/s, est. speed input: 46637.94 toks/s, output: 45.54 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:16<00:06, 42.55it/s, est. speed input: 46602.95 toks/s, output: 45.51 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:16<00:05, 42.55it/s, est. speed input: 46568.98 toks/s, output: 45.48 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:17<00:05, 42.52it/s, est. speed input: 46535.05 toks/s, output: 45.44 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:17<00:05, 42.53it/s, est. speed input: 46503.03 toks/s, output: 45.41 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:17<00:05, 42.49it/s, est. speed input: 46469.54 toks/s, output: 45.38 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:17<00:05, 42.51it/s, est. speed input: 46438.77 toks/s, output: 45.35 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:17<00:05, 42.52it/s, est. speed input: 46408.56 toks/s, output: 45.32 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:18<00:04, 42.50it/s, est. speed input: 46377.98 toks/s, output: 45.29 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:18<00:04, 42.53it/s, est. speed input: 46349.69 toks/s, output: 45.26 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:18<00:04, 42.54it/s, est. speed input: 46321.33 toks/s, output: 45.24 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:18<00:04, 42.55it/s, est. speed input: 46293.86 toks/s, output: 45.21 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:18<00:04, 42.52it/s, est. speed input: 46265.74 toks/s, output: 45.18 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:19<00:03, 42.52it/s, est. speed input: 46238.64 toks/s, output: 45.15 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:19<00:03, 42.51it/s, est. speed input: 46211.85 toks/s, output: 45.13 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:19<00:03, 42.52it/s, est. speed input: 46186.22 toks/s, output: 45.10 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:19<00:03, 42.53it/s, est. speed input: 46160.98 toks/s, output: 45.08 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:19<00:03, 42.50it/s, est. speed input: 46135.06 toks/s, output: 45.05 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:19<00:02, 42.49it/s, est. speed input: 46110.02 toks/s, output: 45.03 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:20<00:02, 42.49it/s, est. speed input: 46085.80 toks/s, output: 45.01 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:20<00:02, 42.49it/s, est. speed input: 46061.87 toks/s, output: 44.98 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:20<00:02, 42.48it/s, est. speed input: 46038.12 toks/s, output: 44.96 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:20<00:02, 42.46it/s, est. speed input: 46014.37 toks/s, output: 44.94 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:20<00:02, 42.46it/s, est. speed input: 45991.34 toks/s, output: 44.91 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:21<00:01, 42.46it/s, est. speed input: 45969.03 toks/s, output: 44.89 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:21<00:01, 42.45it/s, est. speed input: 45946.73 toks/s, output: 44.87 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:21<00:01, 42.48it/s, est. speed input: 45925.82 toks/s, output: 44.85 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:21<00:01, 42.48it/s, est. speed input: 45904.83 toks/s, output: 44.83 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:21<00:01, 42.46it/s, est. speed input: 45883.29 toks/s, output: 44.81 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:22<00:00, 42.47it/s, est. speed input: 45862.95 toks/s, output: 44.79 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:22<00:00, 42.46it/s, est. speed input: 45842.71 toks/s, output: 44.77 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:22<00:00, 42.48it/s, est. speed input: 45823.40 toks/s, output: 44.75 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:22<00:00, 42.49it/s, est. speed input: 45804.25 toks/s, output: 44.73 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:22<00:00, 44.26it/s, est. speed input: 45835.70 toks/s, output: 44.76 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:22<00:00, 44.26it/s, est. speed input: 46105.60 toks/s, output: 45.02 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:22<00:00, 45.02it/s, est. speed input: 46105.60 toks/s, output: 45.02 toks/s]
[rank0]:[W128 00:10:44.382514474 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.5s

测试结果:
  Requests/s:   42.46
  Tokens/s:     43522.98
  Total Reqs:   1024
  Elapsed:      24.12s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     43480.52

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:10:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3460875) WARNING 01-28 00:11:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.02 requests/s, 43069.22 total tokens/s, 42.02 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 00:10:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:10:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:10:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:10:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:10:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:10:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:10:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:10:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:10:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:10:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:10:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:10:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:10:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:10:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:10:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:10:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:10:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:10:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:10:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3460875) [2026-01-28 00:10:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3460875) [2026-01-28 00:10:59] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3460875) [2026-01-28 00:10:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3460875) [2026-01-28 00:10:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3460875) [2026-01-28 00:10:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3460875) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3460875) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.94it/s]
(EngineCore_DP0 pid=3460875) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.94it/s]
(EngineCore_DP0 pid=3460875) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3460875) 2026-01-28 00:11:11,053 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3460875) 2026-01-28 00:11:11,072 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3460875) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  6.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 19.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 17.26it/s]
(EngineCore_DP0 pid=3460875) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.70it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 17.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.75it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 72/2048 [00:00<00:02, 718.95it/s]
Adding requests:   7%|▋         | 148/2048 [00:00<00:02, 736.66it/s]
Adding requests:  11%|█         | 222/2048 [00:00<00:02, 737.89it/s]
Adding requests:  15%|█▍        | 299/2048 [00:00<00:02, 748.78it/s]
Adding requests:  18%|█▊        | 376/2048 [00:00<00:02, 753.35it/s]
Adding requests:  22%|██▏       | 453/2048 [00:00<00:02, 755.25it/s]
Adding requests:  26%|██▌       | 529/2048 [00:00<00:02, 746.65it/s]
Adding requests:  29%|██▉       | 604/2048 [00:00<00:01, 746.95it/s]
Adding requests:  33%|███▎      | 683/2048 [00:00<00:01, 758.69it/s]
Adding requests:  37%|███▋      | 759/2048 [00:01<00:01, 753.19it/s]
Adding requests:  41%|████      | 835/2048 [00:01<00:01, 739.22it/s]
Adding requests:  45%|████▍     | 914/2048 [00:01<00:01, 752.24it/s]
Adding requests:  48%|████▊     | 990/2048 [00:01<00:01, 753.50it/s]
Adding requests:  52%|█████▏    | 1068/2048 [00:01<00:01, 760.26it/s]
Adding requests:  56%|█████▌    | 1145/2048 [00:01<00:01, 757.09it/s]
Adding requests:  60%|█████▉    | 1225/2048 [00:01<00:01, 766.70it/s]
Adding requests:  64%|██████▎   | 1302/2048 [00:01<00:00, 761.06it/s]
Adding requests:  67%|██████▋   | 1380/2048 [00:01<00:00, 764.46it/s]
Adding requests:  71%|███████   | 1459/2048 [00:01<00:00, 771.90it/s]
Adding requests:  75%|███████▌  | 1537/2048 [00:02<00:00, 771.05it/s]
Adding requests:  79%|███████▉  | 1615/2048 [00:02<00:00, 771.76it/s]
Adding requests:  83%|████████▎ | 1693/2048 [00:02<00:00, 765.58it/s]
Adding requests:  86%|████████▋ | 1770/2048 [00:02<00:00, 760.59it/s]
Adding requests:  90%|█████████ | 1847/2048 [00:02<00:00, 759.81it/s]
Adding requests:  94%|█████████▍| 1923/2048 [00:02<00:00, 747.67it/s]
Adding requests:  98%|█████████▊| 1998/2048 [00:02<00:00, 747.49it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 755.46it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:00<00:06, 286.13it/s, est. speed input: 293010.42 toks/s, output: 286.14 toks/s]
Processed prompts:   7%|▋         | 143/2048 [00:00<00:11, 165.33it/s, est. speed input: 188315.75 toks/s, output: 183.90 toks/s]
Processed prompts:   8%|▊         | 160/2048 [00:01<00:17, 110.91it/s, est. speed input: 141622.72 toks/s, output: 138.30 toks/s]
Processed prompts:   8%|▊         | 171/2048 [00:01<00:23, 78.77it/s, est. speed input: 113970.28 toks/s, output: 111.30 toks/s] 
Processed prompts:   9%|▊         | 179/2048 [00:01<00:32, 58.05it/s, est. speed input: 95665.10 toks/s, output: 93.42 toks/s]  
Processed prompts:   9%|▉         | 194/2048 [00:02<00:35, 51.78it/s, est. speed input: 86553.59 toks/s, output: 84.52 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:02<00:37, 48.64it/s, est. speed input: 80396.33 toks/s, output: 78.51 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:03<00:39, 46.57it/s, est. speed input: 75767.81 toks/s, output: 73.99 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:03<00:39, 45.20it/s, est. speed input: 72167.15 toks/s, output: 70.48 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:03<00:40, 44.26it/s, est. speed input: 69280.82 toks/s, output: 67.66 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:04<00:40, 43.61it/s, est. speed input: 66914.69 toks/s, output: 65.35 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:04<00:40, 43.17it/s, est. speed input: 64944.01 toks/s, output: 63.42 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:04<00:40, 42.86it/s, est. speed input: 63272.77 toks/s, output: 61.79 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:05<00:40, 42.64it/s, est. speed input: 61841.19 toks/s, output: 60.39 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:05<00:40, 42.50it/s, est. speed input: 60601.82 toks/s, output: 59.18 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:06<00:39, 42.38it/s, est. speed input: 59509.21 toks/s, output: 58.11 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:06<00:39, 42.30it/s, est. speed input: 58547.50 toks/s, output: 57.18 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:06<00:39, 42.26it/s, est. speed input: 57695.59 toks/s, output: 56.34 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:07<00:38, 42.21it/s, est. speed input: 56929.63 toks/s, output: 55.60 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:07<00:38, 42.19it/s, est. speed input: 56241.76 toks/s, output: 54.92 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:07<00:38, 42.16it/s, est. speed input: 55616.27 toks/s, output: 54.31 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:08<00:37, 42.15it/s, est. speed input: 55050.20 toks/s, output: 53.76 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:08<00:37, 42.14it/s, est. speed input: 54533.14 toks/s, output: 53.25 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:09<00:37, 42.13it/s, est. speed input: 54057.13 toks/s, output: 52.79 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:09<00:36, 42.12it/s, est. speed input: 53620.51 toks/s, output: 52.36 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:09<00:36, 42.12it/s, est. speed input: 53216.91 toks/s, output: 51.97 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:10<00:36, 42.12it/s, est. speed input: 52843.59 toks/s, output: 51.61 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:10<00:35, 42.12it/s, est. speed input: 52496.93 toks/s, output: 51.27 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:11<00:35, 42.10it/s, est. speed input: 52172.54 toks/s, output: 50.95 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:11<00:34, 42.11it/s, est. speed input: 51872.04 toks/s, output: 50.66 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:11<00:34, 42.11it/s, est. speed input: 51589.25 toks/s, output: 50.38 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:12<00:34, 42.08it/s, est. speed input: 51321.49 toks/s, output: 50.12 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:12<00:33, 42.09it/s, est. speed input: 51072.94 toks/s, output: 49.88 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:12<00:33, 42.08it/s, est. speed input: 50837.41 toks/s, output: 49.65 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:13<00:33, 42.09it/s, est. speed input: 50617.49 toks/s, output: 49.43 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:13<00:32, 42.09it/s, est. speed input: 50408.21 toks/s, output: 49.23 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:14<00:32, 42.07it/s, est. speed input: 50208.90 toks/s, output: 49.03 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:14<00:31, 42.07it/s, est. speed input: 50021.04 toks/s, output: 48.85 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:14<00:31, 42.07it/s, est. speed input: 49843.23 toks/s, output: 48.67 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:15<00:31, 42.05it/s, est. speed input: 49671.97 toks/s, output: 48.51 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:15<00:30, 42.04it/s, est. speed input: 49509.81 toks/s, output: 48.35 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:15<00:30, 42.03it/s, est. speed input: 49355.34 toks/s, output: 48.20 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:16<00:30, 42.04it/s, est. speed input: 49208.53 toks/s, output: 48.06 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:16<00:29, 42.03it/s, est. speed input: 49067.60 toks/s, output: 47.92 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:17<00:29, 42.03it/s, est. speed input: 48933.40 toks/s, output: 47.79 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:17<00:28, 42.02it/s, est. speed input: 48804.94 toks/s, output: 47.66 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:17<00:28, 42.01it/s, est. speed input: 48681.17 toks/s, output: 47.54 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:18<00:28, 42.02it/s, est. speed input: 48563.73 toks/s, output: 47.43 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:18<00:27, 42.02it/s, est. speed input: 48450.63 toks/s, output: 47.32 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:19<00:27, 42.00it/s, est. speed input: 48340.72 toks/s, output: 47.21 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:19<00:27, 42.00it/s, est. speed input: 48235.55 toks/s, output: 47.11 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:19<00:26, 42.00it/s, est. speed input: 48135.48 toks/s, output: 47.01 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:20<00:26, 42.00it/s, est. speed input: 48038.24 toks/s, output: 46.91 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:20<00:25, 42.00it/s, est. speed input: 47944.95 toks/s, output: 46.82 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:20<00:25, 42.00it/s, est. speed input: 47854.84 toks/s, output: 46.73 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:21<00:25, 42.00it/s, est. speed input: 47768.11 toks/s, output: 46.65 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:21<00:24, 42.01it/s, est. speed input: 47685.52 toks/s, output: 46.57 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:22<00:24, 42.00it/s, est. speed input: 47604.02 toks/s, output: 46.49 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:22<00:23, 42.00it/s, est. speed input: 47526.31 toks/s, output: 46.41 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:22<00:23, 41.99it/s, est. speed input: 47450.25 toks/s, output: 46.34 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:23<00:23, 41.98it/s, est. speed input: 47376.40 toks/s, output: 46.27 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:23<00:22, 41.99it/s, est. speed input: 47306.22 toks/s, output: 46.20 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:23<00:22, 41.99it/s, est. speed input: 47237.54 toks/s, output: 46.13 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:24<00:22, 41.98it/s, est. speed input: 47170.98 toks/s, output: 46.07 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:24<00:21, 41.98it/s, est. speed input: 47106.48 toks/s, output: 46.00 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:25<00:20, 42.76it/s, est. speed input: 47087.45 toks/s, output: 45.98 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:25<00:20, 42.52it/s, est. speed input: 47026.07 toks/s, output: 45.92 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:25<00:20, 42.36it/s, est. speed input: 46966.81 toks/s, output: 45.87 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:26<00:20, 42.26it/s, est. speed input: 46909.38 toks/s, output: 45.81 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:26<00:19, 42.16it/s, est. speed input: 46852.77 toks/s, output: 45.75 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:27<00:19, 42.09it/s, est. speed input: 46797.38 toks/s, output: 45.70 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:27<00:18, 42.06it/s, est. speed input: 46744.22 toks/s, output: 45.65 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:27<00:18, 42.03it/s, est. speed input: 46692.62 toks/s, output: 45.60 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:28<00:18, 42.01it/s, est. speed input: 46641.93 toks/s, output: 45.55 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:28<00:17, 41.99it/s, est. speed input: 46592.82 toks/s, output: 45.50 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:28<00:17, 41.98it/s, est. speed input: 46544.85 toks/s, output: 45.45 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:29<00:17, 41.98it/s, est. speed input: 46498.78 toks/s, output: 45.41 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:29<00:16, 41.99it/s, est. speed input: 46454.14 toks/s, output: 45.37 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:30<00:16, 41.98it/s, est. speed input: 46409.54 toks/s, output: 45.32 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:30<00:15, 41.97it/s, est. speed input: 46366.45 toks/s, output: 45.28 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:30<00:15, 41.97it/s, est. speed input: 46324.56 toks/s, output: 45.24 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:31<00:15, 41.97it/s, est. speed input: 46283.64 toks/s, output: 45.20 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:31<00:14, 41.97it/s, est. speed input: 46243.57 toks/s, output: 45.16 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:31<00:14, 41.95it/s, est. speed input: 46203.91 toks/s, output: 45.12 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:32<00:14, 41.96it/s, est. speed input: 46165.79 toks/s, output: 45.08 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:32<00:13, 41.96it/s, est. speed input: 46128.71 toks/s, output: 45.05 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:33<00:13, 41.95it/s, est. speed input: 46091.78 toks/s, output: 45.01 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:33<00:12, 41.95it/s, est. speed input: 46056.19 toks/s, output: 44.98 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:33<00:12, 41.94it/s, est. speed input: 46020.63 toks/s, output: 44.94 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:34<00:12, 41.95it/s, est. speed input: 45986.81 toks/s, output: 44.91 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:34<00:11, 41.96it/s, est. speed input: 45953.72 toks/s, output: 44.88 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:35<00:11, 41.95it/s, est. speed input: 45921.06 toks/s, output: 44.84 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:35<00:11, 41.95it/s, est. speed input: 45889.12 toks/s, output: 44.81 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:35<00:10, 41.95it/s, est. speed input: 45857.78 toks/s, output: 44.78 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:36<00:10, 41.95it/s, est. speed input: 45827.01 toks/s, output: 44.75 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:36<00:09, 41.95it/s, est. speed input: 45797.01 toks/s, output: 44.72 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:36<00:09, 41.94it/s, est. speed input: 45767.24 toks/s, output: 44.69 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:37<00:09, 41.94it/s, est. speed input: 45738.55 toks/s, output: 44.67 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:37<00:08, 41.95it/s, est. speed input: 45710.46 toks/s, output: 44.64 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:38<00:08, 41.93it/s, est. speed input: 45682.29 toks/s, output: 44.61 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:38<00:07, 41.93it/s, est. speed input: 45655.10 toks/s, output: 44.59 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:38<00:07, 41.94it/s, est. speed input: 45628.54 toks/s, output: 44.56 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:39<00:07, 41.94it/s, est. speed input: 45602.51 toks/s, output: 44.53 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:39<00:06, 41.93it/s, est. speed input: 45576.65 toks/s, output: 44.51 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:39<00:06, 41.93it/s, est. speed input: 45551.43 toks/s, output: 44.48 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:40<00:06, 41.93it/s, est. speed input: 45526.74 toks/s, output: 44.46 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:40<00:05, 41.94it/s, est. speed input: 45502.80 toks/s, output: 44.44 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:41<00:05, 41.92it/s, est. speed input: 45478.44 toks/s, output: 44.41 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:41<00:04, 41.93it/s, est. speed input: 45455.18 toks/s, output: 44.39 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:41<00:04, 41.92it/s, est. speed input: 45432.04 toks/s, output: 44.37 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:42<00:04, 42.71it/s, est. speed input: 45434.61 toks/s, output: 44.37 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:42<00:03, 42.47it/s, est. speed input: 45412.32 toks/s, output: 44.35 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:42<00:03, 42.30it/s, est. speed input: 45390.11 toks/s, output: 44.33 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:43<00:02, 42.20it/s, est. speed input: 45368.74 toks/s, output: 44.31 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:43<00:02, 42.11it/s, est. speed input: 45347.29 toks/s, output: 44.28 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:44<00:02, 42.05it/s, est. speed input: 45326.28 toks/s, output: 44.26 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:44<00:01, 42.02it/s, est. speed input: 45305.89 toks/s, output: 44.24 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:44<00:01, 41.97it/s, est. speed input: 45285.15 toks/s, output: 44.22 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:45<00:01, 41.96it/s, est. speed input: 45265.25 toks/s, output: 44.20 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:45<00:00, 41.94it/s, est. speed input: 45245.70 toks/s, output: 44.19 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:46<00:00, 42.79it/s, est. speed input: 45251.44 toks/s, output: 44.19 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:46<00:00, 42.79it/s, est. speed input: 45562.55 toks/s, output: 44.49 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:46<00:00, 44.49it/s, est. speed input: 45562.55 toks/s, output: 44.49 toks/s]
[rank0]:[W128 00:12:01.495823778 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 77.1s

测试结果:
  Requests/s:   42.02
  Tokens/s:     43069.22
  Total Reqs:   2048
  Elapsed:      48.74s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     43027.20

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:12:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3462524) WARNING 01-28 00:12:26 [backends.py:609] Failed to read file <frozen os>
Throughput: 41.96 requests/s, 43005.88 total tokens/s, 41.96 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 00:12:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:12:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:12:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:12:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:12:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:12:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:12:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:12:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:12:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:12:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:12:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:12:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:12:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:12:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:12:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:12:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:12:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:12:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:12:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3462524) [2026-01-28 00:12:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3462524) [2026-01-28 00:12:22] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3462524) [2026-01-28 00:12:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3462524) [2026-01-28 00:12:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3462524) [2026-01-28 00:12:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3462524) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3462524) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.94it/s]
(EngineCore_DP0 pid=3462524) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.93it/s]
(EngineCore_DP0 pid=3462524) 
(EngineCore_DP0 pid=3462524) [rank0]:W0128 00:12:29.931000 3462524 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3462524) [rank0]:W0128 00:12:30.608000 3462524 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3462524) [rank0]:W0128 00:12:31.803000 3462524 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3462524) [rank0]:W0128 00:12:31.879000 3462524 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3462524) 2026-01-28 00:12:35,252 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3462524) 2026-01-28 00:12:35,293 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3462524) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:04,  2.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  5.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 10.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 12.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 10.16it/s]
(EngineCore_DP0 pid=3462524) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.17it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 21.23it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.87it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:05, 714.31it/s]
Adding requests:   4%|▎         | 148/4096 [00:00<00:05, 737.80it/s]
Adding requests:   5%|▌         | 225/4096 [00:00<00:05, 750.01it/s]
Adding requests:   7%|▋         | 301/4096 [00:00<00:05, 753.75it/s]
Adding requests:   9%|▉         | 379/4096 [00:00<00:04, 759.54it/s]
Adding requests:  11%|█         | 455/4096 [00:00<00:04, 759.65it/s]
Adding requests:  13%|█▎        | 531/4096 [00:00<00:04, 741.58it/s]
Adding requests:  15%|█▍        | 608/4096 [00:00<00:04, 747.64it/s]
Adding requests:  17%|█▋        | 688/4096 [00:00<00:04, 761.22it/s]
Adding requests:  19%|█▊        | 765/4096 [00:01<00:04, 760.96it/s]
Adding requests:  21%|██        | 842/4096 [00:01<00:04, 745.80it/s]
Adding requests:  22%|██▏       | 918/4096 [00:01<00:04, 749.43it/s]
Adding requests:  24%|██▍       | 995/4096 [00:01<00:04, 753.03it/s]
Adding requests:  26%|██▌       | 1071/4096 [00:01<00:04, 735.31it/s]
Adding requests:  28%|██▊       | 1145/4096 [00:01<00:04, 736.25it/s]
Adding requests:  30%|██▉       | 1223/4096 [00:01<00:03, 746.72it/s]
Adding requests:  32%|███▏      | 1298/4096 [00:01<00:03, 743.83it/s]
Adding requests:  34%|███▎      | 1374/4096 [00:01<00:03, 746.06it/s]
Adding requests:  35%|███▌      | 1450/4096 [00:01<00:03, 749.20it/s]
Adding requests:  37%|███▋      | 1529/4096 [00:02<00:03, 761.17it/s]
Adding requests:  39%|███▉      | 1608/4096 [00:02<00:03, 769.48it/s]
Adding requests:  41%|████      | 1685/4096 [00:02<00:03, 763.88it/s]
Adding requests:  43%|████▎     | 1764/4096 [00:02<00:03, 768.75it/s]
Adding requests:  45%|████▍     | 1841/4096 [00:02<00:02, 767.19it/s]
Adding requests:  47%|████▋     | 1918/4096 [00:02<00:02, 765.95it/s]
Adding requests:  49%|████▊     | 1995/4096 [00:02<00:02, 765.41it/s]
Adding requests:  51%|█████     | 2073/4096 [00:02<00:02, 769.74it/s]
Adding requests:  52%|█████▏    | 2150/4096 [00:02<00:02, 758.85it/s]
Adding requests:  54%|█████▍    | 2226/4096 [00:02<00:02, 742.83it/s]
Adding requests:  56%|█████▋    | 2304/4096 [00:03<00:02, 753.09it/s]
Adding requests:  58%|█████▊    | 2382/4096 [00:03<00:02, 759.00it/s]
Adding requests:  60%|██████    | 2459/4096 [00:03<00:02, 759.36it/s]
Adding requests:  62%|██████▏   | 2536/4096 [00:03<00:02, 760.93it/s]
Adding requests:  64%|██████▍   | 2614/4096 [00:03<00:01, 765.59it/s]
Adding requests:  66%|██████▌   | 2691/4096 [00:03<00:01, 759.33it/s]
Adding requests:  68%|██████▊   | 2767/4096 [00:03<00:01, 757.92it/s]
Adding requests:  69%|██████▉   | 2843/4096 [00:03<00:01, 756.85it/s]
Adding requests:  71%|███████▏  | 2920/4096 [00:03<00:01, 760.43it/s]
Adding requests:  73%|███████▎  | 2997/4096 [00:03<00:01, 755.29it/s]
Adding requests:  75%|███████▌  | 3073/4096 [00:04<00:01, 753.90it/s]
Adding requests:  77%|███████▋  | 3149/4096 [00:04<00:01, 755.43it/s]
Adding requests:  79%|███████▊  | 3225/4096 [00:04<00:01, 755.09it/s]
Adding requests:  81%|████████  | 3303/4096 [00:04<00:01, 759.73it/s]
Adding requests:  83%|████████▎ | 3380/4096 [00:04<00:00, 761.04it/s]
Adding requests:  84%|████████▍ | 3457/4096 [00:04<00:00, 758.39it/s]
Adding requests:  86%|████████▋ | 3533/4096 [00:04<00:00, 752.97it/s]
Adding requests:  88%|████████▊ | 3609/4096 [00:04<00:00, 739.49it/s]
Adding requests:  90%|████████▉ | 3684/4096 [00:04<00:00, 738.31it/s]
Adding requests:  92%|█████████▏| 3761/4096 [00:04<00:00, 746.61it/s]
Adding requests:  94%|█████████▎| 3839/4096 [00:05<00:00, 755.07it/s]
Adding requests:  96%|█████████▌| 3919/4096 [00:05<00:00, 767.85it/s]
Adding requests:  98%|█████████▊| 3997/4096 [00:05<00:00, 770.36it/s]
Adding requests:  99%|█████████▉| 4075/4096 [00:05<00:00, 772.26it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 756.42it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:00<00:12, 302.54it/s, est. speed input: 309807.53 toks/s, output: 302.54 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:01<00:25, 148.36it/s, est. speed input: 175413.46 toks/s, output: 171.30 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:02<00:38, 99.67it/s, est. speed input: 131079.61 toks/s, output: 128.01 toks/s] 
Processed prompts:   8%|▊         | 322/4096 [00:03<00:49, 76.90it/s, est. speed input: 108998.52 toks/s, output: 106.44 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:03<00:58, 64.23it/s, est. speed input: 95690.28 toks/s, output: 93.45 toks/s]  
Processed prompts:   9%|▉         | 386/4096 [00:04<01:05, 56.71it/s, est. speed input: 86908.27 toks/s, output: 84.87 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:05<01:10, 51.94it/s, est. speed input: 80639.94 toks/s, output: 78.75 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:06<01:14, 48.81it/s, est. speed input: 75940.06 toks/s, output: 74.16 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:06<01:17, 46.71it/s, est. speed input: 72284.03 toks/s, output: 70.59 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:07<01:19, 45.28it/s, est. speed input: 69357.93 toks/s, output: 67.73 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:08<01:20, 44.29it/s, est. speed input: 66963.76 toks/s, output: 65.39 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:09<01:20, 43.61it/s, est. speed input: 64968.09 toks/s, output: 63.45 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:09<01:20, 43.15it/s, est. speed input: 63281.06 toks/s, output: 61.80 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:10<01:20, 42.81it/s, est. speed input: 61831.76 toks/s, output: 60.38 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:11<01:20, 42.58it/s, est. speed input: 60577.05 toks/s, output: 59.16 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:12<01:19, 42.41it/s, est. speed input: 59478.68 toks/s, output: 58.08 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:12<01:19, 42.30it/s, est. speed input: 58509.05 toks/s, output: 57.14 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:13<01:18, 42.21it/s, est. speed input: 57646.85 toks/s, output: 56.30 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:14<01:18, 42.15it/s, est. speed input: 56875.24 toks/s, output: 55.54 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:15<01:17, 42.11it/s, est. speed input: 56180.43 toks/s, output: 54.86 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:15<01:16, 42.08it/s, est. speed input: 55551.81 toks/s, output: 54.25 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:16<01:16, 42.06it/s, est. speed input: 54980.96 toks/s, output: 53.69 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:17<01:15, 42.04it/s, est. speed input: 54459.45 toks/s, output: 53.18 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:18<01:14, 42.02it/s, est. speed input: 53980.16 toks/s, output: 52.71 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:19<01:13, 42.02it/s, est. speed input: 53540.82 toks/s, output: 52.29 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:19<01:13, 42.01it/s, est. speed input: 53135.32 toks/s, output: 51.89 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:20<01:12, 42.00it/s, est. speed input: 52758.44 toks/s, output: 51.52 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:21<01:11, 42.00it/s, est. speed input: 52409.33 toks/s, output: 51.18 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:22<01:10, 42.00it/s, est. speed input: 52084.65 toks/s, output: 50.86 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:22<01:10, 42.00it/s, est. speed input: 51781.01 toks/s, output: 50.57 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:23<01:09, 41.99it/s, est. speed input: 51496.48 toks/s, output: 50.29 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:24<01:08, 41.98it/s, est. speed input: 51229.68 toks/s, output: 50.03 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:25<01:07, 41.98it/s, est. speed input: 50979.16 toks/s, output: 49.78 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:25<01:07, 41.98it/s, est. speed input: 50744.02 toks/s, output: 49.55 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:26<01:06, 41.98it/s, est. speed input: 50521.31 toks/s, output: 49.34 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:27<01:05, 41.98it/s, est. speed input: 50311.47 toks/s, output: 49.13 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:28<01:04, 41.97it/s, est. speed input: 50112.97 toks/s, output: 48.94 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:28<01:03, 41.97it/s, est. speed input: 49924.56 toks/s, output: 48.75 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:29<01:03, 41.97it/s, est. speed input: 49745.83 toks/s, output: 48.58 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:30<01:02, 41.97it/s, est. speed input: 49576.80 toks/s, output: 48.41 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:31<01:01, 41.96it/s, est. speed input: 49414.64 toks/s, output: 48.26 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:31<01:00, 41.95it/s, est. speed input: 49259.63 toks/s, output: 48.11 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:32<01:00, 41.95it/s, est. speed input: 49112.57 toks/s, output: 47.96 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:33<00:59, 41.95it/s, est. speed input: 48972.15 toks/s, output: 47.82 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:34<00:58, 41.94it/s, est. speed input: 48837.91 toks/s, output: 47.69 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:35<00:57, 41.95it/s, est. speed input: 48710.19 toks/s, output: 47.57 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:35<00:57, 41.95it/s, est. speed input: 48587.62 toks/s, output: 47.45 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:36<00:56, 41.95it/s, est. speed input: 48469.76 toks/s, output: 47.33 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:37<00:55, 41.95it/s, est. speed input: 48356.90 toks/s, output: 47.22 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:38<00:54, 41.94it/s, est. speed input: 48248.56 toks/s, output: 47.12 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:38<00:54, 41.94it/s, est. speed input: 48144.38 toks/s, output: 47.02 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:39<00:52, 42.30it/s, est. speed input: 48070.25 toks/s, output: 46.94 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:40<00:52, 42.19it/s, est. speed input: 47973.25 toks/s, output: 46.85 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:41<00:51, 42.11it/s, est. speed input: 47879.86 toks/s, output: 46.76 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:41<00:50, 42.06it/s, est. speed input: 47790.17 toks/s, output: 46.67 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:42<00:50, 42.02it/s, est. speed input: 47703.27 toks/s, output: 46.59 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:43<00:49, 42.00it/s, est. speed input: 47619.54 toks/s, output: 46.50 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:44<00:48, 41.97it/s, est. speed input: 47538.34 toks/s, output: 46.42 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:44<00:47, 41.96it/s, est. speed input: 47460.12 toks/s, output: 46.35 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:45<00:47, 41.95it/s, est. speed input: 47384.54 toks/s, output: 46.27 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:46<00:46, 41.94it/s, est. speed input: 47311.24 toks/s, output: 46.20 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:47<00:45, 41.94it/s, est. speed input: 47240.56 toks/s, output: 46.13 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:47<00:44, 41.93it/s, est. speed input: 47171.98 toks/s, output: 46.07 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:48<00:44, 41.93it/s, est. speed input: 47105.43 toks/s, output: 46.00 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:49<00:43, 41.92it/s, est. speed input: 47040.69 toks/s, output: 45.94 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:50<00:42, 41.92it/s, est. speed input: 46978.04 toks/s, output: 45.88 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:51<00:41, 41.92it/s, est. speed input: 46917.36 toks/s, output: 45.82 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:51<00:41, 41.91it/s, est. speed input: 46858.31 toks/s, output: 45.76 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:52<00:40, 41.92it/s, est. speed input: 46801.15 toks/s, output: 45.70 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:53<00:39, 41.91it/s, est. speed input: 46745.40 toks/s, output: 45.65 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:54<00:38, 41.92it/s, est. speed input: 46691.55 toks/s, output: 45.60 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:54<00:37, 42.28it/s, est. speed input: 46657.69 toks/s, output: 45.56 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:55<00:37, 42.17it/s, est. speed input: 46606.32 toks/s, output: 45.51 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:56<00:36, 42.09it/s, est. speed input: 46556.33 toks/s, output: 45.47 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:57<00:35, 42.04it/s, est. speed input: 46507.73 toks/s, output: 45.42 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:57<00:35, 42.00it/s, est. speed input: 46460.27 toks/s, output: 45.37 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:58<00:34, 41.97it/s, est. speed input: 46414.05 toks/s, output: 45.33 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:59<00:33, 41.95it/s, est. speed input: 46368.92 toks/s, output: 45.28 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [01:00<00:32, 41.94it/s, est. speed input: 46325.10 toks/s, output: 45.24 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [01:00<00:32, 41.93it/s, est. speed input: 46282.29 toks/s, output: 45.20 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [01:01<00:31, 41.93it/s, est. speed input: 46241.00 toks/s, output: 45.16 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [01:02<00:30, 41.91it/s, est. speed input: 46199.89 toks/s, output: 45.12 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [01:03<00:29, 41.91it/s, est. speed input: 46160.20 toks/s, output: 45.08 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [01:03<00:28, 41.91it/s, est. speed input: 46121.34 toks/s, output: 45.04 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [01:04<00:28, 41.92it/s, est. speed input: 46083.88 toks/s, output: 45.00 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [01:05<00:27, 41.90it/s, est. speed input: 46046.17 toks/s, output: 44.97 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:06<00:26, 41.90it/s, est. speed input: 46009.90 toks/s, output: 44.93 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:07<00:25, 41.90it/s, est. speed input: 45974.62 toks/s, output: 44.90 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:07<00:25, 41.90it/s, est. speed input: 45940.11 toks/s, output: 44.86 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:08<00:24, 41.89it/s, est. speed input: 45906.00 toks/s, output: 44.83 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:09<00:23, 41.89it/s, est. speed input: 45872.76 toks/s, output: 44.80 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:10<00:22, 41.89it/s, est. speed input: 45840.42 toks/s, output: 44.77 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:10<00:22, 41.89it/s, est. speed input: 45808.72 toks/s, output: 44.74 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:11<00:21, 41.89it/s, est. speed input: 45777.68 toks/s, output: 44.70 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:12<00:20, 41.89it/s, est. speed input: 45747.37 toks/s, output: 44.68 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:13<00:19, 41.90it/s, est. speed input: 45717.77 toks/s, output: 44.65 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:13<00:19, 41.90it/s, est. speed input: 45688.61 toks/s, output: 44.62 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:14<00:18, 41.89it/s, est. speed input: 45660.04 toks/s, output: 44.59 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:15<00:17, 41.89it/s, est. speed input: 45631.94 toks/s, output: 44.56 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:16<00:16, 41.89it/s, est. speed input: 45604.57 toks/s, output: 44.54 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:16<00:15, 41.90it/s, est. speed input: 45577.83 toks/s, output: 44.51 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:17<00:15, 41.89it/s, est. speed input: 45551.38 toks/s, output: 44.48 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:18<00:14, 41.88it/s, est. speed input: 45525.29 toks/s, output: 44.46 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:19<00:13, 41.88it/s, est. speed input: 45499.89 toks/s, output: 44.43 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:20<00:12, 41.89it/s, est. speed input: 45475.19 toks/s, output: 44.41 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:20<00:12, 41.90it/s, est. speed input: 45451.09 toks/s, output: 44.39 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:21<00:11, 41.88it/s, est. speed input: 45426.63 toks/s, output: 44.36 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:22<00:10, 41.88it/s, est. speed input: 45402.88 toks/s, output: 44.34 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:23<00:09, 41.88it/s, est. speed input: 45379.92 toks/s, output: 44.32 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:23<00:09, 41.88it/s, est. speed input: 45357.23 toks/s, output: 44.29 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:24<00:08, 41.88it/s, est. speed input: 45334.83 toks/s, output: 44.27 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:25<00:07, 41.89it/s, est. speed input: 45313.08 toks/s, output: 44.25 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:26<00:06, 41.88it/s, est. speed input: 45291.46 toks/s, output: 44.23 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:26<00:06, 41.89it/s, est. speed input: 45270.54 toks/s, output: 44.21 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:27<00:05, 41.88it/s, est. speed input: 45249.48 toks/s, output: 44.19 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:28<00:04, 41.88it/s, est. speed input: 45229.02 toks/s, output: 44.17 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:29<00:03, 41.88it/s, est. speed input: 45208.93 toks/s, output: 44.15 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:29<00:03, 41.87it/s, est. speed input: 45188.89 toks/s, output: 44.13 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:30<00:02, 41.87it/s, est. speed input: 45169.47 toks/s, output: 44.11 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:31<00:01, 42.23it/s, est. speed input: 45161.03 toks/s, output: 44.10 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:32<00:00, 42.55it/s, est. speed input: 45154.60 toks/s, output: 44.10 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:32<00:00, 42.55it/s, est. speed input: 45487.63 toks/s, output: 44.42 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:32<00:00, 44.42it/s, est. speed input: 45487.63 toks/s, output: 44.42 toks/s]
[rank0]:[W128 00:14:15.350825513 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 133.9s

测试结果:
  Requests/s:   41.96
  Tokens/s:     43005.88
  Total Reqs:   4096
  Elapsed:      97.62s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     42963.93


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,52.2861,26822.7827,2.4481
1024,1024,1,128,128,42.3216,43379.6609,3.0245
2048,1024,2,256,128,44.9539,46077.7946,5.6947
4096,1024,4,512,128,43.7874,44882.0587,11.6929
8192,1024,8,1024,128,42.4614,43522.9772,24.1160
16384,1024,16,2048,128,42.0188,43069.2219,48.7401
32768,1024,32,4096,128,41.9570,43005.8837,97.6239

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:14:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3464671) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3464671) WARNING 01-28 00:14:30 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.30 requests/s, 28371.25 total tokens/s, 55.30 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:14:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:14:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:14:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:14:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:14:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:14:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:14:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:14:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:14:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:14:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:14:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:14:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:14:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:14:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:14:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:14:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:14:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:14:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3464671) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3464671) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=3464671) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=3464671) 
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3464671) [2026-01-28 00:14:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3464671) 2026-01-28 00:14:36,664 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3464671) 2026-01-28 00:14:36,680 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3464671) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.04it/s]
(EngineCore_DP0 pid=3464671) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.37it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  91%|█████████▏| 117/128 [00:00<00:00, 1165.80it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1179.10it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:20,  6.22it/s, est. speed input: 3184.66 toks/s, output: 6.22 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 34.02it/s, est. speed input: 14919.69 toks/s, output: 29.14 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 45.19it/s, est. speed input: 19677.30 toks/s, output: 38.43 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 51.66it/s, est. speed input: 22450.60 toks/s, output: 43.85 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 55.51it/s, est. speed input: 24218.28 toks/s, output: 47.30 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 58.17it/s, est. speed input: 25491.47 toks/s, output: 49.79 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 59.12it/s, est. speed input: 26281.17 toks/s, output: 51.33 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 59.98it/s, est. speed input: 26921.87 toks/s, output: 52.58 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 60.74it/s, est. speed input: 27454.01 toks/s, output: 53.62 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 61.25it/s, est. speed input: 27884.12 toks/s, output: 54.46 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 61.72it/s, est. speed input: 28253.56 toks/s, output: 55.18 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 61.46it/s, est. speed input: 28492.77 toks/s, output: 55.65 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 61.77it/s, est. speed input: 28752.18 toks/s, output: 56.16 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 61.97it/s, est. speed input: 28974.12 toks/s, output: 56.59 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 61.99it/s, est. speed input: 29155.20 toks/s, output: 56.94 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 62.09it/s, est. speed input: 29321.96 toks/s, output: 57.27 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 62.28it/s, est. speed input: 29480.50 toks/s, output: 57.58 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 62.51it/s, est. speed input: 29630.59 toks/s, output: 57.87 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 62.40it/s, est. speed input: 29743.43 toks/s, output: 58.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 62.40it/s, est. speed input: 29733.67 toks/s, output: 58.07 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 58.07it/s, est. speed input: 29733.67 toks/s, output: 58.07 toks/s]
[rank0]:[W128 00:14:40.689756700 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.3s

测试结果:
  Requests/s:   55.30
  Tokens/s:     28371.25
  Total Reqs:   128
  Elapsed:      2.31s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     28315.95

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:14:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3465408) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3465408) WARNING 01-28 00:14:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.17 requests/s, 54494.70 total tokens/s, 53.17 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:14:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:14:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:14:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:14:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:14:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:14:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:14:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:14:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:14:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:14:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:14:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:14:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:14:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:14:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:14:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:14:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:14:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:14:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:14:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3465408) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3465408) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.57it/s]
(EngineCore_DP0 pid=3465408) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.57it/s]
(EngineCore_DP0 pid=3465408) 
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3465408) [2026-01-28 00:14:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3465408) 2026-01-28 00:15:01,701 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3465408) 2026-01-28 00:15:01,717 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3465408) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 23.25it/s]
(EngineCore_DP0 pid=3465408) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.39it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  54%|█████▍    | 69/128 [00:00<00:00, 688.29it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 708.39it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 17.73it/s, est. speed input: 18155.05 toks/s, output: 17.73 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:02, 41.17it/s, est. speed input: 38362.05 toks/s, output: 37.46 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:02, 49.14it/s, est. speed input: 45477.71 toks/s, output: 44.41 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:02, 52.50it/s, est. speed input: 48838.00 toks/s, output: 47.69 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:01, 55.09it/s, est. speed input: 51242.06 toks/s, output: 50.04 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 56.72it/s, est. speed input: 52880.74 toks/s, output: 51.64 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:01, 57.70it/s, est. speed input: 54037.91 toks/s, output: 52.77 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 58.41it/s, est. speed input: 54933.12 toks/s, output: 53.65 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 58.87it/s, est. speed input: 55631.18 toks/s, output: 54.33 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 59.15it/s, est. speed input: 56180.03 toks/s, output: 54.86 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 59.34it/s, est. speed input: 56628.46 toks/s, output: 55.30 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 59.50it/s, est. speed input: 57012.69 toks/s, output: 55.68 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:00, 59.46it/s, est. speed input: 57301.37 toks/s, output: 55.96 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:00, 59.59it/s, est. speed input: 57585.43 toks/s, output: 56.24 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:00, 59.57it/s, est. speed input: 57809.17 toks/s, output: 56.45 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 59.64it/s, est. speed input: 58019.46 toks/s, output: 56.66 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 59.79it/s, est. speed input: 58255.40 toks/s, output: 56.89 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 59.89it/s, est. speed input: 58461.20 toks/s, output: 57.09 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 59.88it/s, est. speed input: 58606.49 toks/s, output: 57.23 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 59.94it/s, est. speed input: 58769.49 toks/s, output: 57.39 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 60.00it/s, est. speed input: 58918.41 toks/s, output: 57.54 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 60.00it/s, est. speed input: 58910.76 toks/s, output: 57.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 57.53it/s, est. speed input: 58910.76 toks/s, output: 57.53 toks/s]
[rank0]:[W128 00:15:05.237774466 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.7s

测试结果:
  Requests/s:   53.17
  Tokens/s:     54494.70
  Total Reqs:   128
  Elapsed:      2.41s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     54441.54

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:15:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3466048) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3466048) WARNING 01-28 00:15:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 60.72 requests/s, 62239.24 total tokens/s, 60.72 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 00:15:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:15:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:15:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:15:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:15:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:15:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:15:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:15:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:15:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:15:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:15:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:15:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:15:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:15:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:15:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:15:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:15:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:15:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:15] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3466048) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3466048) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.56it/s]
(EngineCore_DP0 pid=3466048) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.56it/s]
(EngineCore_DP0 pid=3466048) 
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3466048) [2026-01-28 00:15:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3466048) 2026-01-28 00:15:26,663 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3466048) 2026-01-28 00:15:26,679 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3466048) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 19.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 19.03it/s]
(EngineCore_DP0 pid=3466048) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  6.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.41it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  25%|██▌       | 64/256 [00:00<00:00, 637.66it/s]
Adding requests:  54%|█████▎    | 137/256 [00:00<00:00, 691.63it/s]
Adding requests:  82%|████████▏ | 209/256 [00:00<00:00, 700.38it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 703.48it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:00<00:01, 213.29it/s, est. speed input: 218430.74 toks/s, output: 213.30 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:00<00:02, 89.39it/s, est. speed input: 100688.72 toks/s, output: 98.33 toks/s]  
Processed prompts:  23%|██▎       | 59/256 [00:00<00:02, 80.66it/s, est. speed input: 91277.38 toks/s, output: 89.14 toks/s] 
Processed prompts:  27%|██▋       | 69/256 [00:00<00:02, 74.63it/s, est. speed input: 85747.88 toks/s, output: 83.74 toks/s]
Processed prompts:  30%|███       | 78/256 [00:00<00:02, 68.73it/s, est. speed input: 81035.41 toks/s, output: 79.14 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:01<00:02, 66.92it/s, est. speed input: 78979.70 toks/s, output: 77.13 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:01<00:02, 65.50it/s, est. speed input: 77335.24 toks/s, output: 75.52 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:01<00:02, 64.52it/s, est. speed input: 76036.78 toks/s, output: 74.25 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:01<00:02, 63.65it/s, est. speed input: 74903.61 toks/s, output: 73.15 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:01<00:02, 63.11it/s, est. speed input: 73982.70 toks/s, output: 72.25 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:01<00:02, 62.77it/s, est. speed input: 73210.65 toks/s, output: 71.49 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:01<00:01, 62.42it/s, est. speed input: 72512.99 toks/s, output: 70.81 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:02<00:01, 62.21it/s, est. speed input: 71916.82 toks/s, output: 70.23 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:02<00:01, 62.00it/s, est. speed input: 71375.79 toks/s, output: 69.70 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:02<00:01, 61.93it/s, est. speed input: 70913.48 toks/s, output: 69.25 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:02<00:01, 61.91it/s, est. speed input: 70507.57 toks/s, output: 68.85 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:02<00:01, 61.95it/s, est. speed input: 70154.68 toks/s, output: 68.51 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:02<00:01, 61.94it/s, est. speed input: 69826.79 toks/s, output: 68.19 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:02<00:01, 61.89it/s, est. speed input: 69523.34 toks/s, output: 67.89 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:02<00:00, 61.80it/s, est. speed input: 69237.21 toks/s, output: 67.61 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:03<00:00, 61.77it/s, est. speed input: 68980.46 toks/s, output: 67.36 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:03<00:00, 61.80it/s, est. speed input: 68751.51 toks/s, output: 67.14 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:03<00:00, 61.88it/s, est. speed input: 68550.00 toks/s, output: 66.94 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:03<00:00, 61.84it/s, est. speed input: 68349.29 toks/s, output: 66.75 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:03<00:00, 61.78it/s, est. speed input: 68159.59 toks/s, output: 66.56 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:03<00:00, 61.85it/s, est. speed input: 67997.70 toks/s, output: 66.40 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:03<00:00, 61.91it/s, est. speed input: 67847.62 toks/s, output: 66.26 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 61.91it/s, est. speed input: 68088.02 toks/s, output: 66.49 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 66.49it/s, est. speed input: 68088.02 toks/s, output: 66.49 toks/s]
[rank0]:[W128 00:15:32.169082103 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.8s

测试结果:
  Requests/s:   60.72
  Tokens/s:     62239.24
  Total Reqs:   256
  Elapsed:      4.22s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     62178.52

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:15:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3466709) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3466709) WARNING 01-28 00:15:47 [backends.py:609] Failed to read file <frozen os>
Throughput: 60.03 requests/s, 61532.88 total tokens/s, 60.03 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 00:15:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:15:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:15:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:15:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:15:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:15:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:15:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:15:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:15:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:15:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:15:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:15:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:15:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:15:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:15:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:15:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:15:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:15:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:15:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3466709) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3466709) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.56it/s]
(EngineCore_DP0 pid=3466709) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.56it/s]
(EngineCore_DP0 pid=3466709) 
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3466709) [2026-01-28 00:15:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3466709) 2026-01-28 00:15:54,111 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3466709) 2026-01-28 00:15:54,127 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3466709) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 16.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.52it/s]
(EngineCore_DP0 pid=3466709) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.71it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 69/512 [00:00<00:00, 684.10it/s]
Adding requests:  28%|██▊       | 145/512 [00:00<00:00, 724.05it/s]
Adding requests:  43%|████▎     | 221/512 [00:00<00:00, 736.52it/s]
Adding requests:  58%|█████▊    | 295/512 [00:00<00:00, 734.52it/s]
Adding requests:  72%|███████▏  | 370/512 [00:00<00:00, 739.59it/s]
Adding requests:  87%|████████▋ | 446/512 [00:00<00:00, 744.86it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 738.81it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:00<00:01, 334.69it/s, est. speed input: 342757.14 toks/s, output: 334.70 toks/s]
Processed prompts:  15%|█▍        | 76/512 [00:00<00:04, 102.87it/s, est. speed input: 119001.92 toks/s, output: 116.21 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:00<00:05, 81.97it/s, est. speed input: 97867.59 toks/s, output: 95.57 toks/s]   
Processed prompts:  21%|██        | 107/512 [00:01<00:05, 77.56it/s, est. speed input: 92739.65 toks/s, output: 90.57 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:01<00:05, 77.13it/s, est. speed input: 91180.98 toks/s, output: 89.04 toks/s]
Processed prompts:  25%|██▍       | 127/512 [00:01<00:05, 69.07it/s, est. speed input: 86046.44 toks/s, output: 84.03 toks/s]
Processed prompts:  26%|██▋       | 135/512 [00:01<00:05, 67.20it/s, est. speed input: 84136.48 toks/s, output: 82.16 toks/s]
Processed prompts:  28%|██▊       | 143/512 [00:01<00:05, 65.60it/s, est. speed input: 82497.98 toks/s, output: 80.56 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:01<00:05, 62.36it/s, est. speed input: 80547.90 toks/s, output: 78.66 toks/s]
Processed prompts:  31%|███       | 158/512 [00:02<00:05, 61.94it/s, est. speed input: 79369.24 toks/s, output: 77.51 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:02<00:05, 61.55it/s, est. speed input: 78311.83 toks/s, output: 76.48 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:02<00:05, 61.37it/s, est. speed input: 77401.79 toks/s, output: 75.59 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:02<00:05, 61.17it/s, est. speed input: 76574.57 toks/s, output: 74.78 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:02<00:05, 60.98it/s, est. speed input: 75822.67 toks/s, output: 74.05 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:02<00:05, 60.86it/s, est. speed input: 75146.72 toks/s, output: 73.39 toks/s]
Processed prompts:  40%|████      | 206/512 [00:02<00:05, 60.70it/s, est. speed input: 74519.65 toks/s, output: 72.77 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:02<00:04, 60.74it/s, est. speed input: 73977.04 toks/s, output: 72.24 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:03<00:04, 60.75it/s, est. speed input: 73477.12 toks/s, output: 71.75 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:03<00:04, 60.65it/s, est. speed input: 73000.45 toks/s, output: 71.29 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:03<00:04, 60.66it/s, est. speed input: 72574.41 toks/s, output: 70.87 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:03<00:04, 60.67it/s, est. speed input: 72180.88 toks/s, output: 70.49 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:03<00:04, 60.73it/s, est. speed input: 71822.66 toks/s, output: 70.14 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:03<00:04, 60.66it/s, est. speed input: 71475.72 toks/s, output: 69.80 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:03<00:03, 60.62it/s, est. speed input: 71152.92 toks/s, output: 69.49 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:04<00:03, 60.61it/s, est. speed input: 70853.03 toks/s, output: 69.19 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:04<00:03, 60.58it/s, est. speed input: 70570.19 toks/s, output: 68.92 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:04<00:03, 60.60it/s, est. speed input: 70308.60 toks/s, output: 68.66 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:04<00:03, 60.57it/s, est. speed input: 70058.53 toks/s, output: 68.42 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:04<00:03, 60.61it/s, est. speed input: 69829.87 toks/s, output: 68.19 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:04<00:03, 60.56it/s, est. speed input: 69605.57 toks/s, output: 67.97 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:04<00:03, 60.54it/s, est. speed input: 69395.34 toks/s, output: 67.77 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:04<00:02, 60.57it/s, est. speed input: 69199.74 toks/s, output: 67.58 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:05<00:02, 60.62it/s, est. speed input: 69017.66 toks/s, output: 67.40 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:05<00:02, 60.55it/s, est. speed input: 68835.03 toks/s, output: 67.22 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:05<00:02, 60.61it/s, est. speed input: 68671.46 toks/s, output: 67.06 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:05<00:02, 60.68it/s, est. speed input: 68518.79 toks/s, output: 66.91 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:05<00:02, 60.71it/s, est. speed input: 68370.67 toks/s, output: 66.77 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:05<00:02, 60.69it/s, est. speed input: 68226.84 toks/s, output: 66.63 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:05<00:02, 60.68it/s, est. speed input: 68089.29 toks/s, output: 66.49 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:05<00:01, 60.60it/s, est. speed input: 67951.88 toks/s, output: 66.36 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:06<00:01, 60.56it/s, est. speed input: 67821.60 toks/s, output: 66.23 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:06<00:01, 60.56it/s, est. speed input: 67699.31 toks/s, output: 66.11 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:06<00:01, 60.61it/s, est. speed input: 67585.32 toks/s, output: 66.00 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:06<00:01, 60.67it/s, est. speed input: 67478.45 toks/s, output: 65.90 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:06<00:01, 60.63it/s, est. speed input: 67369.21 toks/s, output: 65.79 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:06<00:01, 60.52it/s, est. speed input: 67258.46 toks/s, output: 65.68 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:06<00:00, 60.48it/s, est. speed input: 67154.66 toks/s, output: 65.58 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:07<00:00, 60.50it/s, est. speed input: 67057.98 toks/s, output: 65.49 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:07<00:00, 60.46it/s, est. speed input: 66961.99 toks/s, output: 65.39 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:07<00:00, 60.47it/s, est. speed input: 66870.75 toks/s, output: 65.30 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:07<00:00, 60.53it/s, est. speed input: 66786.81 toks/s, output: 65.22 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:07<00:00, 60.59it/s, est. speed input: 66707.04 toks/s, output: 65.14 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:07<00:00, 60.63it/s, est. speed input: 66629.79 toks/s, output: 65.07 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:07<00:00, 62.52it/s, est. speed input: 66666.42 toks/s, output: 65.10 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 62.52it/s, est. speed input: 66927.07 toks/s, output: 65.36 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 65.36it/s, est. speed input: 66927.07 toks/s, output: 65.36 toks/s]
[rank0]:[W128 00:16:04.016773484 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 31.9s

测试结果:
  Requests/s:   60.03
  Tokens/s:     61532.88
  Total Reqs:   512
  Elapsed:      8.53s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     61472.85

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:16:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3467439) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3467439) WARNING 01-28 00:16:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.34 requests/s, 57745.47 total tokens/s, 56.34 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 00:16:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:16:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:16:12] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:16:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:12] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:12] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:16:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:16:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:16:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:16:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:16:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:16:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:16:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:16:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:16:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:16:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:16:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:16:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:16:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:16:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:16] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:16] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:16] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:16] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:16] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3467439) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3467439) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.59it/s]
(EngineCore_DP0 pid=3467439) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.59it/s]
(EngineCore_DP0 pid=3467439) 
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3467439) [2026-01-28 00:16:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3467439) 2026-01-28 00:16:27,416 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3467439) 2026-01-28 00:16:27,432 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3467439) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 13.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 12.82it/s]
(EngineCore_DP0 pid=3467439) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.31it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 72/1024 [00:00<00:01, 718.61it/s]
Adding requests:  15%|█▍        | 149/1024 [00:00<00:01, 746.11it/s]
Adding requests:  22%|██▏       | 225/1024 [00:00<00:01, 750.69it/s]
Adding requests:  29%|██▉       | 301/1024 [00:00<00:00, 750.58it/s]
Adding requests:  37%|███▋      | 378/1024 [00:00<00:00, 755.45it/s]
Adding requests:  44%|████▍     | 454/1024 [00:00<00:00, 754.32it/s]
Adding requests:  52%|█████▏    | 530/1024 [00:00<00:00, 737.07it/s]
Adding requests:  59%|█████▉    | 607/1024 [00:00<00:00, 744.93it/s]
Adding requests:  67%|██████▋   | 683/1024 [00:00<00:00, 747.18it/s]
Adding requests:  74%|███████▍  | 759/1024 [00:01<00:00, 750.56it/s]
Adding requests:  82%|████████▏ | 835/1024 [00:01<00:00, 735.40it/s]
Adding requests:  89%|████████▉ | 912/1024 [00:01<00:00, 745.05it/s]
Adding requests:  96%|█████████▋| 988/1024 [00:01<00:00, 748.22it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 747.36it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:00<00:01, 557.36it/s, est. speed input: 570803.85 toks/s, output: 557.39 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:01<00:08, 99.66it/s, est. speed input: 118702.31 toks/s, output: 115.92 toks/s]
Processed prompts:  15%|█▌        | 157/1024 [00:01<00:09, 86.80it/s, est. speed input: 104077.30 toks/s, output: 101.64 toks/s]
Processed prompts:  17%|█▋        | 175/1024 [00:01<00:10, 80.90it/s, est. speed input: 98057.28 toks/s, output: 95.76 toks/s]  
Processed prompts:  18%|█▊        | 188/1024 [00:02<00:11, 71.59it/s, est. speed input: 91239.20 toks/s, output: 89.10 toks/s]
Processed prompts:  19%|█▉        | 198/1024 [00:02<00:11, 71.49it/s, est. speed input: 90074.21 toks/s, output: 87.96 toks/s]
Processed prompts:  20%|██        | 207/1024 [00:02<00:11, 70.03it/s, est. speed input: 88604.11 toks/s, output: 86.53 toks/s]
Processed prompts:  21%|██        | 216/1024 [00:02<00:11, 68.67it/s, est. speed input: 87290.57 toks/s, output: 85.24 toks/s]
Processed prompts:  22%|██▏       | 224/1024 [00:02<00:12, 65.86it/s, est. speed input: 85732.11 toks/s, output: 83.72 toks/s]
Processed prompts:  23%|██▎       | 231/1024 [00:02<00:12, 61.80it/s, est. speed input: 83972.11 toks/s, output: 82.00 toks/s]
Processed prompts:  23%|██▎       | 238/1024 [00:02<00:13, 58.57it/s, est. speed input: 82379.44 toks/s, output: 80.45 toks/s]
Processed prompts:  24%|██▍       | 244/1024 [00:03<00:14, 54.17it/s, est. speed input: 80599.72 toks/s, output: 78.71 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:03<00:15, 50.90it/s, est. speed input: 78983.40 toks/s, output: 77.13 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:03<00:14, 52.54it/s, est. speed input: 78106.95 toks/s, output: 76.28 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:03<00:14, 53.72it/s, est. speed input: 77299.84 toks/s, output: 75.49 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:03<00:13, 54.58it/s, est. speed input: 76556.23 toks/s, output: 74.76 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:03<00:13, 55.17it/s, est. speed input: 75865.85 toks/s, output: 74.09 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:03<00:13, 55.58it/s, est. speed input: 75222.23 toks/s, output: 73.46 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:04<00:12, 55.87it/s, est. speed input: 74623.35 toks/s, output: 72.87 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:04<00:12, 56.11it/s, est. speed input: 74069.42 toks/s, output: 72.33 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:04<00:12, 56.29it/s, est. speed input: 73552.76 toks/s, output: 71.83 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:04<00:12, 56.35it/s, est. speed input: 73060.86 toks/s, output: 71.35 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:04<00:12, 56.41it/s, est. speed input: 72599.38 toks/s, output: 70.90 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:04<00:12, 56.47it/s, est. speed input: 72169.20 toks/s, output: 70.48 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:04<00:12, 56.50it/s, est. speed input: 71760.59 toks/s, output: 70.08 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:05<00:11, 56.54it/s, est. speed input: 71377.45 toks/s, output: 69.70 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:05<00:11, 56.58it/s, est. speed input: 71016.39 toks/s, output: 69.35 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:05<00:11, 56.60it/s, est. speed input: 70673.75 toks/s, output: 69.02 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:05<00:11, 56.58it/s, est. speed input: 70345.40 toks/s, output: 68.70 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:05<00:11, 56.57it/s, est. speed input: 70033.06 toks/s, output: 68.39 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:05<00:11, 56.57it/s, est. speed input: 69737.84 toks/s, output: 68.10 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:05<00:11, 56.52it/s, est. speed input: 69450.87 toks/s, output: 67.82 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:06<00:10, 56.53it/s, est. speed input: 69181.50 toks/s, output: 67.56 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:06<00:10, 56.53it/s, est. speed input: 68924.05 toks/s, output: 67.31 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:06<00:10, 56.54it/s, est. speed input: 68679.36 toks/s, output: 67.07 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:06<00:10, 56.56it/s, est. speed input: 68445.89 toks/s, output: 66.84 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:06<00:10, 56.54it/s, est. speed input: 68219.49 toks/s, output: 66.62 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:06<00:10, 56.55it/s, est. speed input: 68004.84 toks/s, output: 66.41 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:06<00:10, 56.53it/s, est. speed input: 67797.05 toks/s, output: 66.21 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:07<00:09, 56.56it/s, est. speed input: 67600.53 toks/s, output: 66.02 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:07<00:09, 56.60it/s, est. speed input: 67413.65 toks/s, output: 65.83 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:07<00:09, 56.62it/s, est. speed input: 67233.26 toks/s, output: 65.66 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:07<00:09, 56.61it/s, est. speed input: 67058.79 toks/s, output: 65.49 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:07<00:09, 56.63it/s, est. speed input: 66890.25 toks/s, output: 65.32 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:07<00:09, 56.65it/s, est. speed input: 66730.05 toks/s, output: 65.17 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:07<00:09, 56.63it/s, est. speed input: 66572.46 toks/s, output: 65.01 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:08<00:08, 56.58it/s, est. speed input: 66418.39 toks/s, output: 64.86 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:08<00:08, 56.55it/s, est. speed input: 66270.40 toks/s, output: 64.72 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:08<00:08, 56.58it/s, est. speed input: 66129.88 toks/s, output: 64.58 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:08<00:08, 56.56it/s, est. speed input: 65992.02 toks/s, output: 64.45 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:08<00:08, 56.55it/s, est. speed input: 65858.58 toks/s, output: 64.31 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:08<00:08, 56.51it/s, est. speed input: 65727.81 toks/s, output: 64.19 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:08<00:08, 56.53it/s, est. speed input: 65603.84 toks/s, output: 64.07 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:09<00:07, 56.50it/s, est. speed input: 65481.24 toks/s, output: 63.95 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:09<00:07, 56.49it/s, est. speed input: 65363.27 toks/s, output: 63.83 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:09<00:07, 56.52it/s, est. speed input: 65250.56 toks/s, output: 63.72 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:09<00:07, 56.56it/s, est. speed input: 65142.81 toks/s, output: 63.62 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:09<00:07, 56.58it/s, est. speed input: 65037.25 toks/s, output: 63.51 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:09<00:07, 56.59it/s, est. speed input: 64934.65 toks/s, output: 63.41 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:09<00:07, 56.63it/s, est. speed input: 64836.92 toks/s, output: 63.32 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:10<00:06, 56.58it/s, est. speed input: 64738.06 toks/s, output: 63.22 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:10<00:06, 56.57it/s, est. speed input: 64643.00 toks/s, output: 63.13 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:10<00:06, 56.58it/s, est. speed input: 64551.19 toks/s, output: 63.04 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:10<00:06, 56.60it/s, est. speed input: 64462.94 toks/s, output: 62.95 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:10<00:06, 56.62it/s, est. speed input: 64377.15 toks/s, output: 62.87 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:10<00:06, 56.58it/s, est. speed input: 64290.95 toks/s, output: 62.78 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:10<00:06, 56.57it/s, est. speed input: 64208.05 toks/s, output: 62.70 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:11<00:05, 56.53it/s, est. speed input: 64125.37 toks/s, output: 62.62 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:11<00:05, 56.52it/s, est. speed input: 64045.71 toks/s, output: 62.54 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:11<00:05, 56.53it/s, est. speed input: 63969.18 toks/s, output: 62.47 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:11<00:05, 56.51it/s, est. speed input: 63892.80 toks/s, output: 62.40 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:11<00:05, 56.52it/s, est. speed input: 63819.47 toks/s, output: 62.32 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:11<00:05, 56.52it/s, est. speed input: 63747.69 toks/s, output: 62.25 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:11<00:05, 56.54it/s, est. speed input: 63678.75 toks/s, output: 62.19 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:12<00:04, 56.51it/s, est. speed input: 63609.38 toks/s, output: 62.12 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:12<00:04, 56.48it/s, est. speed input: 63541.01 toks/s, output: 62.05 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:12<00:04, 56.50it/s, est. speed input: 63476.29 toks/s, output: 61.99 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:12<00:04, 56.55it/s, est. speed input: 63414.21 toks/s, output: 61.93 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:12<00:04, 56.57it/s, est. speed input: 63353.06 toks/s, output: 61.87 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:12<00:04, 56.57it/s, est. speed input: 63292.64 toks/s, output: 61.81 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:12<00:04, 56.52it/s, est. speed input: 63231.80 toks/s, output: 61.75 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:13<00:03, 56.47it/s, est. speed input: 63171.48 toks/s, output: 61.69 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:13<00:03, 56.49it/s, est. speed input: 63114.38 toks/s, output: 61.64 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:13<00:03, 56.52it/s, est. speed input: 63059.57 toks/s, output: 61.58 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:13<00:03, 56.55it/s, est. speed input: 63005.97 toks/s, output: 61.53 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:13<00:03, 56.57it/s, est. speed input: 62953.65 toks/s, output: 61.48 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:13<00:03, 56.59it/s, est. speed input: 62902.62 toks/s, output: 61.43 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:13<00:03, 56.60it/s, est. speed input: 62852.31 toks/s, output: 61.38 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:13<00:02, 56.56it/s, est. speed input: 62801.54 toks/s, output: 61.33 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:14<00:02, 56.57it/s, est. speed input: 62752.82 toks/s, output: 61.28 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:14<00:02, 56.55it/s, est. speed input: 62704.40 toks/s, output: 61.23 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:14<00:02, 56.56it/s, est. speed input: 62657.56 toks/s, output: 61.19 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:14<00:02, 56.54it/s, est. speed input: 62611.01 toks/s, output: 61.14 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:14<00:02, 56.53it/s, est. speed input: 62565.33 toks/s, output: 61.10 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:14<00:02, 56.53it/s, est. speed input: 62520.77 toks/s, output: 61.06 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:14<00:01, 56.50it/s, est. speed input: 62475.88 toks/s, output: 61.01 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:15<00:01, 56.48it/s, est. speed input: 62431.91 toks/s, output: 60.97 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:15<00:01, 56.52it/s, est. speed input: 62390.44 toks/s, output: 60.93 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:15<00:01, 56.56it/s, est. speed input: 62350.26 toks/s, output: 60.89 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:15<00:01, 56.57it/s, est. speed input: 62310.23 toks/s, output: 60.85 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:15<00:01, 56.58it/s, est. speed input: 62270.94 toks/s, output: 60.81 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:15<00:01, 56.60it/s, est. speed input: 62232.98 toks/s, output: 60.77 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:15<00:00, 56.55it/s, est. speed input: 62193.67 toks/s, output: 60.74 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:16<00:00, 56.53it/s, est. speed input: 62155.26 toks/s, output: 60.70 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:16<00:00, 56.52it/s, est. speed input: 62117.84 toks/s, output: 60.66 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:16<00:00, 56.51it/s, est. speed input: 62080.78 toks/s, output: 60.63 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:16<00:00, 56.54it/s, est. speed input: 62045.70 toks/s, output: 60.59 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:16<00:00, 56.54it/s, est. speed input: 62010.46 toks/s, output: 60.56 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:16<00:00, 58.43it/s, est. speed input: 62032.09 toks/s, output: 60.58 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:16<00:00, 58.43it/s, est. speed input: 62397.23 toks/s, output: 60.93 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:16<00:00, 60.93it/s, est. speed input: 62397.23 toks/s, output: 60.93 toks/s]
[rank0]:[W128 00:16:47.210827977 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.2s

测试结果:
  Requests/s:   56.34
  Tokens/s:     57745.47
  Total Reqs:   1024
  Elapsed:      18.18s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     57689.13

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:16:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3468353) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3468353) WARNING 01-28 00:17:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.06 requests/s, 56438.76 total tokens/s, 55.06 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 00:16:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:16:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:16:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:16:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:16:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:16:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:16:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:16:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:16:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:16:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:17:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:17:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:17:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:17:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:17:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:17:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:17:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:17:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:17:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:17:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3468353) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3468353) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.60it/s]
(EngineCore_DP0 pid=3468353) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.60it/s]
(EngineCore_DP0 pid=3468353) 
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3468353) [2026-01-28 00:17:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3468353) 2026-01-28 00:17:13,755 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3468353) 2026-01-28 00:17:13,772 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3468353) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 18.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 24.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 23.30it/s]
(EngineCore_DP0 pid=3468353) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.45it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.64it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 71/2048 [00:00<00:02, 703.04it/s]
Adding requests:   7%|▋         | 148/2048 [00:00<00:02, 736.46it/s]
Adding requests:  11%|█         | 226/2048 [00:00<00:02, 752.29it/s]
Adding requests:  15%|█▍        | 303/2048 [00:00<00:02, 755.87it/s]
Adding requests:  19%|█▊        | 379/2048 [00:00<00:02, 755.41it/s]
Adding requests:  22%|██▏       | 457/2048 [00:00<00:02, 762.80it/s]
Adding requests:  26%|██▌       | 534/2048 [00:00<00:02, 751.97it/s]
Adding requests:  30%|██▉       | 613/2048 [00:00<00:01, 761.74it/s]
Adding requests:  34%|███▎      | 690/2048 [00:00<00:01, 758.13it/s]
Adding requests:  37%|███▋      | 767/2048 [00:01<00:01, 760.04it/s]
Adding requests:  41%|████      | 844/2048 [00:01<00:01, 745.61it/s]
Adding requests:  45%|████▌     | 925/2048 [00:01<00:01, 761.62it/s]
Adding requests:  49%|████▉     | 1002/2048 [00:01<00:01, 760.82it/s]
Adding requests:  53%|█████▎    | 1080/2048 [00:01<00:01, 764.26it/s]
Adding requests:  56%|█████▋    | 1157/2048 [00:01<00:01, 762.55it/s]
Adding requests:  60%|██████    | 1236/2048 [00:01<00:01, 768.57it/s]
Adding requests:  64%|██████▍   | 1313/2048 [00:01<00:00, 764.18it/s]
Adding requests:  68%|██████▊   | 1393/2048 [00:01<00:00, 772.68it/s]
Adding requests:  72%|███████▏  | 1471/2048 [00:01<00:00, 768.94it/s]
Adding requests:  76%|███████▌  | 1549/2048 [00:02<00:00, 771.71it/s]
Adding requests:  80%|███████▉  | 1629/2048 [00:02<00:00, 777.65it/s]
Adding requests:  83%|████████▎ | 1707/2048 [00:02<00:00, 772.73it/s]
Adding requests:  87%|████████▋ | 1785/2048 [00:02<00:00, 773.10it/s]
Adding requests:  91%|█████████ | 1863/2048 [00:02<00:00, 759.60it/s]
Adding requests:  95%|█████████▍| 1941/2048 [00:02<00:00, 763.18it/s]
Adding requests:  99%|█████████▊| 2018/2048 [00:02<00:00, 759.85it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 761.80it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:00<00:03, 503.38it/s, est. speed input: 515496.77 toks/s, output: 503.39 toks/s]
Processed prompts:  10%|▉         | 197/2048 [00:01<00:12, 142.83it/s, est. speed input: 173961.32 toks/s, output: 169.88 toks/s]
Processed prompts:  11%|█         | 222/2048 [00:01<00:14, 127.04it/s, est. speed input: 156809.12 toks/s, output: 153.13 toks/s]
Processed prompts:  12%|█▏        | 240/2048 [00:01<00:16, 108.58it/s, est. speed input: 141291.75 toks/s, output: 137.98 toks/s]
Processed prompts:  12%|█▏        | 253/2048 [00:02<00:19, 90.18it/s, est. speed input: 127683.73 toks/s, output: 124.69 toks/s] 
Processed prompts:  13%|█▎        | 263/2048 [00:02<00:24, 73.92it/s, est. speed input: 116149.31 toks/s, output: 113.43 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:02<00:28, 63.32it/s, est. speed input: 107559.28 toks/s, output: 105.04 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:02<00:28, 60.91it/s, est. speed input: 102457.12 toks/s, output: 100.06 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:03<00:29, 59.23it/s, est. speed input: 98287.55 toks/s, output: 95.98 toks/s]  
Processed prompts:  16%|█▌        | 322/2048 [00:03<00:29, 58.02it/s, est. speed input: 94802.94 toks/s, output: 92.58 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:03<00:29, 57.18it/s, est. speed input: 91857.77 toks/s, output: 89.70 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:04<00:30, 56.42it/s, est. speed input: 89269.98 toks/s, output: 87.18 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:04<00:29, 56.06it/s, est. speed input: 87091.43 toks/s, output: 85.05 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:04<00:29, 55.79it/s, est. speed input: 85178.29 toks/s, output: 83.18 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:04<00:29, 55.61it/s, est. speed input: 83492.71 toks/s, output: 81.54 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:05<00:29, 55.50it/s, est. speed input: 81998.02 toks/s, output: 80.08 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:05<00:29, 55.40it/s, est. speed input: 80655.35 toks/s, output: 78.76 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:05<00:28, 55.32it/s, est. speed input: 79445.57 toks/s, output: 77.58 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:06<00:28, 55.28it/s, est. speed input: 78352.94 toks/s, output: 76.52 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:06<00:28, 55.24it/s, est. speed input: 77357.61 toks/s, output: 75.54 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:06<00:28, 55.22it/s, est. speed input: 76450.46 toks/s, output: 74.66 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:06<00:27, 55.20it/s, est. speed input: 75618.00 toks/s, output: 73.85 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:07<00:27, 55.20it/s, est. speed input: 74855.55 toks/s, output: 73.10 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:07<00:27, 55.18it/s, est. speed input: 74146.65 toks/s, output: 72.41 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:07<00:26, 55.18it/s, est. speed input: 73493.33 toks/s, output: 71.77 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:08<00:26, 55.16it/s, est. speed input: 72884.25 toks/s, output: 71.18 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:08<00:26, 55.15it/s, est. speed input: 72317.03 toks/s, output: 70.62 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:08<00:26, 55.15it/s, est. speed input: 71788.18 toks/s, output: 70.11 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:08<00:25, 55.16it/s, est. speed input: 71295.91 toks/s, output: 69.62 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:09<00:25, 55.15it/s, est. speed input: 70831.81 toks/s, output: 69.17 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:09<00:25, 55.14it/s, est. speed input: 70395.54 toks/s, output: 68.75 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:09<00:24, 55.15it/s, est. speed input: 69986.05 toks/s, output: 68.35 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:10<00:24, 55.14it/s, est. speed input: 69599.17 toks/s, output: 67.97 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:10<00:24, 55.12it/s, est. speed input: 69232.21 toks/s, output: 67.61 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:10<00:24, 55.14it/s, est. speed input: 68888.41 toks/s, output: 67.27 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:11<00:23, 55.16it/s, est. speed input: 68563.79 toks/s, output: 66.96 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:11<00:23, 55.15it/s, est. speed input: 68252.88 toks/s, output: 66.65 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:11<00:23, 55.13it/s, est. speed input: 67956.59 toks/s, output: 66.36 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:11<00:22, 55.14it/s, est. speed input: 67676.79 toks/s, output: 66.09 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:12<00:22, 55.15it/s, est. speed input: 67410.21 toks/s, output: 65.83 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:12<00:22, 55.13it/s, est. speed input: 67154.52 toks/s, output: 65.58 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:12<00:22, 55.13it/s, est. speed input: 66910.98 toks/s, output: 65.34 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:13<00:21, 55.15it/s, est. speed input: 66680.10 toks/s, output: 65.12 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:13<00:21, 55.15it/s, est. speed input: 66458.40 toks/s, output: 64.90 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:13<00:21, 55.13it/s, est. speed input: 66244.49 toks/s, output: 64.69 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:13<00:20, 55.14it/s, est. speed input: 66041.06 toks/s, output: 64.49 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:14<00:20, 55.15it/s, est. speed input: 65846.52 toks/s, output: 64.30 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:14<00:20, 55.13it/s, est. speed input: 65657.42 toks/s, output: 64.12 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:14<00:19, 55.15it/s, est. speed input: 65478.63 toks/s, output: 63.94 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:15<00:19, 55.15it/s, est. speed input: 65305.19 toks/s, output: 63.77 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:15<00:19, 55.13it/s, est. speed input: 65137.36 toks/s, output: 63.61 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:15<00:19, 55.12it/s, est. speed input: 64975.92 toks/s, output: 63.45 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:15<00:18, 55.12it/s, est. speed input: 64820.11 toks/s, output: 63.30 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:16<00:18, 55.12it/s, est. speed input: 64670.52 toks/s, output: 63.15 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:16<00:18, 55.10it/s, est. speed input: 64524.88 toks/s, output: 63.01 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:16<00:17, 55.12it/s, est. speed input: 64386.09 toks/s, output: 62.88 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:17<00:17, 55.12it/s, est. speed input: 64251.56 toks/s, output: 62.75 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:17<00:17, 55.11it/s, est. speed input: 64120.42 toks/s, output: 62.62 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:17<00:17, 55.10it/s, est. speed input: 63993.79 toks/s, output: 62.49 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:17<00:16, 55.11it/s, est. speed input: 63872.20 toks/s, output: 62.38 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:18<00:16, 55.10it/s, est. speed input: 63753.14 toks/s, output: 62.26 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:18<00:15, 55.95it/s, est. speed input: 63688.95 toks/s, output: 62.20 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:18<00:15, 55.70it/s, est. speed input: 63577.78 toks/s, output: 62.09 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:19<00:15, 55.53it/s, est. speed input: 63469.66 toks/s, output: 61.98 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:19<00:15, 55.39it/s, est. speed input: 63363.94 toks/s, output: 61.88 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:19<00:15, 55.31it/s, est. speed input: 63261.85 toks/s, output: 61.78 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:20<00:14, 55.25it/s, est. speed input: 63162.68 toks/s, output: 61.68 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:20<00:14, 55.22it/s, est. speed input: 63066.98 toks/s, output: 61.59 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:20<00:14, 55.17it/s, est. speed input: 62972.40 toks/s, output: 61.50 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:20<00:13, 55.15it/s, est. speed input: 62881.44 toks/s, output: 61.41 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:21<00:13, 55.14it/s, est. speed input: 62793.25 toks/s, output: 61.32 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:21<00:13, 55.11it/s, est. speed input: 62706.06 toks/s, output: 61.24 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:21<00:13, 55.10it/s, est. speed input: 62621.68 toks/s, output: 61.15 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:22<00:12, 55.10it/s, est. speed input: 62540.28 toks/s, output: 61.07 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:22<00:12, 55.10it/s, est. speed input: 62460.40 toks/s, output: 61.00 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:22<00:12, 55.05it/s, est. speed input: 62380.76 toks/s, output: 60.92 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:22<00:11, 55.07it/s, est. speed input: 62305.36 toks/s, output: 60.85 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:23<00:11, 55.08it/s, est. speed input: 62231.69 toks/s, output: 60.77 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:23<00:11, 55.09it/s, est. speed input: 62160.17 toks/s, output: 60.70 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:23<00:11, 55.08it/s, est. speed input: 62089.48 toks/s, output: 60.63 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:24<00:10, 55.06it/s, est. speed input: 62019.72 toks/s, output: 60.57 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:24<00:10, 55.04it/s, est. speed input: 61951.76 toks/s, output: 60.50 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:24<00:10, 55.05it/s, est. speed input: 61886.10 toks/s, output: 60.44 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:24<00:09, 55.06it/s, est. speed input: 61822.17 toks/s, output: 60.37 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:25<00:09, 55.08it/s, est. speed input: 61760.47 toks/s, output: 60.31 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:25<00:09, 55.06it/s, est. speed input: 61698.74 toks/s, output: 60.25 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:25<00:08, 55.05it/s, est. speed input: 61638.59 toks/s, output: 60.19 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:26<00:08, 55.06it/s, est. speed input: 61580.13 toks/s, output: 60.14 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:26<00:08, 55.05it/s, est. speed input: 61522.47 toks/s, output: 60.08 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:26<00:08, 55.05it/s, est. speed input: 61466.56 toks/s, output: 60.03 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:26<00:07, 55.05it/s, est. speed input: 61411.44 toks/s, output: 59.97 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:27<00:07, 55.05it/s, est. speed input: 61357.91 toks/s, output: 59.92 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:27<00:07, 55.03it/s, est. speed input: 61304.34 toks/s, output: 59.87 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:27<00:06, 55.03it/s, est. speed input: 61252.69 toks/s, output: 59.82 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:28<00:06, 55.02it/s, est. speed input: 61201.83 toks/s, output: 59.77 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:28<00:06, 55.02it/s, est. speed input: 61152.14 toks/s, output: 59.72 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:28<00:06, 55.04it/s, est. speed input: 61104.14 toks/s, output: 59.67 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:29<00:05, 55.05it/s, est. speed input: 61056.81 toks/s, output: 59.63 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:29<00:05, 55.06it/s, est. speed input: 61010.77 toks/s, output: 59.58 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:29<00:05, 55.02it/s, est. speed input: 60963.76 toks/s, output: 59.53 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:29<00:04, 55.03it/s, est. speed input: 60919.22 toks/s, output: 59.49 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:30<00:04, 55.04it/s, est. speed input: 60875.55 toks/s, output: 59.45 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:30<00:04, 55.03it/s, est. speed input: 60832.21 toks/s, output: 59.41 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:30<00:04, 55.03it/s, est. speed input: 60789.93 toks/s, output: 59.37 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:31<00:03, 55.05it/s, est. speed input: 60748.89 toks/s, output: 59.33 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:31<00:03, 55.07it/s, est. speed input: 60709.01 toks/s, output: 59.29 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:31<00:03, 56.05it/s, est. speed input: 60701.91 toks/s, output: 59.28 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:31<00:02, 55.73it/s, est. speed input: 60661.99 toks/s, output: 59.24 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:32<00:02, 55.53it/s, est. speed input: 60623.47 toks/s, output: 59.20 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:32<00:02, 55.38it/s, est. speed input: 60585.10 toks/s, output: 59.17 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:32<00:01, 55.27it/s, est. speed input: 60547.42 toks/s, output: 59.13 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:33<00:01, 55.20it/s, est. speed input: 60510.72 toks/s, output: 59.09 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:33<00:01, 55.13it/s, est. speed input: 60473.81 toks/s, output: 59.06 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:33<00:01, 55.10it/s, est. speed input: 60438.07 toks/s, output: 59.02 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:33<00:00, 55.08it/s, est. speed input: 60403.18 toks/s, output: 58.99 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:34<00:00, 55.07it/s, est. speed input: 60368.84 toks/s, output: 58.95 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:34<00:00, 56.00it/s, est. speed input: 60363.57 toks/s, output: 58.95 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:34<00:00, 56.00it/s, est. speed input: 60778.73 toks/s, output: 59.35 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:34<00:00, 59.35it/s, est. speed input: 60778.73 toks/s, output: 59.35 toks/s]
[rank0]:[W128 00:17:52.481882590 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 65.3s

测试结果:
  Requests/s:   55.06
  Tokens/s:     56438.76
  Total Reqs:   2048
  Elapsed:      37.19s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     56383.70

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:18:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3469613) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3469613) WARNING 01-28 00:18:18 [backends.py:609] Failed to read file <frozen os>
Throughput: 54.86 requests/s, 56227.92 total tokens/s, 54.86 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 00:18:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:18:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:18:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:18:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:18:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:18:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:18:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:18:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:18:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:18:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:18:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:18:12] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:18:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:12] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:12] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:18:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:18:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:18:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:18:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:18:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:18:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3469613) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3469613) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.58it/s]
(EngineCore_DP0 pid=3469613) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.58it/s]
(EngineCore_DP0 pid=3469613) 
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3469613) [2026-01-28 00:18:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3469613) [rank0]:W0128 00:18:21.052000 3469613 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3469613) [rank0]:W0128 00:18:21.100000 3469613 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3469613) [rank0]:W0128 00:18:21.777000 3469613 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3469613) [rank0]:W0128 00:18:21.853000 3469613 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3469613) 2026-01-28 00:18:24,510 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3469613) 2026-01-28 00:18:24,528 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3469613) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:03,  3.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 10.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 15.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 19.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 15.32it/s]
(EngineCore_DP0 pid=3469613) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.29it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.48it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 23.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 20.37it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:05, 716.18it/s]
Adding requests:   4%|▎         | 148/4096 [00:00<00:05, 739.87it/s]
Adding requests:   5%|▌         | 224/4096 [00:00<00:05, 748.30it/s]
Adding requests:   7%|▋         | 299/4096 [00:00<00:05, 737.14it/s]
Adding requests:   9%|▉         | 375/4096 [00:00<00:05, 743.18it/s]
Adding requests:  11%|█         | 452/4096 [00:00<00:04, 749.86it/s]
Adding requests:  13%|█▎        | 528/4096 [00:00<00:04, 737.79it/s]
Adding requests:  15%|█▍        | 605/4096 [00:00<00:04, 746.40it/s]
Adding requests:  17%|█▋        | 685/4096 [00:00<00:04, 760.96it/s]
Adding requests:  19%|█▊        | 762/4096 [00:01<00:04, 755.43it/s]
Adding requests:  20%|██        | 838/4096 [00:01<00:04, 742.69it/s]
Adding requests:  22%|██▏       | 917/4096 [00:01<00:04, 755.51it/s]
Adding requests:  24%|██▍       | 993/4096 [00:01<00:04, 753.46it/s]
Adding requests:  26%|██▌       | 1069/4096 [00:01<00:04, 754.31it/s]
Adding requests:  28%|██▊       | 1145/4096 [00:01<00:04, 730.90it/s]
Adding requests:  30%|██▉       | 1225/4096 [00:01<00:03, 748.38it/s]
Adding requests:  32%|███▏      | 1300/4096 [00:01<00:03, 747.99it/s]
Adding requests:  34%|███▎      | 1378/4096 [00:01<00:03, 756.79it/s]
Adding requests:  36%|███▌      | 1456/4096 [00:01<00:03, 762.62it/s]
Adding requests:  37%|███▋      | 1534/4096 [00:02<00:03, 767.77it/s]
Adding requests:  39%|███▉      | 1611/4096 [00:02<00:03, 766.46it/s]
Adding requests:  41%|████      | 1688/4096 [00:02<00:03, 764.18it/s]
Adding requests:  43%|████▎     | 1766/4096 [00:02<00:03, 767.13it/s]
Adding requests:  45%|████▌     | 1845/4096 [00:02<00:02, 773.85it/s]
Adding requests:  47%|████▋     | 1923/4096 [00:02<00:02, 772.67it/s]
Adding requests:  49%|████▉     | 2002/4096 [00:02<00:02, 777.64it/s]
Adding requests:  51%|█████     | 2082/4096 [00:02<00:02, 783.48it/s]
Adding requests:  53%|█████▎    | 2161/4096 [00:02<00:02, 767.37it/s]
Adding requests:  55%|█████▍    | 2239/4096 [00:02<00:02, 770.50it/s]
Adding requests:  57%|█████▋    | 2317/4096 [00:03<00:02, 757.99it/s]
Adding requests:  58%|█████▊    | 2393/4096 [00:03<00:02, 756.95it/s]
Adding requests:  60%|██████    | 2470/4096 [00:03<00:02, 758.94it/s]
Adding requests:  62%|██████▏   | 2546/4096 [00:03<00:02, 752.25it/s]
Adding requests:  64%|██████▍   | 2623/4096 [00:03<00:01, 755.36it/s]
Adding requests:  66%|██████▌   | 2699/4096 [00:03<00:01, 750.06it/s]
Adding requests:  68%|██████▊   | 2775/4096 [00:03<00:01, 748.88it/s]
Adding requests:  70%|██████▉   | 2851/4096 [00:03<00:01, 752.11it/s]
Adding requests:  71%|███████▏  | 2927/4096 [00:03<00:01, 751.77it/s]
Adding requests:  73%|███████▎  | 3003/4096 [00:03<00:01, 747.52it/s]
Adding requests:  75%|███████▌  | 3079/4096 [00:04<00:01, 750.34it/s]
Adding requests:  77%|███████▋  | 3155/4096 [00:04<00:01, 752.84it/s]
Adding requests:  79%|███████▉  | 3231/4096 [00:04<00:01, 752.42it/s]
Adding requests:  81%|████████  | 3310/4096 [00:04<00:01, 762.10it/s]
Adding requests:  83%|████████▎ | 3388/4096 [00:04<00:00, 767.40it/s]
Adding requests:  85%|████████▍ | 3465/4096 [00:04<00:00, 760.07it/s]
Adding requests:  86%|████████▋ | 3543/4096 [00:04<00:00, 764.60it/s]
Adding requests:  88%|████████▊ | 3620/4096 [00:04<00:00, 762.97it/s]
Adding requests:  90%|█████████ | 3697/4096 [00:04<00:00, 748.89it/s]
Adding requests:  92%|█████████▏| 3776/4096 [00:04<00:00, 760.44it/s]
Adding requests:  94%|█████████▍| 3855/4096 [00:05<00:00, 769.15it/s]
Adding requests:  96%|█████████▌| 3932/4096 [00:05<00:00, 769.35it/s]
Adding requests:  98%|█████████▊| 4010/4096 [00:05<00:00, 769.81it/s]
Adding requests: 100%|█████████▉| 4088/4096 [00:05<00:00, 760.70it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 757.34it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:00<00:06, 586.23it/s, est. speed input: 600313.43 toks/s, output: 586.23 toks/s]
Processed prompts:   9%|▊         | 349/4096 [00:01<00:13, 282.32it/s, est. speed input: 332006.86 toks/s, output: 324.22 toks/s]
Processed prompts:   9%|▉         | 380/4096 [00:01<00:21, 174.07it/s, est. speed input: 234625.36 toks/s, output: 229.13 toks/s]
Processed prompts:  10%|▉         | 399/4096 [00:02<00:31, 117.14it/s, est. speed input: 182427.34 toks/s, output: 178.15 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:02<00:42, 86.27it/s, est. speed input: 151705.99 toks/s, output: 148.15 toks/s] 
Processed prompts:  11%|█         | 450/4096 [00:03<00:48, 75.53it/s, est. speed input: 135386.51 toks/s, output: 132.21 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:03<00:52, 68.77it/s, est. speed input: 123838.51 toks/s, output: 120.94 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:04<00:55, 64.37it/s, est. speed input: 115234.29 toks/s, output: 112.53 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:05<00:57, 61.43it/s, est. speed input: 108576.01 toks/s, output: 106.03 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:05<00:59, 59.43it/s, est. speed input: 103267.19 toks/s, output: 100.85 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:06<01:00, 58.07it/s, est. speed input: 98939.25 toks/s, output: 96.62 toks/s]  
Processed prompts:  16%|█▌        | 642/4096 [00:06<01:00, 57.12it/s, est. speed input: 95337.34 toks/s, output: 93.10 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:07<01:00, 56.46it/s, est. speed input: 92293.33 toks/s, output: 90.13 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:08<01:00, 56.03it/s, est. speed input: 89699.18 toks/s, output: 87.60 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:08<01:00, 55.70it/s, est. speed input: 87445.91 toks/s, output: 85.40 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:09<00:59, 55.49it/s, est. speed input: 85482.68 toks/s, output: 83.48 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:09<00:59, 55.33it/s, est. speed input: 83747.73 toks/s, output: 81.78 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:10<00:59, 55.21it/s, est. speed input: 82205.83 toks/s, output: 80.28 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:10<00:58, 55.14it/s, est. speed input: 80832.29 toks/s, output: 78.94 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:11<00:58, 55.08it/s, est. speed input: 79592.72 toks/s, output: 77.73 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:12<00:57, 55.05it/s, est. speed input: 78474.61 toks/s, output: 76.64 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:12<00:56, 55.00it/s, est. speed input: 77454.54 toks/s, output: 75.64 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:13<00:56, 54.99it/s, est. speed input: 76527.09 toks/s, output: 74.73 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:13<00:55, 54.96it/s, est. speed input: 75675.04 toks/s, output: 73.90 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:14<00:55, 54.97it/s, est. speed input: 74894.80 toks/s, output: 73.14 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:15<00:54, 54.94it/s, est. speed input: 74170.05 toks/s, output: 72.43 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:15<00:54, 54.93it/s, est. speed input: 73500.39 toks/s, output: 71.78 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:16<00:53, 55.37it/s, est. speed input: 72951.25 toks/s, output: 71.24 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:16<00:52, 55.23it/s, est. speed input: 72369.49 toks/s, output: 70.67 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:17<00:52, 55.12it/s, est. speed input: 71826.23 toks/s, output: 70.14 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:17<00:51, 55.05it/s, est. speed input: 71318.29 toks/s, output: 69.65 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:18<00:51, 55.00it/s, est. speed input: 70842.72 toks/s, output: 69.18 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:19<00:50, 54.97it/s, est. speed input: 70396.83 toks/s, output: 68.75 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:19<00:50, 54.93it/s, est. speed input: 69975.12 toks/s, output: 68.34 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:20<00:49, 54.87it/s, est. speed input: 69572.90 toks/s, output: 67.94 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:20<00:48, 54.87it/s, est. speed input: 69199.35 toks/s, output: 67.58 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:21<00:48, 54.86it/s, est. speed input: 68844.38 toks/s, output: 67.23 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:22<00:47, 54.87it/s, est. speed input: 68510.45 toks/s, output: 66.90 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:22<00:47, 54.86it/s, est. speed input: 68190.96 toks/s, output: 66.59 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:23<00:46, 54.86it/s, est. speed input: 67889.13 toks/s, output: 66.30 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:23<00:46, 54.87it/s, est. speed input: 67602.64 toks/s, output: 66.02 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:24<00:45, 54.87it/s, est. speed input: 67329.18 toks/s, output: 65.75 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:24<00:44, 54.86it/s, est. speed input: 67068.32 toks/s, output: 65.50 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:25<00:44, 54.86it/s, est. speed input: 66818.95 toks/s, output: 65.25 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:26<00:43, 54.86it/s, est. speed input: 66581.86 toks/s, output: 65.02 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:26<00:43, 54.85it/s, est. speed input: 66353.23 toks/s, output: 64.80 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:27<00:42, 54.85it/s, est. speed input: 66135.45 toks/s, output: 64.59 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:27<00:41, 54.85it/s, est. speed input: 65926.62 toks/s, output: 64.38 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:28<00:41, 54.84it/s, est. speed input: 65725.87 toks/s, output: 64.19 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:29<00:40, 55.33it/s, est. speed input: 65572.38 toks/s, output: 64.04 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:29<00:39, 55.18it/s, est. speed input: 65386.16 toks/s, output: 63.85 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:30<00:39, 55.08it/s, est. speed input: 65208.46 toks/s, output: 63.68 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:30<00:38, 55.01it/s, est. speed input: 65036.75 toks/s, output: 63.51 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:31<00:38, 54.97it/s, est. speed input: 64871.93 toks/s, output: 63.35 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:31<00:37, 54.92it/s, est. speed input: 64712.36 toks/s, output: 63.20 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:32<00:37, 54.90it/s, est. speed input: 64559.15 toks/s, output: 63.05 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:33<00:36, 54.87it/s, est. speed input: 64410.21 toks/s, output: 62.90 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:33<00:36, 54.86it/s, est. speed input: 64266.85 toks/s, output: 62.76 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:34<00:35, 54.87it/s, est. speed input: 64129.79 toks/s, output: 62.63 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:34<00:34, 54.84it/s, est. speed input: 63995.15 toks/s, output: 62.50 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:35<00:34, 54.84it/s, est. speed input: 63866.01 toks/s, output: 62.37 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:36<00:33, 54.84it/s, est. speed input: 63741.19 toks/s, output: 62.25 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:36<00:33, 54.83it/s, est. speed input: 63619.60 toks/s, output: 62.13 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:37<00:32, 54.84it/s, est. speed input: 63502.66 toks/s, output: 62.01 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:37<00:32, 54.83it/s, est. speed input: 63388.49 toks/s, output: 61.90 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:38<00:31, 54.82it/s, est. speed input: 63278.08 toks/s, output: 61.79 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:38<00:30, 54.82it/s, est. speed input: 63170.81 toks/s, output: 61.69 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:39<00:30, 54.82it/s, est. speed input: 63066.95 toks/s, output: 61.59 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:40<00:29, 54.82it/s, est. speed input: 62965.91 toks/s, output: 61.49 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:40<00:28, 55.29it/s, est. speed input: 62893.63 toks/s, output: 61.42 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:41<00:28, 55.15it/s, est. speed input: 62798.00 toks/s, output: 61.33 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:41<00:27, 55.04it/s, est. speed input: 62704.56 toks/s, output: 61.23 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:42<00:27, 54.97it/s, est. speed input: 62613.94 toks/s, output: 61.15 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:43<00:26, 54.92it/s, est. speed input: 62525.88 toks/s, output: 61.06 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:43<00:26, 54.88it/s, est. speed input: 62439.56 toks/s, output: 60.98 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:44<00:25, 54.86it/s, est. speed input: 62356.16 toks/s, output: 60.89 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:44<00:25, 54.84it/s, est. speed input: 62274.61 toks/s, output: 60.82 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:45<00:24, 54.84it/s, est. speed input: 62195.79 toks/s, output: 60.74 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:45<00:23, 54.81it/s, est. speed input: 62117.74 toks/s, output: 60.66 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:46<00:23, 54.80it/s, est. speed input: 62042.03 toks/s, output: 60.59 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:47<00:22, 54.80it/s, est. speed input: 61968.73 toks/s, output: 60.52 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:47<00:22, 54.80it/s, est. speed input: 61896.72 toks/s, output: 60.45 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:48<00:21, 54.80it/s, est. speed input: 61826.93 toks/s, output: 60.38 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:48<00:20, 54.79it/s, est. speed input: 61758.38 toks/s, output: 60.31 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:49<00:20, 54.79it/s, est. speed input: 61691.37 toks/s, output: 60.25 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:50<00:19, 54.80it/s, est. speed input: 61626.50 toks/s, output: 60.18 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:50<00:19, 54.80it/s, est. speed input: 61563.05 toks/s, output: 60.12 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:51<00:18, 54.80it/s, est. speed input: 61500.70 toks/s, output: 60.06 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:51<00:18, 54.78it/s, est. speed input: 61439.40 toks/s, output: 60.00 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:52<00:17, 54.79it/s, est. speed input: 61380.00 toks/s, output: 59.94 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:52<00:16, 54.78it/s, est. speed input: 61321.43 toks/s, output: 59.88 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:53<00:16, 54.79it/s, est. speed input: 61264.59 toks/s, output: 59.83 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:54<00:15, 54.81it/s, est. speed input: 61209.75 toks/s, output: 59.78 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:54<00:15, 54.79it/s, est. speed input: 61154.61 toks/s, output: 59.72 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:55<00:14, 54.80it/s, est. speed input: 61101.56 toks/s, output: 59.67 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:55<00:13, 54.79it/s, est. speed input: 61049.15 toks/s, output: 59.62 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:56<00:13, 54.78it/s, est. speed input: 60997.74 toks/s, output: 59.57 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:57<00:12, 54.78it/s, est. speed input: 60947.35 toks/s, output: 59.52 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:57<00:12, 54.79it/s, est. speed input: 60898.48 toks/s, output: 59.47 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:58<00:11, 54.79it/s, est. speed input: 60850.52 toks/s, output: 59.42 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:58<00:11, 54.78it/s, est. speed input: 60802.93 toks/s, output: 59.38 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:59<00:10, 54.79it/s, est. speed input: 60756.83 toks/s, output: 59.33 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:59<00:09, 54.79it/s, est. speed input: 60711.50 toks/s, output: 59.29 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:00<00:09, 54.78it/s, est. speed input: 60666.82 toks/s, output: 59.24 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:01<00:08, 54.79it/s, est. speed input: 60623.27 toks/s, output: 59.20 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:01<00:08, 54.77it/s, est. speed input: 60579.75 toks/s, output: 59.16 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:02<00:07, 54.78it/s, est. speed input: 60537.98 toks/s, output: 59.12 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:02<00:06, 54.79it/s, est. speed input: 60497.10 toks/s, output: 59.08 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:03<00:06, 54.77it/s, est. speed input: 60455.94 toks/s, output: 59.04 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:04<00:05, 54.78it/s, est. speed input: 60416.41 toks/s, output: 59.00 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:04<00:05, 54.77it/s, est. speed input: 60376.92 toks/s, output: 58.96 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:05<00:04, 54.77it/s, est. speed input: 60338.57 toks/s, output: 58.92 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:05<00:04, 54.77it/s, est. speed input: 60300.73 toks/s, output: 58.89 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:06<00:03, 54.78it/s, est. speed input: 60263.98 toks/s, output: 58.85 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:06<00:02, 54.77it/s, est. speed input: 60227.42 toks/s, output: 58.82 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:07<00:02, 54.77it/s, est. speed input: 60191.39 toks/s, output: 58.78 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:08<00:01, 54.77it/s, est. speed input: 60156.08 toks/s, output: 58.75 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:08<00:01, 55.24it/s, est. speed input: 60136.14 toks/s, output: 58.73 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:09<00:00, 55.62it/s, est. speed input: 60117.53 toks/s, output: 58.71 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:09<00:00, 55.62it/s, est. speed input: 60560.84 toks/s, output: 59.14 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:09<00:00, 59.14it/s, est. speed input: 60560.84 toks/s, output: 59.14 toks/s]
[rank0]:[W128 00:19:41.236033306 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 108.7s

测试结果:
  Requests/s:   54.86
  Tokens/s:     56227.92
  Total Reqs:   4096
  Elapsed:      74.67s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     56173.06


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,55.3046,28371.2531,2.3145
1024,1024,1,128,128,53.1656,54494.7047,2.4076
2048,1024,2,256,128,60.7212,62239.2434,4.2160
4096,1024,4,512,128,60.0321,61532.8791,8.5288
8192,1024,8,1024,128,56.3370,57745.4650,18.1763
16384,1024,16,2048,128,55.0622,56438.7611,37.1943
32768,1024,32,4096,128,54.8565,56227.9180,74.6675

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:19:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3471216) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3471216) WARNING 01-28 00:19:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 54.95 requests/s, 28188.79 total tokens/s, 54.95 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:19:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:19:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:19:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:19:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:19:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:19:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:19:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:19:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:19:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:19:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:19:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:19:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:19:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:19:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:19:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:19:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:19:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:19:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3471216) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3471216) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3471216) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3471216) 
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3471216) 2026-01-28 00:20:01,838 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3471216) 2026-01-28 00:20:01,853 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3471216) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.86it/s]
(EngineCore_DP0 pid=3471216) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.38it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  88%|████████▊ | 112/128 [00:00<00:00, 1112.80it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1132.65it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:22,  5.57it/s, est. speed input: 2850.30 toks/s, output: 5.57 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 32.17it/s, est. speed input: 13966.49 toks/s, output: 27.28 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 44.36it/s, est. speed input: 18963.77 toks/s, output: 37.04 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 51.17it/s, est. speed input: 21839.63 toks/s, output: 42.65 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 55.06it/s, est. speed input: 23652.60 toks/s, output: 46.20 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 57.43it/s, est. speed input: 24900.49 toks/s, output: 48.63 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 58.17it/s, est. speed input: 25590.36 toks/s, output: 49.98 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 59.62it/s, est. speed input: 26354.26 toks/s, output: 51.47 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 60.50it/s, est. speed input: 26944.79 toks/s, output: 52.63 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 60.94it/s, est. speed input: 27401.15 toks/s, output: 53.52 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:00, 61.53it/s, est. speed input: 27814.74 toks/s, output: 54.33 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:00, 61.90it/s, est. speed input: 28158.71 toks/s, output: 55.00 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:00, 62.12it/s, est. speed input: 28446.79 toks/s, output: 55.56 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 62.24it/s, est. speed input: 28693.21 toks/s, output: 56.04 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:01<00:00, 62.43it/s, est. speed input: 28917.65 toks/s, output: 56.48 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 62.39it/s, est. speed input: 29098.56 toks/s, output: 56.83 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 62.22it/s, est. speed input: 29246.12 toks/s, output: 57.12 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 62.46it/s, est. speed input: 29408.20 toks/s, output: 57.44 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 62.69it/s, est. speed input: 29558.42 toks/s, output: 57.73 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 62.69it/s, est. speed input: 29592.81 toks/s, output: 57.80 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 57.80it/s, est. speed input: 29592.81 toks/s, output: 57.80 toks/s]
[rank0]:[W128 00:20:05.727060062 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.6s

测试结果:
  Requests/s:   54.95
  Tokens/s:     28188.79
  Total Reqs:   128
  Elapsed:      2.33s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     28133.84

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:20:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3471892) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3471892) WARNING 01-28 00:20:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 45.50 requests/s, 46634.28 total tokens/s, 45.50 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:20:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:20:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:20:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:20:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:20:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:20:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:20:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:20:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:20:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:20:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:20:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:20:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:20:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:20:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:20:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:20:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:20:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:20:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3471892) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3471892) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.83it/s]
(EngineCore_DP0 pid=3471892) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.83it/s]
(EngineCore_DP0 pid=3471892) 
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3471892) 2026-01-28 00:20:26,958 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3471892) 2026-01-28 00:20:26,974 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3471892) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.69it/s]
(EngineCore_DP0 pid=3471892) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.57it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.57it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 633.15it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 679.85it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:04, 26.67it/s, est. speed input: 27311.95 toks/s, output: 26.67 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 39.58it/s, est. speed input: 38442.70 toks/s, output: 37.54 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:02, 44.14it/s, est. speed input: 42474.06 toks/s, output: 41.48 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 46.28it/s, est. speed input: 44507.41 toks/s, output: 43.46 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 47.51it/s, est. speed input: 45758.11 toks/s, output: 44.68 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 48.31it/s, est. speed input: 46625.82 toks/s, output: 45.53 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 48.80it/s, est. speed input: 47241.66 toks/s, output: 46.13 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:01, 49.11it/s, est. speed input: 47700.30 toks/s, output: 46.58 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 49.35it/s, est. speed input: 48069.12 toks/s, output: 46.94 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:01, 49.38it/s, est. speed input: 48324.86 toks/s, output: 47.19 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 49.51it/s, est. speed input: 48566.13 toks/s, output: 47.43 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 49.59it/s, est. speed input: 48763.96 toks/s, output: 47.62 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 49.66it/s, est. speed input: 48934.55 toks/s, output: 47.79 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 49.68it/s, est. speed input: 49076.39 toks/s, output: 47.93 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 49.62it/s, est. speed input: 49182.13 toks/s, output: 48.03 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 49.68it/s, est. speed input: 49295.50 toks/s, output: 48.14 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:00, 49.82it/s, est. speed input: 49433.08 toks/s, output: 48.27 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 49.87it/s, est. speed input: 49546.46 toks/s, output: 48.38 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 49.80it/s, est. speed input: 49611.62 toks/s, output: 48.45 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 49.79it/s, est. speed input: 49675.86 toks/s, output: 48.51 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 49.78it/s, est. speed input: 49735.33 toks/s, output: 48.57 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 49.83it/s, est. speed input: 49808.91 toks/s, output: 48.64 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 49.81it/s, est. speed input: 49857.46 toks/s, output: 48.69 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 49.79it/s, est. speed input: 49900.52 toks/s, output: 48.73 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 49.80it/s, est. speed input: 49943.75 toks/s, output: 48.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 49.80it/s, est. speed input: 49965.50 toks/s, output: 48.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.79it/s, est. speed input: 49965.50 toks/s, output: 48.79 toks/s]
[rank0]:[W128 00:20:31.992028323 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.1s

测试结果:
  Requests/s:   45.50
  Tokens/s:     46634.28
  Total Reqs:   128
  Elapsed:      2.81s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     46588.78

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:20:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3472542) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3472542) WARNING 01-28 00:20:46 [backends.py:609] Failed to read file <frozen os>
Throughput: 49.24 requests/s, 50468.17 total tokens/s, 49.24 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 00:20:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:20:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:20:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:20:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:20:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:20:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:20:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:20:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:20:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:20:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:20:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:20:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:20:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:20:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:20:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:20:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:20:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:20:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3472542) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3472542) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=3472542) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=3472542) 
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3472542) 2026-01-28 00:20:52,153 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3472542) 2026-01-28 00:20:52,168 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3472542) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 22.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 22.88it/s]
(EngineCore_DP0 pid=3472542) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.96it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:38,  6.63it/s]
Adding requests:  29%|██▉       | 74/256 [00:00<00:00, 358.06it/s]
Adding requests:  58%|█████▊    | 149/256 [00:00<00:00, 516.11it/s]
Adding requests:  87%|████████▋ | 223/256 [00:00<00:00, 597.03it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 513.09it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 171.73it/s, est. speed input: 175867.20 toks/s, output: 171.73 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:00<00:02, 74.07it/s, est. speed input: 83329.30 toks/s, output: 81.38 toks/s]   
Processed prompts:  19%|█▉        | 49/256 [00:00<00:03, 67.09it/s, est. speed input: 75751.76 toks/s, output: 73.98 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:00<00:03, 59.54it/s, est. speed input: 69244.01 toks/s, output: 67.62 toks/s]
Processed prompts:  25%|██▌       | 65/256 [00:00<00:03, 59.64it/s, est. speed input: 68302.20 toks/s, output: 66.70 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:03, 55.10it/s, est. speed input: 65234.75 toks/s, output: 63.71 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:03, 54.20it/s, est. speed input: 64061.86 toks/s, output: 62.56 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:03, 53.47it/s, est. speed input: 63087.81 toks/s, output: 61.61 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:03, 52.83it/s, est. speed input: 62236.83 toks/s, output: 60.78 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:01<00:03, 52.33it/s, est. speed input: 61503.01 toks/s, output: 60.06 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:01<00:02, 52.02it/s, est. speed input: 60886.45 toks/s, output: 59.46 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:01<00:02, 51.82it/s, est. speed input: 60356.82 toks/s, output: 58.94 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:01<00:02, 51.71it/s, est. speed input: 59895.79 toks/s, output: 58.49 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:02<00:02, 51.60it/s, est. speed input: 59482.61 toks/s, output: 58.09 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:02<00:02, 51.55it/s, est. speed input: 59118.53 toks/s, output: 57.73 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:02<00:02, 51.51it/s, est. speed input: 58788.80 toks/s, output: 57.41 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:02<00:02, 51.51it/s, est. speed input: 58497.92 toks/s, output: 57.13 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:02<00:02, 51.37it/s, est. speed input: 58208.45 toks/s, output: 56.84 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:02<00:02, 51.36it/s, est. speed input: 57960.18 toks/s, output: 56.60 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:02<00:01, 51.32it/s, est. speed input: 57727.18 toks/s, output: 56.37 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:02<00:01, 51.27it/s, est. speed input: 57509.91 toks/s, output: 56.16 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:03<00:01, 51.17it/s, est. speed input: 57300.47 toks/s, output: 55.96 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:03<00:01, 51.24it/s, est. speed input: 57126.15 toks/s, output: 55.79 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:03<00:01, 51.30it/s, est. speed input: 56965.58 toks/s, output: 55.63 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:03<00:01, 51.34it/s, est. speed input: 56816.32 toks/s, output: 55.48 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:03<00:01, 51.30it/s, est. speed input: 56668.63 toks/s, output: 55.34 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:03<00:01, 51.21it/s, est. speed input: 56522.88 toks/s, output: 55.20 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:03<00:01, 51.28it/s, est. speed input: 56401.53 toks/s, output: 55.08 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:03<00:00, 51.24it/s, est. speed input: 56277.51 toks/s, output: 54.96 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:03<00:00, 51.21it/s, est. speed input: 56161.70 toks/s, output: 54.85 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:04<00:00, 51.28it/s, est. speed input: 56060.74 toks/s, output: 54.75 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:04<00:00, 51.32it/s, est. speed input: 55965.50 toks/s, output: 54.65 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:04<00:00, 51.37it/s, est. speed input: 55877.02 toks/s, output: 54.57 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:04<00:00, 51.40it/s, est. speed input: 55793.74 toks/s, output: 54.49 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:04<00:00, 51.29it/s, est. speed input: 55701.61 toks/s, output: 54.40 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:04<00:00, 51.27it/s, est. speed input: 55619.47 toks/s, output: 54.32 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 51.27it/s, est. speed input: 55793.25 toks/s, output: 54.49 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 54.48it/s, est. speed input: 55793.25 toks/s, output: 54.49 toks/s]
[rank0]:[W128 00:20:58.569137193 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 27.5s

测试结果:
  Requests/s:   49.24
  Tokens/s:     50468.17
  Total Reqs:   256
  Elapsed:      5.20s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     50418.93

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:21:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3473210) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3473210) WARNING 01-28 00:21:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 50.12 requests/s, 51369.33 total tokens/s, 50.12 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 00:21:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:21:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:21:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:21:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:21:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:21:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:21:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:21:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:21:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:21:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:21:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:21:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:21:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:21:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:21:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:21:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:21:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:21:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3473210) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3473210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3473210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3473210) 
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3473210) 2026-01-28 00:21:20,996 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3473210) 2026-01-28 00:21:21,013 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3473210) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.57it/s]
(EngineCore_DP0 pid=3473210) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.33it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 69/512 [00:00<00:00, 683.39it/s]
Adding requests:  28%|██▊       | 143/512 [00:00<00:00, 713.43it/s]
Adding requests:  42%|████▏     | 216/512 [00:00<00:00, 720.80it/s]
Adding requests:  57%|█████▋    | 292/512 [00:00<00:00, 733.10it/s]
Adding requests:  72%|███████▏  | 367/512 [00:00<00:00, 736.08it/s]
Adding requests:  86%|████████▌ | 441/512 [00:00<00:00, 736.63it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 731.18it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:00<00:02, 219.04it/s, est. speed input: 224308.63 toks/s, output: 219.04 toks/s]
Processed prompts:  12%|█▏        | 60/512 [00:00<00:04, 93.78it/s, est. speed input: 107737.30 toks/s, output: 105.21 toks/s] 
Processed prompts:  14%|█▍        | 73/512 [00:00<00:05, 78.28it/s, est. speed input: 92491.34 toks/s, output: 90.32 toks/s]  
Processed prompts:  16%|█▌        | 83/512 [00:01<00:06, 65.07it/s, est. speed input: 81156.35 toks/s, output: 79.25 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:01<00:06, 61.37it/s, est. speed input: 77306.45 toks/s, output: 75.49 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:01<00:07, 56.70it/s, est. speed input: 73542.38 toks/s, output: 71.82 toks/s]
Processed prompts:  21%|██        | 106/512 [00:01<00:07, 54.98it/s, est. speed input: 71266.37 toks/s, output: 69.60 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:01<00:07, 53.71it/s, est. speed input: 69424.14 toks/s, output: 67.80 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:01<00:07, 52.81it/s, est. speed input: 67907.41 toks/s, output: 66.32 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:01<00:07, 52.16it/s, est. speed input: 66632.69 toks/s, output: 65.07 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:02<00:07, 51.70it/s, est. speed input: 65545.27 toks/s, output: 64.01 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:02<00:07, 51.32it/s, est. speed input: 64591.02 toks/s, output: 63.08 toks/s]
Processed prompts:  30%|███       | 154/512 [00:02<00:07, 51.04it/s, est. speed input: 63755.17 toks/s, output: 62.26 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:02<00:06, 50.87it/s, est. speed input: 63028.38 toks/s, output: 61.55 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:02<00:06, 50.73it/s, est. speed input: 62378.79 toks/s, output: 60.92 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:02<00:06, 50.68it/s, est. speed input: 61809.26 toks/s, output: 60.36 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:03<00:06, 50.57it/s, est. speed input: 61284.06 toks/s, output: 59.85 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:03<00:06, 50.54it/s, est. speed input: 60817.40 toks/s, output: 59.39 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:03<00:06, 50.53it/s, est. speed input: 60396.77 toks/s, output: 58.98 toks/s]
Processed prompts:  41%|████      | 210/512 [00:03<00:05, 50.51it/s, est. speed input: 60011.42 toks/s, output: 58.60 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:03<00:05, 50.48it/s, est. speed input: 59654.90 toks/s, output: 58.26 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:03<00:05, 50.47it/s, est. speed input: 59330.02 toks/s, output: 57.94 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:04<00:05, 50.45it/s, est. speed input: 59029.37 toks/s, output: 57.65 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:04<00:05, 50.45it/s, est. speed input: 58752.01 toks/s, output: 57.37 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:04<00:05, 50.39it/s, est. speed input: 58486.12 toks/s, output: 57.12 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:04<00:05, 50.40it/s, est. speed input: 58245.84 toks/s, output: 56.88 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:04<00:04, 50.41it/s, est. speed input: 58023.82 toks/s, output: 56.66 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:04<00:04, 50.41it/s, est. speed input: 57814.45 toks/s, output: 56.46 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:05<00:04, 50.47it/s, est. speed input: 57624.88 toks/s, output: 56.27 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:05<00:04, 50.44it/s, est. speed input: 57439.12 toks/s, output: 56.09 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:05<00:04, 50.41it/s, est. speed input: 57263.93 toks/s, output: 55.92 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:05<00:04, 50.43it/s, est. speed input: 57102.99 toks/s, output: 55.76 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:05<00:03, 50.45it/s, est. speed input: 56950.95 toks/s, output: 55.62 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:05<00:03, 50.46it/s, est. speed input: 56807.41 toks/s, output: 55.48 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:05<00:03, 50.48it/s, est. speed input: 56673.14 toks/s, output: 55.34 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:06<00:03, 50.50it/s, est. speed input: 56545.57 toks/s, output: 55.22 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:06<00:03, 50.46it/s, est. speed input: 56420.39 toks/s, output: 55.10 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:06<00:03, 50.48it/s, est. speed input: 56304.96 toks/s, output: 54.99 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:06<00:02, 50.47it/s, est. speed input: 56193.67 toks/s, output: 54.88 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:06<00:02, 50.43it/s, est. speed input: 56083.98 toks/s, output: 54.77 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:06<00:02, 50.43it/s, est. speed input: 55982.37 toks/s, output: 54.67 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:07<00:02, 50.44it/s, est. speed input: 55885.89 toks/s, output: 54.58 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:07<00:02, 50.40it/s, est. speed input: 55789.54 toks/s, output: 54.48 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:07<00:02, 50.36it/s, est. speed input: 55696.92 toks/s, output: 54.39 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:07<00:02, 50.40it/s, est. speed input: 55613.02 toks/s, output: 54.31 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:07<00:01, 50.43it/s, est. speed input: 55532.81 toks/s, output: 54.23 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:07<00:01, 50.44it/s, est. speed input: 55455.08 toks/s, output: 54.16 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:08<00:01, 50.46it/s, est. speed input: 55381.31 toks/s, output: 54.08 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:08<00:01, 50.45it/s, est. speed input: 55308.97 toks/s, output: 54.01 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:08<00:01, 50.43it/s, est. speed input: 55238.14 toks/s, output: 53.94 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:08<00:01, 50.46it/s, est. speed input: 55172.70 toks/s, output: 53.88 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:08<00:00, 50.47it/s, est. speed input: 55109.57 toks/s, output: 53.82 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:08<00:00, 50.43it/s, est. speed input: 55045.10 toks/s, output: 53.75 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:08<00:00, 50.46it/s, est. speed input: 54986.63 toks/s, output: 53.70 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:09<00:00, 50.47it/s, est. speed input: 54930.08 toks/s, output: 53.64 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:09<00:00, 50.43it/s, est. speed input: 54872.29 toks/s, output: 53.59 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:09<00:00, 50.42it/s, est. speed input: 54817.53 toks/s, output: 53.53 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 50.42it/s, est. speed input: 55100.81 toks/s, output: 53.81 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 53.81it/s, est. speed input: 55100.81 toks/s, output: 53.81 toks/s]
[rank0]:[W128 00:21:32.597433692 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.0s

测试结果:
  Requests/s:   50.12
  Tokens/s:     51369.33
  Total Reqs:   512
  Elapsed:      10.22s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     51319.21

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:21:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3473980) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3473980) WARNING 01-28 00:21:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 47.70 requests/s, 48890.71 total tokens/s, 47.70 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 00:21:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:21:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:21:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:21:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:21:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:21:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:21:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:21:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:21:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:21:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:21:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:21:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:21:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:21:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:21:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:21:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:21:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:21:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3473980) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3473980) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.87it/s]
(EngineCore_DP0 pid=3473980) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.87it/s]
(EngineCore_DP0 pid=3473980) 
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3473980) 2026-01-28 00:21:55,891 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3473980) 2026-01-28 00:21:55,907 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3473980) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  3.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 12.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 11.88it/s]
(EngineCore_DP0 pid=3473980) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.98it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 72/1024 [00:00<00:01, 713.50it/s]
Adding requests:  15%|█▍        | 149/1024 [00:00<00:01, 741.36it/s]
Adding requests:  22%|██▏       | 226/1024 [00:00<00:01, 750.07it/s]
Adding requests:  29%|██▉       | 302/1024 [00:00<00:00, 751.40it/s]
Adding requests:  37%|███▋      | 378/1024 [00:00<00:00, 749.40it/s]
Adding requests:  44%|████▍     | 454/1024 [00:00<00:00, 750.48it/s]
Adding requests:  52%|█████▏    | 530/1024 [00:00<00:00, 738.64it/s]
Adding requests:  59%|█████▉    | 607/1024 [00:00<00:00, 747.49it/s]
Adding requests:  67%|██████▋   | 685/1024 [00:00<00:00, 755.89it/s]
Adding requests:  74%|███████▍  | 761/1024 [00:01<00:00, 754.69it/s]
Adding requests:  82%|████████▏ | 837/1024 [00:01<00:00, 738.13it/s]
Adding requests:  89%|████████▉ | 916/1024 [00:01<00:00, 753.09it/s]
Adding requests:  97%|█████████▋| 993/1024 [00:01<00:00, 757.71it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 751.28it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:00<00:03, 294.85it/s, est. speed input: 301950.86 toks/s, output: 294.86 toks/s]
Processed prompts:   9%|▉         | 96/1024 [00:00<00:08, 115.90it/s, est. speed input: 135661.65 toks/s, output: 132.48 toks/s]
Processed prompts:  11%|█         | 112/1024 [00:01<00:10, 87.33it/s, est. speed input: 108358.11 toks/s, output: 105.82 toks/s]
Processed prompts:  12%|█▏        | 123/1024 [00:01<00:13, 66.91it/s, est. speed input: 90452.23 toks/s, output: 88.33 toks/s]  
Processed prompts:  13%|█▎        | 131/1024 [00:01<00:14, 62.88it/s, est. speed input: 86015.78 toks/s, output: 84.00 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:01<00:15, 58.03it/s, est. speed input: 81854.77 toks/s, output: 79.94 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:01<00:15, 55.51it/s, est. speed input: 78959.86 toks/s, output: 77.11 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:02<00:16, 53.54it/s, est. speed input: 76547.31 toks/s, output: 74.75 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:02<00:16, 52.00it/s, est. speed input: 74484.12 toks/s, output: 72.74 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:02<00:16, 50.86it/s, est. speed input: 72713.42 toks/s, output: 71.01 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:02<00:16, 50.00it/s, est. speed input: 71167.34 toks/s, output: 69.50 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:02<00:16, 49.40it/s, est. speed input: 69814.55 toks/s, output: 68.18 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:02<00:16, 48.93it/s, est. speed input: 68607.97 toks/s, output: 67.00 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:03<00:16, 48.64it/s, est. speed input: 67544.21 toks/s, output: 65.96 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:03<00:16, 48.44it/s, est. speed input: 66592.92 toks/s, output: 65.03 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:03<00:16, 48.27it/s, est. speed input: 65727.51 toks/s, output: 64.19 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:03<00:16, 48.14it/s, est. speed input: 64942.30 toks/s, output: 63.42 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:03<00:16, 48.08it/s, est. speed input: 64233.78 toks/s, output: 62.73 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:03<00:16, 48.01it/s, est. speed input: 63580.07 toks/s, output: 62.09 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:04<00:16, 47.97it/s, est. speed input: 62981.99 toks/s, output: 61.51 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:04<00:15, 47.95it/s, est. speed input: 62432.39 toks/s, output: 60.97 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:15, 47.92it/s, est. speed input: 61921.72 toks/s, output: 60.47 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:04<00:15, 47.90it/s, est. speed input: 61448.35 toks/s, output: 60.01 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:04<00:15, 47.87it/s, est. speed input: 61006.52 toks/s, output: 59.58 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:04<00:15, 47.88it/s, est. speed input: 60598.96 toks/s, output: 59.18 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:15, 47.88it/s, est. speed input: 60217.70 toks/s, output: 58.81 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:14, 47.89it/s, est. speed input: 59861.73 toks/s, output: 58.46 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:05<00:14, 47.89it/s, est. speed input: 59526.81 toks/s, output: 58.13 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:05<00:14, 47.85it/s, est. speed input: 59207.48 toks/s, output: 57.82 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:05<00:14, 47.85it/s, est. speed input: 58910.38 toks/s, output: 57.53 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:05<00:14, 47.88it/s, est. speed input: 58632.57 toks/s, output: 57.26 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:06<00:14, 47.87it/s, est. speed input: 58367.63 toks/s, output: 57.00 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:06<00:13, 47.88it/s, est. speed input: 58117.81 toks/s, output: 56.76 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:06<00:13, 47.88it/s, est. speed input: 57880.51 toks/s, output: 56.52 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:06<00:13, 47.87it/s, est. speed input: 57654.77 toks/s, output: 56.30 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:06<00:13, 47.84it/s, est. speed input: 57437.65 toks/s, output: 56.09 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:06<00:13, 47.87it/s, est. speed input: 57236.11 toks/s, output: 55.89 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:07<00:13, 47.86it/s, est. speed input: 57040.50 toks/s, output: 55.70 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:07<00:12, 47.86it/s, est. speed input: 56855.49 toks/s, output: 55.52 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:07<00:12, 47.86it/s, est. speed input: 56678.67 toks/s, output: 55.35 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:07<00:12, 47.83it/s, est. speed input: 56506.19 toks/s, output: 55.18 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:07<00:12, 47.82it/s, est. speed input: 56343.29 toks/s, output: 55.02 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:07<00:12, 47.85it/s, est. speed input: 56189.28 toks/s, output: 54.87 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:08<00:12, 47.85it/s, est. speed input: 56040.87 toks/s, output: 54.73 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:08<00:11, 47.84it/s, est. speed input: 55897.35 toks/s, output: 54.59 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:08<00:11, 47.84it/s, est. speed input: 55759.61 toks/s, output: 54.45 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:08<00:11, 47.84it/s, est. speed input: 55627.54 toks/s, output: 54.32 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:08<00:11, 47.84it/s, est. speed input: 55501.03 toks/s, output: 54.20 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:08<00:11, 47.87it/s, est. speed input: 55380.39 toks/s, output: 54.08 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:09<00:11, 47.87it/s, est. speed input: 55263.47 toks/s, output: 53.97 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:09<00:10, 47.86it/s, est. speed input: 55149.96 toks/s, output: 53.86 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:09<00:10, 47.82it/s, est. speed input: 55038.34 toks/s, output: 53.75 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:09<00:10, 47.83it/s, est. speed input: 54933.07 toks/s, output: 53.65 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:09<00:10, 47.84it/s, est. speed input: 54831.23 toks/s, output: 53.55 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:09<00:10, 47.85it/s, est. speed input: 54733.43 toks/s, output: 53.45 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:10<00:10, 47.84it/s, est. speed input: 54637.82 toks/s, output: 53.36 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:10<00:09, 47.84it/s, est. speed input: 54545.31 toks/s, output: 53.27 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:10<00:09, 47.84it/s, est. speed input: 54456.52 toks/s, output: 53.18 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:10<00:09, 47.81it/s, est. speed input: 54368.00 toks/s, output: 53.09 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:10<00:09, 47.82it/s, est. speed input: 54284.33 toks/s, output: 53.01 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:10<00:09, 47.85it/s, est. speed input: 54204.69 toks/s, output: 52.93 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:11<00:09, 47.86it/s, est. speed input: 54126.85 toks/s, output: 52.86 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:11<00:08, 47.85it/s, est. speed input: 54050.21 toks/s, output: 52.78 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:11<00:08, 47.86it/s, est. speed input: 53976.80 toks/s, output: 52.71 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:11<00:08, 47.84it/s, est. speed input: 53904.27 toks/s, output: 52.64 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:11<00:08, 47.84it/s, est. speed input: 53834.25 toks/s, output: 52.57 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:11<00:08, 47.86it/s, est. speed input: 53767.06 toks/s, output: 52.51 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:12<00:08, 47.87it/s, est. speed input: 53701.92 toks/s, output: 52.44 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:12<00:07, 47.87it/s, est. speed input: 53638.28 toks/s, output: 52.38 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:12<00:07, 47.86it/s, est. speed input: 53575.56 toks/s, output: 52.32 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:12<00:07, 47.80it/s, est. speed input: 53511.96 toks/s, output: 52.26 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:12<00:07, 47.81it/s, est. speed input: 53452.75 toks/s, output: 52.20 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:12<00:07, 47.84it/s, est. speed input: 53395.63 toks/s, output: 52.14 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:13<00:07, 47.81it/s, est. speed input: 53338.03 toks/s, output: 52.09 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:13<00:06, 47.82it/s, est. speed input: 53283.29 toks/s, output: 52.03 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:13<00:06, 47.84it/s, est. speed input: 53230.63 toks/s, output: 51.98 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:13<00:06, 47.81it/s, est. speed input: 53176.75 toks/s, output: 51.93 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:13<00:06, 47.81it/s, est. speed input: 53125.51 toks/s, output: 51.88 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:13<00:06, 47.83it/s, est. speed input: 53076.15 toks/s, output: 51.83 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:14<00:06, 47.82it/s, est. speed input: 53027.35 toks/s, output: 51.78 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:14<00:05, 47.84it/s, est. speed input: 52980.66 toks/s, output: 51.74 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:14<00:05, 47.84it/s, est. speed input: 52934.23 toks/s, output: 51.69 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:14<00:05, 47.82it/s, est. speed input: 52888.18 toks/s, output: 51.65 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:14<00:05, 47.82it/s, est. speed input: 52843.89 toks/s, output: 51.61 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:14<00:05, 47.82it/s, est. speed input: 52800.47 toks/s, output: 51.56 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:15<00:05, 47.84it/s, est. speed input: 52758.86 toks/s, output: 51.52 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:15<00:04, 47.85it/s, est. speed input: 52717.78 toks/s, output: 51.48 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:15<00:04, 47.86it/s, est. speed input: 52677.79 toks/s, output: 51.44 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:15<00:04, 47.84it/s, est. speed input: 52637.64 toks/s, output: 51.40 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:15<00:04, 47.83it/s, est. speed input: 52598.79 toks/s, output: 51.37 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:15<00:04, 47.81it/s, est. speed input: 52560.02 toks/s, output: 51.33 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:16<00:04, 47.83it/s, est. speed input: 52523.19 toks/s, output: 51.29 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:16<00:03, 47.82it/s, est. speed input: 52486.50 toks/s, output: 51.26 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:16<00:03, 47.80it/s, est. speed input: 52450.06 toks/s, output: 51.22 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:16<00:03, 47.77it/s, est. speed input: 52413.71 toks/s, output: 51.19 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:16<00:03, 47.77it/s, est. speed input: 52378.70 toks/s, output: 51.15 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:16<00:03, 47.77it/s, est. speed input: 52344.38 toks/s, output: 51.12 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:17<00:03, 47.80it/s, est. speed input: 52312.02 toks/s, output: 51.09 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:17<00:02, 47.81it/s, est. speed input: 52279.65 toks/s, output: 51.05 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:17<00:02, 47.78it/s, est. speed input: 52246.77 toks/s, output: 51.02 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:17<00:02, 47.79it/s, est. speed input: 52215.41 toks/s, output: 50.99 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:17<00:02, 47.81it/s, est. speed input: 52185.35 toks/s, output: 50.96 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:17<00:02, 47.80it/s, est. speed input: 52154.88 toks/s, output: 50.93 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:18<00:02, 47.79it/s, est. speed input: 52124.90 toks/s, output: 50.90 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:18<00:01, 47.80it/s, est. speed input: 52095.94 toks/s, output: 50.87 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:18<00:01, 47.77it/s, est. speed input: 52066.31 toks/s, output: 50.85 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:18<00:01, 47.75it/s, est. speed input: 52037.27 toks/s, output: 50.82 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:18<00:01, 47.75it/s, est. speed input: 52009.32 toks/s, output: 50.79 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:18<00:01, 47.76it/s, est. speed input: 51982.28 toks/s, output: 50.76 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:19<00:01, 47.79it/s, est. speed input: 51956.05 toks/s, output: 50.74 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:19<00:00, 47.78it/s, est. speed input: 51929.66 toks/s, output: 50.71 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:19<00:00, 47.77it/s, est. speed input: 51903.52 toks/s, output: 50.69 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:19<00:00, 47.73it/s, est. speed input: 51876.80 toks/s, output: 50.66 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:19<00:00, 47.75it/s, est. speed input: 51851.87 toks/s, output: 50.64 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:19<00:00, 47.76it/s, est. speed input: 51827.54 toks/s, output: 50.61 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:20<00:00, 49.45it/s, est. speed input: 51852.42 toks/s, output: 50.64 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:20<00:00, 49.45it/s, est. speed input: 52157.73 toks/s, output: 50.94 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:20<00:00, 50.94it/s, est. speed input: 52157.73 toks/s, output: 50.94 toks/s]
[rank0]:[W128 00:22:19.950428348 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 46.3s

测试结果:
  Requests/s:   47.70
  Tokens/s:     48890.71
  Total Reqs:   1024
  Elapsed:      21.47s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     48843.01

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:22:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3475003) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3475003) WARNING 01-28 00:22:39 [backends.py:609] Failed to read file <frozen os>
Throughput: 46.59 requests/s, 47756.89 total tokens/s, 46.59 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 00:22:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:22:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:22:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:22:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:22:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:22:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:22:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:22:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:22:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:22:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:22:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:22:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:22:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:22:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:22:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:22:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:22:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:22:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3475003) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3475003) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]
(EngineCore_DP0 pid=3475003) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]
(EngineCore_DP0 pid=3475003) 
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3475003) 2026-01-28 00:22:45,151 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3475003) 2026-01-28 00:22:45,195 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3475003) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  4.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 10.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.84it/s]
(EngineCore_DP0 pid=3475003) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.66it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 68/2048 [00:00<00:02, 679.29it/s]
Adding requests:   7%|▋         | 141/2048 [00:00<00:02, 708.45it/s]
Adding requests:  11%|█         | 216/2048 [00:00<00:02, 724.31it/s]
Adding requests:  14%|█▍        | 294/2048 [00:00<00:02, 745.39it/s]
Adding requests:  18%|█▊        | 369/2048 [00:00<00:02, 746.44it/s]
Adding requests:  22%|██▏       | 445/2048 [00:00<00:02, 748.04it/s]
Adding requests:  25%|██▌       | 520/2048 [00:00<00:02, 741.89it/s]
Adding requests:  29%|██▉       | 597/2048 [00:00<00:01, 749.02it/s]
Adding requests:  33%|███▎      | 673/2048 [00:00<00:01, 749.99it/s]
Adding requests:  37%|███▋      | 749/2048 [00:01<00:01, 750.67it/s]
Adding requests:  40%|████      | 825/2048 [00:01<00:01, 739.57it/s]
Adding requests:  44%|████▍     | 902/2048 [00:01<00:01, 747.26it/s]
Adding requests:  48%|████▊     | 977/2048 [00:01<00:01, 721.98it/s]
Adding requests:  51%|█████▏    | 1054/2048 [00:01<00:01, 735.83it/s]
Adding requests:  55%|█████▌    | 1128/2048 [00:01<00:01, 735.44it/s]
Adding requests:  59%|█████▉    | 1206/2048 [00:01<00:01, 747.68it/s]
Adding requests:  63%|██████▎   | 1281/2048 [00:01<00:01, 739.96it/s]
Adding requests:  66%|██████▋   | 1358/2048 [00:01<00:00, 746.21it/s]
Adding requests:  70%|███████   | 1436/2048 [00:01<00:00, 753.88it/s]
Adding requests:  74%|███████▍  | 1517/2048 [00:02<00:00, 767.53it/s]
Adding requests:  78%|███████▊  | 1598/2048 [00:02<00:00, 777.50it/s]
Adding requests:  82%|████████▏ | 1676/2048 [00:02<00:00, 775.02it/s]
Adding requests:  86%|████████▌ | 1754/2048 [00:02<00:00, 769.00it/s]
Adding requests:  89%|████████▉ | 1831/2048 [00:02<00:00, 755.56it/s]
Adding requests:  93%|█████████▎| 1907/2048 [00:02<00:00, 753.20it/s]
Adding requests:  97%|█████████▋| 1984/2048 [00:02<00:00, 756.47it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 749.05it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:00<00:06, 303.63it/s, est. speed input: 310930.37 toks/s, output: 303.64 toks/s]
Processed prompts:   8%|▊         | 161/2048 [00:00<00:09, 189.91it/s, est. speed input: 213868.65 toks/s, output: 208.86 toks/s]
Processed prompts:   9%|▊         | 179/2048 [00:01<00:20, 91.18it/s, est. speed input: 125962.07 toks/s, output: 123.01 toks/s] 
Processed prompts:   9%|▉         | 194/2048 [00:01<00:24, 76.90it/s, est. speed input: 110508.86 toks/s, output: 107.92 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:02<00:27, 67.84it/s, est. speed input: 100498.87 toks/s, output: 98.14 toks/s] 
Processed prompts:  11%|█         | 226/2048 [00:02<00:29, 61.49it/s, est. speed input: 93238.15 toks/s, output: 91.05 toks/s] 
Processed prompts:  12%|█▏        | 242/2048 [00:02<00:31, 57.04it/s, est. speed input: 87727.46 toks/s, output: 85.67 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:03<00:33, 53.95it/s, est. speed input: 83420.91 toks/s, output: 81.47 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:03<00:34, 51.80it/s, est. speed input: 79956.40 toks/s, output: 78.08 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:03<00:34, 50.25it/s, est. speed input: 77091.44 toks/s, output: 75.28 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:04<00:35, 49.19it/s, est. speed input: 74703.16 toks/s, output: 72.95 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:04<00:35, 48.34it/s, est. speed input: 72634.42 toks/s, output: 70.93 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:04<00:35, 47.84it/s, est. speed input: 70888.75 toks/s, output: 69.23 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:05<00:35, 47.50it/s, est. speed input: 69378.47 toks/s, output: 67.75 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:05<00:35, 47.27it/s, est. speed input: 68052.87 toks/s, output: 66.46 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:05<00:35, 47.09it/s, est. speed input: 66877.51 toks/s, output: 65.31 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:06<00:35, 46.98it/s, est. speed input: 65834.43 toks/s, output: 64.29 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:06<00:34, 46.89it/s, est. speed input: 64897.95 toks/s, output: 63.38 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:06<00:34, 46.82it/s, est. speed input: 64051.32 toks/s, output: 62.55 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:07<00:34, 46.80it/s, est. speed input: 63291.55 toks/s, output: 61.81 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:07<00:33, 46.77it/s, est. speed input: 62596.31 toks/s, output: 61.13 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:07<00:33, 46.74it/s, est. speed input: 61959.18 toks/s, output: 60.51 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:08<00:33, 46.73it/s, est. speed input: 61375.46 toks/s, output: 59.94 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:08<00:32, 46.72it/s, est. speed input: 60839.44 toks/s, output: 59.41 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:08<00:32, 46.71it/s, est. speed input: 60341.94 toks/s, output: 58.93 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:09<00:32, 46.70it/s, est. speed input: 59881.76 toks/s, output: 58.48 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:09<00:31, 46.70it/s, est. speed input: 59455.13 toks/s, output: 58.06 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:10<00:31, 46.68it/s, est. speed input: 59054.72 toks/s, output: 57.67 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:10<00:31, 46.68it/s, est. speed input: 58681.76 toks/s, output: 57.31 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:10<00:30, 46.68it/s, est. speed input: 58333.97 toks/s, output: 56.97 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:11<00:30, 46.67it/s, est. speed input: 58005.76 toks/s, output: 56.65 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:11<00:30, 46.67it/s, est. speed input: 57698.99 toks/s, output: 56.35 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:11<00:29, 46.66it/s, est. speed input: 57408.23 toks/s, output: 56.06 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:12<00:29, 46.66it/s, est. speed input: 57135.17 toks/s, output: 55.80 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:12<00:29, 46.67it/s, est. speed input: 56877.72 toks/s, output: 55.54 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:12<00:28, 46.65it/s, est. speed input: 56631.03 toks/s, output: 55.30 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:13<00:28, 46.65it/s, est. speed input: 56399.96 toks/s, output: 55.08 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:13<00:28, 46.65it/s, est. speed input: 56179.83 toks/s, output: 54.86 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:13<00:27, 46.63it/s, est. speed input: 55969.10 toks/s, output: 54.66 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:14<00:27, 46.64it/s, est. speed input: 55769.97 toks/s, output: 54.46 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:14<00:27, 46.64it/s, est. speed input: 55580.66 toks/s, output: 54.28 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:14<00:26, 46.63it/s, est. speed input: 55398.64 toks/s, output: 54.10 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:15<00:26, 46.64it/s, est. speed input: 55226.05 toks/s, output: 53.93 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:15<00:26, 46.64it/s, est. speed input: 55061.41 toks/s, output: 53.77 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:15<00:25, 46.62it/s, est. speed input: 54901.76 toks/s, output: 53.61 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:16<00:25, 46.63it/s, est. speed input: 54750.68 toks/s, output: 53.47 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:16<00:25, 46.63it/s, est. speed input: 54605.31 toks/s, output: 53.33 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:16<00:24, 46.62it/s, est. speed input: 54465.02 toks/s, output: 53.19 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:17<00:24, 46.62it/s, est. speed input: 54331.04 toks/s, output: 53.06 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:17<00:23, 46.62it/s, est. speed input: 54202.30 toks/s, output: 52.93 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:17<00:23, 46.61it/s, est. speed input: 54077.73 toks/s, output: 52.81 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:18<00:23, 46.62it/s, est. speed input: 53958.91 toks/s, output: 52.69 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:18<00:22, 46.62it/s, est. speed input: 53844.00 toks/s, output: 52.58 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:18<00:22, 46.61it/s, est. speed input: 53732.77 toks/s, output: 52.47 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:19<00:22, 46.62it/s, est. speed input: 53626.31 toks/s, output: 52.37 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:19<00:21, 46.61it/s, est. speed input: 53522.77 toks/s, output: 52.27 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:19<00:21, 46.59it/s, est. speed input: 53421.84 toks/s, output: 52.17 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:20<00:21, 46.60it/s, est. speed input: 53325.99 toks/s, output: 52.08 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:20<00:20, 46.59it/s, est. speed input: 53232.29 toks/s, output: 51.98 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:21<00:20, 46.59it/s, est. speed input: 53142.03 toks/s, output: 51.90 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:21<00:20, 46.59it/s, est. speed input: 53054.60 toks/s, output: 51.81 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:21<00:19, 46.60it/s, est. speed input: 52970.50 toks/s, output: 51.73 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:22<00:19, 46.59it/s, est. speed input: 52887.80 toks/s, output: 51.65 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:22<00:18, 47.34it/s, est. speed input: 52851.55 toks/s, output: 51.61 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:22<00:18, 47.11it/s, est. speed input: 52773.69 toks/s, output: 51.54 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:23<00:18, 46.95it/s, est. speed input: 52698.11 toks/s, output: 51.46 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:23<00:18, 46.84it/s, est. speed input: 52624.81 toks/s, output: 51.39 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:23<00:17, 46.77it/s, est. speed input: 52553.70 toks/s, output: 51.32 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:24<00:17, 46.72it/s, est. speed input: 52484.70 toks/s, output: 51.25 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:24<00:17, 46.69it/s, est. speed input: 52417.95 toks/s, output: 51.19 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:24<00:16, 46.64it/s, est. speed input: 52351.93 toks/s, output: 51.12 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:25<00:16, 46.63it/s, est. speed input: 52288.72 toks/s, output: 51.06 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:25<00:16, 46.63it/s, est. speed input: 52227.23 toks/s, output: 51.00 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:25<00:15, 46.60it/s, est. speed input: 52166.08 toks/s, output: 50.94 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:26<00:15, 46.60it/s, est. speed input: 52107.55 toks/s, output: 50.89 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:26<00:15, 46.60it/s, est. speed input: 52050.64 toks/s, output: 50.83 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:26<00:14, 46.59it/s, est. speed input: 51994.59 toks/s, output: 50.78 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:27<00:14, 46.59it/s, est. speed input: 51940.64 toks/s, output: 50.72 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:27<00:14, 46.60it/s, est. speed input: 51888.25 toks/s, output: 50.67 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:27<00:13, 46.58it/s, est. speed input: 51835.94 toks/s, output: 50.62 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:28<00:13, 46.59it/s, est. speed input: 51785.84 toks/s, output: 50.57 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:28<00:13, 46.57it/s, est. speed input: 51736.07 toks/s, output: 50.52 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:28<00:12, 46.57it/s, est. speed input: 51687.80 toks/s, output: 50.48 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:29<00:12, 46.57it/s, est. speed input: 51640.80 toks/s, output: 50.43 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:29<00:11, 46.58it/s, est. speed input: 51595.50 toks/s, output: 50.39 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:29<00:11, 46.58it/s, est. speed input: 51550.65 toks/s, output: 50.34 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:30<00:11, 46.58it/s, est. speed input: 51507.02 toks/s, output: 50.30 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:30<00:10, 46.59it/s, est. speed input: 51464.41 toks/s, output: 50.26 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:30<00:10, 46.57it/s, est. speed input: 51421.95 toks/s, output: 50.22 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:31<00:10, 46.58it/s, est. speed input: 51381.33 toks/s, output: 50.18 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:31<00:09, 46.57it/s, est. speed input: 51341.16 toks/s, output: 50.14 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:31<00:09, 46.56it/s, est. speed input: 51301.51 toks/s, output: 50.10 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:32<00:09, 46.56it/s, est. speed input: 51262.91 toks/s, output: 50.06 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:32<00:08, 46.56it/s, est. speed input: 51225.33 toks/s, output: 50.02 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:33<00:08, 46.56it/s, est. speed input: 51188.16 toks/s, output: 49.99 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:33<00:08, 46.56it/s, est. speed input: 51152.05 toks/s, output: 49.95 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:33<00:07, 46.57it/s, est. speed input: 51117.14 toks/s, output: 49.92 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:34<00:07, 46.56it/s, est. speed input: 51082.07 toks/s, output: 49.88 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:34<00:07, 46.57it/s, est. speed input: 51048.22 toks/s, output: 49.85 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:34<00:06, 46.55it/s, est. speed input: 51014.40 toks/s, output: 49.82 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:35<00:06, 46.55it/s, est. speed input: 50981.71 toks/s, output: 49.79 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:35<00:06, 46.55it/s, est. speed input: 50949.53 toks/s, output: 49.76 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:35<00:05, 46.55it/s, est. speed input: 50917.89 toks/s, output: 49.72 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:36<00:05, 46.56it/s, est. speed input: 50887.16 toks/s, output: 49.69 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:36<00:05, 46.55it/s, est. speed input: 50856.67 toks/s, output: 49.66 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:36<00:04, 46.55it/s, est. speed input: 50826.74 toks/s, output: 49.64 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:37<00:04, 46.55it/s, est. speed input: 50797.48 toks/s, output: 49.61 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:37<00:04, 46.55it/s, est. speed input: 50768.84 toks/s, output: 49.58 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:37<00:03, 47.40it/s, est. speed input: 50768.11 toks/s, output: 49.58 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:38<00:03, 47.15it/s, est. speed input: 50740.37 toks/s, output: 49.55 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:38<00:03, 46.97it/s, est. speed input: 50713.10 toks/s, output: 49.52 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:38<00:02, 46.84it/s, est. speed input: 50685.95 toks/s, output: 49.50 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:39<00:02, 46.76it/s, est. speed input: 50659.67 toks/s, output: 49.47 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:39<00:02, 46.71it/s, est. speed input: 50634.06 toks/s, output: 49.45 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:39<00:01, 46.65it/s, est. speed input: 50608.11 toks/s, output: 49.42 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:40<00:01, 46.62it/s, est. speed input: 50583.02 toks/s, output: 49.40 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:40<00:00, 46.58it/s, est. speed input: 50557.83 toks/s, output: 49.37 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:40<00:00, 46.56it/s, est. speed input: 50533.16 toks/s, output: 49.35 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:41<00:00, 47.21it/s, est. speed input: 50528.52 toks/s, output: 49.34 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:41<00:00, 47.21it/s, est. speed input: 50876.08 toks/s, output: 49.68 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:41<00:00, 49.68it/s, est. speed input: 50876.08 toks/s, output: 49.68 toks/s]
[rank0]:[W128 00:23:31.833944993 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 71.9s

测试结果:
  Requests/s:   46.59
  Tokens/s:     47756.89
  Total Reqs:   2048
  Elapsed:      43.96s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     47710.30

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:23:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3476461) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3476461) WARNING 01-28 00:23:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 46.51 requests/s, 47671.14 total tokens/s, 46.51 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 00:23:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:23:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:23:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:23:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:23:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:23:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:23:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:23:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:23:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:23:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:23:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:23:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:23:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:23:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:23:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:23:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:23:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:23:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3476461) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3476461) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3476461) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3476461) 
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3476461) [rank0]:W0128 00:23:59.859000 3476461 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3476461) [rank0]:W0128 00:23:59.908000 3476461 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3476461) [rank0]:W0128 00:24:00.589000 3476461 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3476461) [rank0]:W0128 00:24:00.665000 3476461 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3476461) 2026-01-28 00:24:02,951 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3476461) 2026-01-28 00:24:02,968 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3476461) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:04,  2.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  7.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 12.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 16.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 12.36it/s]
(EngineCore_DP0 pid=3476461) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.38it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 22.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 20.27it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 71/4096 [00:00<00:05, 708.81it/s]
Adding requests:   3%|▎         | 142/4096 [00:00<00:05, 698.62it/s]
Adding requests:   5%|▌         | 212/4096 [00:00<00:05, 697.51it/s]
Adding requests:   7%|▋         | 289/4096 [00:00<00:05, 724.08it/s]
Adding requests:   9%|▉         | 365/4096 [00:00<00:05, 736.42it/s]
Adding requests:  11%|█         | 441/4096 [00:00<00:04, 743.65it/s]
Adding requests:  13%|█▎        | 516/4096 [00:00<00:04, 736.75it/s]
Adding requests:  14%|█▍        | 590/4096 [00:00<00:04, 733.79it/s]
Adding requests:  16%|█▋        | 666/4096 [00:00<00:04, 740.05it/s]
Adding requests:  18%|█▊        | 743/4096 [00:01<00:04, 749.16it/s]
Adding requests:  20%|█▉        | 818/4096 [00:01<00:04, 739.68it/s]
Adding requests:  22%|██▏       | 895/4096 [00:01<00:04, 746.07it/s]
Adding requests:  24%|██▎       | 970/4096 [00:01<00:04, 744.01it/s]
Adding requests:  26%|██▌       | 1045/4096 [00:01<00:04, 732.17it/s]
Adding requests:  27%|██▋       | 1119/4096 [00:01<00:04, 734.27it/s]
Adding requests:  29%|██▉       | 1197/4096 [00:01<00:03, 746.92it/s]
Adding requests:  31%|███       | 1272/4096 [00:01<00:03, 745.11it/s]
Adding requests:  33%|███▎      | 1350/4096 [00:01<00:03, 754.16it/s]
Adding requests:  35%|███▍      | 1429/4096 [00:01<00:03, 762.70it/s]
Adding requests:  37%|███▋      | 1509/4096 [00:02<00:03, 773.57it/s]
Adding requests:  39%|███▉      | 1589/4096 [00:02<00:03, 779.22it/s]
Adding requests:  41%|████      | 1667/4096 [00:02<00:03, 773.27it/s]
Adding requests:  43%|████▎     | 1745/4096 [00:02<00:03, 773.73it/s]
Adding requests:  45%|████▍     | 1823/4096 [00:02<00:02, 769.60it/s]
Adding requests:  46%|████▋     | 1901/4096 [00:02<00:02, 771.85it/s]
Adding requests:  48%|████▊     | 1979/4096 [00:02<00:02, 772.02it/s]
Adding requests:  50%|█████     | 2057/4096 [00:02<00:02, 768.71it/s]
Adding requests:  52%|█████▏    | 2134/4096 [00:02<00:02, 743.20it/s]
Adding requests:  54%|█████▍    | 2209/4096 [00:02<00:02, 730.59it/s]
Adding requests:  56%|█████▌    | 2288/4096 [00:03<00:02, 745.13it/s]
Adding requests:  58%|█████▊    | 2365/4096 [00:03<00:02, 750.24it/s]
Adding requests:  60%|█████▉    | 2442/4096 [00:03<00:02, 754.05it/s]
Adding requests:  61%|██████▏   | 2518/4096 [00:03<00:02, 754.80it/s]
Adding requests:  63%|██████▎   | 2595/4096 [00:03<00:01, 758.45it/s]
Adding requests:  65%|██████▌   | 2673/4096 [00:03<00:01, 763.92it/s]
Adding requests:  67%|██████▋   | 2750/4096 [00:03<00:01, 765.18it/s]
Adding requests:  69%|██████▉   | 2827/4096 [00:03<00:01, 755.60it/s]
Adding requests:  71%|███████   | 2905/4096 [00:03<00:01, 762.73it/s]
Adding requests:  73%|███████▎  | 2982/4096 [00:03<00:01, 759.16it/s]
Adding requests:  75%|███████▍  | 3059/4096 [00:04<00:01, 759.46it/s]
Adding requests:  77%|███████▋  | 3136/4096 [00:04<00:01, 758.71it/s]
Adding requests:  78%|███████▊  | 3212/4096 [00:04<00:01, 757.92it/s]
Adding requests:  80%|████████  | 3288/4096 [00:04<00:01, 757.90it/s]
Adding requests:  82%|████████▏ | 3365/4096 [00:04<00:00, 761.22it/s]
Adding requests:  84%|████████▍ | 3442/4096 [00:04<00:00, 760.64it/s]
Adding requests:  86%|████████▌ | 3519/4096 [00:04<00:00, 756.38it/s]
Adding requests:  88%|████████▊ | 3595/4096 [00:04<00:00, 747.09it/s]
Adding requests:  90%|████████▉ | 3671/4096 [00:04<00:00, 749.77it/s]
Adding requests:  91%|█████████▏| 3747/4096 [00:04<00:00, 752.78it/s]
Adding requests:  93%|█████████▎| 3828/4096 [00:05<00:00, 767.97it/s]
Adding requests:  95%|█████████▌| 3907/4096 [00:05<00:00, 773.09it/s]
Adding requests:  97%|█████████▋| 3985/4096 [00:05<00:00, 767.65it/s]
Adding requests:  99%|█████████▉| 4062/4096 [00:05<00:00, 766.73it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 754.35it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:00<00:02, 1694.23it/s, est. speed input: 1735092.16 toks/s, output: 1694.29 toks/s]
Processed prompts:  10%|▉         | 396/4096 [00:03<00:39, 93.21it/s, est. speed input: 113870.18 toks/s, output: 111.20 toks/s]    
Processed prompts:  11%|█▏        | 469/4096 [00:04<00:45, 78.91it/s, est. speed input: 97355.17 toks/s, output: 95.07 toks/s]  
Processed prompts:  12%|█▏        | 511/4096 [00:05<00:47, 75.31it/s, est. speed input: 93123.21 toks/s, output: 90.94 toks/s]
Processed prompts:  13%|█▎        | 539/4096 [00:06<00:52, 67.54it/s, est. speed input: 87539.22 toks/s, output: 85.49 toks/s]
Processed prompts:  14%|█▎        | 559/4096 [00:06<01:00, 58.20it/s, est. speed input: 81878.05 toks/s, output: 79.96 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:07<01:09, 50.33it/s, est. speed input: 77096.65 toks/s, output: 75.29 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:08<01:10, 49.33it/s, est. speed input: 74686.23 toks/s, output: 72.94 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:09<01:11, 48.57it/s, est. speed input: 72642.44 toks/s, output: 70.94 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:09<01:11, 48.02it/s, est. speed input: 70886.20 toks/s, output: 69.22 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:10<01:11, 47.62it/s, est. speed input: 69363.23 toks/s, output: 67.74 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:11<01:10, 47.32it/s, est. speed input: 68025.66 toks/s, output: 66.43 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:11<01:10, 47.11it/s, est. speed input: 66843.49 toks/s, output: 65.28 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:12<01:10, 46.96it/s, est. speed input: 65792.72 toks/s, output: 64.25 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:13<01:09, 46.85it/s, est. speed input: 64849.58 toks/s, output: 63.33 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:13<01:09, 46.78it/s, est. speed input: 64000.56 toks/s, output: 62.50 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:14<01:08, 46.72it/s, est. speed input: 63231.64 toks/s, output: 61.75 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:15<01:07, 46.69it/s, est. speed input: 62532.32 toks/s, output: 61.07 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:15<01:07, 46.65it/s, est. speed input: 61891.59 toks/s, output: 60.44 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:16<01:06, 46.63it/s, est. speed input: 61304.92 toks/s, output: 59.87 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:17<01:05, 46.61it/s, est. speed input: 60762.54 toks/s, output: 59.34 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:17<01:05, 46.59it/s, est. speed input: 60262.38 toks/s, output: 58.85 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:18<01:04, 46.58it/s, est. speed input: 59799.03 toks/s, output: 58.40 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:19<01:03, 46.58it/s, est. speed input: 59368.79 toks/s, output: 57.98 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:20<01:03, 46.65it/s, est. speed input: 58979.38 toks/s, output: 57.60 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:20<01:02, 46.63it/s, est. speed input: 58605.12 toks/s, output: 57.23 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:21<01:01, 46.61it/s, est. speed input: 58254.16 toks/s, output: 56.89 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:22<01:01, 46.59it/s, est. speed input: 57924.87 toks/s, output: 56.57 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:22<01:00, 46.53it/s, est. speed input: 57609.31 toks/s, output: 56.26 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:23<00:59, 46.55it/s, est. speed input: 57319.26 toks/s, output: 55.98 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:24<00:59, 46.55it/s, est. speed input: 57044.53 toks/s, output: 55.71 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:24<00:58, 46.55it/s, est. speed input: 56785.52 toks/s, output: 55.45 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:25<00:57, 46.56it/s, est. speed input: 56540.70 toks/s, output: 55.22 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:26<00:57, 46.55it/s, est. speed input: 56308.01 toks/s, output: 54.99 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:26<00:56, 46.55it/s, est. speed input: 56086.73 toks/s, output: 54.77 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:27<00:55, 46.55it/s, est. speed input: 55877.58 toks/s, output: 54.57 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:28<00:54, 46.55it/s, est. speed input: 55677.88 toks/s, output: 54.37 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:28<00:54, 46.54it/s, est. speed input: 55486.95 toks/s, output: 54.19 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:29<00:53, 46.54it/s, est. speed input: 55305.68 toks/s, output: 54.01 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:30<00:52, 46.54it/s, est. speed input: 55132.35 toks/s, output: 53.84 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:31<00:52, 46.53it/s, est. speed input: 54966.04 toks/s, output: 53.68 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:31<00:51, 46.54it/s, est. speed input: 54807.78 toks/s, output: 53.52 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:32<00:50, 46.54it/s, est. speed input: 54656.04 toks/s, output: 53.38 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:33<00:50, 46.53it/s, est. speed input: 54509.68 toks/s, output: 53.23 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:33<00:49, 46.52it/s, est. speed input: 54369.66 toks/s, output: 53.10 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:34<00:48, 46.52it/s, est. speed input: 54235.36 toks/s, output: 52.96 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:35<00:48, 46.08it/s, est. speed input: 54072.57 toks/s, output: 52.81 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:35<00:47, 46.21it/s, est. speed input: 53948.96 toks/s, output: 52.68 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:36<00:46, 46.31it/s, est. speed input: 53830.87 toks/s, output: 52.57 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:37<00:46, 46.37it/s, est. speed input: 53716.18 toks/s, output: 52.46 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:37<00:45, 46.41it/s, est. speed input: 53605.38 toks/s, output: 52.35 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:38<00:44, 46.44it/s, est. speed input: 53498.96 toks/s, output: 52.25 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:39<00:44, 46.46it/s, est. speed input: 53396.01 toks/s, output: 52.14 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:40<00:43, 46.47it/s, est. speed input: 53296.86 toks/s, output: 52.05 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:40<00:42, 46.48it/s, est. speed input: 53200.69 toks/s, output: 51.95 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:41<00:41, 46.49it/s, est. speed input: 53108.17 toks/s, output: 51.86 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:42<00:41, 46.49it/s, est. speed input: 53018.18 toks/s, output: 51.78 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:42<00:40, 46.49it/s, est. speed input: 52931.19 toks/s, output: 51.69 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:43<00:39, 46.50it/s, est. speed input: 52847.27 toks/s, output: 51.61 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:44<00:39, 46.49it/s, est. speed input: 52765.23 toks/s, output: 51.53 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:44<00:38, 46.51it/s, est. speed input: 52686.91 toks/s, output: 51.45 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:45<00:37, 46.50it/s, est. speed input: 52609.99 toks/s, output: 51.38 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:46<00:37, 46.50it/s, est. speed input: 52535.76 toks/s, output: 51.30 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:46<00:36, 46.50it/s, est. speed input: 52463.25 toks/s, output: 51.23 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:47<00:35, 46.49it/s, est. speed input: 52392.54 toks/s, output: 51.16 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:48<00:35, 46.49it/s, est. speed input: 52324.15 toks/s, output: 51.10 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:48<00:34, 46.89it/s, est. speed input: 52278.94 toks/s, output: 51.05 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:49<00:33, 46.77it/s, est. speed input: 52213.94 toks/s, output: 50.99 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:50<00:32, 46.69it/s, est. speed input: 52150.98 toks/s, output: 50.93 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:50<00:32, 46.62it/s, est. speed input: 52089.22 toks/s, output: 50.87 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:51<00:31, 46.58it/s, est. speed input: 52029.69 toks/s, output: 50.81 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:52<00:30, 46.55it/s, est. speed input: 51971.34 toks/s, output: 50.75 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:53<00:30, 46.52it/s, est. speed input: 51914.32 toks/s, output: 50.70 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:53<00:29, 46.51it/s, est. speed input: 51858.96 toks/s, output: 50.64 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:54<00:28, 46.49it/s, est. speed input: 51804.70 toks/s, output: 50.59 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:55<00:28, 46.50it/s, est. speed input: 51752.62 toks/s, output: 50.54 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:55<00:27, 46.48it/s, est. speed input: 51700.67 toks/s, output: 50.49 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:56<00:26, 46.49it/s, est. speed input: 51651.12 toks/s, output: 50.44 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:57<00:26, 46.47it/s, est. speed input: 51601.42 toks/s, output: 50.39 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:57<00:25, 46.46it/s, est. speed input: 51553.30 toks/s, output: 50.35 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:58<00:24, 46.46it/s, est. speed input: 51506.69 toks/s, output: 50.30 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:59<00:24, 46.46it/s, est. speed input: 51460.81 toks/s, output: 50.25 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:59<00:23, 46.46it/s, est. speed input: 51416.01 toks/s, output: 50.21 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:00<00:22, 46.46it/s, est. speed input: 51372.67 toks/s, output: 50.17 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:01<00:21, 46.46it/s, est. speed input: 51330.06 toks/s, output: 50.13 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:02<00:21, 46.45it/s, est. speed input: 51287.86 toks/s, output: 50.09 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:02<00:20, 46.46it/s, est. speed input: 51247.15 toks/s, output: 50.05 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:03<00:19, 46.46it/s, est. speed input: 51207.19 toks/s, output: 50.01 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:04<00:19, 46.46it/s, est. speed input: 51168.18 toks/s, output: 49.97 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:04<00:18, 46.46it/s, est. speed input: 51129.94 toks/s, output: 49.93 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:05<00:17, 46.46it/s, est. speed input: 51092.70 toks/s, output: 49.90 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:06<00:17, 46.46it/s, est. speed input: 51056.08 toks/s, output: 49.86 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:06<00:16, 46.46it/s, est. speed input: 51020.18 toks/s, output: 49.82 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:07<00:15, 46.46it/s, est. speed input: 50985.08 toks/s, output: 49.79 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:08<00:15, 46.46it/s, est. speed input: 50950.44 toks/s, output: 49.76 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:08<00:14, 46.44it/s, est. speed input: 50916.24 toks/s, output: 49.72 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:09<00:13, 46.45it/s, est. speed input: 50883.32 toks/s, output: 49.69 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:10<00:13, 46.45it/s, est. speed input: 50850.85 toks/s, output: 49.66 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:10<00:12, 46.45it/s, est. speed input: 50818.86 toks/s, output: 49.63 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:11<00:11, 46.46it/s, est. speed input: 50787.82 toks/s, output: 49.60 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:12<00:10, 46.45it/s, est. speed input: 50757.07 toks/s, output: 49.57 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:13<00:10, 46.45it/s, est. speed input: 50726.89 toks/s, output: 49.54 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:13<00:09, 46.45it/s, est. speed input: 50697.35 toks/s, output: 49.51 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:14<00:08, 46.45it/s, est. speed input: 50668.43 toks/s, output: 49.48 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:15<00:08, 46.45it/s, est. speed input: 50639.81 toks/s, output: 49.45 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:15<00:07, 46.45it/s, est. speed input: 50611.76 toks/s, output: 49.43 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:16<00:06, 46.45it/s, est. speed input: 50584.36 toks/s, output: 49.40 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:17<00:06, 46.45it/s, est. speed input: 50557.30 toks/s, output: 49.37 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:17<00:05, 46.45it/s, est. speed input: 50530.92 toks/s, output: 49.35 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:18<00:04, 46.45it/s, est. speed input: 50504.81 toks/s, output: 49.32 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:19<00:04, 46.44it/s, est. speed input: 50479.01 toks/s, output: 49.30 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:19<00:03, 46.44it/s, est. speed input: 50453.67 toks/s, output: 49.27 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:20<00:02, 46.44it/s, est. speed input: 50428.83 toks/s, output: 49.25 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:21<00:02, 46.44it/s, est. speed input: 50404.45 toks/s, output: 49.22 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:21<00:01, 46.85it/s, est. speed input: 50392.97 toks/s, output: 49.21 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:22<00:00, 47.19it/s, est. speed input: 50382.90 toks/s, output: 49.20 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:22<00:00, 47.19it/s, est. speed input: 50754.47 toks/s, output: 49.56 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:22<00:00, 49.56it/s, est. speed input: 50754.47 toks/s, output: 49.56 toks/s]
[rank0]:[W128 00:25:33.274863278 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 122.4s

测试结果:
  Requests/s:   46.51
  Tokens/s:     47671.14
  Total Reqs:   4096
  Elapsed:      88.07s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     47624.63


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,54.9489,28188.7923,2.3294
1024,1024,1,128,128,45.4969,46634.2806,2.8134
2048,1024,2,256,128,49.2372,50468.1719,5.1993
4096,1024,4,512,128,50.1164,51369.3295,10.2162
8192,1024,8,1024,128,47.6983,48890.7076,21.4683
16384,1024,16,2048,128,46.5921,47756.8899,43.9560
32768,1024,32,4096,128,46.5084,47671.1365,88.0701

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:25:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3478272) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3478272) WARNING 01-28 00:25:47 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.61 requests/s, 28528.51 total tokens/s, 55.61 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:25:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:25:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:25:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:25:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:25:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:25:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:25:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:25:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:25:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:25:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:25:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:25:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:25:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:25:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:25:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:25:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:25:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:25:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:25:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3478272) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3478272) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.67it/s]
(EngineCore_DP0 pid=3478272) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.67it/s]
(EngineCore_DP0 pid=3478272) 
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3478272) [2026-01-28 00:25:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3478272) 2026-01-28 00:25:53,999 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3478272) 2026-01-28 00:25:54,016 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3478272) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.07it/s]
(EngineCore_DP0 pid=3478272) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.28it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  92%|█████████▏| 118/128 [00:00<00:00, 1174.94it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1188.36it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:20,  6.12it/s, est. speed input: 3133.93 toks/s, output: 6.12 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 33.96it/s, est. speed input: 14855.39 toks/s, output: 29.01 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 45.91it/s, est. speed input: 19850.86 toks/s, output: 38.77 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 52.31it/s, est. speed input: 22637.02 toks/s, output: 44.21 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 56.07it/s, est. speed input: 24405.80 toks/s, output: 47.67 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 58.34it/s, est. speed input: 25612.59 toks/s, output: 50.02 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 60.03it/s, est. speed input: 26537.82 toks/s, output: 51.83 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 60.80it/s, est. speed input: 27185.55 toks/s, output: 53.10 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 61.25it/s, est. speed input: 27684.62 toks/s, output: 54.07 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 61.59it/s, est. speed input: 28092.29 toks/s, output: 54.87 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 61.65it/s, est. speed input: 28405.39 toks/s, output: 55.48 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 62.17it/s, est. speed input: 28728.26 toks/s, output: 56.11 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 62.53it/s, est. speed input: 29000.57 toks/s, output: 56.64 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 62.44it/s, est. speed input: 29200.42 toks/s, output: 57.03 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 61.85it/s, est. speed input: 29319.80 toks/s, output: 57.27 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 62.30it/s, est. speed input: 29508.24 toks/s, output: 57.63 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 62.50it/s, est. speed input: 29663.38 toks/s, output: 57.94 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 62.12it/s, est. speed input: 29757.83 toks/s, output: 58.12 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 62.18it/s, est. speed input: 29868.45 toks/s, output: 58.34 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 62.18it/s, est. speed input: 29884.56 toks/s, output: 58.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 58.37it/s, est. speed input: 29884.56 toks/s, output: 58.37 toks/s]
[rank0]:[W128 00:25:58.864529107 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.6s

测试结果:
  Requests/s:   55.61
  Tokens/s:     28528.51
  Total Reqs:   128
  Elapsed:      2.30s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     28472.90

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:26:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3478948) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3478948) WARNING 01-28 00:26:12 [backends.py:609] Failed to read file <frozen os>
Throughput: 44.32 requests/s, 45430.53 total tokens/s, 44.32 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:26:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:26:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:26:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:26:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:26:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:26:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:26:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:26:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:26:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:26:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:26:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:26:07] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:26:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:07] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:07] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:26:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:26:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:26:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:26:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:26:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3478948) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3478948) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.63it/s]
(EngineCore_DP0 pid=3478948) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.63it/s]
(EngineCore_DP0 pid=3478948) 
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3478948) [2026-01-28 00:26:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3478948) 2026-01-28 00:26:18,815 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3478948) 2026-01-28 00:26:18,831 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3478948) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 23.69it/s]
(EngineCore_DP0 pid=3478948) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.40it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 634.32it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 679.61it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:06, 19.34it/s, est. speed input: 19809.59 toks/s, output: 19.34 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 36.41it/s, est. speed input: 34666.83 toks/s, output: 33.85 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:02, 41.95it/s, est. speed input: 39672.00 toks/s, output: 38.74 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 44.57it/s, est. speed input: 42192.08 toks/s, output: 41.20 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 46.10it/s, est. speed input: 43746.43 toks/s, output: 42.72 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 47.01it/s, est. speed input: 44774.29 toks/s, output: 43.72 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 47.46it/s, est. speed input: 45460.30 toks/s, output: 44.39 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 47.81it/s, est. speed input: 45991.43 toks/s, output: 44.91 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 48.15it/s, est. speed input: 46443.29 toks/s, output: 45.35 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:01, 48.37it/s, est. speed input: 46801.84 toks/s, output: 45.70 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 48.41it/s, est. speed input: 47060.90 toks/s, output: 45.96 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 48.26it/s, est. speed input: 47232.56 toks/s, output: 46.12 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 48.37it/s, est. speed input: 47427.58 toks/s, output: 46.32 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 48.42it/s, est. speed input: 47589.71 toks/s, output: 46.47 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 48.51it/s, est. speed input: 47743.43 toks/s, output: 46.62 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 48.61it/s, est. speed input: 47883.54 toks/s, output: 46.76 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:00, 48.62it/s, est. speed input: 47998.05 toks/s, output: 46.87 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 48.65it/s, est. speed input: 48101.75 toks/s, output: 46.97 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 48.65it/s, est. speed input: 48192.71 toks/s, output: 47.06 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 48.68it/s, est. speed input: 48278.47 toks/s, output: 47.15 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 48.65it/s, est. speed input: 48348.45 toks/s, output: 47.21 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 48.56it/s, est. speed input: 48402.29 toks/s, output: 47.27 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 48.50it/s, est. speed input: 48451.14 toks/s, output: 47.32 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 48.47it/s, est. speed input: 48500.48 toks/s, output: 47.36 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 48.30it/s, est. speed input: 48519.28 toks/s, output: 47.38 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 48.45it/s, est. speed input: 48576.08 toks/s, output: 47.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.45it/s, est. speed input: 48584.80 toks/s, output: 47.45 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 47.44it/s, est. speed input: 48584.80 toks/s, output: 47.45 toks/s]
[rank0]:[W128 00:26:23.841813717 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.0s

测试结果:
  Requests/s:   44.32
  Tokens/s:     45430.53
  Total Reqs:   128
  Elapsed:      2.89s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     45386.21

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:26:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3479584) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3479584) WARNING 01-28 00:26:38 [backends.py:609] Failed to read file <frozen os>
Throughput: 49.46 requests/s, 50694.47 total tokens/s, 49.46 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 00:26:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:26:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:26:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:26:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:26:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:26:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:26:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:26:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:26:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:26:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:26:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:26:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:26:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:26:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:26:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:26:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:26:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:26:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3479584) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3479584) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.61it/s]
(EngineCore_DP0 pid=3479584) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.61it/s]
(EngineCore_DP0 pid=3479584) 
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3479584) [2026-01-28 00:26:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3479584) 2026-01-28 00:26:44,073 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3479584) 2026-01-28 00:26:44,088 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3479584) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 22.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 22.46it/s]
(EngineCore_DP0 pid=3479584) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.61it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  27%|██▋       | 69/256 [00:00<00:00, 689.92it/s]
Adding requests:  57%|█████▋    | 145/256 [00:00<00:00, 726.68it/s]
Adding requests:  86%|████████▌ | 219/256 [00:00<00:00, 732.54it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 734.69it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 151.56it/s, est. speed input: 155205.66 toks/s, output: 151.56 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:00<00:03, 72.84it/s, est. speed input: 81663.51 toks/s, output: 79.75 toks/s]   
Processed prompts:  18%|█▊        | 46/256 [00:00<00:03, 63.49it/s, est. speed input: 72312.65 toks/s, output: 70.62 toks/s]
Processed prompts:  21%|██        | 54/256 [00:00<00:03, 59.22it/s, est. speed input: 68174.34 toks/s, output: 66.58 toks/s]
Processed prompts:  24%|██▍       | 61/256 [00:00<00:03, 59.02it/s, est. speed input: 67100.13 toks/s, output: 65.53 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:01<00:03, 54.10it/s, est. speed input: 63864.87 toks/s, output: 62.37 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:03, 53.03it/s, est. speed input: 62608.68 toks/s, output: 61.14 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:03, 52.28it/s, est. speed input: 61604.51 toks/s, output: 60.16 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:01<00:03, 51.65it/s, est. speed input: 60745.20 toks/s, output: 59.32 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:01<00:03, 51.26it/s, est. speed input: 60040.27 toks/s, output: 58.63 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:01<00:03, 50.86it/s, est. speed input: 59403.13 toks/s, output: 58.01 toks/s]
Processed prompts:  41%|████      | 104/256 [00:01<00:02, 50.68it/s, est. speed input: 58878.98 toks/s, output: 57.50 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:01<00:02, 50.54it/s, est. speed input: 58415.79 toks/s, output: 57.05 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:02<00:02, 50.41it/s, est. speed input: 58000.38 toks/s, output: 56.64 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:02<00:02, 50.35it/s, est. speed input: 57636.12 toks/s, output: 56.28 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:02<00:02, 50.23it/s, est. speed input: 57295.65 toks/s, output: 55.95 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:02<00:02, 50.20it/s, est. speed input: 56999.47 toks/s, output: 55.66 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:02<00:02, 50.15it/s, est. speed input: 56725.01 toks/s, output: 55.40 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:02<00:02, 50.12it/s, est. speed input: 56478.24 toks/s, output: 55.15 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:02<00:02, 49.99it/s, est. speed input: 56233.83 toks/s, output: 54.92 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:02<00:01, 49.97it/s, est. speed input: 56020.79 toks/s, output: 54.71 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:03<00:01, 49.96it/s, est. speed input: 55826.42 toks/s, output: 54.52 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:03<00:01, 50.01it/s, est. speed input: 55653.57 toks/s, output: 54.35 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:03<00:01, 50.06it/s, est. speed input: 55495.96 toks/s, output: 54.20 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:03<00:01, 50.07it/s, est. speed input: 55347.01 toks/s, output: 54.05 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:03<00:01, 50.08it/s, est. speed input: 55208.33 toks/s, output: 53.91 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:03<00:01, 50.07it/s, est. speed input: 55077.18 toks/s, output: 53.79 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:03<00:01, 50.07it/s, est. speed input: 54954.14 toks/s, output: 53.67 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:03<00:00, 50.07it/s, est. speed input: 54839.03 toks/s, output: 53.55 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:03<00:00, 50.05it/s, est. speed input: 54729.19 toks/s, output: 53.45 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:04<00:00, 50.08it/s, est. speed input: 54630.73 toks/s, output: 53.35 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:04<00:00, 50.08it/s, est. speed input: 54534.82 toks/s, output: 53.26 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:04<00:00, 50.08it/s, est. speed input: 54444.99 toks/s, output: 53.17 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:04<00:00, 50.10it/s, est. speed input: 54361.33 toks/s, output: 53.09 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:04<00:00, 50.09it/s, est. speed input: 54280.59 toks/s, output: 53.01 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:04<00:00, 50.05it/s, est. speed input: 54200.53 toks/s, output: 52.93 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:04<00:00, 50.04it/s, est. speed input: 54125.79 toks/s, output: 52.86 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 50.04it/s, est. speed input: 54318.51 toks/s, output: 53.05 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 53.04it/s, est. speed input: 54318.51 toks/s, output: 53.05 toks/s]
[rank0]:[W128 00:26:50.518337144 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 27.7s

测试结果:
  Requests/s:   49.46
  Tokens/s:     50694.47
  Total Reqs:   256
  Elapsed:      5.18s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     50645.01

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:26:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3480250) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3480250) WARNING 01-28 00:27:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 49.14 requests/s, 50368.63 total tokens/s, 49.14 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 00:26:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:26:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:26:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:26:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:26:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:26:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:26:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:26:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:26:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:26:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:27:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:27:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:27:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:27:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:27:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:27:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:27:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:27:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:27:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3480250) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3480250) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=3480250) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=3480250) 
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3480250) [2026-01-28 00:27:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3480250) 2026-01-28 00:27:12,224 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3480250) 2026-01-28 00:27:12,240 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3480250) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 25.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 23.77it/s]
(EngineCore_DP0 pid=3480250) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.81it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 73/512 [00:00<00:00, 724.06it/s]
Adding requests:  29%|██▊       | 147/512 [00:00<00:00, 729.11it/s]
Adding requests:  43%|████▎     | 221/512 [00:00<00:00, 729.69it/s]
Adding requests:  58%|█████▊    | 298/512 [00:00<00:00, 741.94it/s]
Adding requests:  73%|███████▎  | 373/512 [00:00<00:00, 741.78it/s]
Adding requests:  88%|████████▊ | 448/512 [00:00<00:00, 743.62it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 739.57it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:00<00:01, 300.38it/s, est. speed input: 307616.40 toks/s, output: 300.39 toks/s]
Processed prompts:  13%|█▎        | 65/512 [00:00<00:05, 85.19it/s, est. speed input: 98286.15 toks/s, output: 95.98 toks/s]   
Processed prompts:  16%|█▌        | 81/512 [00:01<00:06, 70.32it/s, est. speed input: 82943.66 toks/s, output: 81.00 toks/s]
Processed prompts:  18%|█▊        | 92/512 [00:01<00:06, 62.60it/s, est. speed input: 75826.75 toks/s, output: 74.05 toks/s]
Processed prompts:  20%|█▉        | 101/512 [00:01<00:06, 61.09it/s, est. speed input: 73698.88 toks/s, output: 71.97 toks/s]
Processed prompts:  21%|██▏       | 109/512 [00:01<00:06, 58.25it/s, est. speed input: 71325.35 toks/s, output: 69.65 toks/s]
Processed prompts:  23%|██▎       | 116/512 [00:01<00:07, 54.34it/s, est. speed input: 68789.16 toks/s, output: 67.18 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:01<00:07, 49.71it/s, est. speed input: 66166.83 toks/s, output: 64.62 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:02<00:07, 49.62it/s, est. speed input: 64933.22 toks/s, output: 63.41 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:02<00:07, 49.63it/s, est. speed input: 63906.70 toks/s, output: 62.41 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:02<00:07, 49.65it/s, est. speed input: 63022.61 toks/s, output: 61.55 toks/s]
Processed prompts:  30%|███       | 154/512 [00:02<00:07, 49.66it/s, est. speed input: 62250.81 toks/s, output: 60.79 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:02<00:07, 49.64it/s, est. speed input: 61565.58 toks/s, output: 60.12 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:02<00:06, 49.62it/s, est. speed input: 60953.42 toks/s, output: 59.52 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:03<00:06, 49.58it/s, est. speed input: 60404.31 toks/s, output: 58.99 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:03<00:06, 49.55it/s, est. speed input: 59909.43 toks/s, output: 58.51 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:03<00:06, 49.56it/s, est. speed input: 59467.07 toks/s, output: 58.07 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:03<00:06, 49.54it/s, est. speed input: 59062.36 toks/s, output: 57.68 toks/s]
Processed prompts:  41%|████      | 210/512 [00:03<00:06, 49.56it/s, est. speed input: 58698.79 toks/s, output: 57.32 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:03<00:05, 49.49it/s, est. speed input: 58351.94 toks/s, output: 56.98 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:03<00:05, 49.46it/s, est. speed input: 58035.97 toks/s, output: 56.68 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:04<00:05, 49.41it/s, est. speed input: 57739.37 toks/s, output: 56.39 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:04<00:05, 49.45it/s, est. speed input: 57477.11 toks/s, output: 56.13 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:04<00:05, 49.48it/s, est. speed input: 57233.91 toks/s, output: 55.89 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:04<00:05, 49.51it/s, est. speed input: 57009.10 toks/s, output: 55.67 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:04<00:04, 49.52it/s, est. speed input: 56797.61 toks/s, output: 55.47 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:04<00:04, 49.52it/s, est. speed input: 56598.79 toks/s, output: 55.27 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:05<00:04, 49.52it/s, est. speed input: 56412.80 toks/s, output: 55.09 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:05<00:04, 49.56it/s, est. speed input: 56242.73 toks/s, output: 54.92 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:05<00:04, 49.56it/s, est. speed input: 56080.91 toks/s, output: 54.77 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:05<00:04, 49.56it/s, est. speed input: 55926.86 toks/s, output: 54.62 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:05<00:03, 49.54it/s, est. speed input: 55779.47 toks/s, output: 54.47 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:05<00:03, 49.50it/s, est. speed input: 55637.40 toks/s, output: 54.33 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:06<00:03, 49.45it/s, est. speed input: 55501.66 toks/s, output: 54.20 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:06<00:03, 49.48it/s, est. speed input: 55378.08 toks/s, output: 54.08 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:06<00:03, 49.53it/s, est. speed input: 55264.09 toks/s, output: 53.97 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:06<00:03, 49.53it/s, est. speed input: 55152.49 toks/s, output: 53.86 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:06<00:03, 49.51it/s, est. speed input: 55044.73 toks/s, output: 53.75 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:06<00:02, 49.50it/s, est. speed input: 54941.81 toks/s, output: 53.65 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:07<00:02, 49.46it/s, est. speed input: 54840.99 toks/s, output: 53.56 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:07<00:02, 49.44it/s, est. speed input: 54745.48 toks/s, output: 53.46 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:07<00:02, 49.48it/s, est. speed input: 54658.58 toks/s, output: 53.38 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:07<00:02, 49.50it/s, est. speed input: 54574.61 toks/s, output: 53.30 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:07<00:02, 49.52it/s, est. speed input: 54494.25 toks/s, output: 53.22 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:07<00:01, 49.49it/s, est. speed input: 54414.45 toks/s, output: 53.14 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:08<00:01, 49.43it/s, est. speed input: 54334.71 toks/s, output: 53.06 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:08<00:01, 49.44it/s, est. speed input: 54262.03 toks/s, output: 52.99 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:08<00:01, 49.45it/s, est. speed input: 54192.46 toks/s, output: 52.92 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:08<00:01, 49.52it/s, est. speed input: 54129.46 toks/s, output: 52.86 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:08<00:01, 49.56it/s, est. speed input: 54068.21 toks/s, output: 52.80 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:08<00:00, 49.53it/s, est. speed input: 54005.97 toks/s, output: 52.74 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:08<00:00, 49.43it/s, est. speed input: 53940.58 toks/s, output: 52.68 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:09<00:00, 49.46it/s, est. speed input: 53883.90 toks/s, output: 52.62 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:09<00:00, 49.48it/s, est. speed input: 53828.50 toks/s, output: 52.57 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:09<00:00, 49.45it/s, est. speed input: 53772.88 toks/s, output: 52.51 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:09<00:00, 49.45it/s, est. speed input: 53720.38 toks/s, output: 52.46 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 49.45it/s, est. speed input: 53905.99 toks/s, output: 52.64 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 52.64it/s, est. speed input: 53905.99 toks/s, output: 52.64 toks/s]
[rank0]:[W128 00:27:24.063735838 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.7s

测试结果:
  Requests/s:   49.14
  Tokens/s:     50368.63
  Total Reqs:   512
  Elapsed:      10.42s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     50319.49

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:27:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3481095) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3481095) WARNING 01-28 00:27:42 [backends.py:609] Failed to read file <frozen os>
Throughput: 46.80 requests/s, 47969.10 total tokens/s, 46.80 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 00:27:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:27:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:27:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:27:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:27:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:27:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:27:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:27:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:27:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:27:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:27:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:27:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:27:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:27:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:27:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:27:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:27:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:27:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:27:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3481095) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3481095) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.64it/s]
(EngineCore_DP0 pid=3481095) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.64it/s]
(EngineCore_DP0 pid=3481095) 
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3481095) [2026-01-28 00:27:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3481095) 2026-01-28 00:27:47,896 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3481095) 2026-01-28 00:27:47,913 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3481095) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  5.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 10.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 12.75it/s]
(EngineCore_DP0 pid=3481095) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.96it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 69/1024 [00:00<00:01, 687.55it/s]
Adding requests:  14%|█▍        | 147/1024 [00:00<00:01, 739.73it/s]
Adding requests:  22%|██▏       | 224/1024 [00:00<00:01, 752.90it/s]
Adding requests:  29%|██▉       | 300/1024 [00:00<00:00, 745.41it/s]
Adding requests:  37%|███▋      | 377/1024 [00:00<00:00, 751.53it/s]
Adding requests:  44%|████▍     | 453/1024 [00:00<00:00, 747.26it/s]
Adding requests:  52%|█████▏    | 528/1024 [00:00<00:00, 740.45it/s]
Adding requests:  59%|█████▉    | 603/1024 [00:00<00:00, 738.99it/s]
Adding requests:  67%|██████▋   | 682/1024 [00:00<00:00, 753.59it/s]
Adding requests:  74%|███████▍  | 760/1024 [00:01<00:00, 758.57it/s]
Adding requests:  82%|████████▏ | 836/1024 [00:01<00:00, 748.14it/s]
Adding requests:  89%|████████▉ | 916/1024 [00:01<00:00, 763.37it/s]
Adding requests:  97%|█████████▋| 995/1024 [00:01<00:00, 768.60it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 754.18it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:00<00:03, 257.67it/s, est. speed input: 263864.80 toks/s, output: 257.67 toks/s]
Processed prompts:   9%|▉         | 92/1024 [00:00<00:08, 104.55it/s, est. speed input: 122762.07 toks/s, output: 119.88 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:01<00:11, 78.34it/s, est. speed input: 97977.70 toks/s, output: 95.68 toks/s]  
Processed prompts:  11%|█▏        | 116/1024 [00:01<00:12, 73.87it/s, est. speed input: 92937.09 toks/s, output: 90.76 toks/s]
Processed prompts:  12%|█▏        | 124/1024 [00:01<00:13, 67.25it/s, est. speed input: 87657.18 toks/s, output: 85.60 toks/s]
Processed prompts:  13%|█▎        | 131/1024 [00:01<00:14, 60.43it/s, est. speed input: 82845.69 toks/s, output: 80.90 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:01<00:16, 55.21it/s, est. speed input: 78972.69 toks/s, output: 77.12 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:01<00:16, 52.95it/s, est. speed input: 76307.22 toks/s, output: 74.52 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:02<00:16, 51.28it/s, est. speed input: 74063.86 toks/s, output: 72.33 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:02<00:17, 50.05it/s, est. speed input: 72147.98 toks/s, output: 70.46 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:02<00:17, 49.17it/s, est. speed input: 70497.28 toks/s, output: 68.84 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:02<00:17, 48.52it/s, est. speed input: 69050.24 toks/s, output: 67.43 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:02<00:17, 48.10it/s, est. speed input: 67792.19 toks/s, output: 66.20 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:02<00:17, 47.78it/s, est. speed input: 66669.51 toks/s, output: 65.11 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:03<00:17, 47.58it/s, est. speed input: 65675.91 toks/s, output: 64.14 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:03<00:17, 47.38it/s, est. speed input: 64769.86 toks/s, output: 63.25 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:03<00:17, 47.24it/s, est. speed input: 63953.62 toks/s, output: 62.45 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:03<00:16, 47.15it/s, est. speed input: 63212.44 toks/s, output: 61.73 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:03<00:16, 47.09it/s, est. speed input: 62538.89 toks/s, output: 61.07 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:04<00:16, 47.05it/s, est. speed input: 61924.64 toks/s, output: 60.47 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:04<00:16, 47.07it/s, est. speed input: 61368.00 toks/s, output: 59.93 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:04<00:16, 47.06it/s, est. speed input: 60850.08 toks/s, output: 59.42 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:16, 47.02it/s, est. speed input: 60367.23 toks/s, output: 58.95 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:04<00:15, 47.02it/s, est. speed input: 59923.21 toks/s, output: 58.52 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:04<00:15, 47.03it/s, est. speed input: 59510.96 toks/s, output: 58.12 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:05<00:15, 47.04it/s, est. speed input: 59128.00 toks/s, output: 57.74 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:15, 47.03it/s, est. speed input: 58768.56 toks/s, output: 57.39 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:15, 47.01it/s, est. speed input: 58429.04 toks/s, output: 57.06 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:05<00:15, 46.96it/s, est. speed input: 58106.84 toks/s, output: 56.74 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:05<00:14, 46.98it/s, est. speed input: 57809.08 toks/s, output: 56.45 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:05<00:14, 46.94it/s, est. speed input: 57522.92 toks/s, output: 56.17 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:06<00:14, 46.95it/s, est. speed input: 57257.32 toks/s, output: 55.92 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:06<00:14, 46.96it/s, est. speed input: 57007.21 toks/s, output: 55.67 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:06<00:14, 46.95it/s, est. speed input: 56768.26 toks/s, output: 55.44 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:06<00:14, 46.94it/s, est. speed input: 56541.68 toks/s, output: 55.22 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:06<00:13, 46.92it/s, est. speed input: 56324.61 toks/s, output: 55.00 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:06<00:13, 46.93it/s, est. speed input: 56120.80 toks/s, output: 54.81 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:07<00:13, 46.94it/s, est. speed input: 55927.11 toks/s, output: 54.62 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:07<00:13, 46.96it/s, est. speed input: 55744.24 toks/s, output: 54.44 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:07<00:13, 46.96it/s, est. speed input: 55568.05 toks/s, output: 54.27 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:07<00:13, 46.93it/s, est. speed input: 55396.94 toks/s, output: 54.10 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:07<00:12, 46.96it/s, est. speed input: 55237.81 toks/s, output: 53.94 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:07<00:12, 46.92it/s, est. speed input: 55081.18 toks/s, output: 53.79 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:08<00:12, 46.94it/s, est. speed input: 54933.97 toks/s, output: 53.65 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:08<00:12, 46.95it/s, est. speed input: 54793.54 toks/s, output: 53.51 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:08<00:12, 46.99it/s, est. speed input: 54660.39 toks/s, output: 53.38 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:08<00:12, 46.95it/s, est. speed input: 54528.22 toks/s, output: 53.25 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:08<00:11, 46.96it/s, est. speed input: 54403.43 toks/s, output: 53.13 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:08<00:11, 47.00it/s, est. speed input: 54285.78 toks/s, output: 53.01 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:09<00:11, 46.99it/s, est. speed input: 54170.44 toks/s, output: 52.90 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:09<00:11, 46.99it/s, est. speed input: 54059.38 toks/s, output: 52.79 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:09<00:11, 46.97it/s, est. speed input: 53951.08 toks/s, output: 52.69 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:09<00:11, 46.94it/s, est. speed input: 53845.00 toks/s, output: 52.58 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:09<00:10, 46.93it/s, est. speed input: 53744.03 toks/s, output: 52.48 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:09<00:10, 46.95it/s, est. speed input: 53647.76 toks/s, output: 52.39 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:10<00:10, 46.92it/s, est. speed input: 53552.17 toks/s, output: 52.30 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:10<00:10, 46.94it/s, est. speed input: 53462.43 toks/s, output: 52.21 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:10<00:10, 46.98it/s, est. speed input: 53377.19 toks/s, output: 52.13 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:10<00:10, 46.96it/s, est. speed input: 53291.49 toks/s, output: 52.04 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:10<00:09, 46.97it/s, est. speed input: 53210.38 toks/s, output: 51.96 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:10<00:09, 46.97it/s, est. speed input: 53130.87 toks/s, output: 51.89 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:11<00:09, 46.96it/s, est. speed input: 53053.60 toks/s, output: 51.81 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:11<00:09, 46.94it/s, est. speed input: 52977.74 toks/s, output: 51.74 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:11<00:09, 46.93it/s, est. speed input: 52904.31 toks/s, output: 51.66 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:11<00:08, 46.91it/s, est. speed input: 52832.18 toks/s, output: 51.59 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:11<00:08, 46.92it/s, est. speed input: 52763.81 toks/s, output: 51.53 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:12<00:08, 46.91it/s, est. speed input: 52696.30 toks/s, output: 51.46 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:12<00:08, 46.93it/s, est. speed input: 52631.83 toks/s, output: 51.40 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:12<00:08, 46.94it/s, est. speed input: 52569.18 toks/s, output: 51.34 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:12<00:08, 46.91it/s, est. speed input: 52506.40 toks/s, output: 51.28 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:12<00:07, 46.91it/s, est. speed input: 52446.31 toks/s, output: 51.22 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:12<00:07, 46.89it/s, est. speed input: 52387.12 toks/s, output: 51.16 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:13<00:07, 46.93it/s, est. speed input: 52331.52 toks/s, output: 51.10 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:13<00:07, 46.92it/s, est. speed input: 52275.95 toks/s, output: 51.05 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:13<00:07, 46.92it/s, est. speed input: 52222.03 toks/s, output: 51.00 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:13<00:07, 46.89it/s, est. speed input: 52168.13 toks/s, output: 50.95 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:13<00:06, 46.90it/s, est. speed input: 52116.87 toks/s, output: 50.90 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:13<00:06, 46.91it/s, est. speed input: 52067.05 toks/s, output: 50.85 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:14<00:06, 46.92it/s, est. speed input: 52018.68 toks/s, output: 50.80 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:14<00:06, 46.90it/s, est. speed input: 51970.31 toks/s, output: 50.75 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:14<00:06, 46.93it/s, est. speed input: 51924.71 toks/s, output: 50.71 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:14<00:06, 46.93it/s, est. speed input: 51879.26 toks/s, output: 50.66 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:14<00:05, 46.92it/s, est. speed input: 51834.95 toks/s, output: 50.62 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:14<00:05, 46.94it/s, est. speed input: 51792.28 toks/s, output: 50.58 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:15<00:05, 46.96it/s, est. speed input: 51751.09 toks/s, output: 50.54 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:15<00:05, 46.96it/s, est. speed input: 51710.25 toks/s, output: 50.50 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:15<00:05, 46.95it/s, est. speed input: 51669.54 toks/s, output: 50.46 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:15<00:05, 46.91it/s, est. speed input: 51628.89 toks/s, output: 50.42 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:15<00:04, 46.89it/s, est. speed input: 51589.11 toks/s, output: 50.38 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:15<00:04, 46.89it/s, est. speed input: 51550.79 toks/s, output: 50.34 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:16<00:04, 46.88it/s, est. speed input: 51512.98 toks/s, output: 50.31 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:16<00:04, 46.86it/s, est. speed input: 51475.48 toks/s, output: 50.27 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:16<00:04, 46.87it/s, est. speed input: 51439.60 toks/s, output: 50.23 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:16<00:04, 46.85it/s, est. speed input: 51403.62 toks/s, output: 50.20 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:16<00:03, 46.86it/s, est. speed input: 51369.13 toks/s, output: 50.17 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:16<00:03, 46.88it/s, est. speed input: 51335.71 toks/s, output: 50.13 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:17<00:03, 46.86it/s, est. speed input: 51301.90 toks/s, output: 50.10 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:17<00:03, 46.89it/s, est. speed input: 51270.21 toks/s, output: 50.07 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:17<00:03, 46.89it/s, est. speed input: 51238.42 toks/s, output: 50.04 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:17<00:03, 46.87it/s, est. speed input: 51206.42 toks/s, output: 50.01 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:17<00:02, 46.87it/s, est. speed input: 51175.55 toks/s, output: 49.98 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:17<00:02, 46.86it/s, est. speed input: 51145.19 toks/s, output: 49.95 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:18<00:02, 46.86it/s, est. speed input: 51115.35 toks/s, output: 49.92 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:18<00:02, 46.85it/s, est. speed input: 51085.84 toks/s, output: 49.89 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:18<00:02, 46.82it/s, est. speed input: 51056.26 toks/s, output: 49.86 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:18<00:02, 46.79it/s, est. speed input: 51026.76 toks/s, output: 49.83 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:18<00:01, 46.84it/s, est. speed input: 51000.15 toks/s, output: 49.80 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:19<00:01, 46.85it/s, est. speed input: 50973.23 toks/s, output: 49.78 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:19<00:01, 46.87it/s, est. speed input: 50947.01 toks/s, output: 49.75 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:19<00:01, 46.89it/s, est. speed input: 50921.60 toks/s, output: 49.73 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:19<00:01, 46.87it/s, est. speed input: 50895.46 toks/s, output: 49.70 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:19<00:00, 46.87it/s, est. speed input: 50870.44 toks/s, output: 49.68 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:19<00:00, 46.88it/s, est. speed input: 50846.04 toks/s, output: 49.65 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:20<00:00, 46.87it/s, est. speed input: 50821.40 toks/s, output: 49.63 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:20<00:00, 46.86it/s, est. speed input: 50797.17 toks/s, output: 49.61 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:20<00:00, 46.84it/s, est. speed input: 50773.01 toks/s, output: 49.58 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:20<00:00, 48.43it/s, est. speed input: 50796.03 toks/s, output: 49.61 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:20<00:00, 48.43it/s, est. speed input: 51095.09 toks/s, output: 49.90 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:20<00:00, 49.90it/s, est. speed input: 51095.09 toks/s, output: 49.90 toks/s]
[rank0]:[W128 00:28:11.344654030 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.1s

测试结果:
  Requests/s:   46.80
  Tokens/s:     47969.10
  Total Reqs:   1024
  Elapsed:      21.88s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     47922.30

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:28:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3482057) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3482057) WARNING 01-28 00:28:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 45.71 requests/s, 46852.80 total tokens/s, 45.71 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 00:28:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:28:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:28:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:28:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:28:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:28:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:28:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:28:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:28:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:28:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:28:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:28:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:28:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:28:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:28:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:28:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:28:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:28:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:28:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:28:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:28:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:28:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:28:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:28:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:28:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:28:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:28:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:28:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3482057) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3482057) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.63it/s]
(EngineCore_DP0 pid=3482057) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.63it/s]
(EngineCore_DP0 pid=3482057) 
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3482057) [2026-01-28 00:28:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3482057) 2026-01-28 00:28:37,403 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3482057) 2026-01-28 00:28:37,419 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3482057) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 18.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 21.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 19.46it/s]
(EngineCore_DP0 pid=3482057) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.59it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.60it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 74/2048 [00:00<00:02, 731.13it/s]
Adding requests:   7%|▋         | 148/2048 [00:00<00:02, 730.05it/s]
Adding requests:  11%|█         | 223/2048 [00:00<00:02, 738.39it/s]
Adding requests:  15%|█▍        | 299/2048 [00:00<00:02, 744.12it/s]
Adding requests:  18%|█▊        | 374/2048 [00:00<00:02, 740.97it/s]
Adding requests:  22%|██▏       | 450/2048 [00:00<00:02, 744.28it/s]
Adding requests:  26%|██▌       | 525/2048 [00:00<00:02, 742.05it/s]
Adding requests:  29%|██▉       | 603/2048 [00:00<00:01, 751.66it/s]
Adding requests:  33%|███▎      | 682/2048 [00:00<00:01, 763.26it/s]
Adding requests:  37%|███▋      | 759/2048 [00:01<00:01, 755.86it/s]
Adding requests:  41%|████      | 835/2048 [00:01<00:01, 736.93it/s]
Adding requests:  45%|████▍     | 912/2048 [00:01<00:01, 743.69it/s]
Adding requests:  48%|████▊     | 989/2048 [00:01<00:01, 751.14it/s]
Adding requests:  52%|█████▏    | 1066/2048 [00:01<00:01, 756.48it/s]
Adding requests:  56%|█████▌    | 1142/2048 [00:01<00:01, 749.00it/s]
Adding requests:  60%|█████▉    | 1223/2048 [00:01<00:01, 765.82it/s]
Adding requests:  63%|██████▎   | 1300/2048 [00:01<00:00, 752.19it/s]
Adding requests:  67%|██████▋   | 1376/2048 [00:01<00:00, 748.41it/s]
Adding requests:  71%|███████   | 1451/2048 [00:01<00:00, 746.68it/s]
Adding requests:  75%|███████▍  | 1529/2048 [00:02<00:00, 756.50it/s]
Adding requests:  79%|███████▊  | 1608/2048 [00:02<00:00, 765.25it/s]
Adding requests:  82%|████████▏ | 1685/2048 [00:02<00:00, 761.00it/s]
Adding requests:  86%|████████▌ | 1762/2048 [00:02<00:00, 760.19it/s]
Adding requests:  90%|████████▉ | 1839/2048 [00:02<00:00, 756.83it/s]
Adding requests:  94%|█████████▎| 1916/2048 [00:02<00:00, 760.40it/s]
Adding requests:  97%|█████████▋| 1993/2048 [00:02<00:00, 746.45it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 751.77it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:00<00:03, 596.17it/s, est. speed input: 610546.16 toks/s, output: 596.21 toks/s]
Processed prompts:   8%|▊         | 174/2048 [00:01<00:15, 118.49it/s, est. speed input: 144016.60 toks/s, output: 140.64 toks/s]
Processed prompts:  10%|▉         | 202/2048 [00:01<00:22, 82.63it/s, est. speed input: 106924.86 toks/s, output: 104.42 toks/s] 
Processed prompts:  11%|█         | 220/2048 [00:02<00:24, 75.02it/s, est. speed input: 98692.17 toks/s, output: 96.38 toks/s]  
Processed prompts:  11%|█▏        | 233/2048 [00:02<00:27, 65.19it/s, est. speed input: 90675.06 toks/s, output: 88.55 toks/s]
Processed prompts:  12%|█▏        | 243/2048 [00:02<00:32, 55.29it/s, est. speed input: 83502.13 toks/s, output: 81.54 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:03<00:34, 51.87it/s, est. speed input: 79373.97 toks/s, output: 77.51 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:03<00:35, 50.15it/s, est. speed input: 76295.38 toks/s, output: 74.51 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:04<00:35, 48.91it/s, est. speed input: 73755.14 toks/s, output: 72.03 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:04<00:36, 48.04it/s, est. speed input: 71627.43 toks/s, output: 69.95 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:04<00:36, 47.27it/s, est. speed input: 69760.08 toks/s, output: 68.12 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:05<00:36, 46.88it/s, est. speed input: 68200.19 toks/s, output: 66.60 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:05<00:36, 46.58it/s, est. speed input: 66834.91 toks/s, output: 65.27 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:05<00:36, 46.34it/s, est. speed input: 65628.12 toks/s, output: 64.09 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:06<00:35, 46.21it/s, est. speed input: 64567.82 toks/s, output: 63.05 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:06<00:35, 46.12it/s, est. speed input: 63622.07 toks/s, output: 62.13 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:06<00:35, 46.04it/s, est. speed input: 62768.22 toks/s, output: 61.30 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:07<00:35, 45.98it/s, est. speed input: 61996.82 toks/s, output: 60.54 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:07<00:34, 45.95it/s, est. speed input: 61299.87 toks/s, output: 59.86 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:07<00:34, 45.91it/s, est. speed input: 60661.64 toks/s, output: 59.24 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:08<00:34, 45.90it/s, est. speed input: 60081.58 toks/s, output: 58.67 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:08<00:33, 45.90it/s, est. speed input: 59548.29 toks/s, output: 58.15 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:08<00:33, 45.88it/s, est. speed input: 59054.53 toks/s, output: 57.67 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:09<00:33, 45.87it/s, est. speed input: 58598.36 toks/s, output: 57.22 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:09<00:32, 45.87it/s, est. speed input: 58175.68 toks/s, output: 56.81 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:09<00:32, 45.84it/s, est. speed input: 57779.69 toks/s, output: 56.43 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:10<00:32, 45.85it/s, est. speed input: 57413.95 toks/s, output: 56.07 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:10<00:31, 45.83it/s, est. speed input: 57068.93 toks/s, output: 55.73 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:11<00:31, 45.83it/s, est. speed input: 56747.49 toks/s, output: 55.42 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:11<00:31, 45.84it/s, est. speed input: 56446.75 toks/s, output: 55.12 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:11<00:30, 45.83it/s, est. speed input: 56162.20 toks/s, output: 54.85 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:12<00:30, 45.83it/s, est. speed input: 55894.56 toks/s, output: 54.58 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:12<00:29, 45.84it/s, est. speed input: 55643.00 toks/s, output: 54.34 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:12<00:29, 45.81it/s, est. speed input: 55401.82 toks/s, output: 54.10 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:13<00:29, 45.81it/s, est. speed input: 55174.98 toks/s, output: 53.88 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:13<00:28, 45.81it/s, est. speed input: 54960.65 toks/s, output: 53.67 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:13<00:28, 45.82it/s, est. speed input: 54757.57 toks/s, output: 53.47 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:14<00:28, 45.81it/s, est. speed input: 54563.81 toks/s, output: 53.28 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:14<00:27, 45.82it/s, est. speed input: 54380.16 toks/s, output: 53.11 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:14<00:27, 45.80it/s, est. speed input: 54203.24 toks/s, output: 52.93 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:15<00:27, 45.80it/s, est. speed input: 54034.74 toks/s, output: 52.77 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:15<00:26, 45.80it/s, est. speed input: 53874.87 toks/s, output: 52.61 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:15<00:26, 45.79it/s, est. speed input: 53720.41 toks/s, output: 52.46 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:16<00:26, 45.79it/s, est. speed input: 53573.48 toks/s, output: 52.32 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:16<00:25, 45.80it/s, est. speed input: 53433.28 toks/s, output: 52.18 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:16<00:25, 45.78it/s, est. speed input: 53296.99 toks/s, output: 52.05 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:17<00:25, 45.79it/s, est. speed input: 53167.94 toks/s, output: 51.92 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:17<00:24, 45.78it/s, est. speed input: 53043.36 toks/s, output: 51.80 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:17<00:24, 45.79it/s, est. speed input: 52924.11 toks/s, output: 51.68 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:18<00:24, 45.79it/s, est. speed input: 52809.09 toks/s, output: 51.57 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:18<00:23, 45.78it/s, est. speed input: 52697.86 toks/s, output: 51.46 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:19<00:23, 45.78it/s, est. speed input: 52591.07 toks/s, output: 51.36 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:19<00:23, 45.79it/s, est. speed input: 52488.58 toks/s, output: 51.26 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:19<00:22, 45.76it/s, est. speed input: 52387.75 toks/s, output: 51.16 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:20<00:22, 45.76it/s, est. speed input: 52291.63 toks/s, output: 51.07 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:20<00:21, 45.76it/s, est. speed input: 52198.49 toks/s, output: 50.98 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:20<00:21, 45.76it/s, est. speed input: 52109.03 toks/s, output: 50.89 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:21<00:21, 45.77it/s, est. speed input: 52022.28 toks/s, output: 50.80 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:21<00:20, 45.77it/s, est. speed input: 51938.52 toks/s, output: 50.72 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:21<00:20, 45.77it/s, est. speed input: 51857.27 toks/s, output: 50.64 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:22<00:20, 45.76it/s, est. speed input: 51778.33 toks/s, output: 50.56 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:22<00:19, 45.76it/s, est. speed input: 51701.75 toks/s, output: 50.49 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:22<00:19, 46.44it/s, est. speed input: 51666.65 toks/s, output: 50.46 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:23<00:18, 46.24it/s, est. speed input: 51594.33 toks/s, output: 50.39 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:23<00:18, 46.09it/s, est. speed input: 51524.14 toks/s, output: 50.32 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:23<00:18, 45.99it/s, est. speed input: 51455.58 toks/s, output: 50.25 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:24<00:18, 45.92it/s, est. speed input: 51389.57 toks/s, output: 50.19 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:24<00:17, 45.87it/s, est. speed input: 51325.15 toks/s, output: 50.12 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:24<00:17, 45.82it/s, est. speed input: 51261.50 toks/s, output: 50.06 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:25<00:17, 45.80it/s, est. speed input: 51200.60 toks/s, output: 50.00 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:25<00:16, 45.78it/s, est. speed input: 51141.01 toks/s, output: 49.94 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:26<00:16, 45.77it/s, est. speed input: 51083.54 toks/s, output: 49.89 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:26<00:16, 45.76it/s, est. speed input: 51027.25 toks/s, output: 49.83 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:26<00:15, 45.74it/s, est. speed input: 50971.54 toks/s, output: 49.78 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:27<00:15, 45.74it/s, est. speed input: 50917.97 toks/s, output: 49.72 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:27<00:15, 45.73it/s, est. speed input: 50865.70 toks/s, output: 49.67 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:27<00:14, 45.72it/s, est. speed input: 50814.37 toks/s, output: 49.62 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:28<00:14, 45.74it/s, est. speed input: 50765.38 toks/s, output: 49.58 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:28<00:13, 45.74it/s, est. speed input: 50717.17 toks/s, output: 49.53 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:28<00:13, 45.73it/s, est. speed input: 50669.77 toks/s, output: 49.48 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:29<00:13, 45.73it/s, est. speed input: 50623.62 toks/s, output: 49.44 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:29<00:12, 45.74it/s, est. speed input: 50579.08 toks/s, output: 49.39 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:29<00:12, 45.73it/s, est. speed input: 50534.65 toks/s, output: 49.35 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:30<00:12, 45.74it/s, est. speed input: 50492.13 toks/s, output: 49.31 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:30<00:11, 45.73it/s, est. speed input: 50450.08 toks/s, output: 49.27 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:30<00:11, 45.72it/s, est. speed input: 50408.53 toks/s, output: 49.23 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:31<00:11, 45.73it/s, est. speed input: 50368.69 toks/s, output: 49.19 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:31<00:10, 45.72it/s, est. speed input: 50329.22 toks/s, output: 49.15 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:31<00:10, 45.72it/s, est. speed input: 50290.93 toks/s, output: 49.11 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:32<00:10, 45.72it/s, est. speed input: 50253.31 toks/s, output: 49.08 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:32<00:09, 45.72it/s, est. speed input: 50216.52 toks/s, output: 49.04 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:33<00:09, 45.73it/s, est. speed input: 50180.69 toks/s, output: 49.00 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:33<00:09, 45.73it/s, est. speed input: 50145.53 toks/s, output: 48.97 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:33<00:08, 45.72it/s, est. speed input: 50110.72 toks/s, output: 48.94 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:34<00:08, 45.71it/s, est. speed input: 50076.45 toks/s, output: 48.90 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:34<00:08, 45.72it/s, est. speed input: 50043.56 toks/s, output: 48.87 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:34<00:07, 45.70it/s, est. speed input: 50010.42 toks/s, output: 48.84 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:35<00:07, 45.72it/s, est. speed input: 49979.01 toks/s, output: 48.81 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:35<00:06, 45.71it/s, est. speed input: 49947.64 toks/s, output: 48.78 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:35<00:06, 45.69it/s, est. speed input: 49916.32 toks/s, output: 48.75 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:36<00:06, 45.71it/s, est. speed input: 49886.70 toks/s, output: 48.72 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:36<00:05, 45.72it/s, est. speed input: 49857.51 toks/s, output: 48.69 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:36<00:05, 45.70it/s, est. speed input: 49828.11 toks/s, output: 48.66 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:37<00:05, 45.71it/s, est. speed input: 49799.64 toks/s, output: 48.63 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:37<00:04, 45.71it/s, est. speed input: 49771.76 toks/s, output: 48.61 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:37<00:04, 45.70it/s, est. speed input: 49744.24 toks/s, output: 48.58 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:38<00:04, 45.71it/s, est. speed input: 49717.65 toks/s, output: 48.55 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:38<00:03, 46.54it/s, est. speed input: 49717.81 toks/s, output: 48.55 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:38<00:03, 46.28it/s, est. speed input: 49691.38 toks/s, output: 48.53 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:39<00:03, 46.12it/s, est. speed input: 49666.05 toks/s, output: 48.50 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:39<00:02, 45.98it/s, est. speed input: 49640.29 toks/s, output: 48.48 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:39<00:02, 45.89it/s, est. speed input: 49615.44 toks/s, output: 48.45 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:40<00:02, 45.84it/s, est. speed input: 49591.24 toks/s, output: 48.43 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:40<00:01, 45.79it/s, est. speed input: 49566.89 toks/s, output: 48.41 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:41<00:01, 45.77it/s, est. speed input: 49543.33 toks/s, output: 48.38 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:41<00:01, 45.74it/s, est. speed input: 49519.79 toks/s, output: 48.36 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:41<00:00, 45.72it/s, est. speed input: 49496.80 toks/s, output: 48.34 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:42<00:00, 46.53it/s, est. speed input: 49498.32 toks/s, output: 48.34 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:42<00:00, 46.53it/s, est. speed input: 49838.60 toks/s, output: 48.67 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:42<00:00, 48.67it/s, est. speed input: 49838.60 toks/s, output: 48.67 toks/s]
[rank0]:[W128 00:29:23.765900163 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 72.5s

测试结果:
  Requests/s:   45.71
  Tokens/s:     46852.80
  Total Reqs:   2048
  Elapsed:      44.80s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     46807.09

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:29:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3483406) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3483406) WARNING 01-28 00:29:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 45.79 requests/s, 46939.63 total tokens/s, 45.79 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 00:29:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:29:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:29:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:29:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:29:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:29:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:29:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:29:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:29:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:29:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:29:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:29:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:29:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:29:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:29:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:29:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:29:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:29:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:29:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:44] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3483406) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3483406) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=3483406) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=3483406) 
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3483406) [2026-01-28 00:29:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3483406) [rank0]:W0128 00:29:52.550000 3483406 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3483406) [rank0]:W0128 00:29:52.598000 3483406 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3483406) [rank0]:W0128 00:29:53.292000 3483406 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3483406) [rank0]:W0128 00:29:53.368000 3483406 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3483406) 2026-01-28 00:29:55,621 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3483406) 2026-01-28 00:29:55,638 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3483406) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:04,  2.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  6.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 12.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 16.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 12.90it/s]
(EngineCore_DP0 pid=3483406) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.38it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 22.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 19.89it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:05, 719.08it/s]
Adding requests:   4%|▎         | 148/4096 [00:00<00:05, 741.01it/s]
Adding requests:   5%|▌         | 225/4096 [00:00<00:05, 750.08it/s]
Adding requests:   7%|▋         | 301/4096 [00:00<00:05, 753.71it/s]
Adding requests:   9%|▉         | 377/4096 [00:00<00:04, 755.13it/s]
Adding requests:  11%|█         | 453/4096 [00:00<00:04, 749.47it/s]
Adding requests:  13%|█▎        | 528/4096 [00:00<00:04, 726.83it/s]
Adding requests:  15%|█▍        | 601/4096 [00:00<00:04, 724.85it/s]
Adding requests:  17%|█▋        | 680/4096 [00:00<00:04, 743.60it/s]
Adding requests:  19%|█▊        | 758/4096 [00:01<00:04, 752.06it/s]
Adding requests:  20%|██        | 834/4096 [00:01<00:04, 732.33it/s]
Adding requests:  22%|██▏       | 912/4096 [00:01<00:04, 744.16it/s]
Adding requests:  24%|██▍       | 989/4096 [00:01<00:04, 749.95it/s]
Adding requests:  26%|██▌       | 1067/4096 [00:01<00:04, 756.05it/s]
Adding requests:  28%|██▊       | 1143/4096 [00:01<00:04, 733.05it/s]
Adding requests:  30%|██▉       | 1222/4096 [00:01<00:03, 745.80it/s]
Adding requests:  32%|███▏      | 1297/4096 [00:01<00:03, 746.49it/s]
Adding requests:  34%|███▎      | 1376/4096 [00:01<00:03, 758.60it/s]
Adding requests:  35%|███▌      | 1454/4096 [00:01<00:03, 764.88it/s]
Adding requests:  37%|███▋      | 1534/4096 [00:02<00:03, 774.91it/s]
Adding requests:  39%|███▉      | 1612/4096 [00:02<00:03, 760.22it/s]
Adding requests:  41%|████      | 1689/4096 [00:02<00:03, 749.49it/s]
Adding requests:  43%|████▎     | 1765/4096 [00:02<00:03, 751.75it/s]
Adding requests:  45%|████▍     | 1842/4096 [00:02<00:02, 756.75it/s]
Adding requests:  47%|████▋     | 1918/4096 [00:02<00:02, 754.03it/s]
Adding requests:  49%|████▊     | 1994/4096 [00:02<00:02, 749.82it/s]
Adding requests:  51%|█████     | 2070/4096 [00:02<00:02, 751.59it/s]
Adding requests:  52%|█████▏    | 2146/4096 [00:02<00:02, 741.04it/s]
Adding requests:  54%|█████▍    | 2221/4096 [00:02<00:02, 743.64it/s]
Adding requests:  56%|█████▌    | 2300/4096 [00:03<00:02, 754.14it/s]
Adding requests:  58%|█████▊    | 2376/4096 [00:03<00:02, 741.78it/s]
Adding requests:  60%|█████▉    | 2452/4096 [00:03<00:02, 746.34it/s]
Adding requests:  62%|██████▏   | 2530/4096 [00:03<00:02, 754.86it/s]
Adding requests:  64%|██████▎   | 2608/4096 [00:03<00:01, 760.80it/s]
Adding requests:  66%|██████▌   | 2686/4096 [00:03<00:01, 764.84it/s]
Adding requests:  67%|██████▋   | 2763/4096 [00:03<00:01, 765.56it/s]
Adding requests:  69%|██████▉   | 2840/4096 [00:03<00:01, 764.48it/s]
Adding requests:  71%|███████   | 2918/4096 [00:03<00:01, 766.48it/s]
Adding requests:  73%|███████▎  | 2995/4096 [00:03<00:01, 766.27it/s]
Adding requests:  75%|███████▌  | 3072/4096 [00:04<00:01, 762.44it/s]
Adding requests:  77%|███████▋  | 3149/4096 [00:04<00:01, 764.49it/s]
Adding requests:  79%|███████▉  | 3227/4096 [00:04<00:01, 766.93it/s]
Adding requests:  81%|████████  | 3305/4096 [00:04<00:01, 770.74it/s]
Adding requests:  83%|████████▎ | 3383/4096 [00:04<00:00, 772.31it/s]
Adding requests:  84%|████████▍ | 3461/4096 [00:04<00:00, 758.62it/s]
Adding requests:  86%|████████▋ | 3537/4096 [00:04<00:00, 757.06it/s]
Adding requests:  88%|████████▊ | 3613/4096 [00:04<00:00, 757.39it/s]
Adding requests:  90%|█████████ | 3689/4096 [00:04<00:00, 732.29it/s]
Adding requests:  92%|█████████▏| 3765/4096 [00:05<00:00, 738.15it/s]
Adding requests:  94%|█████████▍| 3841/4096 [00:05<00:00, 744.24it/s]
Adding requests:  96%|█████████▌| 3918/4096 [00:05<00:00, 750.67it/s]
Adding requests:  98%|█████████▊| 3994/4096 [00:05<00:00, 748.56it/s]
Adding requests:  99%|█████████▉| 4070/4096 [00:05<00:00, 750.01it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 752.28it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:00<00:03, 1063.36it/s, est. speed input: 1088963.22 toks/s, output: 1063.39 toks/s]
Processed prompts:   8%|▊         | 333/4096 [00:02<00:31, 118.52it/s, est. speed input: 148166.76 toks/s, output: 144.69 toks/s]   
Processed prompts:   9%|▉         | 380/4096 [00:02<00:36, 102.80it/s, est. speed input: 129786.71 toks/s, output: 126.74 toks/s]
Processed prompts:  10%|▉         | 409/4096 [00:03<00:43, 84.11it/s, est. speed input: 113352.58 toks/s, output: 110.70 toks/s] 
Processed prompts:  10%|█         | 428/4096 [00:04<00:54, 66.85it/s, est. speed input: 99805.18 toks/s, output: 97.47 toks/s]  
Processed prompts:  11%|█         | 450/4096 [00:05<01:04, 56.18it/s, est. speed input: 90568.08 toks/s, output: 88.45 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:05<01:08, 53.09it/s, est. speed input: 85326.23 toks/s, output: 83.33 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:06<01:10, 50.92it/s, est. speed input: 81203.55 toks/s, output: 79.30 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:07<01:11, 49.41it/s, est. speed input: 77881.05 toks/s, output: 76.06 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:07<01:12, 48.36it/s, est. speed input: 75150.37 toks/s, output: 73.39 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:08<01:13, 47.61it/s, est. speed input: 72858.68 toks/s, output: 71.15 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:09<01:13, 47.09it/s, est. speed input: 70912.16 toks/s, output: 69.25 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:09<01:13, 46.73it/s, est. speed input: 69238.73 toks/s, output: 67.62 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:10<01:12, 46.48it/s, est. speed input: 67783.75 toks/s, output: 66.19 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:11<01:12, 46.29it/s, est. speed input: 66504.27 toks/s, output: 64.95 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:12<01:12, 46.17it/s, est. speed input: 65376.75 toks/s, output: 63.84 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:12<01:11, 46.08it/s, est. speed input: 64370.94 toks/s, output: 62.86 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:13<01:10, 46.02it/s, est. speed input: 63468.02 toks/s, output: 61.98 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:14<01:10, 45.97it/s, est. speed input: 62653.50 toks/s, output: 61.19 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:14<01:09, 45.94it/s, est. speed input: 61916.96 toks/s, output: 60.47 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:15<01:08, 45.92it/s, est. speed input: 61246.01 toks/s, output: 59.81 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:16<01:08, 45.90it/s, est. speed input: 60632.02 toks/s, output: 59.21 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:16<01:07, 45.87it/s, est. speed input: 60066.90 toks/s, output: 58.66 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:17<01:06, 45.87it/s, est. speed input: 59547.76 toks/s, output: 58.15 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:18<01:06, 45.85it/s, est. speed input: 59066.69 toks/s, output: 57.68 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:19<01:05, 45.85it/s, est. speed input: 58622.77 toks/s, output: 57.25 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:19<01:04, 45.84it/s, est. speed input: 58209.05 toks/s, output: 56.84 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:20<01:04, 45.84it/s, est. speed input: 57823.82 toks/s, output: 56.47 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:21<01:03, 45.83it/s, est. speed input: 57463.35 toks/s, output: 56.12 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:21<01:02, 45.83it/s, est. speed input: 57126.50 toks/s, output: 55.79 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:22<01:02, 45.83it/s, est. speed input: 56810.71 toks/s, output: 55.48 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:23<01:01, 45.79it/s, est. speed input: 56508.61 toks/s, output: 55.18 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:23<01:00, 45.80it/s, est. speed input: 56229.35 toks/s, output: 54.91 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:24<01:00, 45.81it/s, est. speed input: 55965.15 toks/s, output: 54.65 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:25<00:59, 45.81it/s, est. speed input: 55716.08 toks/s, output: 54.41 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:26<00:58, 45.82it/s, est. speed input: 55480.20 toks/s, output: 54.18 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:26<00:57, 45.83it/s, est. speed input: 55257.23 toks/s, output: 53.96 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:27<00:57, 45.81it/s, est. speed input: 55043.58 toks/s, output: 53.75 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:28<00:56, 45.81it/s, est. speed input: 54841.52 toks/s, output: 53.56 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:28<00:55, 45.82it/s, est. speed input: 54650.44 toks/s, output: 53.37 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:29<00:55, 45.80it/s, est. speed input: 54465.70 toks/s, output: 53.19 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:30<00:54, 45.80it/s, est. speed input: 54290.41 toks/s, output: 53.02 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:30<00:53, 45.80it/s, est. speed input: 54123.36 toks/s, output: 52.85 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:31<00:53, 45.80it/s, est. speed input: 53963.91 toks/s, output: 52.70 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:32<00:52, 45.79it/s, est. speed input: 53810.38 toks/s, output: 52.55 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:33<00:51, 45.80it/s, est. speed input: 53664.61 toks/s, output: 52.41 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:33<00:50, 45.80it/s, est. speed input: 53524.04 toks/s, output: 52.27 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:34<00:50, 45.80it/s, est. speed input: 53389.37 toks/s, output: 52.14 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:35<00:49, 45.79it/s, est. speed input: 53259.74 toks/s, output: 52.01 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:35<00:48, 46.18it/s, est. speed input: 53164.86 toks/s, output: 51.92 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:36<00:47, 46.06it/s, est. speed input: 53044.54 toks/s, output: 51.80 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:37<00:47, 45.97it/s, est. speed input: 52928.16 toks/s, output: 51.69 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:37<00:46, 45.93it/s, est. speed input: 52817.60 toks/s, output: 51.58 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:38<00:45, 45.87it/s, est. speed input: 52709.07 toks/s, output: 51.47 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:39<00:45, 45.84it/s, est. speed input: 52604.83 toks/s, output: 51.37 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:39<00:44, 45.82it/s, est. speed input: 52504.80 toks/s, output: 51.27 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:40<00:43, 45.81it/s, est. speed input: 52408.01 toks/s, output: 51.18 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:41<00:43, 45.79it/s, est. speed input: 52314.28 toks/s, output: 51.09 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:42<00:42, 45.79it/s, est. speed input: 52223.87 toks/s, output: 51.00 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:42<00:41, 45.79it/s, est. speed input: 52136.58 toks/s, output: 50.91 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:43<00:41, 45.78it/s, est. speed input: 52051.97 toks/s, output: 50.83 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:44<00:40, 45.79it/s, est. speed input: 51970.57 toks/s, output: 50.75 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:44<00:39, 45.77it/s, est. speed input: 51890.39 toks/s, output: 50.67 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:45<00:39, 45.76it/s, est. speed input: 51812.84 toks/s, output: 50.60 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:46<00:38, 45.77it/s, est. speed input: 51738.32 toks/s, output: 50.53 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:46<00:37, 45.77it/s, est. speed input: 51665.85 toks/s, output: 50.45 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:47<00:37, 45.77it/s, est. speed input: 51595.66 toks/s, output: 50.39 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:48<00:36, 45.76it/s, est. speed input: 51526.61 toks/s, output: 50.32 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:49<00:35, 45.77it/s, est. speed input: 51460.93 toks/s, output: 50.25 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:49<00:34, 46.17it/s, est. speed input: 51416.85 toks/s, output: 50.21 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:50<00:34, 46.04it/s, est. speed input: 51353.61 toks/s, output: 50.15 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:51<00:33, 45.95it/s, est. speed input: 51291.96 toks/s, output: 50.09 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:51<00:32, 45.90it/s, est. speed input: 51232.20 toks/s, output: 50.03 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:52<00:32, 45.85it/s, est. speed input: 51173.84 toks/s, output: 49.97 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:53<00:31, 45.81it/s, est. speed input: 51116.57 toks/s, output: 49.92 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:53<00:30, 45.80it/s, est. speed input: 51061.35 toks/s, output: 49.86 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:54<00:30, 45.78it/s, est. speed input: 51007.32 toks/s, output: 49.81 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:55<00:29, 45.77it/s, est. speed input: 50954.68 toks/s, output: 49.76 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:56<00:28, 45.77it/s, est. speed input: 50903.61 toks/s, output: 49.71 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:56<00:27, 45.77it/s, est. speed input: 50853.77 toks/s, output: 49.66 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:57<00:27, 45.75it/s, est. speed input: 50804.48 toks/s, output: 49.61 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:58<00:26, 45.75it/s, est. speed input: 50756.83 toks/s, output: 49.57 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:58<00:25, 45.75it/s, est. speed input: 50710.43 toks/s, output: 49.52 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:59<00:25, 45.75it/s, est. speed input: 50664.96 toks/s, output: 49.48 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:00<00:24, 45.74it/s, est. speed input: 50620.08 toks/s, output: 49.43 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:00<00:23, 45.74it/s, est. speed input: 50576.96 toks/s, output: 49.39 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:01<00:23, 45.74it/s, est. speed input: 50534.55 toks/s, output: 49.35 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:02<00:22, 45.74it/s, est. speed input: 50492.98 toks/s, output: 49.31 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:03<00:21, 45.74it/s, est. speed input: 50452.31 toks/s, output: 49.27 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:03<00:20, 45.74it/s, est. speed input: 50412.57 toks/s, output: 49.23 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:04<00:20, 45.74it/s, est. speed input: 50373.68 toks/s, output: 49.19 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:05<00:19, 45.73it/s, est. speed input: 50335.55 toks/s, output: 49.16 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:05<00:18, 45.74it/s, est. speed input: 50298.41 toks/s, output: 49.12 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:06<00:18, 45.73it/s, est. speed input: 50261.68 toks/s, output: 49.08 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:07<00:17, 45.72it/s, est. speed input: 50225.60 toks/s, output: 49.05 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:07<00:16, 45.75it/s, est. speed input: 50191.52 toks/s, output: 49.02 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:08<00:16, 45.73it/s, est. speed input: 50156.65 toks/s, output: 48.98 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:09<00:15, 45.72it/s, est. speed input: 50122.77 toks/s, output: 48.95 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:10<00:14, 45.73it/s, est. speed input: 50090.00 toks/s, output: 48.92 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:10<00:13, 45.73it/s, est. speed input: 50057.75 toks/s, output: 48.88 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:11<00:13, 45.73it/s, est. speed input: 50026.09 toks/s, output: 48.85 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:12<00:12, 45.72it/s, est. speed input: 49994.73 toks/s, output: 48.82 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:12<00:11, 45.72it/s, est. speed input: 49964.34 toks/s, output: 48.79 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:13<00:11, 45.73it/s, est. speed input: 49934.47 toks/s, output: 48.76 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:14<00:10, 45.73it/s, est. speed input: 49905.13 toks/s, output: 48.74 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:14<00:09, 45.72it/s, est. speed input: 49876.20 toks/s, output: 48.71 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:15<00:09, 45.72it/s, est. speed input: 49847.98 toks/s, output: 48.68 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:16<00:08, 45.72it/s, est. speed input: 49820.01 toks/s, output: 48.65 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:17<00:07, 45.72it/s, est. speed input: 49792.77 toks/s, output: 48.63 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:17<00:06, 45.72it/s, est. speed input: 49766.01 toks/s, output: 48.60 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:18<00:06, 45.72it/s, est. speed input: 49739.60 toks/s, output: 48.57 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:19<00:05, 45.72it/s, est. speed input: 49713.66 toks/s, output: 48.55 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:19<00:04, 45.72it/s, est. speed input: 49688.29 toks/s, output: 48.52 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:20<00:04, 45.71it/s, est. speed input: 49663.23 toks/s, output: 48.50 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:21<00:03, 45.71it/s, est. speed input: 49638.41 toks/s, output: 48.47 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:21<00:02, 45.70it/s, est. speed input: 49614.01 toks/s, output: 48.45 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:22<00:02, 45.70it/s, est. speed input: 49590.17 toks/s, output: 48.43 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:23<00:01, 46.10it/s, est. speed input: 49578.78 toks/s, output: 48.42 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:23<00:00, 46.43it/s, est. speed input: 49568.98 toks/s, output: 48.41 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:23<00:00, 46.43it/s, est. speed input: 49934.35 toks/s, output: 48.76 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:23<00:00, 48.76it/s, est. speed input: 49934.35 toks/s, output: 48.76 toks/s]
[rank0]:[W128 00:31:27.296873324 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 123.6s

测试结果:
  Requests/s:   45.79
  Tokens/s:     46939.63
  Total Reqs:   4096
  Elapsed:      89.44s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     46893.84


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,55.6111,28528.5073,2.3017
1024,1024,1,128,128,44.3225,45430.5288,2.8879
2048,1024,2,256,128,49.4580,50694.4654,5.1761
4096,1024,4,512,128,49.1401,50368.6305,10.4192
8192,1024,8,1024,128,46.7991,47969.1016,21.8808
16384,1024,16,2048,128,45.7100,46852.7952,44.8042
32768,1024,32,4096,128,45.7948,46939.6325,89.4425

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:31:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3485217) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3485217) WARNING 01-28 00:31:42 [backends.py:609] Failed to read file <frozen os>
Throughput: 54.10 requests/s, 27751.36 total tokens/s, 54.10 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:31:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:31:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:31:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:31:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:31:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:31:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:31:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:31:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:31:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:31:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:31:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:31:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:31:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:31:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:31:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:31:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:31:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:31:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3485217) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3485217) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.30it/s]
(EngineCore_DP0 pid=3485217) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.30it/s]
(EngineCore_DP0 pid=3485217) 
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3485217) [2026-01-28 00:31:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3485217) 2026-01-28 00:31:48,262 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3485217) 2026-01-28 00:31:48,277 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3485217) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.12it/s]
(EngineCore_DP0 pid=3485217) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.35it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  95%|█████████▍| 121/128 [00:00<00:00, 1208.57it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1213.96it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:21,  5.91it/s, est. speed input: 3026.45 toks/s, output: 5.91 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 30.41it/s, est. speed input: 13223.20 toks/s, output: 25.83 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:02, 43.67it/s, est. speed input: 18642.53 toks/s, output: 36.41 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 50.33it/s, est. speed input: 21555.11 toks/s, output: 42.10 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:01, 54.11it/s, est. speed input: 23370.32 toks/s, output: 45.64 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:01, 56.43it/s, est. speed input: 24612.32 toks/s, output: 48.07 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 58.21it/s, est. speed input: 25568.87 toks/s, output: 49.94 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 58.92it/s, est. speed input: 26219.24 toks/s, output: 51.21 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 59.69it/s, est. speed input: 26774.29 toks/s, output: 52.29 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 60.54it/s, est. speed input: 27269.50 toks/s, output: 53.26 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:00, 60.72it/s, est. speed input: 27625.17 toks/s, output: 53.96 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:00, 60.37it/s, est. speed input: 27864.37 toks/s, output: 54.42 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:00, 60.67it/s, est. speed input: 28130.24 toks/s, output: 54.94 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 61.06it/s, est. speed input: 28377.60 toks/s, output: 55.42 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:01<00:00, 60.86it/s, est. speed input: 28545.73 toks/s, output: 55.75 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 59.54it/s, est. speed input: 28577.30 toks/s, output: 55.81 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 59.70it/s, est. speed input: 28704.69 toks/s, output: 56.06 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 60.06it/s, est. speed input: 28839.34 toks/s, output: 56.33 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 60.47it/s, est. speed input: 28973.68 toks/s, output: 56.59 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 60.47it/s, est. speed input: 29011.79 toks/s, output: 56.66 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 56.66it/s, est. speed input: 29011.79 toks/s, output: 56.66 toks/s]
[rank0]:[W128 00:31:52.162190349 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.0s

测试结果:
  Requests/s:   54.10
  Tokens/s:     27751.36
  Total Reqs:   128
  Elapsed:      2.37s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     27697.26

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:31:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3485898) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3485898) WARNING 01-28 00:32:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 44.07 requests/s, 45174.60 total tokens/s, 44.07 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:31:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:31:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:31:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:31:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:31:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:31:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:31:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:31:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:31:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:31:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:32:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:32:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:32:01] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:32:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:01] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:01] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:32:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:32:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:32:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:32:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:32:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3485898) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3485898) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]
(EngineCore_DP0 pid=3485898) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]
(EngineCore_DP0 pid=3485898) 
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3485898) [2026-01-28 00:32:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3485898) 2026-01-28 00:32:13,274 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3485898) 2026-01-28 00:32:13,290 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3485898) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.76it/s]
(EngineCore_DP0 pid=3485898) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.31it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 632.77it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 679.08it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:04, 26.42it/s, est. speed input: 27052.70 toks/s, output: 26.42 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 38.62it/s, est. speed input: 37593.62 toks/s, output: 36.71 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:02, 42.91it/s, est. speed input: 41394.82 toks/s, output: 40.42 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 44.96it/s, est. speed input: 43332.27 toks/s, output: 42.32 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 46.03it/s, est. speed input: 44468.08 toks/s, output: 43.43 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 46.66it/s, est. speed input: 45222.00 toks/s, output: 44.16 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 46.97it/s, est. speed input: 45721.50 toks/s, output: 44.65 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:01, 47.33it/s, est. speed input: 46159.55 toks/s, output: 45.08 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 47.52it/s, est. speed input: 46481.41 toks/s, output: 45.39 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:01, 47.71it/s, est. speed input: 46759.45 toks/s, output: 45.66 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 47.85it/s, est. speed input: 46990.52 toks/s, output: 45.89 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 47.95it/s, est. speed input: 47182.08 toks/s, output: 46.08 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 47.99it/s, est. speed input: 47340.45 toks/s, output: 46.23 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 48.01it/s, est. speed input: 47471.28 toks/s, output: 46.36 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 48.02it/s, est. speed input: 47585.79 toks/s, output: 46.47 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 48.01it/s, est. speed input: 47682.40 toks/s, output: 46.56 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:00, 48.08it/s, est. speed input: 47783.14 toks/s, output: 46.66 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 48.13it/s, est. speed input: 47873.06 toks/s, output: 46.75 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:01<00:00, 48.17it/s, est. speed input: 47953.77 toks/s, output: 46.83 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 48.14it/s, est. speed input: 48016.28 toks/s, output: 46.89 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 48.13it/s, est. speed input: 48075.77 toks/s, output: 46.95 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 48.14it/s, est. speed input: 48132.24 toks/s, output: 47.00 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 48.15it/s, est. speed input: 48183.46 toks/s, output: 47.05 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 48.17it/s, est. speed input: 48232.78 toks/s, output: 47.10 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 48.12it/s, est. speed input: 48270.39 toks/s, output: 47.14 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.02it/s, est. speed input: 48295.55 toks/s, output: 47.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.02it/s, est. speed input: 48295.55 toks/s, output: 47.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 47.16it/s, est. speed input: 48295.55 toks/s, output: 47.16 toks/s]
[rank0]:[W128 00:32:17.367515802 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.0s

测试结果:
  Requests/s:   44.07
  Tokens/s:     45174.60
  Total Reqs:   128
  Elapsed:      2.90s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     45130.53

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:32:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3486539) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3486539) WARNING 01-28 00:32:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 48.69 requests/s, 49902.92 total tokens/s, 48.69 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 00:32:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:32:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:32:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:32:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:32:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:32:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:32:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:32:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:32:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:32:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:32:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:32:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:32:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:32:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:32:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:32:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:32:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:32:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:27] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:27] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:27] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:27] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:27] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3486539) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3486539) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.52it/s]
(EngineCore_DP0 pid=3486539) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.52it/s]
(EngineCore_DP0 pid=3486539) 
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3486539) [2026-01-28 00:32:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3486539) 2026-01-28 00:32:38,654 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3486539) 2026-01-28 00:32:38,669 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3486539) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 21.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 21.87it/s]
(EngineCore_DP0 pid=3486539) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  6.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 10.28it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  25%|██▌       | 65/256 [00:00<00:00, 647.96it/s]
Adding requests:  55%|█████▍    | 140/256 [00:00<00:00, 702.61it/s]
Adding requests:  83%|████████▎ | 213/256 [00:00<00:00, 711.88it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 714.88it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 148.79it/s, est. speed input: 152375.67 toks/s, output: 148.80 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:00<00:02, 76.88it/s, est. speed input: 85835.68 toks/s, output: 83.82 toks/s]   
Processed prompts:  18%|█▊        | 45/256 [00:00<00:03, 64.86it/s, est. speed input: 74241.71 toks/s, output: 72.50 toks/s]
Processed prompts:  21%|██        | 53/256 [00:00<00:03, 59.71it/s, est. speed input: 69326.98 toks/s, output: 67.70 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:00<00:03, 54.46it/s, est. speed input: 65056.42 toks/s, output: 63.53 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:01<00:03, 53.11it/s, est. speed input: 63380.73 toks/s, output: 61.89 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:03, 52.13it/s, est. speed input: 62079.94 toks/s, output: 60.62 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:03, 51.39it/s, est. speed input: 61019.95 toks/s, output: 59.59 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:03, 50.83it/s, est. speed input: 60134.04 toks/s, output: 58.72 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:03, 50.43it/s, est. speed input: 59390.85 toks/s, output: 58.00 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:01<00:03, 50.10it/s, est. speed input: 58742.78 toks/s, output: 57.37 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:01<00:03, 49.91it/s, est. speed input: 58195.53 toks/s, output: 56.83 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:01<00:02, 49.80it/s, est. speed input: 57722.45 toks/s, output: 56.37 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:02, 49.59it/s, est. speed input: 57275.88 toks/s, output: 55.93 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:02<00:02, 49.51it/s, est. speed input: 56894.02 toks/s, output: 55.56 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:02<00:02, 49.39it/s, est. speed input: 56541.36 toks/s, output: 55.22 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:02<00:02, 49.37it/s, est. speed input: 56234.70 toks/s, output: 54.92 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:02<00:02, 49.35it/s, est. speed input: 55958.17 toks/s, output: 54.65 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:02<00:02, 49.29it/s, est. speed input: 55698.57 toks/s, output: 54.39 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:02<00:02, 49.29it/s, est. speed input: 55469.78 toks/s, output: 54.17 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:02<00:02, 49.31it/s, est. speed input: 55262.36 toks/s, output: 53.97 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:03<00:01, 49.35it/s, est. speed input: 55074.86 toks/s, output: 53.78 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:03<00:01, 49.29it/s, est. speed input: 54891.05 toks/s, output: 53.60 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:03<00:01, 49.25it/s, est. speed input: 54720.16 toks/s, output: 53.44 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:03<00:01, 49.31it/s, est. speed input: 54572.68 toks/s, output: 53.29 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:03<00:01, 49.34it/s, est. speed input: 54435.26 toks/s, output: 53.16 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:03<00:01, 49.38it/s, est. speed input: 54308.56 toks/s, output: 53.04 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:03<00:01, 49.43it/s, est. speed input: 54192.61 toks/s, output: 52.92 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:03<00:01, 49.41it/s, est. speed input: 54078.44 toks/s, output: 52.81 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:03<00:00, 49.36it/s, est. speed input: 53967.11 toks/s, output: 52.70 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:04<00:00, 49.30it/s, est. speed input: 53858.92 toks/s, output: 52.60 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:04<00:00, 49.36it/s, est. speed input: 53768.36 toks/s, output: 52.51 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:04<00:00, 49.37it/s, est. speed input: 53679.08 toks/s, output: 52.42 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:04<00:00, 49.40it/s, est. speed input: 53597.52 toks/s, output: 52.34 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:04<00:00, 49.34it/s, est. speed input: 53512.03 toks/s, output: 52.26 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:04<00:00, 49.26it/s, est. speed input: 53427.45 toks/s, output: 52.18 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:04<00:00, 49.23it/s, est. speed input: 53349.67 toks/s, output: 52.10 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 49.23it/s, est. speed input: 53516.67 toks/s, output: 52.26 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 52.26it/s, est. speed input: 53516.67 toks/s, output: 52.26 toks/s]
[rank0]:[W128 00:32:45.179903421 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 27.8s

测试结果:
  Requests/s:   48.69
  Tokens/s:     49902.92
  Total Reqs:   256
  Elapsed:      5.26s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     49854.23

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:32:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3487211) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3487211) WARNING 01-28 00:33:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 47.93 requests/s, 49133.34 total tokens/s, 47.93 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 00:32:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:32:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:32:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:32:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:32:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:32:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:32:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:32:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:32:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:32:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:32:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:32:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:32:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:32:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:32:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:32:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:32:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:32:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:32:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3487211) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3487211) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.58it/s]
(EngineCore_DP0 pid=3487211) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.58it/s]
(EngineCore_DP0 pid=3487211) 
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3487211) [2026-01-28 00:32:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3487211) 2026-01-28 00:33:07,070 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3487211) 2026-01-28 00:33:07,087 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3487211) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 25.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 23.86it/s]
(EngineCore_DP0 pid=3487211) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.33it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 69/512 [00:00<00:00, 688.49it/s]
Adding requests:  28%|██▊       | 143/512 [00:00<00:00, 714.19it/s]
Adding requests:  42%|████▏     | 216/512 [00:00<00:00, 720.40it/s]
Adding requests:  57%|█████▋    | 293/512 [00:00<00:00, 738.20it/s]
Adding requests:  72%|███████▏  | 367/512 [00:00<00:00, 738.61it/s]
Adding requests:  86%|████████▋ | 442/512 [00:00<00:00, 740.15it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 734.79it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:00<00:01, 262.04it/s, est. speed input: 268359.49 toks/s, output: 262.05 toks/s]
Processed prompts:  12%|█▏        | 61/512 [00:00<00:05, 86.38it/s, est. speed input: 99625.37 toks/s, output: 97.29 toks/s]   
Processed prompts:  15%|█▍        | 75/512 [00:00<00:06, 66.76it/s, est. speed input: 80190.67 toks/s, output: 78.31 toks/s]
Processed prompts:  17%|█▋        | 85/512 [00:01<00:06, 65.22it/s, est. speed input: 77483.84 toks/s, output: 75.67 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:01<00:07, 55.31it/s, est. speed input: 70156.29 toks/s, output: 68.51 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:01<00:07, 53.53it/s, est. speed input: 67896.37 toks/s, output: 66.30 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:01<00:07, 52.14it/s, est. speed input: 66085.98 toks/s, output: 64.54 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:01<00:07, 51.12it/s, est. speed input: 64620.27 toks/s, output: 63.11 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:02<00:07, 50.36it/s, est. speed input: 63394.86 toks/s, output: 61.91 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:02<00:07, 49.79it/s, est. speed input: 62353.96 toks/s, output: 60.89 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:02<00:07, 49.33it/s, est. speed input: 61441.24 toks/s, output: 60.00 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:02<00:07, 49.04it/s, est. speed input: 60661.72 toks/s, output: 59.24 toks/s]
Processed prompts:  31%|███       | 158/512 [00:02<00:07, 48.83it/s, est. speed input: 59975.25 toks/s, output: 58.57 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:02<00:07, 48.60it/s, est. speed input: 59349.47 toks/s, output: 57.96 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:03<00:06, 48.52it/s, est. speed input: 58810.36 toks/s, output: 57.43 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:03<00:06, 48.44it/s, est. speed input: 58322.58 toks/s, output: 56.96 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:03<00:06, 48.32it/s, est. speed input: 57870.20 toks/s, output: 56.51 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:03<00:06, 48.26it/s, est. speed input: 57464.77 toks/s, output: 56.12 toks/s]
Processed prompts:  40%|████      | 206/512 [00:03<00:06, 48.21it/s, est. speed input: 57095.72 toks/s, output: 55.76 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:03<00:06, 48.21it/s, est. speed input: 56763.86 toks/s, output: 55.43 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:04<00:06, 48.25it/s, est. speed input: 56464.61 toks/s, output: 55.14 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:04<00:05, 48.27it/s, est. speed input: 56188.21 toks/s, output: 54.87 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:04<00:05, 48.22it/s, est. speed input: 55924.81 toks/s, output: 54.61 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:04<00:05, 48.24it/s, est. speed input: 55686.77 toks/s, output: 54.38 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:04<00:05, 48.28it/s, est. speed input: 55470.29 toks/s, output: 54.17 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:04<00:05, 48.28it/s, est. speed input: 55263.86 toks/s, output: 53.97 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:05<00:05, 48.32it/s, est. speed input: 55076.33 toks/s, output: 53.79 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:05<00:04, 48.31it/s, est. speed input: 54897.23 toks/s, output: 53.61 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:05<00:04, 48.25it/s, est. speed input: 54722.01 toks/s, output: 53.44 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:05<00:04, 48.24it/s, est. speed input: 54561.33 toks/s, output: 53.28 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:05<00:04, 48.19it/s, est. speed input: 54405.47 toks/s, output: 53.13 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:05<00:04, 48.19it/s, est. speed input: 54261.34 toks/s, output: 52.99 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:06<00:04, 48.19it/s, est. speed input: 54125.70 toks/s, output: 52.86 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:06<00:03, 48.18it/s, est. speed input: 53996.62 toks/s, output: 52.73 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:06<00:03, 48.13it/s, est. speed input: 53869.54 toks/s, output: 52.61 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:06<00:03, 48.12it/s, est. speed input: 53751.47 toks/s, output: 52.49 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:06<00:03, 48.18it/s, est. speed input: 53645.41 toks/s, output: 52.39 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:06<00:03, 48.23it/s, est. speed input: 53545.46 toks/s, output: 52.29 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:07<00:03, 48.26it/s, est. speed input: 53450.07 toks/s, output: 52.20 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:07<00:02, 48.25it/s, est. speed input: 53356.39 toks/s, output: 52.11 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:07<00:02, 48.21it/s, est. speed input: 53263.47 toks/s, output: 52.02 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:07<00:02, 48.20it/s, est. speed input: 53176.51 toks/s, output: 51.93 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:07<00:02, 48.17it/s, est. speed input: 53091.34 toks/s, output: 51.85 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:07<00:02, 48.15it/s, est. speed input: 53010.39 toks/s, output: 51.77 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:08<00:02, 48.19it/s, est. speed input: 52936.54 toks/s, output: 51.70 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:08<00:01, 48.24it/s, est. speed input: 52867.18 toks/s, output: 51.63 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:08<00:01, 48.19it/s, est. speed input: 52794.28 toks/s, output: 51.56 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:08<00:01, 48.24it/s, est. speed input: 52730.90 toks/s, output: 51.49 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:08<00:01, 48.26it/s, est. speed input: 52668.21 toks/s, output: 51.43 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:08<00:01, 48.26it/s, est. speed input: 52607.26 toks/s, output: 51.37 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:09<00:01, 48.23it/s, est. speed input: 52546.51 toks/s, output: 51.31 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:09<00:00, 48.25it/s, est. speed input: 52490.76 toks/s, output: 51.26 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:09<00:00, 48.20it/s, est. speed input: 52432.90 toks/s, output: 51.20 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:09<00:00, 48.15it/s, est. speed input: 52376.18 toks/s, output: 51.15 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:09<00:00, 48.18it/s, est. speed input: 52325.25 toks/s, output: 51.10 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:09<00:00, 48.15it/s, est. speed input: 52272.64 toks/s, output: 51.05 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:09<00:00, 49.64it/s, est. speed input: 52310.55 toks/s, output: 51.08 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 49.64it/s, est. speed input: 52515.16 toks/s, output: 51.28 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 51.28it/s, est. speed input: 52515.16 toks/s, output: 51.28 toks/s]
[rank0]:[W128 00:33:19.043729385 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.9s

测试结果:
  Requests/s:   47.93
  Tokens/s:     49133.34
  Total Reqs:   512
  Elapsed:      10.68s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     49085.40

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:33:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3487976) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3487976) WARNING 01-28 00:33:36 [backends.py:609] Failed to read file <frozen os>
Throughput: 45.46 requests/s, 46600.67 total tokens/s, 45.46 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 00:33:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:33:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:33:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:33:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:33:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:33:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:33:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:33:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:33:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:33:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:33:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:33:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:33:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:33:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:33:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:33:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:33:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:33:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:33:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3487976) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3487976) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]
(EngineCore_DP0 pid=3487976) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]
(EngineCore_DP0 pid=3487976) 
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3487976) [2026-01-28 00:33:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3487976) 2026-01-28 00:33:42,112 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3487976) 2026-01-28 00:33:42,137 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3487976) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  3.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 12.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 11.25it/s]
(EngineCore_DP0 pid=3487976) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.79it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 73/1024 [00:00<00:01, 724.25it/s]
Adding requests:  15%|█▍        | 149/1024 [00:00<00:01, 743.35it/s]
Adding requests:  22%|██▏       | 226/1024 [00:00<00:01, 751.57it/s]
Adding requests:  29%|██▉       | 302/1024 [00:00<00:00, 754.46it/s]
Adding requests:  37%|███▋      | 378/1024 [00:00<00:00, 752.46it/s]
Adding requests:  44%|████▍     | 454/1024 [00:00<00:00, 738.99it/s]
Adding requests:  52%|█████▏    | 528/1024 [00:00<00:00, 724.12it/s]
Adding requests:  59%|█████▉    | 604/1024 [00:00<00:00, 733.16it/s]
Adding requests:  67%|██████▋   | 683/1024 [00:00<00:00, 748.10it/s]
Adding requests:  74%|███████▍  | 758/1024 [00:01<00:00, 747.28it/s]
Adding requests:  81%|████████▏ | 833/1024 [00:01<00:00, 735.06it/s]
Adding requests:  89%|████████▉ | 910/1024 [00:01<00:00, 744.20it/s]
Adding requests:  96%|█████████▋| 986/1024 [00:01<00:00, 747.53it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 744.31it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:00<00:01, 567.29it/s, est. speed input: 580991.18 toks/s, output: 567.32 toks/s]
Processed prompts:  11%|█         | 115/1024 [00:01<00:12, 75.31it/s, est. speed input: 88761.93 toks/s, output: 86.68 toks/s]  
Processed prompts:  14%|█▍        | 142/1024 [00:01<00:13, 66.54it/s, est. speed input: 78518.07 toks/s, output: 76.68 toks/s]
Processed prompts:  16%|█▌        | 159/1024 [00:02<00:13, 61.88it/s, est. speed input: 73934.41 toks/s, output: 72.20 toks/s]
Processed prompts:  17%|█▋        | 171/1024 [00:02<00:15, 54.44it/s, est. speed input: 68611.81 toks/s, output: 67.00 toks/s]
Processed prompts:  18%|█▊        | 180/1024 [00:02<00:15, 53.96it/s, est. speed input: 67594.49 toks/s, output: 66.01 toks/s]
Processed prompts:  18%|█▊        | 188/1024 [00:02<00:15, 52.40it/s, est. speed input: 66335.59 toks/s, output: 64.78 toks/s]
Processed prompts:  19%|█▉        | 195/1024 [00:03<00:16, 49.79it/s, est. speed input: 64896.39 toks/s, output: 63.38 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:03<00:17, 47.52it/s, est. speed input: 63606.94 toks/s, output: 62.12 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:03<00:17, 47.06it/s, est. speed input: 62745.90 toks/s, output: 61.28 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:03<00:17, 46.69it/s, est. speed input: 61967.99 toks/s, output: 60.52 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:03<00:17, 46.42it/s, est. speed input: 61264.88 toks/s, output: 59.83 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:03<00:17, 46.23it/s, est. speed input: 60627.21 toks/s, output: 59.21 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:04<00:16, 46.06it/s, est. speed input: 60037.36 toks/s, output: 58.63 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:04<00:16, 45.94it/s, est. speed input: 59494.99 toks/s, output: 58.10 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:04<00:16, 45.87it/s, est. speed input: 58999.20 toks/s, output: 57.62 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:16, 45.82it/s, est. speed input: 58540.41 toks/s, output: 57.17 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:04<00:16, 45.79it/s, est. speed input: 58115.54 toks/s, output: 56.75 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:05<00:16, 45.77it/s, est. speed input: 57720.93 toks/s, output: 56.37 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:05<00:16, 45.75it/s, est. speed input: 57350.65 toks/s, output: 56.01 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:15, 45.74it/s, est. speed input: 57006.67 toks/s, output: 55.67 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:15, 45.74it/s, est. speed input: 56684.84 toks/s, output: 55.36 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:05<00:15, 45.70it/s, est. speed input: 56377.47 toks/s, output: 55.06 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:05<00:15, 45.68it/s, est. speed input: 56090.41 toks/s, output: 54.78 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:06<00:15, 45.68it/s, est. speed input: 55820.20 toks/s, output: 54.51 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:06<00:15, 45.66it/s, est. speed input: 55563.99 toks/s, output: 54.26 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:06<00:14, 45.66it/s, est. speed input: 55323.16 toks/s, output: 54.03 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:06<00:14, 45.69it/s, est. speed input: 55097.75 toks/s, output: 53.81 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:06<00:14, 45.67it/s, est. speed input: 54880.24 toks/s, output: 53.59 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:06<00:14, 45.67it/s, est. speed input: 54674.84 toks/s, output: 53.39 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:07<00:14, 45.65it/s, est. speed input: 54477.80 toks/s, output: 53.20 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:07<00:13, 45.67it/s, est. speed input: 54294.29 toks/s, output: 53.02 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:07<00:13, 45.70it/s, est. speed input: 54120.26 toks/s, output: 52.85 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:07<00:13, 45.67it/s, est. speed input: 53949.10 toks/s, output: 52.68 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:07<00:13, 45.68it/s, est. speed input: 53788.80 toks/s, output: 52.53 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:07<00:13, 45.68it/s, est. speed input: 53635.24 toks/s, output: 52.38 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:08<00:13, 45.65it/s, est. speed input: 53485.29 toks/s, output: 52.23 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:08<00:12, 45.68it/s, est. speed input: 53345.61 toks/s, output: 52.10 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:08<00:12, 45.68it/s, est. speed input: 53210.80 toks/s, output: 51.96 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:08<00:12, 45.67it/s, est. speed input: 53080.31 toks/s, output: 51.84 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:08<00:12, 45.67it/s, est. speed input: 52955.03 toks/s, output: 51.71 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:09<00:12, 45.65it/s, est. speed input: 52833.83 toks/s, output: 51.60 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:09<00:12, 45.64it/s, est. speed input: 52717.01 toks/s, output: 51.48 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:09<00:11, 45.63it/s, est. speed input: 52604.43 toks/s, output: 51.37 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:09<00:11, 45.64it/s, est. speed input: 52497.70 toks/s, output: 51.27 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:09<00:11, 45.66it/s, est. speed input: 52395.37 toks/s, output: 51.17 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:09<00:11, 45.64it/s, est. speed input: 52293.84 toks/s, output: 51.07 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:10<00:11, 45.62it/s, est. speed input: 52196.07 toks/s, output: 50.97 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:10<00:11, 45.62it/s, est. speed input: 52102.62 toks/s, output: 50.88 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:10<00:10, 45.67it/s, est. speed input: 52014.96 toks/s, output: 50.80 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:10<00:10, 45.68it/s, est. speed input: 51929.27 toks/s, output: 50.71 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:10<00:10, 45.70it/s, est. speed input: 51846.73 toks/s, output: 50.63 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:10<00:10, 45.69it/s, est. speed input: 51765.32 toks/s, output: 50.55 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:11<00:10, 45.64it/s, est. speed input: 51684.02 toks/s, output: 50.47 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:11<00:09, 45.64it/s, est. speed input: 51607.55 toks/s, output: 50.40 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:11<00:09, 45.66it/s, est. speed input: 51534.36 toks/s, output: 50.33 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:11<00:09, 45.68it/s, est. speed input: 51463.61 toks/s, output: 50.26 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:11<00:09, 45.66it/s, est. speed input: 51392.96 toks/s, output: 50.19 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:12<00:09, 45.65it/s, est. speed input: 51325.15 toks/s, output: 50.12 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:12<00:09, 45.63it/s, est. speed input: 51258.04 toks/s, output: 50.06 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:12<00:08, 45.62it/s, est. speed input: 51193.21 toks/s, output: 49.99 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:12<00:08, 45.63it/s, est. speed input: 51131.16 toks/s, output: 49.93 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:12<00:08, 45.65it/s, est. speed input: 51071.19 toks/s, output: 49.87 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:12<00:08, 45.66it/s, est. speed input: 51013.23 toks/s, output: 49.82 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:13<00:08, 45.65it/s, est. speed input: 50955.65 toks/s, output: 49.76 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:13<00:08, 45.64it/s, est. speed input: 50899.21 toks/s, output: 49.71 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:13<00:07, 45.62it/s, est. speed input: 50843.87 toks/s, output: 49.65 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:13<00:07, 45.63it/s, est. speed input: 50790.87 toks/s, output: 49.60 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:13<00:07, 45.62it/s, est. speed input: 50738.99 toks/s, output: 49.55 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:13<00:07, 45.61it/s, est. speed input: 50687.89 toks/s, output: 49.50 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:14<00:07, 45.58it/s, est. speed input: 50636.75 toks/s, output: 49.45 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:14<00:06, 45.60it/s, est. speed input: 50589.25 toks/s, output: 49.40 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:14<00:06, 45.59it/s, est. speed input: 50541.35 toks/s, output: 49.36 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:14<00:06, 45.60it/s, est. speed input: 50495.67 toks/s, output: 49.31 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:14<00:06, 45.62it/s, est. speed input: 50451.35 toks/s, output: 49.27 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:14<00:06, 45.61it/s, est. speed input: 50407.48 toks/s, output: 49.23 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:15<00:06, 45.59it/s, est. speed input: 50363.79 toks/s, output: 49.18 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:15<00:05, 45.60it/s, est. speed input: 50322.08 toks/s, output: 49.14 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:15<00:05, 45.61it/s, est. speed input: 50281.46 toks/s, output: 49.10 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:15<00:05, 45.60it/s, est. speed input: 50241.07 toks/s, output: 49.06 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:15<00:05, 45.62it/s, est. speed input: 50202.58 toks/s, output: 49.03 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:16<00:05, 45.60it/s, est. speed input: 50163.66 toks/s, output: 48.99 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:16<00:05, 45.56it/s, est. speed input: 50124.50 toks/s, output: 48.95 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:16<00:04, 45.58it/s, est. speed input: 50088.25 toks/s, output: 48.91 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:16<00:04, 45.58it/s, est. speed input: 50052.00 toks/s, output: 48.88 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:16<00:04, 45.58it/s, est. speed input: 50016.58 toks/s, output: 48.84 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:16<00:04, 45.61it/s, est. speed input: 49983.21 toks/s, output: 48.81 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:17<00:04, 45.58it/s, est. speed input: 49948.22 toks/s, output: 48.78 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:17<00:03, 45.57it/s, est. speed input: 49914.81 toks/s, output: 48.74 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:17<00:03, 45.59it/s, est. speed input: 49882.86 toks/s, output: 48.71 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:17<00:03, 45.63it/s, est. speed input: 49852.25 toks/s, output: 48.68 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:17<00:03, 45.60it/s, est. speed input: 49820.43 toks/s, output: 48.65 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:17<00:03, 45.60it/s, est. speed input: 49790.06 toks/s, output: 48.62 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:18<00:03, 45.56it/s, est. speed input: 49758.77 toks/s, output: 48.59 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:18<00:02, 45.56it/s, est. speed input: 49728.82 toks/s, output: 48.56 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:18<00:02, 45.57it/s, est. speed input: 49700.00 toks/s, output: 48.54 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:18<00:02, 45.59it/s, est. speed input: 49672.10 toks/s, output: 48.51 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:18<00:02, 45.61it/s, est. speed input: 49644.96 toks/s, output: 48.48 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:19<00:02, 45.60it/s, est. speed input: 49617.66 toks/s, output: 48.45 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:19<00:02, 45.55it/s, est. speed input: 49589.25 toks/s, output: 48.43 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:19<00:01, 45.60it/s, est. speed input: 49564.25 toks/s, output: 48.40 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:19<00:01, 45.60it/s, est. speed input: 49538.33 toks/s, output: 48.38 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:19<00:01, 45.60it/s, est. speed input: 49513.12 toks/s, output: 48.35 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:19<00:01, 45.61it/s, est. speed input: 49488.54 toks/s, output: 48.33 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:20<00:01, 45.58it/s, est. speed input: 49463.18 toks/s, output: 48.30 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:20<00:01, 45.54it/s, est. speed input: 49438.01 toks/s, output: 48.28 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:20<00:00, 45.57it/s, est. speed input: 49414.74 toks/s, output: 48.26 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:20<00:00, 45.61it/s, est. speed input: 49392.55 toks/s, output: 48.23 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:20<00:00, 45.61it/s, est. speed input: 49369.93 toks/s, output: 48.21 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:20<00:00, 45.61it/s, est. speed input: 49347.47 toks/s, output: 48.19 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:21<00:00, 44.63it/s, est. speed input: 49295.79 toks/s, output: 48.14 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:21<00:00, 44.63it/s, est. speed input: 49586.05 toks/s, output: 48.42 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:21<00:00, 48.42it/s, est. speed input: 49586.05 toks/s, output: 48.42 toks/s]
[rank0]:[W128 00:34:06.385366272 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.4s

测试结果:
  Requests/s:   45.46
  Tokens/s:     46600.67
  Total Reqs:   1024
  Elapsed:      22.52s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     46555.20

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:34:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3488953) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3488953) WARNING 01-28 00:34:26 [backends.py:609] Failed to read file <frozen os>
Throughput: 44.61 requests/s, 45725.09 total tokens/s, 44.61 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 00:34:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:34:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:34:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:34:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:34:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:34:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:34:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:34:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:34:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:34:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:34:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:34:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:34:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:34:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:34:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:34:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:34:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:34:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:34:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3488953) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3488953) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.54it/s]
(EngineCore_DP0 pid=3488953) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.54it/s]
(EngineCore_DP0 pid=3488953) 
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3488953) [2026-01-28 00:34:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3488953) 2026-01-28 00:34:32,763 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3488953) 2026-01-28 00:34:32,780 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3488953) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  6.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 11.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 13.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 13.09it/s]
(EngineCore_DP0 pid=3488953) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.09it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.16it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 73/2048 [00:00<00:02, 727.68it/s]
Adding requests:   7%|▋         | 149/2048 [00:00<00:02, 744.29it/s]
Adding requests:  11%|█         | 226/2048 [00:00<00:02, 753.98it/s]
Adding requests:  15%|█▍        | 303/2048 [00:00<00:02, 756.78it/s]
Adding requests:  19%|█▊        | 379/2048 [00:00<00:02, 750.56it/s]
Adding requests:  22%|██▏       | 455/2048 [00:00<00:02, 749.22it/s]
Adding requests:  26%|██▌       | 530/2048 [00:00<00:02, 727.86it/s]
Adding requests:  30%|██▉       | 607/2048 [00:00<00:01, 739.87it/s]
Adding requests:  33%|███▎      | 686/2048 [00:00<00:01, 754.25it/s]
Adding requests:  37%|███▋      | 762/2048 [00:01<00:01, 750.88it/s]
Adding requests:  41%|████      | 838/2048 [00:01<00:01, 737.09it/s]
Adding requests:  45%|████▍     | 917/2048 [00:01<00:01, 750.57it/s]
Adding requests:  49%|████▊     | 995/2048 [00:01<00:01, 756.79it/s]
Adding requests:  52%|█████▏    | 1071/2048 [00:01<00:01, 749.95it/s]
Adding requests:  56%|█████▌    | 1147/2048 [00:01<00:01, 750.70it/s]
Adding requests:  60%|█████▉    | 1227/2048 [00:01<00:01, 763.83it/s]
Adding requests:  64%|██████▎   | 1304/2048 [00:01<00:00, 755.14it/s]
Adding requests:  67%|██████▋   | 1381/2048 [00:01<00:00, 758.59it/s]
Adding requests:  71%|███████   | 1457/2048 [00:01<00:00, 754.00it/s]
Adding requests:  75%|███████▌  | 1536/2048 [00:02<00:00, 761.77it/s]
Adding requests:  79%|███████▉  | 1615/2048 [00:02<00:00, 767.54it/s]
Adding requests:  83%|████████▎ | 1692/2048 [00:02<00:00, 764.60it/s]
Adding requests:  86%|████████▋ | 1769/2048 [00:02<00:00, 762.58it/s]
Adding requests:  90%|█████████ | 1847/2048 [00:02<00:00, 766.71it/s]
Adding requests:  94%|█████████▍| 1924/2048 [00:02<00:00, 750.24it/s]
Adding requests:  98%|█████████▊| 2000/2048 [00:02<00:00, 745.92it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 752.37it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:00<00:03, 518.39it/s, est. speed input: 530873.55 toks/s, output: 518.41 toks/s]
Processed prompts:   8%|▊         | 166/2048 [00:01<00:17, 107.46it/s, est. speed input: 131522.31 toks/s, output: 128.44 toks/s]
Processed prompts:   9%|▉         | 191/2048 [00:01<00:19, 96.41it/s, est. speed input: 118526.62 toks/s, output: 115.75 toks/s] 
Processed prompts:  10%|█         | 208/2048 [00:02<00:22, 81.96it/s, est. speed input: 106084.93 toks/s, output: 103.60 toks/s]
Processed prompts:  11%|█         | 220/2048 [00:02<00:27, 67.57it/s, est. speed input: 95237.59 toks/s, output: 93.01 toks/s]  
Processed prompts:  11%|█         | 229/2048 [00:02<00:33, 54.94it/s, est. speed input: 86114.72 toks/s, output: 84.10 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:03<00:36, 49.41it/s, est. speed input: 80453.12 toks/s, output: 78.57 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:03<00:37, 48.01it/s, est. speed input: 76844.78 toks/s, output: 75.04 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:03<00:37, 47.05it/s, est. speed input: 73929.71 toks/s, output: 72.20 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:04<00:37, 46.35it/s, est. speed input: 71504.02 toks/s, output: 69.83 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:04<00:37, 45.86it/s, est. speed input: 69464.66 toks/s, output: 67.84 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:04<00:38, 45.41it/s, est. speed input: 67687.78 toks/s, output: 66.10 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:05<00:37, 45.20it/s, est. speed input: 66187.09 toks/s, output: 64.64 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:05<00:37, 45.06it/s, est. speed input: 64883.28 toks/s, output: 63.36 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:05<00:37, 44.95it/s, est. speed input: 63732.10 toks/s, output: 62.24 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:06<00:37, 44.88it/s, est. speed input: 62713.48 toks/s, output: 61.24 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:06<00:36, 44.83it/s, est. speed input: 61803.38 toks/s, output: 60.35 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:07<00:36, 44.80it/s, est. speed input: 60987.71 toks/s, output: 59.56 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:07<00:36, 44.76it/s, est. speed input: 60247.02 toks/s, output: 58.83 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:07<00:35, 44.74it/s, est. speed input: 59577.35 toks/s, output: 58.18 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:08<00:35, 44.73it/s, est. speed input: 58966.20 toks/s, output: 57.58 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:08<00:35, 44.70it/s, est. speed input: 58404.40 toks/s, output: 57.04 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:08<00:34, 44.69it/s, est. speed input: 57889.83 toks/s, output: 56.53 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:09<00:34, 44.69it/s, est. speed input: 57416.19 toks/s, output: 56.07 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:09<00:33, 44.70it/s, est. speed input: 56979.58 toks/s, output: 55.64 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:09<00:33, 44.71it/s, est. speed input: 56575.02 toks/s, output: 55.25 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:10<00:33, 44.71it/s, est. speed input: 56197.31 toks/s, output: 54.88 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:10<00:32, 44.70it/s, est. speed input: 55844.43 toks/s, output: 54.54 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:10<00:32, 44.69it/s, est. speed input: 55513.64 toks/s, output: 54.21 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:11<00:32, 44.67it/s, est. speed input: 55202.37 toks/s, output: 53.91 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:11<00:31, 44.68it/s, est. speed input: 54913.19 toks/s, output: 53.63 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:12<00:31, 44.68it/s, est. speed input: 54640.93 toks/s, output: 53.36 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:12<00:31, 44.67it/s, est. speed input: 54382.97 toks/s, output: 53.11 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:12<00:30, 44.67it/s, est. speed input: 54139.57 toks/s, output: 52.87 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:13<00:30, 44.66it/s, est. speed input: 53909.51 toks/s, output: 52.65 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:13<00:30, 44.67it/s, est. speed input: 53692.53 toks/s, output: 52.43 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:13<00:29, 44.67it/s, est. speed input: 53487.13 toks/s, output: 52.23 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:14<00:29, 44.66it/s, est. speed input: 53290.01 toks/s, output: 52.04 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:14<00:28, 44.66it/s, est. speed input: 53103.72 toks/s, output: 51.86 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:14<00:28, 44.64it/s, est. speed input: 52925.08 toks/s, output: 51.68 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:15<00:28, 44.64it/s, est. speed input: 52755.59 toks/s, output: 51.52 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:15<00:27, 44.64it/s, est. speed input: 52594.03 toks/s, output: 51.36 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:15<00:27, 44.65it/s, est. speed input: 52440.50 toks/s, output: 51.21 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:16<00:27, 44.64it/s, est. speed input: 52291.92 toks/s, output: 51.07 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:16<00:26, 44.64it/s, est. speed input: 52150.93 toks/s, output: 50.93 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:17<00:26, 44.64it/s, est. speed input: 52015.22 toks/s, output: 50.80 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:17<00:26, 44.62it/s, est. speed input: 51884.20 toks/s, output: 50.67 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:17<00:25, 44.64it/s, est. speed input: 51760.56 toks/s, output: 50.55 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:18<00:25, 44.63it/s, est. speed input: 51639.91 toks/s, output: 50.43 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:18<00:25, 44.63it/s, est. speed input: 51524.78 toks/s, output: 50.32 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:18<00:24, 44.64it/s, est. speed input: 51414.45 toks/s, output: 50.21 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:19<00:24, 44.61it/s, est. speed input: 51306.39 toks/s, output: 50.10 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:19<00:23, 44.63it/s, est. speed input: 51204.47 toks/s, output: 50.00 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:19<00:23, 44.63it/s, est. speed input: 51105.32 toks/s, output: 49.91 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:20<00:23, 44.62it/s, est. speed input: 51009.10 toks/s, output: 49.81 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:20<00:22, 44.62it/s, est. speed input: 50917.00 toks/s, output: 49.72 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:20<00:22, 44.62it/s, est. speed input: 50827.41 toks/s, output: 49.64 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:21<00:22, 44.61it/s, est. speed input: 50740.68 toks/s, output: 49.55 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:21<00:21, 44.62it/s, est. speed input: 50657.39 toks/s, output: 49.47 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:22<00:21, 44.62it/s, est. speed input: 50577.15 toks/s, output: 49.39 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:22<00:21, 44.60it/s, est. speed input: 50497.69 toks/s, output: 49.31 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:22<00:20, 44.60it/s, est. speed input: 50421.69 toks/s, output: 49.24 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:23<00:20, 44.56it/s, est. speed input: 50345.86 toks/s, output: 49.17 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:23<00:19, 45.31it/s, est. speed input: 50316.09 toks/s, output: 49.14 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:23<00:19, 45.11it/s, est. speed input: 50246.80 toks/s, output: 49.07 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:24<00:19, 44.95it/s, est. speed input: 50178.71 toks/s, output: 49.00 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:24<00:18, 44.85it/s, est. speed input: 50113.35 toks/s, output: 48.94 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:24<00:18, 44.77it/s, est. speed input: 50049.29 toks/s, output: 48.88 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:25<00:18, 44.71it/s, est. speed input: 49986.45 toks/s, output: 48.81 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:25<00:17, 44.69it/s, est. speed input: 49926.72 toks/s, output: 48.76 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:25<00:17, 44.65it/s, est. speed input: 49867.27 toks/s, output: 48.70 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:26<00:17, 44.63it/s, est. speed input: 49809.75 toks/s, output: 48.64 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:26<00:16, 44.62it/s, est. speed input: 49754.11 toks/s, output: 48.59 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:27<00:16, 44.61it/s, est. speed input: 49699.96 toks/s, output: 48.54 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:27<00:16, 44.59it/s, est. speed input: 49646.52 toks/s, output: 48.48 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:27<00:15, 44.61it/s, est. speed input: 49595.70 toks/s, output: 48.43 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:28<00:15, 44.60it/s, est. speed input: 49545.66 toks/s, output: 48.38 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:28<00:15, 44.60it/s, est. speed input: 49496.86 toks/s, output: 48.34 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:28<00:14, 44.61it/s, est. speed input: 49449.63 toks/s, output: 48.29 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:29<00:14, 44.60it/s, est. speed input: 49403.02 toks/s, output: 48.25 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:29<00:13, 44.60it/s, est. speed input: 49357.84 toks/s, output: 48.20 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:29<00:13, 44.60it/s, est. speed input: 49313.45 toks/s, output: 48.16 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:30<00:13, 44.58it/s, est. speed input: 49269.58 toks/s, output: 48.11 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:30<00:12, 44.58it/s, est. speed input: 49227.38 toks/s, output: 48.07 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:31<00:12, 44.59it/s, est. speed input: 49186.20 toks/s, output: 48.03 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:31<00:12, 44.58it/s, est. speed input: 49145.42 toks/s, output: 47.99 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:31<00:11, 44.58it/s, est. speed input: 49105.75 toks/s, output: 47.95 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:32<00:11, 44.57it/s, est. speed input: 49066.88 toks/s, output: 47.92 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:32<00:11, 44.58it/s, est. speed input: 49029.19 toks/s, output: 47.88 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:32<00:10, 44.58it/s, est. speed input: 48992.48 toks/s, output: 47.84 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:33<00:10, 44.58it/s, est. speed input: 48956.32 toks/s, output: 47.81 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:33<00:10, 44.58it/s, est. speed input: 48920.82 toks/s, output: 47.77 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:33<00:09, 44.59it/s, est. speed input: 48886.41 toks/s, output: 47.74 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:34<00:09, 44.58it/s, est. speed input: 48852.33 toks/s, output: 47.71 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:34<00:08, 44.58it/s, est. speed input: 48819.11 toks/s, output: 47.67 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:34<00:08, 44.57it/s, est. speed input: 48786.34 toks/s, output: 47.64 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:35<00:08, 44.55it/s, est. speed input: 48753.41 toks/s, output: 47.61 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:35<00:07, 44.55it/s, est. speed input: 48722.00 toks/s, output: 47.58 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:36<00:07, 44.57it/s, est. speed input: 48691.72 toks/s, output: 47.55 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:36<00:07, 44.57it/s, est. speed input: 48661.78 toks/s, output: 47.52 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:36<00:06, 44.58it/s, est. speed input: 48632.44 toks/s, output: 47.49 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:37<00:06, 44.56it/s, est. speed input: 48602.85 toks/s, output: 47.46 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:37<00:06, 44.55it/s, est. speed input: 48574.23 toks/s, output: 47.44 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:37<00:05, 44.56it/s, est. speed input: 48546.57 toks/s, output: 47.41 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:38<00:05, 44.55it/s, est. speed input: 48518.73 toks/s, output: 47.38 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:38<00:04, 44.57it/s, est. speed input: 48492.31 toks/s, output: 47.36 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:38<00:04, 44.57it/s, est. speed input: 48466.02 toks/s, output: 47.33 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:39<00:04, 44.56it/s, est. speed input: 48439.95 toks/s, output: 47.30 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:39<00:03, 45.37it/s, est. speed input: 48440.45 toks/s, output: 47.31 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:39<00:03, 45.11it/s, est. speed input: 48414.88 toks/s, output: 47.28 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:40<00:03, 44.94it/s, est. speed input: 48389.83 toks/s, output: 47.26 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:40<00:02, 44.83it/s, est. speed input: 48365.57 toks/s, output: 47.23 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:41<00:02, 44.74it/s, est. speed input: 48341.54 toks/s, output: 47.21 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:41<00:02, 44.68it/s, est. speed input: 48317.72 toks/s, output: 47.19 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:41<00:01, 44.64it/s, est. speed input: 48294.66 toks/s, output: 47.16 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:42<00:01, 44.60it/s, est. speed input: 48271.29 toks/s, output: 47.14 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:42<00:01, 44.59it/s, est. speed input: 48249.06 toks/s, output: 47.12 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:42<00:00, 44.58it/s, est. speed input: 48227.20 toks/s, output: 47.10 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:43<00:00, 45.37it/s, est. speed input: 48229.04 toks/s, output: 47.10 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:43<00:00, 45.37it/s, est. speed input: 48560.79 toks/s, output: 47.42 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:43<00:00, 47.42it/s, est. speed input: 48560.79 toks/s, output: 47.42 toks/s]
[rank0]:[W128 00:35:20.412242238 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 74.0s

测试结果:
  Requests/s:   44.61
  Tokens/s:     45725.09
  Total Reqs:   2048
  Elapsed:      45.91s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     45680.48

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:35:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3490316) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3490316) WARNING 01-28 00:35:46 [backends.py:609] Failed to read file <frozen os>
Throughput: 44.60 requests/s, 45717.12 total tokens/s, 44.60 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 00:35:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:35:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:35:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:35:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:35:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:35:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:35:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:35:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:35:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:35:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:35:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:35:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:35:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:35:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:35:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:35:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:35:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:35:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:35:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3490316) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3490316) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]
(EngineCore_DP0 pid=3490316) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]
(EngineCore_DP0 pid=3490316) 
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3490316) [2026-01-28 00:35:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3490316) [rank0]:W0128 00:35:49.740000 3490316 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3490316) [rank0]:W0128 00:35:49.788000 3490316 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3490316) [rank0]:W0128 00:35:50.441000 3490316 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3490316) [rank0]:W0128 00:35:50.516000 3490316 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3490316) 2026-01-28 00:35:52,801 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3490316) 2026-01-28 00:35:52,818 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3490316) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:04,  2.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 13.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 16.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 12.54it/s]
(EngineCore_DP0 pid=3490316) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  6.46it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 15.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 16.73it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 71/4096 [00:00<00:05, 707.38it/s]
Adding requests:   4%|▎         | 147/4096 [00:00<00:05, 735.02it/s]
Adding requests:   5%|▌         | 222/4096 [00:00<00:05, 741.56it/s]
Adding requests:   7%|▋         | 299/4096 [00:00<00:05, 748.93it/s]
Adding requests:   9%|▉         | 375/4096 [00:00<00:04, 752.22it/s]
Adding requests:  11%|█         | 451/4096 [00:00<00:04, 754.44it/s]
Adding requests:  13%|█▎        | 527/4096 [00:00<00:04, 737.10it/s]
Adding requests:  15%|█▍        | 603/4096 [00:00<00:04, 742.74it/s]
Adding requests:  17%|█▋        | 683/4096 [00:00<00:04, 758.65it/s]
Adding requests:  19%|█▊        | 760/4096 [00:01<00:04, 761.56it/s]
Adding requests:  20%|██        | 837/4096 [00:01<00:04, 745.38it/s]
Adding requests:  22%|██▏       | 916/4096 [00:01<00:04, 758.10it/s]
Adding requests:  24%|██▍       | 992/4096 [00:01<00:04, 758.34it/s]
Adding requests:  26%|██▌       | 1068/4096 [00:01<00:04, 746.23it/s]
Adding requests:  28%|██▊       | 1143/4096 [00:01<00:03, 740.96it/s]
Adding requests:  30%|██▉       | 1222/4096 [00:01<00:03, 753.25it/s]
Adding requests:  32%|███▏      | 1298/4096 [00:01<00:03, 750.35it/s]
Adding requests:  34%|███▎      | 1374/4096 [00:01<00:03, 743.97it/s]
Adding requests:  35%|███▌      | 1449/4096 [00:01<00:03, 739.36it/s]
Adding requests:  37%|███▋      | 1526/4096 [00:02<00:03, 747.71it/s]
Adding requests:  39%|███▉      | 1603/4096 [00:02<00:03, 751.61it/s]
Adding requests:  41%|████      | 1679/4096 [00:02<00:03, 743.46it/s]
Adding requests:  43%|████▎     | 1754/4096 [00:02<00:03, 743.54it/s]
Adding requests:  45%|████▍     | 1829/4096 [00:02<00:03, 739.02it/s]
Adding requests:  46%|████▋     | 1904/4096 [00:02<00:02, 741.30it/s]
Adding requests:  48%|████▊     | 1980/4096 [00:02<00:02, 746.18it/s]
Adding requests:  50%|█████     | 2056/4096 [00:02<00:02, 748.84it/s]
Adding requests:  52%|█████▏    | 2131/4096 [00:02<00:02, 745.75it/s]
Adding requests:  54%|█████▍    | 2206/4096 [00:02<00:02, 723.96it/s]
Adding requests:  56%|█████▌    | 2283/4096 [00:03<00:02, 736.37it/s]
Adding requests:  58%|█████▊    | 2360/4096 [00:03<00:02, 745.10it/s]
Adding requests:  59%|█████▉    | 2437/4096 [00:03<00:02, 751.01it/s]
Adding requests:  61%|██████▏   | 2513/4096 [00:03<00:02, 752.01it/s]
Adding requests:  63%|██████▎   | 2589/4096 [00:03<00:02, 752.51it/s]
Adding requests:  65%|██████▌   | 2666/4096 [00:03<00:01, 757.22it/s]
Adding requests:  67%|██████▋   | 2742/4096 [00:03<00:01, 755.82it/s]
Adding requests:  69%|██████▉   | 2818/4096 [00:03<00:01, 755.49it/s]
Adding requests:  71%|███████   | 2895/4096 [00:03<00:01, 759.01it/s]
Adding requests:  73%|███████▎  | 2971/4096 [00:03<00:01, 744.70it/s]
Adding requests:  74%|███████▍  | 3046/4096 [00:04<00:01, 744.00it/s]
Adding requests:  76%|███████▌  | 3123/4096 [00:04<00:01, 748.41it/s]
Adding requests:  78%|███████▊  | 3198/4096 [00:04<00:01, 742.79it/s]
Adding requests:  80%|███████▉  | 3275/4096 [00:04<00:01, 747.95it/s]
Adding requests:  82%|████████▏ | 3352/4096 [00:04<00:00, 752.47it/s]
Adding requests:  84%|████████▎ | 3429/4096 [00:04<00:00, 756.86it/s]
Adding requests:  86%|████████▌ | 3505/4096 [00:04<00:00, 747.27it/s]
Adding requests:  87%|████████▋ | 3580/4096 [00:04<00:00, 732.81it/s]
Adding requests:  89%|████████▉ | 3654/4096 [00:04<00:00, 731.81it/s]
Adding requests:  91%|█████████ | 3730/4096 [00:04<00:00, 737.79it/s]
Adding requests:  93%|█████████▎| 3807/4096 [00:05<00:00, 747.13it/s]
Adding requests:  95%|█████████▍| 3883/4096 [00:05<00:00, 749.05it/s]
Adding requests:  97%|█████████▋| 3959/4096 [00:05<00:00, 751.43it/s]
Adding requests:  99%|█████████▊| 4035/4096 [00:05<00:00, 745.11it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 746.73it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:00<00:05, 719.52it/s, est. speed input: 736821.31 toks/s, output: 719.53 toks/s]
Processed prompts:   7%|▋         | 298/4096 [00:01<00:27, 139.54it/s, est. speed input: 174976.50 toks/s, output: 170.88 toks/s]
Processed prompts:   8%|▊         | 331/4096 [00:02<00:36, 103.83it/s, est. speed input: 137845.40 toks/s, output: 134.61 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:03<00:47, 78.50it/s, est. speed input: 114200.78 toks/s, output: 111.52 toks/s] 
Processed prompts:   9%|▉         | 386/4096 [00:03<00:55, 67.17it/s, est. speed input: 101623.46 toks/s, output: 99.24 toks/s] 
Processed prompts:  10%|█         | 418/4096 [00:04<01:01, 59.93it/s, est. speed input: 92971.74 toks/s, output: 90.79 toks/s] 
Processed prompts:  11%|█         | 450/4096 [00:05<01:06, 55.10it/s, est. speed input: 86618.60 toks/s, output: 84.59 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:06<01:09, 51.86it/s, est. speed input: 81774.40 toks/s, output: 79.86 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:06<01:12, 49.67it/s, est. speed input: 77963.88 toks/s, output: 76.14 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:07<01:13, 48.16it/s, est. speed input: 74880.55 toks/s, output: 73.13 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:08<01:14, 47.11it/s, est. speed input: 72334.08 toks/s, output: 70.64 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:08<01:15, 46.37it/s, est. speed input: 70195.30 toks/s, output: 68.55 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:09<01:15, 45.88it/s, est. speed input: 68381.05 toks/s, output: 66.78 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:10<01:15, 45.51it/s, est. speed input: 66807.79 toks/s, output: 65.24 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:11<01:14, 45.26it/s, est. speed input: 65442.99 toks/s, output: 63.91 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:11<01:14, 45.09it/s, est. speed input: 64245.34 toks/s, output: 62.74 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:12<01:13, 44.96it/s, est. speed input: 63183.32 toks/s, output: 61.70 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:13<01:13, 44.88it/s, est. speed input: 62237.80 toks/s, output: 60.78 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:13<01:12, 44.81it/s, est. speed input: 61387.77 toks/s, output: 59.95 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:14<01:12, 44.78it/s, est. speed input: 60622.74 toks/s, output: 59.20 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:15<01:11, 44.74it/s, est. speed input: 59927.93 toks/s, output: 58.52 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:16<01:10, 44.72it/s, est. speed input: 59294.53 toks/s, output: 57.90 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:16<01:10, 44.70it/s, est. speed input: 58715.83 toks/s, output: 57.34 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:17<01:09, 44.69it/s, est. speed input: 58183.40 toks/s, output: 56.82 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:18<01:08, 44.67it/s, est. speed input: 57691.79 toks/s, output: 56.34 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:18<01:08, 44.66it/s, est. speed input: 57238.06 toks/s, output: 55.90 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:19<01:07, 44.66it/s, est. speed input: 56818.13 toks/s, output: 55.49 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:20<01:06, 44.65it/s, est. speed input: 56426.63 toks/s, output: 55.10 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:21<01:05, 44.64it/s, est. speed input: 56061.61 toks/s, output: 54.75 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:21<01:05, 44.64it/s, est. speed input: 55721.21 toks/s, output: 54.42 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:22<01:04, 44.64it/s, est. speed input: 55402.46 toks/s, output: 54.10 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:23<01:03, 44.64it/s, est. speed input: 55103.21 toks/s, output: 53.81 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:23<01:03, 44.60it/s, est. speed input: 54816.77 toks/s, output: 53.53 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:24<01:02, 44.61it/s, est. speed input: 54552.49 toks/s, output: 53.27 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:25<01:01, 44.62it/s, est. speed input: 54302.14 toks/s, output: 53.03 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:26<01:00, 44.62it/s, est. speed input: 54065.71 toks/s, output: 52.80 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:26<01:00, 44.61it/s, est. speed input: 53841.06 toks/s, output: 52.58 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:27<00:59, 44.63it/s, est. speed input: 53629.88 toks/s, output: 52.37 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:28<00:58, 44.63it/s, est. speed input: 53428.68 toks/s, output: 52.18 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:28<00:58, 44.62it/s, est. speed input: 53236.97 toks/s, output: 51.99 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:29<00:57, 44.63it/s, est. speed input: 53054.95 toks/s, output: 51.81 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:30<00:56, 44.62it/s, est. speed input: 52880.99 toks/s, output: 51.64 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:31<00:55, 44.62it/s, est. speed input: 52714.97 toks/s, output: 51.48 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:31<00:55, 44.62it/s, est. speed input: 52556.61 toks/s, output: 51.32 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:32<00:54, 44.61it/s, est. speed input: 52404.90 toks/s, output: 51.18 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:33<00:53, 44.60it/s, est. speed input: 52259.16 toks/s, output: 51.03 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:33<00:53, 44.61it/s, est. speed input: 52120.57 toks/s, output: 50.90 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:34<00:52, 44.60it/s, est. speed input: 51987.04 toks/s, output: 50.77 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:35<00:51, 44.61it/s, est. speed input: 51859.68 toks/s, output: 50.64 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:36<00:50, 44.60it/s, est. speed input: 51736.31 toks/s, output: 50.52 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:36<00:49, 44.95it/s, est. speed input: 51644.38 toks/s, output: 50.43 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:37<00:49, 44.85it/s, est. speed input: 51530.43 toks/s, output: 50.32 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:38<00:48, 44.77it/s, est. speed input: 51420.38 toks/s, output: 50.22 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:38<00:47, 44.71it/s, est. speed input: 51313.99 toks/s, output: 50.11 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:39<00:47, 44.67it/s, est. speed input: 51211.74 toks/s, output: 50.01 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:40<00:46, 44.64it/s, est. speed input: 51112.83 toks/s, output: 49.91 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:41<00:45, 44.63it/s, est. speed input: 51018.08 toks/s, output: 49.82 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:41<00:45, 44.60it/s, est. speed input: 50925.22 toks/s, output: 49.73 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:42<00:44, 44.60it/s, est. speed input: 50836.56 toks/s, output: 49.65 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:43<00:43, 44.60it/s, est. speed input: 50750.67 toks/s, output: 49.56 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:44<00:43, 44.59it/s, est. speed input: 50667.47 toks/s, output: 49.48 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:44<00:42, 44.59it/s, est. speed input: 50586.99 toks/s, output: 49.40 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:45<00:41, 44.58it/s, est. speed input: 50508.93 toks/s, output: 49.33 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:46<00:40, 44.58it/s, est. speed input: 50433.36 toks/s, output: 49.25 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:46<00:40, 44.58it/s, est. speed input: 50359.79 toks/s, output: 49.18 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:47<00:39, 44.57it/s, est. speed input: 50288.46 toks/s, output: 49.11 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:48<00:38, 44.57it/s, est. speed input: 50219.34 toks/s, output: 49.04 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:49<00:38, 44.57it/s, est. speed input: 50152.17 toks/s, output: 48.98 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:49<00:37, 44.57it/s, est. speed input: 50087.18 toks/s, output: 48.91 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:50<00:36, 44.58it/s, est. speed input: 50024.18 toks/s, output: 48.85 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:51<00:35, 44.95it/s, est. speed input: 49982.24 toks/s, output: 48.81 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:51<00:34, 44.83it/s, est. speed input: 49921.91 toks/s, output: 48.75 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:52<00:34, 44.75it/s, est. speed input: 49863.32 toks/s, output: 48.69 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:53<00:33, 44.69it/s, est. speed input: 49806.24 toks/s, output: 48.64 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:54<00:32, 44.65it/s, est. speed input: 49750.63 toks/s, output: 48.58 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:54<00:32, 44.62it/s, est. speed input: 49696.72 toks/s, output: 48.53 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:55<00:31, 44.60it/s, est. speed input: 49643.88 toks/s, output: 48.48 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:56<00:30, 44.59it/s, est. speed input: 49592.67 toks/s, output: 48.43 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:56<00:30, 44.58it/s, est. speed input: 49542.53 toks/s, output: 48.38 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:57<00:29, 44.57it/s, est. speed input: 49493.71 toks/s, output: 48.33 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:58<00:28, 44.57it/s, est. speed input: 49446.18 toks/s, output: 48.29 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:59<00:27, 44.56it/s, est. speed input: 49399.54 toks/s, output: 48.24 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:59<00:27, 44.56it/s, est. speed input: 49354.05 toks/s, output: 48.20 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [01:00<00:26, 44.56it/s, est. speed input: 49309.79 toks/s, output: 48.15 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [01:01<00:25, 44.55it/s, est. speed input: 49266.33 toks/s, output: 48.11 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:01<00:25, 44.55it/s, est. speed input: 49223.86 toks/s, output: 48.07 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:02<00:24, 44.55it/s, est. speed input: 49182.50 toks/s, output: 48.03 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:03<00:23, 44.55it/s, est. speed input: 49142.12 toks/s, output: 47.99 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:04<00:22, 44.53it/s, est. speed input: 49102.10 toks/s, output: 47.95 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:04<00:22, 44.53it/s, est. speed input: 49063.33 toks/s, output: 47.91 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:05<00:21, 44.54it/s, est. speed input: 49025.58 toks/s, output: 47.88 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:06<00:20, 44.54it/s, est. speed input: 48988.36 toks/s, output: 47.84 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:06<00:20, 44.53it/s, est. speed input: 48951.97 toks/s, output: 47.80 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:07<00:19, 44.53it/s, est. speed input: 48916.44 toks/s, output: 47.77 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:08<00:18, 44.53it/s, est. speed input: 48881.51 toks/s, output: 47.74 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:09<00:17, 44.52it/s, est. speed input: 48847.15 toks/s, output: 47.70 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:09<00:17, 44.52it/s, est. speed input: 48813.37 toks/s, output: 47.67 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:10<00:16, 44.52it/s, est. speed input: 48780.65 toks/s, output: 47.64 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:11<00:15, 44.52it/s, est. speed input: 48748.36 toks/s, output: 47.61 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:12<00:15, 44.51it/s, est. speed input: 48716.65 toks/s, output: 47.57 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:12<00:14, 44.52it/s, est. speed input: 48685.96 toks/s, output: 47.54 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:13<00:13, 44.52it/s, est. speed input: 48655.58 toks/s, output: 47.52 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:14<00:12, 44.52it/s, est. speed input: 48625.97 toks/s, output: 47.49 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:14<00:12, 44.52it/s, est. speed input: 48596.64 toks/s, output: 47.46 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:15<00:11, 44.52it/s, est. speed input: 48568.09 toks/s, output: 47.43 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:16<00:10, 44.52it/s, est. speed input: 48540.01 toks/s, output: 47.40 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:17<00:10, 44.52it/s, est. speed input: 48512.33 toks/s, output: 47.38 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:17<00:09, 44.52it/s, est. speed input: 48485.40 toks/s, output: 47.35 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:18<00:08, 44.52it/s, est. speed input: 48458.76 toks/s, output: 47.32 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:19<00:07, 44.52it/s, est. speed input: 48432.69 toks/s, output: 47.30 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:19<00:07, 44.51it/s, est. speed input: 48406.85 toks/s, output: 47.27 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:20<00:06, 44.51it/s, est. speed input: 48381.72 toks/s, output: 47.25 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:21<00:05, 44.51it/s, est. speed input: 48356.74 toks/s, output: 47.22 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:22<00:04, 44.51it/s, est. speed input: 48332.37 toks/s, output: 47.20 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:22<00:04, 44.50it/s, est. speed input: 48308.16 toks/s, output: 47.18 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:23<00:03, 44.51it/s, est. speed input: 48285.07 toks/s, output: 47.15 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:24<00:02, 44.50it/s, est. speed input: 48261.66 toks/s, output: 47.13 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:24<00:02, 44.51it/s, est. speed input: 48238.95 toks/s, output: 47.11 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:25<00:01, 44.89it/s, est. speed input: 48228.20 toks/s, output: 47.10 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:26<00:00, 45.21it/s, est. speed input: 48218.98 toks/s, output: 47.09 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:26<00:00, 45.21it/s, est. speed input: 48574.61 toks/s, output: 47.44 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:26<00:00, 47.44it/s, est. speed input: 48574.61 toks/s, output: 47.44 toks/s]
[rank0]:[W128 00:37:27.928595905 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 126.6s

测试结果:
  Requests/s:   44.60
  Tokens/s:     45717.12
  Total Reqs:   4096
  Elapsed:      91.83s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     45672.52


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,54.0962,27751.3599,2.3662
1024,1024,1,128,128,44.0728,45174.6047,2.9043
2048,1024,2,256,128,48.6858,49902.9182,5.2582
4096,1024,4,512,128,47.9350,49133.3385,10.6811
8192,1024,8,1024,128,45.4641,46600.6651,22.5233
16384,1024,16,2048,128,44.6098,45725.0896,45.9092
32768,1024,32,4096,128,44.6021,45717.1240,91.8343

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_000759.log
[SUCCESS] bitnet1.58-2b-int8 Prefill 完成 (1772.1s)

------------------------------------------------------------
  Prefill Benchmark: bitnet1.58-2b-fp8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-fp8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA GeForce RTX 5080                   ││
│ GPU (short):      RTX5080                                   │
│ Memory:           15.5 GB                                    │
│ CC:               cc120 (Blackwell)                            │
│ SM Code:          sm_120                                    │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.95.05                                 │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-fp8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['prefill']
  M_prefill:        [512, 1024, 2048, 4096, 8192, 16384, 32768]
  M_decode:         [512, 1024, 2048, 4096, 8192, 16384, 32768]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_003731.log


============================================================
  BitNet-2B-FP8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:37:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3492279) WARNING 01-28 00:37:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.17 requests/s, 26249.32 total tokens/s, 51.17 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:37:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:37:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:37:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:37:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:37:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:37:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:37:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:37:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:37:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:37:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:37:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:37:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:37:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:37:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:37:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:37:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:37:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:37:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:37:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:37:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:37:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:37:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:37:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:37:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:37:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:37:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:37:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:37:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3492279) [2026-01-28 00:37:40] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3492279) [2026-01-28 00:37:40] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3492279) [2026-01-28 00:37:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3492279) [2026-01-28 00:37:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3492279) [2026-01-28 00:37:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3492279) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3492279) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.03it/s]
(EngineCore_DP0 pid=3492279) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.03it/s]
(EngineCore_DP0 pid=3492279) 
(EngineCore_DP0 pid=3492279) 2026-01-28 00:37:51,110 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3492279) 2026-01-28 00:37:51,126 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3492279) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.87it/s]
(EngineCore_DP0 pid=3492279) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.11it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  85%|████████▌ | 109/128 [00:00<00:00, 1080.65it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1106.00it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:04, 25.74it/s, est. speed input: 13177.87 toks/s, output: 25.74 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:02, 42.04it/s, est. speed input: 20244.74 toks/s, output: 39.54 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 47.63it/s, est. speed input: 22737.88 toks/s, output: 44.41 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 50.70it/s, est. speed input: 24121.38 toks/s, output: 47.11 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 52.16it/s, est. speed input: 24894.48 toks/s, output: 48.62 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 53.50it/s, est. speed input: 25518.17 toks/s, output: 49.84 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 53.94it/s, est. speed input: 25881.79 toks/s, output: 50.55 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 54.45it/s, est. speed input: 26195.82 toks/s, output: 51.16 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:01, 54.68it/s, est. speed input: 26421.99 toks/s, output: 51.61 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 55.06it/s, est. speed input: 26638.99 toks/s, output: 52.03 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 55.22it/s, est. speed input: 26802.24 toks/s, output: 52.35 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 54.85it/s, est. speed input: 26874.34 toks/s, output: 52.49 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:00, 55.02it/s, est. speed input: 26989.02 toks/s, output: 52.71 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 55.07it/s, est. speed input: 27078.62 toks/s, output: 52.89 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 54.96it/s, est. speed input: 27141.21 toks/s, output: 53.01 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:01<00:00, 54.49it/s, est. speed input: 27154.46 toks/s, output: 53.04 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 54.85it/s, est. speed input: 27234.00 toks/s, output: 53.19 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 54.92it/s, est. speed input: 27286.98 toks/s, output: 53.29 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 55.22it/s, est. speed input: 27356.81 toks/s, output: 53.43 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 54.98it/s, est. speed input: 27382.40 toks/s, output: 53.48 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 55.25it/s, est. speed input: 27439.97 toks/s, output: 53.59 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 55.25it/s, est. speed input: 27490.11 toks/s, output: 53.69 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 53.69it/s, est. speed input: 27490.11 toks/s, output: 53.69 toks/s]
[rank0]:[W128 00:37:55.004993257 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.7s

测试结果:
  Requests/s:   51.17
  Tokens/s:     26249.32
  Total Reqs:   128
  Elapsed:      2.50s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     26198.16

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:38:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3492992) WARNING 01-28 00:38:09 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.36 requests/s, 36240.56 total tokens/s, 35.36 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:38:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:38:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:38:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:38:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:38:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:38:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:38:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:38:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:38:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:38:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:38:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:38:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:38:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:38:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:38:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:38:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:38:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:38:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3492992) [2026-01-28 00:38:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3492992) [2026-01-28 00:38:05] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3492992) [2026-01-28 00:38:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3492992) [2026-01-28 00:38:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3492992) [2026-01-28 00:38:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3492992) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3492992) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.63it/s]
(EngineCore_DP0 pid=3492992) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.63it/s]
(EngineCore_DP0 pid=3492992) 
(EngineCore_DP0 pid=3492992) 2026-01-28 00:38:15,493 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3492992) 2026-01-28 00:38:15,510 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3492992) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 23.16it/s]
(EngineCore_DP0 pid=3492992) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.25it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.24it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 628.52it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 679.75it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 74.92it/s, est. speed input: 76727.36 toks/s, output: 74.92 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 46.74it/s, est. speed input: 50906.58 toks/s, output: 49.71 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 42.01it/s, est. speed input: 46277.08 toks/s, output: 45.19 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 39.95it/s, est. speed input: 44260.10 toks/s, output: 43.22 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 38.69it/s, est. speed input: 42973.50 toks/s, output: 41.97 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 37.88it/s, est. speed input: 42182.31 toks/s, output: 41.19 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 37.33it/s, est. speed input: 41583.77 toks/s, output: 40.61 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 36.91it/s, est. speed input: 41103.19 toks/s, output: 40.14 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 36.67it/s, est. speed input: 40727.02 toks/s, output: 39.77 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 36.48it/s, est. speed input: 40409.35 toks/s, output: 39.46 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 36.38it/s, est. speed input: 40149.40 toks/s, output: 39.21 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 36.19it/s, est. speed input: 39895.65 toks/s, output: 38.96 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 36.10it/s, est. speed input: 39687.92 toks/s, output: 38.76 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 36.06it/s, est. speed input: 39510.22 toks/s, output: 38.58 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 36.05it/s, est. speed input: 39356.57 toks/s, output: 38.43 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 36.00it/s, est. speed input: 39210.87 toks/s, output: 38.29 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 35.99it/s, est. speed input: 39087.59 toks/s, output: 38.17 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 35.99it/s, est. speed input: 38975.91 toks/s, output: 38.06 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 35.99it/s, est. speed input: 38875.98 toks/s, output: 37.96 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 35.99it/s, est. speed input: 38784.69 toks/s, output: 37.88 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 35.93it/s, est. speed input: 38691.99 toks/s, output: 37.79 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 35.93it/s, est. speed input: 38612.77 toks/s, output: 37.71 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 35.96it/s, est. speed input: 38544.14 toks/s, output: 37.64 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 35.96it/s, est. speed input: 38477.71 toks/s, output: 37.58 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 35.89it/s, est. speed input: 38407.07 toks/s, output: 37.51 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 35.88it/s, est. speed input: 38346.75 toks/s, output: 37.45 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 35.90it/s, est. speed input: 38294.33 toks/s, output: 37.40 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.92it/s, est. speed input: 38245.42 toks/s, output: 37.35 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.92it/s, est. speed input: 38211.94 toks/s, output: 37.32 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.32it/s, est. speed input: 38211.94 toks/s, output: 37.32 toks/s]
[rank0]:[W128 00:38:20.248995437 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.3s

测试结果:
  Requests/s:   35.36
  Tokens/s:     36240.56
  Total Reqs:   128
  Elapsed:      3.62s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     36205.20

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:38:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3493619) WARNING 01-28 00:38:35 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.80 requests/s, 36690.99 total tokens/s, 35.80 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 00:38:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:38:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:38:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:38:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:38:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:38:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:38:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:38:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:38:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:38:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:38:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:38:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:38:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:38:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:38:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:38:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:38:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:38:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3493619) [2026-01-28 00:38:31] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3493619) [2026-01-28 00:38:31] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3493619) [2026-01-28 00:38:31] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3493619) [2026-01-28 00:38:31] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3493619) [2026-01-28 00:38:31] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3493619) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3493619) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.63it/s]
(EngineCore_DP0 pid=3493619) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.63it/s]
(EngineCore_DP0 pid=3493619) 
(EngineCore_DP0 pid=3493619) 2026-01-28 00:38:41,734 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3493619) 2026-01-28 00:38:41,751 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3493619) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 16.82it/s]
(EngineCore_DP0 pid=3493619) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.95it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  25%|██▌       | 65/256 [00:00<00:00, 643.08it/s]
Adding requests:  55%|█████▌    | 141/256 [00:00<00:00, 708.52it/s]
Adding requests:  84%|████████▍ | 215/256 [00:00<00:00, 718.91it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 716.38it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:00<00:02, 120.90it/s, est. speed input: 123817.86 toks/s, output: 120.91 toks/s]
Processed prompts:  11%|█         | 27/256 [00:00<00:04, 55.22it/s, est. speed input: 61765.80 toks/s, output: 60.32 toks/s]   
Processed prompts:  14%|█▎        | 35/256 [00:00<00:04, 46.95it/s, est. speed input: 53574.76 toks/s, output: 52.32 toks/s]
Processed prompts:  16%|█▌        | 41/256 [00:00<00:04, 43.52it/s, est. speed input: 50283.81 toks/s, output: 49.11 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:01<00:05, 39.33it/s, est. speed input: 47053.88 toks/s, output: 45.95 toks/s]
Processed prompts:  20%|█▉        | 51/256 [00:01<00:05, 40.66it/s, est. speed input: 46970.82 toks/s, output: 45.87 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:01<00:05, 37.21it/s, est. speed input: 44884.32 toks/s, output: 43.83 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:01<00:05, 36.99it/s, est. speed input: 44270.32 toks/s, output: 43.23 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:01<00:05, 36.76it/s, est. speed input: 43727.65 toks/s, output: 42.70 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:01<00:05, 36.57it/s, est. speed input: 43258.18 toks/s, output: 42.24 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:05, 36.46it/s, est. speed input: 42858.38 toks/s, output: 41.85 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:01<00:04, 36.35it/s, est. speed input: 42499.06 toks/s, output: 41.50 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:04, 36.24it/s, est. speed input: 42176.07 toks/s, output: 41.19 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:02<00:04, 36.15it/s, est. speed input: 41883.91 toks/s, output: 40.90 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:02<00:04, 36.07it/s, est. speed input: 41619.36 toks/s, output: 40.64 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:02<00:04, 36.08it/s, est. speed input: 41393.29 toks/s, output: 40.42 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:02<00:04, 36.10it/s, est. speed input: 41190.10 toks/s, output: 40.22 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:02<00:04, 36.16it/s, est. speed input: 41012.26 toks/s, output: 40.05 toks/s]
Processed prompts:  41%|████      | 104/256 [00:02<00:04, 36.15it/s, est. speed input: 40842.45 toks/s, output: 39.89 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:02<00:04, 36.14it/s, est. speed input: 40684.96 toks/s, output: 39.73 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:03, 36.10it/s, est. speed input: 40535.38 toks/s, output: 39.59 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:02<00:03, 36.10it/s, est. speed input: 40400.85 toks/s, output: 39.45 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:03<00:03, 36.10it/s, est. speed input: 40276.56 toks/s, output: 39.33 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:03<00:03, 36.13it/s, est. speed input: 40163.99 toks/s, output: 39.22 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:03<00:03, 36.10it/s, est. speed input: 40053.26 toks/s, output: 39.11 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:03<00:03, 36.13it/s, est. speed input: 39955.45 toks/s, output: 39.02 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:03<00:03, 36.14it/s, est. speed input: 39863.02 toks/s, output: 38.93 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:03<00:03, 36.16it/s, est. speed input: 39777.13 toks/s, output: 38.84 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:03<00:03, 36.11it/s, est. speed input: 39690.43 toks/s, output: 38.76 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:03<00:02, 36.09it/s, est. speed input: 39609.40 toks/s, output: 38.68 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:03<00:02, 36.10it/s, est. speed input: 39535.43 toks/s, output: 38.61 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:04<00:02, 36.09it/s, est. speed input: 39464.05 toks/s, output: 38.54 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:04<00:02, 36.05it/s, est. speed input: 39393.72 toks/s, output: 38.47 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:04<00:02, 36.03it/s, est. speed input: 39327.28 toks/s, output: 38.41 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:04<00:02, 36.05it/s, est. speed input: 39267.14 toks/s, output: 38.35 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:04<00:02, 36.03it/s, est. speed input: 39207.16 toks/s, output: 38.29 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:04<00:02, 36.03it/s, est. speed input: 39151.31 toks/s, output: 38.23 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:04<00:02, 36.07it/s, est. speed input: 39102.09 toks/s, output: 38.19 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:04<00:01, 36.09it/s, est. speed input: 39054.04 toks/s, output: 38.14 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:04<00:01, 36.05it/s, est. speed input: 39003.33 toks/s, output: 38.09 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:05<00:01, 35.96it/s, est. speed input: 38950.51 toks/s, output: 38.04 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:05<00:01, 35.98it/s, est. speed input: 38905.87 toks/s, output: 37.99 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:05<00:01, 36.03it/s, est. speed input: 38866.22 toks/s, output: 37.96 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:05<00:01, 36.05it/s, est. speed input: 38827.26 toks/s, output: 37.92 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:05<00:01, 36.12it/s, est. speed input: 38793.30 toks/s, output: 37.88 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:05<00:01, 36.12it/s, est. speed input: 38757.83 toks/s, output: 37.85 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:05<00:01, 36.10it/s, est. speed input: 38722.26 toks/s, output: 37.81 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:05<00:00, 36.13it/s, est. speed input: 38690.58 toks/s, output: 37.78 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:05<00:00, 36.07it/s, est. speed input: 38655.14 toks/s, output: 37.75 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:06<00:00, 36.05it/s, est. speed input: 38621.95 toks/s, output: 37.72 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:06<00:00, 36.10it/s, est. speed input: 38594.52 toks/s, output: 37.69 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:06<00:00, 36.05it/s, est. speed input: 38562.65 toks/s, output: 37.66 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:06<00:00, 36.06it/s, est. speed input: 38534.84 toks/s, output: 37.63 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:06<00:00, 36.08it/s, est. speed input: 38508.41 toks/s, output: 37.61 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:06<00:00, 36.11it/s, est. speed input: 38483.83 toks/s, output: 37.58 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:06<00:00, 36.14it/s, est. speed input: 38460.98 toks/s, output: 37.56 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 36.14it/s, est. speed input: 38593.85 toks/s, output: 37.69 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 37.69it/s, est. speed input: 38593.85 toks/s, output: 37.69 toks/s]
[rank0]:[W128 00:38:50.175480723 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 30.0s

测试结果:
  Requests/s:   35.80
  Tokens/s:     36690.99
  Total Reqs:   256
  Elapsed:      7.15s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     36655.20

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:38:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3494328) WARNING 01-28 00:39:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.76 requests/s, 36654.52 total tokens/s, 35.76 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 00:38:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:38:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:38:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:38:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:38:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:38:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:38:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:38:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:38:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:38:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:39:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:39:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:39:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:39:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:39:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:39:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:39:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:39:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:39:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3494328) [2026-01-28 00:39:01] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3494328) [2026-01-28 00:39:01] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3494328) [2026-01-28 00:39:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3494328) [2026-01-28 00:39:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3494328) [2026-01-28 00:39:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3494328) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3494328) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.68it/s]
(EngineCore_DP0 pid=3494328) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.68it/s]
(EngineCore_DP0 pid=3494328) 
(EngineCore_DP0 pid=3494328) 2026-01-28 00:39:11,904 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3494328) 2026-01-28 00:39:11,921 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3494328) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.10it/s]
(EngineCore_DP0 pid=3494328) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.03it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 68/512 [00:00<00:00, 675.21it/s]
Adding requests:  28%|██▊       | 144/512 [00:00<00:00, 722.97it/s]
Adding requests:  43%|████▎     | 218/512 [00:00<00:00, 728.85it/s]
Adding requests:  58%|█████▊    | 295/512 [00:00<00:00, 744.78it/s]
Adding requests:  72%|███████▏  | 371/512 [00:00<00:00, 749.30it/s]
Adding requests:  87%|████████▋ | 447/512 [00:00<00:00, 751.26it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 741.49it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:00<00:03, 152.03it/s, est. speed input: 155688.74 toks/s, output: 152.03 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:00<00:07, 60.51it/s, est. speed input: 69768.80 toks/s, output: 68.13 toks/s]   
Processed prompts:  10%|▉         | 51/512 [00:00<00:08, 53.31it/s, est. speed input: 62261.33 toks/s, output: 60.80 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:01<00:09, 45.85it/s, est. speed input: 55933.27 toks/s, output: 54.62 toks/s]
Processed prompts:  12%|█▎        | 64/512 [00:01<00:09, 47.42it/s, est. speed input: 55862.27 toks/s, output: 54.55 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:01<00:11, 40.03it/s, est. speed input: 51322.09 toks/s, output: 50.12 toks/s]
Processed prompts:  15%|█▍        | 75/512 [00:01<00:10, 41.02it/s, est. speed input: 50924.16 toks/s, output: 49.73 toks/s]
Processed prompts:  16%|█▌        | 80/512 [00:01<00:10, 41.91it/s, est. speed input: 50591.40 toks/s, output: 49.41 toks/s]
Processed prompts:  17%|█▋        | 85/512 [00:01<00:10, 42.64it/s, est. speed input: 50296.11 toks/s, output: 49.12 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:01<00:12, 34.38it/s, est. speed input: 47189.60 toks/s, output: 46.08 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:02<00:12, 34.73it/s, est. speed input: 46627.87 toks/s, output: 45.53 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:02<00:11, 34.97it/s, est. speed input: 46110.21 toks/s, output: 45.03 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:02<00:11, 35.22it/s, est. speed input: 45656.67 toks/s, output: 44.59 toks/s]
Processed prompts:  21%|██        | 106/512 [00:02<00:11, 35.35it/s, est. speed input: 45231.36 toks/s, output: 44.17 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:02<00:11, 35.53it/s, est. speed input: 44861.75 toks/s, output: 43.81 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:02<00:11, 35.63it/s, est. speed input: 44517.21 toks/s, output: 43.47 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:02<00:11, 35.70it/s, est. speed input: 44200.04 toks/s, output: 43.16 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:02<00:10, 35.77it/s, est. speed input: 43910.86 toks/s, output: 42.88 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:02<00:10, 35.83it/s, est. speed input: 43643.86 toks/s, output: 42.62 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:03<00:10, 35.87it/s, est. speed input: 43397.09 toks/s, output: 42.38 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:03<00:10, 35.89it/s, est. speed input: 43166.12 toks/s, output: 42.15 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:03<00:10, 35.91it/s, est. speed input: 42952.14 toks/s, output: 41.95 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:03<00:10, 35.87it/s, est. speed input: 42744.21 toks/s, output: 41.74 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:03<00:10, 35.89it/s, est. speed input: 42556.11 toks/s, output: 41.56 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:03<00:10, 35.92it/s, est. speed input: 42381.19 toks/s, output: 41.39 toks/s]
Processed prompts:  30%|███       | 154/512 [00:03<00:09, 35.92it/s, est. speed input: 42214.04 toks/s, output: 41.22 toks/s]
Processed prompts:  31%|███       | 158/512 [00:03<00:09, 35.94it/s, est. speed input: 42059.21 toks/s, output: 41.07 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:03<00:09, 35.92it/s, est. speed input: 41909.08 toks/s, output: 40.93 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:04<00:09, 35.92it/s, est. speed input: 41768.78 toks/s, output: 40.79 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:04<00:09, 35.88it/s, est. speed input: 41632.16 toks/s, output: 40.66 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:04<00:09, 35.88it/s, est. speed input: 41505.01 toks/s, output: 40.53 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:04<00:09, 35.84it/s, est. speed input: 41380.18 toks/s, output: 40.41 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:04<00:09, 35.85it/s, est. speed input: 41265.38 toks/s, output: 40.30 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:04<00:09, 35.87it/s, est. speed input: 41157.87 toks/s, output: 40.19 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:04<00:08, 35.91it/s, est. speed input: 41057.00 toks/s, output: 40.09 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:04<00:08, 35.93it/s, est. speed input: 40960.68 toks/s, output: 40.00 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:04<00:08, 35.93it/s, est. speed input: 40867.28 toks/s, output: 39.91 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:05<00:08, 35.91it/s, est. speed input: 40776.35 toks/s, output: 39.82 toks/s]
Processed prompts:  40%|████      | 206/512 [00:05<00:08, 35.93it/s, est. speed input: 40691.77 toks/s, output: 39.74 toks/s]
Processed prompts:  41%|████      | 210/512 [00:05<00:08, 35.94it/s, est. speed input: 40610.38 toks/s, output: 39.66 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:05<00:08, 35.90it/s, est. speed input: 40528.92 toks/s, output: 39.58 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:05<00:08, 35.88it/s, est. speed input: 40451.14 toks/s, output: 39.50 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:05<00:08, 35.90it/s, est. speed input: 40379.23 toks/s, output: 39.43 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:05<00:07, 35.93it/s, est. speed input: 40311.07 toks/s, output: 39.37 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:05<00:07, 35.94it/s, est. speed input: 40244.87 toks/s, output: 39.30 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:05<00:07, 35.95it/s, est. speed input: 40181.20 toks/s, output: 39.24 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:06<00:07, 35.94it/s, est. speed input: 40118.60 toks/s, output: 39.18 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:06<00:07, 35.92it/s, est. speed input: 40058.18 toks/s, output: 39.12 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:06<00:07, 35.90it/s, est. speed input: 39998.61 toks/s, output: 39.06 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:06<00:07, 35.86it/s, est. speed input: 39939.86 toks/s, output: 39.00 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:06<00:07, 35.88it/s, est. speed input: 39886.22 toks/s, output: 38.95 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:06<00:07, 35.90it/s, est. speed input: 39834.21 toks/s, output: 38.90 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:06<00:06, 35.94it/s, est. speed input: 39786.05 toks/s, output: 38.85 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:06<00:06, 35.94it/s, est. speed input: 39737.42 toks/s, output: 38.81 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:06<00:06, 35.93it/s, est. speed input: 39689.77 toks/s, output: 38.76 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:07<00:06, 35.92it/s, est. speed input: 39644.02 toks/s, output: 38.71 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:07<00:06, 35.93it/s, est. speed input: 39600.36 toks/s, output: 38.67 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:07<00:06, 35.90it/s, est. speed input: 39555.67 toks/s, output: 38.63 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:07<00:06, 35.88it/s, est. speed input: 39512.64 toks/s, output: 38.59 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:07<00:06, 35.86it/s, est. speed input: 39470.45 toks/s, output: 38.55 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:07<00:06, 35.88it/s, est. speed input: 39431.26 toks/s, output: 38.51 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:07<00:05, 35.90it/s, est. speed input: 39393.52 toks/s, output: 38.47 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:07<00:05, 35.92it/s, est. speed input: 39357.14 toks/s, output: 38.43 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:07<00:05, 35.93it/s, est. speed input: 39321.60 toks/s, output: 38.40 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:08<00:05, 35.94it/s, est. speed input: 39287.26 toks/s, output: 38.37 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:08<00:05, 35.94it/s, est. speed input: 39253.52 toks/s, output: 38.33 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:08<00:05, 35.91it/s, est. speed input: 39219.24 toks/s, output: 38.30 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:08<00:05, 35.91it/s, est. speed input: 39186.73 toks/s, output: 38.27 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:08<00:05, 35.87it/s, est. speed input: 39153.53 toks/s, output: 38.24 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:08<00:05, 35.91it/s, est. speed input: 39124.23 toks/s, output: 38.21 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:08<00:04, 35.92it/s, est. speed input: 39094.76 toks/s, output: 38.18 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:08<00:04, 35.94it/s, est. speed input: 39066.44 toks/s, output: 38.15 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:08<00:04, 35.93it/s, est. speed input: 39038.07 toks/s, output: 38.12 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:09<00:04, 35.95it/s, est. speed input: 39011.35 toks/s, output: 38.10 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:09<00:04, 35.97it/s, est. speed input: 38985.74 toks/s, output: 38.07 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:09<00:04, 35.91it/s, est. speed input: 38957.39 toks/s, output: 38.04 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:09<00:04, 35.92it/s, est. speed input: 38931.95 toks/s, output: 38.02 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:09<00:04, 35.91it/s, est. speed input: 38906.20 toks/s, output: 37.99 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:09<00:04, 35.86it/s, est. speed input: 38879.66 toks/s, output: 37.97 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:09<00:03, 35.88it/s, est. speed input: 38855.55 toks/s, output: 37.94 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:09<00:03, 35.90it/s, est. speed input: 38832.59 toks/s, output: 37.92 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:09<00:03, 35.91it/s, est. speed input: 38809.77 toks/s, output: 37.90 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:10<00:03, 35.92it/s, est. speed input: 38787.65 toks/s, output: 37.88 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:10<00:03, 35.94it/s, est. speed input: 38766.50 toks/s, output: 37.86 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:10<00:03, 35.91it/s, est. speed input: 38744.07 toks/s, output: 37.84 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:10<00:03, 35.89it/s, est. speed input: 38722.30 toks/s, output: 37.81 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:10<00:03, 35.85it/s, est. speed input: 38700.12 toks/s, output: 37.79 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:10<00:03, 35.88it/s, est. speed input: 38680.44 toks/s, output: 37.77 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:10<00:02, 35.90it/s, est. speed input: 38660.77 toks/s, output: 37.75 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:10<00:02, 35.93it/s, est. speed input: 38642.52 toks/s, output: 37.74 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:10<00:02, 35.89it/s, est. speed input: 38622.51 toks/s, output: 37.72 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:11<00:02, 35.89it/s, est. speed input: 38603.38 toks/s, output: 37.70 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:11<00:02, 35.92it/s, est. speed input: 38586.01 toks/s, output: 37.68 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:11<00:02, 35.90it/s, est. speed input: 38567.66 toks/s, output: 37.66 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:11<00:02, 35.88it/s, est. speed input: 38549.21 toks/s, output: 37.65 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:11<00:02, 35.93it/s, est. speed input: 38533.49 toks/s, output: 37.63 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:11<00:02, 35.91it/s, est. speed input: 38516.15 toks/s, output: 37.61 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:11<00:01, 35.88it/s, est. speed input: 38498.62 toks/s, output: 37.60 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:11<00:01, 35.92it/s, est. speed input: 38483.29 toks/s, output: 37.58 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:11<00:01, 35.95it/s, est. speed input: 38468.46 toks/s, output: 37.57 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:12<00:01, 35.97it/s, est. speed input: 38453.96 toks/s, output: 37.55 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:12<00:01, 35.97it/s, est. speed input: 38439.32 toks/s, output: 37.54 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:12<00:01, 35.94it/s, est. speed input: 38423.77 toks/s, output: 37.52 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:12<00:01, 35.90it/s, est. speed input: 38407.80 toks/s, output: 37.51 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:12<00:01, 35.87it/s, est. speed input: 38392.22 toks/s, output: 37.49 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:12<00:01, 35.83it/s, est. speed input: 38376.37 toks/s, output: 37.48 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:12<00:00, 35.82it/s, est. speed input: 38361.29 toks/s, output: 37.46 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:12<00:00, 35.82it/s, est. speed input: 38346.64 toks/s, output: 37.45 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:12<00:00, 35.80it/s, est. speed input: 38331.86 toks/s, output: 37.43 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:13<00:00, 35.79it/s, est. speed input: 38317.30 toks/s, output: 37.42 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:13<00:00, 35.79it/s, est. speed input: 38303.16 toks/s, output: 37.41 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:13<00:00, 35.76it/s, est. speed input: 38288.32 toks/s, output: 37.39 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:13<00:00, 35.74it/s, est. speed input: 38273.88 toks/s, output: 37.38 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:13<00:00, 35.74it/s, est. speed input: 38259.93 toks/s, output: 37.36 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:13<00:00, 35.74it/s, est. speed input: 38478.02 toks/s, output: 37.58 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:13<00:00, 37.58it/s, est. speed input: 38478.02 toks/s, output: 37.58 toks/s]
[rank0]:[W128 00:39:27.631814688 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 37.4s

测试结果:
  Requests/s:   35.76
  Tokens/s:     36654.52
  Total Reqs:   512
  Elapsed:      14.32s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     36618.76

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:39:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3495210) WARNING 01-28 00:39:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.46 requests/s, 35324.99 total tokens/s, 34.46 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 00:39:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:39:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:39:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:39:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:39:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:39:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:39:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:39:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:39:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:39:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:39:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:39:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:39:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:39:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:39:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:39:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:39:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:39:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:39:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3495210) [2026-01-28 00:39:40] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3495210) [2026-01-28 00:39:40] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3495210) [2026-01-28 00:39:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3495210) [2026-01-28 00:39:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3495210) [2026-01-28 00:39:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3495210) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3495210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.52it/s]
(EngineCore_DP0 pid=3495210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.52it/s]
(EngineCore_DP0 pid=3495210) 
(EngineCore_DP0 pid=3495210) 2026-01-28 00:39:50,894 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3495210) 2026-01-28 00:39:50,912 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3495210) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  6.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 12.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 12.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 11.85it/s]
(EngineCore_DP0 pid=3495210) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.74it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 73/1024 [00:00<00:01, 722.85it/s]
Adding requests:  15%|█▍        | 149/1024 [00:00<00:01, 743.66it/s]
Adding requests:  22%|██▏       | 227/1024 [00:00<00:01, 757.19it/s]
Adding requests:  30%|██▉       | 303/1024 [00:00<00:00, 755.26it/s]
Adding requests:  37%|███▋      | 380/1024 [00:00<00:00, 756.94it/s]
Adding requests:  45%|████▍     | 457/1024 [00:00<00:00, 759.68it/s]
Adding requests:  52%|█████▏    | 533/1024 [00:00<00:00, 737.57it/s]
Adding requests:  59%|█████▉    | 607/1024 [00:00<00:00, 737.81it/s]
Adding requests:  67%|██████▋   | 686/1024 [00:00<00:00, 752.16it/s]
Adding requests:  75%|███████▍  | 763/1024 [00:01<00:00, 754.56it/s]
Adding requests:  82%|████████▏ | 839/1024 [00:01<00:00, 740.43it/s]
Adding requests:  90%|████████▉ | 917/1024 [00:01<00:00, 749.85it/s]
Adding requests:  97%|█████████▋| 994/1024 [00:01<00:00, 754.79it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 750.19it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 42/1024 [00:00<00:02, 396.07it/s, est. speed input: 405616.54 toks/s, output: 396.08 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:01<00:16, 56.41it/s, est. speed input: 66528.56 toks/s, output: 64.97 toks/s]   
Processed prompts:  10%|▉         | 101/1024 [00:01<00:18, 51.03it/s, est. speed input: 59946.57 toks/s, output: 58.54 toks/s]
Processed prompts:  11%|█         | 113/1024 [00:01<00:17, 51.19it/s, est. speed input: 59135.15 toks/s, output: 57.75 toks/s]
Processed prompts:  12%|█▏        | 123/1024 [00:02<00:22, 40.46it/s, est. speed input: 52070.25 toks/s, output: 50.85 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:02<00:23, 38.35it/s, est. speed input: 50224.30 toks/s, output: 49.05 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:02<00:23, 37.50it/s, est. speed input: 49036.79 toks/s, output: 47.89 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:03<00:23, 36.77it/s, est. speed input: 48020.62 toks/s, output: 46.89 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:03<00:24, 36.20it/s, est. speed input: 47147.01 toks/s, output: 46.04 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:03<00:24, 35.75it/s, est. speed input: 46385.10 toks/s, output: 45.30 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:03<00:24, 35.42it/s, est. speed input: 45716.16 toks/s, output: 44.64 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:04<00:24, 35.16it/s, est. speed input: 45121.74 toks/s, output: 44.06 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:04<00:23, 35.00it/s, est. speed input: 44596.55 toks/s, output: 43.55 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:04<00:23, 34.87it/s, est. speed input: 44123.33 toks/s, output: 43.09 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:04<00:23, 34.78it/s, est. speed input: 43696.65 toks/s, output: 42.67 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:04<00:23, 34.73it/s, est. speed input: 43311.92 toks/s, output: 42.30 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:05<00:23, 34.68it/s, est. speed input: 42959.96 toks/s, output: 41.95 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:05<00:23, 34.67it/s, est. speed input: 42640.31 toks/s, output: 41.64 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:05<00:22, 34.65it/s, est. speed input: 42345.72 toks/s, output: 41.35 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:05<00:22, 34.62it/s, est. speed input: 42072.36 toks/s, output: 41.09 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:06<00:22, 34.61it/s, est. speed input: 41821.37 toks/s, output: 40.84 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:06<00:22, 34.61it/s, est. speed input: 41588.84 toks/s, output: 40.61 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:06<00:21, 34.58it/s, est. speed input: 41368.75 toks/s, output: 40.40 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:06<00:21, 34.58it/s, est. speed input: 41166.49 toks/s, output: 40.20 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:07<00:21, 34.58it/s, est. speed input: 40977.43 toks/s, output: 40.02 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:07<00:21, 34.59it/s, est. speed input: 40801.35 toks/s, output: 39.84 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:07<00:21, 34.57it/s, est. speed input: 40633.44 toks/s, output: 39.68 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:07<00:20, 34.58it/s, est. speed input: 40478.01 toks/s, output: 39.53 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:07<00:20, 34.57it/s, est. speed input: 40329.81 toks/s, output: 39.38 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:08<00:20, 34.56it/s, est. speed input: 40189.90 toks/s, output: 39.25 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:08<00:20, 34.57it/s, est. speed input: 40058.98 toks/s, output: 39.12 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:08<00:19, 34.56it/s, est. speed input: 39934.12 toks/s, output: 39.00 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:08<00:19, 34.56it/s, est. speed input: 39815.95 toks/s, output: 38.88 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:09<00:19, 34.58it/s, est. speed input: 39705.04 toks/s, output: 38.77 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:09<00:19, 34.58it/s, est. speed input: 39599.05 toks/s, output: 38.67 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:09<00:18, 34.56it/s, est. speed input: 39496.69 toks/s, output: 38.57 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:09<00:18, 34.56it/s, est. speed input: 39399.98 toks/s, output: 38.48 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:10<00:18, 34.56it/s, est. speed input: 39307.41 toks/s, output: 38.39 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:10<00:18, 34.57it/s, est. speed input: 39220.18 toks/s, output: 38.30 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:10<00:17, 34.57it/s, est. speed input: 39136.34 toks/s, output: 38.22 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:10<00:17, 34.57it/s, est. speed input: 39055.92 toks/s, output: 38.14 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:10<00:17, 34.58it/s, est. speed input: 38979.44 toks/s, output: 38.07 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:11<00:17, 34.59it/s, est. speed input: 38906.83 toks/s, output: 37.99 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:11<00:17, 34.59it/s, est. speed input: 38835.88 toks/s, output: 37.93 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:11<00:16, 34.58it/s, est. speed input: 38768.00 toks/s, output: 37.86 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:11<00:16, 34.59it/s, est. speed input: 38703.02 toks/s, output: 37.80 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:12<00:16, 34.57it/s, est. speed input: 38639.51 toks/s, output: 37.73 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:12<00:16, 34.58it/s, est. speed input: 38579.62 toks/s, output: 37.68 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:12<00:15, 34.57it/s, est. speed input: 38520.75 toks/s, output: 37.62 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:12<00:15, 34.57it/s, est. speed input: 38464.58 toks/s, output: 37.56 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:13<00:15, 34.56it/s, est. speed input: 38409.58 toks/s, output: 37.51 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:13<00:15, 34.57it/s, est. speed input: 38357.88 toks/s, output: 37.46 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:13<00:14, 34.56it/s, est. speed input: 38306.31 toks/s, output: 37.41 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:13<00:14, 34.57it/s, est. speed input: 38258.05 toks/s, output: 37.36 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:13<00:14, 34.57it/s, est. speed input: 38210.87 toks/s, output: 37.32 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:14<00:14, 34.55it/s, est. speed input: 38163.84 toks/s, output: 37.27 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:14<00:14, 34.54it/s, est. speed input: 38118.62 toks/s, output: 37.23 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:14<00:13, 34.54it/s, est. speed input: 38075.21 toks/s, output: 37.18 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:14<00:13, 34.55it/s, est. speed input: 38033.98 toks/s, output: 37.14 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:15<00:13, 34.55it/s, est. speed input: 37993.47 toks/s, output: 37.10 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:15<00:13, 34.55it/s, est. speed input: 37954.18 toks/s, output: 37.06 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:15<00:12, 34.55it/s, est. speed input: 37915.59 toks/s, output: 37.03 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:15<00:12, 34.56it/s, est. speed input: 37878.97 toks/s, output: 36.99 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:16<00:12, 34.53it/s, est. speed input: 37841.79 toks/s, output: 36.95 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:16<00:12, 34.52it/s, est. speed input: 37805.76 toks/s, output: 36.92 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:16<00:12, 34.49it/s, est. speed input: 37770.06 toks/s, output: 36.88 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:16<00:11, 34.49it/s, est. speed input: 37735.93 toks/s, output: 36.85 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:17<00:11, 34.51it/s, est. speed input: 37703.86 toks/s, output: 36.82 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:17<00:11, 34.52it/s, est. speed input: 37672.43 toks/s, output: 36.79 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:17<00:11, 34.50it/s, est. speed input: 37640.93 toks/s, output: 36.76 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:17<00:10, 34.50it/s, est. speed input: 37610.50 toks/s, output: 36.73 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:17<00:10, 34.50it/s, est. speed input: 37580.84 toks/s, output: 36.70 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:18<00:10, 34.49it/s, est. speed input: 37551.69 toks/s, output: 36.67 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:18<00:10, 34.48it/s, est. speed input: 37522.88 toks/s, output: 36.64 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:18<00:09, 34.46it/s, est. speed input: 37494.40 toks/s, output: 36.62 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:18<00:09, 34.48it/s, est. speed input: 37468.01 toks/s, output: 36.59 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:19<00:09, 34.47it/s, est. speed input: 37441.64 toks/s, output: 36.56 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:19<00:09, 34.47it/s, est. speed input: 37415.77 toks/s, output: 36.54 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:19<00:09, 34.44it/s, est. speed input: 37389.50 toks/s, output: 36.51 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:19<00:08, 34.46it/s, est. speed input: 37365.19 toks/s, output: 36.49 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:20<00:08, 34.47it/s, est. speed input: 37341.71 toks/s, output: 36.47 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:20<00:08, 34.48it/s, est. speed input: 37318.60 toks/s, output: 36.44 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:20<00:08, 34.45it/s, est. speed input: 37294.48 toks/s, output: 36.42 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:20<00:07, 34.46it/s, est. speed input: 37272.22 toks/s, output: 36.40 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:20<00:07, 34.47it/s, est. speed input: 37250.51 toks/s, output: 36.38 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:21<00:07, 34.47it/s, est. speed input: 37229.08 toks/s, output: 36.36 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:21<00:07, 34.45it/s, est. speed input: 37207.52 toks/s, output: 36.34 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:21<00:06, 34.44it/s, est. speed input: 37186.38 toks/s, output: 36.31 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:21<00:06, 34.44it/s, est. speed input: 37166.08 toks/s, output: 36.29 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:22<00:06, 34.44it/s, est. speed input: 37146.24 toks/s, output: 36.28 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:22<00:06, 34.44it/s, est. speed input: 37126.73 toks/s, output: 36.26 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:22<00:05, 34.43it/s, est. speed input: 37107.08 toks/s, output: 36.24 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:22<00:05, 34.45it/s, est. speed input: 37089.11 toks/s, output: 36.22 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:23<00:05, 34.46it/s, est. speed input: 37071.13 toks/s, output: 36.20 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:23<00:05, 34.46it/s, est. speed input: 37053.33 toks/s, output: 36.18 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:23<00:05, 34.46it/s, est. speed input: 37035.86 toks/s, output: 36.17 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:23<00:04, 34.46it/s, est. speed input: 37018.64 toks/s, output: 36.15 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:23<00:04, 34.46it/s, est. speed input: 37001.81 toks/s, output: 36.13 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:24<00:04, 34.46it/s, est. speed input: 36985.29 toks/s, output: 36.12 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:24<00:04, 34.46it/s, est. speed input: 36969.37 toks/s, output: 36.10 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:24<00:03, 34.44it/s, est. speed input: 36952.81 toks/s, output: 36.09 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:24<00:03, 34.45it/s, est. speed input: 36937.26 toks/s, output: 36.07 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:25<00:03, 34.45it/s, est. speed input: 36922.13 toks/s, output: 36.06 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:25<00:03, 34.47it/s, est. speed input: 36907.71 toks/s, output: 36.04 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:25<00:02, 34.46it/s, est. speed input: 36892.78 toks/s, output: 36.03 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:25<00:02, 34.46it/s, est. speed input: 36878.16 toks/s, output: 36.01 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:26<00:02, 34.46it/s, est. speed input: 36864.07 toks/s, output: 36.00 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:26<00:02, 34.47it/s, est. speed input: 36850.52 toks/s, output: 35.99 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:26<00:02, 34.46it/s, est. speed input: 36836.56 toks/s, output: 35.97 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:26<00:01, 34.45it/s, est. speed input: 36822.75 toks/s, output: 35.96 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:26<00:01, 34.45it/s, est. speed input: 36809.64 toks/s, output: 35.95 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:27<00:01, 34.45it/s, est. speed input: 36796.49 toks/s, output: 35.93 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:27<00:01, 34.46it/s, est. speed input: 36783.78 toks/s, output: 35.92 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:27<00:00, 34.46it/s, est. speed input: 36771.20 toks/s, output: 35.91 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:27<00:00, 34.44it/s, est. speed input: 36758.48 toks/s, output: 35.90 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:28<00:00, 34.45it/s, est. speed input: 36746.37 toks/s, output: 35.89 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:28<00:00, 35.92it/s, est. speed input: 36775.38 toks/s, output: 35.91 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:28<00:00, 35.92it/s, est. speed input: 36991.85 toks/s, output: 36.12 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:28<00:00, 36.12it/s, est. speed input: 36991.85 toks/s, output: 36.12 toks/s]
[rank0]:[W128 00:40:22.208105505 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.6s

测试结果:
  Requests/s:   34.46
  Tokens/s:     35324.99
  Total Reqs:   1024
  Elapsed:      29.71s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     35290.52

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:40:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3496292) WARNING 01-28 00:40:42 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.23 requests/s, 35088.09 total tokens/s, 34.23 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 00:40:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:40:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:40:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:40:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:40:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:40:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:40:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:40:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:40:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:40:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:40:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:40:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:40:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:40:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:40:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:40:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:40:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:40:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:40:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:40:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:40:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:40:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:40:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:40:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:40:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:40:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:40:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:40:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3496292) [2026-01-28 00:40:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3496292) [2026-01-28 00:40:37] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3496292) [2026-01-28 00:40:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3496292) [2026-01-28 00:40:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3496292) [2026-01-28 00:40:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3496292) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3496292) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=3496292) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=3496292) 
(EngineCore_DP0 pid=3496292) 2026-01-28 00:40:48,458 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3496292) 2026-01-28 00:40:48,489 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3496292) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  4.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 13.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 17.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.69it/s]
(EngineCore_DP0 pid=3496292) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.19it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 17.76it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 68/2048 [00:00<00:02, 678.21it/s]
Adding requests:   7%|▋         | 143/2048 [00:00<00:02, 719.56it/s]
Adding requests:  11%|█         | 218/2048 [00:00<00:02, 731.71it/s]
Adding requests:  14%|█▍        | 296/2048 [00:00<00:02, 748.82it/s]
Adding requests:  18%|█▊        | 372/2048 [00:00<00:02, 751.29it/s]
Adding requests:  22%|██▏       | 449/2048 [00:00<00:02, 754.39it/s]
Adding requests:  26%|██▌       | 525/2048 [00:00<00:02, 736.64it/s]
Adding requests:  29%|██▉       | 600/2048 [00:00<00:01, 736.45it/s]
Adding requests:  33%|███▎      | 678/2048 [00:00<00:01, 747.98it/s]
Adding requests:  37%|███▋      | 753/2048 [00:01<00:01, 742.25it/s]
Adding requests:  40%|████      | 828/2048 [00:01<00:01, 727.65it/s]
Adding requests:  44%|████▍     | 905/2048 [00:01<00:01, 738.17it/s]
Adding requests:  48%|████▊     | 979/2048 [00:01<00:01, 735.49it/s]
Adding requests:  52%|█████▏    | 1055/2048 [00:01<00:01, 742.46it/s]
Adding requests:  55%|█████▌    | 1130/2048 [00:01<00:01, 744.03it/s]
Adding requests:  59%|█████▉    | 1209/2048 [00:01<00:01, 755.84it/s]
Adding requests:  63%|██████▎   | 1285/2048 [00:01<00:01, 746.29it/s]
Adding requests:  66%|██████▋   | 1361/2048 [00:01<00:00, 749.05it/s]
Adding requests:  70%|███████   | 1438/2048 [00:01<00:00, 753.25it/s]
Adding requests:  74%|███████▍  | 1517/2048 [00:02<00:00, 762.52it/s]
Adding requests:  78%|███████▊  | 1594/2048 [00:02<00:00, 758.78it/s]
Adding requests:  82%|████████▏ | 1670/2048 [00:02<00:00, 751.98it/s]
Adding requests:  85%|████████▌ | 1747/2048 [00:02<00:00, 757.03it/s]
Adding requests:  89%|████████▉ | 1825/2048 [00:02<00:00, 763.53it/s]
Adding requests:  93%|█████████▎| 1903/2048 [00:02<00:00, 766.09it/s]
Adding requests:  97%|█████████▋| 1980/2048 [00:02<00:00, 746.40it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 747.48it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 82/2048 [00:00<00:03, 569.31it/s, est. speed input: 583024.98 toks/s, output: 569.33 toks/s]
Processed prompts:   7%|▋         | 139/2048 [00:01<00:25, 76.32it/s, est. speed input: 92302.11 toks/s, output: 90.14 toks/s]  
Processed prompts:   8%|▊         | 165/2048 [00:02<00:35, 53.55it/s, est. speed input: 68295.01 toks/s, output: 66.69 toks/s]
Processed prompts:   9%|▉         | 181/2048 [00:02<00:38, 48.72it/s, est. speed input: 63044.34 toks/s, output: 61.57 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:03<00:42, 43.22it/s, est. speed input: 58333.57 toks/s, output: 56.97 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:03<00:45, 40.78it/s, est. speed input: 55540.53 toks/s, output: 54.24 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:04<00:46, 38.98it/s, est. speed input: 53356.16 toks/s, output: 52.11 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:04<00:47, 37.64it/s, est. speed input: 51586.23 toks/s, output: 50.38 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:05<00:48, 36.67it/s, est. speed input: 50130.27 toks/s, output: 48.96 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:05<00:49, 35.97it/s, est. speed input: 48910.61 toks/s, output: 47.76 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:06<00:49, 35.48it/s, est. speed input: 47875.33 toks/s, output: 46.75 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:06<00:49, 35.14it/s, est. speed input: 46985.50 toks/s, output: 45.88 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:07<00:49, 34.89it/s, est. speed input: 46211.53 toks/s, output: 45.13 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:07<00:49, 34.71it/s, est. speed input: 45529.62 toks/s, output: 44.46 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:08<00:48, 34.59it/s, est. speed input: 44929.40 toks/s, output: 43.88 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:08<00:48, 34.50it/s, est. speed input: 44391.56 toks/s, output: 43.35 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:09<00:48, 34.44it/s, est. speed input: 43912.45 toks/s, output: 42.88 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:09<00:47, 34.39it/s, est. speed input: 43477.83 toks/s, output: 42.46 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:09<00:47, 34.36it/s, est. speed input: 43084.80 toks/s, output: 42.07 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:10<00:47, 34.34it/s, est. speed input: 42726.94 toks/s, output: 41.73 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:10<00:46, 34.31it/s, est. speed input: 42398.01 toks/s, output: 41.40 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:11<00:46, 34.29it/s, est. speed input: 42095.01 toks/s, output: 41.11 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:11<00:45, 34.28it/s, est. speed input: 41817.61 toks/s, output: 40.84 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:12<00:45, 34.27it/s, est. speed input: 41561.02 toks/s, output: 40.59 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:12<00:44, 34.26it/s, est. speed input: 41321.61 toks/s, output: 40.35 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:13<00:44, 34.26it/s, est. speed input: 41101.58 toks/s, output: 40.14 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:13<00:43, 34.25it/s, est. speed input: 40894.62 toks/s, output: 39.94 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:14<00:43, 34.25it/s, est. speed input: 40701.85 toks/s, output: 39.75 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:14<00:42, 34.25it/s, est. speed input: 40522.27 toks/s, output: 39.57 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:15<00:42, 34.24it/s, est. speed input: 40351.74 toks/s, output: 39.41 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:15<00:41, 34.24it/s, est. speed input: 40192.58 toks/s, output: 39.25 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:16<00:41, 34.24it/s, est. speed input: 40043.53 toks/s, output: 39.10 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:16<00:41, 34.24it/s, est. speed input: 39901.34 toks/s, output: 38.97 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:16<00:40, 34.24it/s, est. speed input: 39767.83 toks/s, output: 38.84 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:17<00:40, 34.23it/s, est. speed input: 39640.95 toks/s, output: 38.71 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:17<00:39, 34.23it/s, est. speed input: 39520.70 toks/s, output: 38.59 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:18<00:39, 34.23it/s, est. speed input: 39407.10 toks/s, output: 38.48 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:18<00:38, 34.22it/s, est. speed input: 39298.19 toks/s, output: 38.38 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:19<00:38, 34.22it/s, est. speed input: 39194.79 toks/s, output: 38.28 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:19<00:37, 34.21it/s, est. speed input: 39095.51 toks/s, output: 38.18 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:20<00:37, 34.22it/s, est. speed input: 39002.08 toks/s, output: 38.09 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:20<00:36, 34.21it/s, est. speed input: 38912.04 toks/s, output: 38.00 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:21<00:36, 34.21it/s, est. speed input: 38826.52 toks/s, output: 37.92 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:21<00:35, 34.21it/s, est. speed input: 38744.08 toks/s, output: 37.84 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:22<00:35, 34.21it/s, est. speed input: 38665.81 toks/s, output: 37.76 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:22<00:35, 34.21it/s, est. speed input: 38589.97 toks/s, output: 37.69 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:23<00:34, 34.21it/s, est. speed input: 38517.91 toks/s, output: 37.62 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:23<00:34, 34.20it/s, est. speed input: 38447.72 toks/s, output: 37.55 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:23<00:33, 34.21it/s, est. speed input: 38381.57 toks/s, output: 37.48 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:24<00:33, 34.21it/s, est. speed input: 38317.17 toks/s, output: 37.42 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:24<00:32, 34.21it/s, est. speed input: 38255.68 toks/s, output: 37.36 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:25<00:32, 34.21it/s, est. speed input: 38196.34 toks/s, output: 37.30 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:25<00:31, 34.21it/s, est. speed input: 38138.80 toks/s, output: 37.24 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:26<00:31, 34.21it/s, est. speed input: 38083.29 toks/s, output: 37.19 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:26<00:30, 34.20it/s, est. speed input: 38029.45 toks/s, output: 37.14 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:27<00:30, 34.21it/s, est. speed input: 37978.32 toks/s, output: 37.09 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:27<00:29, 34.21it/s, est. speed input: 37928.34 toks/s, output: 37.04 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:28<00:29, 34.21it/s, est. speed input: 37880.20 toks/s, output: 36.99 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:28<00:28, 34.21it/s, est. speed input: 37833.93 toks/s, output: 36.95 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:29<00:28, 34.21it/s, est. speed input: 37788.95 toks/s, output: 36.90 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:29<00:28, 34.21it/s, est. speed input: 37745.25 toks/s, output: 36.86 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:30<00:27, 34.21it/s, est. speed input: 37703.09 toks/s, output: 36.82 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:30<00:27, 34.21it/s, est. speed input: 37661.86 toks/s, output: 36.78 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:30<00:26, 34.21it/s, est. speed input: 37622.46 toks/s, output: 36.74 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:31<00:25, 34.83it/s, est. speed input: 37617.10 toks/s, output: 36.74 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:31<00:25, 34.64it/s, est. speed input: 37579.16 toks/s, output: 36.70 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:32<00:24, 34.52it/s, est. speed input: 37542.67 toks/s, output: 36.66 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:32<00:24, 34.42it/s, est. speed input: 37506.82 toks/s, output: 36.63 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:33<00:24, 34.36it/s, est. speed input: 37472.10 toks/s, output: 36.59 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:33<00:23, 34.31it/s, est. speed input: 37438.14 toks/s, output: 36.56 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:34<00:23, 34.28it/s, est. speed input: 37405.19 toks/s, output: 36.53 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:34<00:22, 34.25it/s, est. speed input: 37372.82 toks/s, output: 36.50 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:35<00:22, 34.24it/s, est. speed input: 37341.72 toks/s, output: 36.47 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:35<00:21, 34.23it/s, est. speed input: 37311.47 toks/s, output: 36.44 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:36<00:21, 34.23it/s, est. speed input: 37281.91 toks/s, output: 36.41 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:36<00:20, 34.22it/s, est. speed input: 37252.85 toks/s, output: 36.38 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:37<00:20, 34.22it/s, est. speed input: 37224.94 toks/s, output: 36.35 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:37<00:20, 34.21it/s, est. speed input: 37197.33 toks/s, output: 36.33 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:37<00:19, 34.21it/s, est. speed input: 37170.66 toks/s, output: 36.30 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:38<00:19, 34.20it/s, est. speed input: 37144.30 toks/s, output: 36.27 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:38<00:18, 34.21it/s, est. speed input: 37119.12 toks/s, output: 36.25 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:39<00:18, 34.22it/s, est. speed input: 37094.52 toks/s, output: 36.23 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:39<00:17, 34.21it/s, est. speed input: 37070.17 toks/s, output: 36.20 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:40<00:17, 34.21it/s, est. speed input: 37046.58 toks/s, output: 36.18 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:40<00:16, 34.21it/s, est. speed input: 37023.31 toks/s, output: 36.16 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:41<00:16, 34.20it/s, est. speed input: 37000.37 toks/s, output: 36.13 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:41<00:15, 34.19it/s, est. speed input: 36977.73 toks/s, output: 36.11 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:42<00:15, 34.19it/s, est. speed input: 36956.00 toks/s, output: 36.09 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:42<00:14, 34.19it/s, est. speed input: 36934.59 toks/s, output: 36.07 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:43<00:14, 34.20it/s, est. speed input: 36913.91 toks/s, output: 36.05 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:43<00:13, 34.19it/s, est. speed input: 36893.31 toks/s, output: 36.03 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:44<00:13, 34.19it/s, est. speed input: 36873.36 toks/s, output: 36.01 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:44<00:13, 34.19it/s, est. speed input: 36853.65 toks/s, output: 35.99 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:44<00:12, 34.19it/s, est. speed input: 36834.62 toks/s, output: 35.97 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:45<00:12, 34.19it/s, est. speed input: 36815.79 toks/s, output: 35.95 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:45<00:11, 34.19it/s, est. speed input: 36797.43 toks/s, output: 35.93 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:46<00:11, 34.20it/s, est. speed input: 36779.80 toks/s, output: 35.92 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:46<00:10, 34.19it/s, est. speed input: 36761.97 toks/s, output: 35.90 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:47<00:10, 34.21it/s, est. speed input: 36745.18 toks/s, output: 35.88 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:47<00:09, 34.20it/s, est. speed input: 36728.03 toks/s, output: 35.87 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:48<00:09, 34.20it/s, est. speed input: 36711.51 toks/s, output: 35.85 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:48<00:08, 34.20it/s, est. speed input: 36695.29 toks/s, output: 35.84 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:49<00:08, 34.20it/s, est. speed input: 36679.48 toks/s, output: 35.82 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:49<00:07, 34.20it/s, est. speed input: 36663.79 toks/s, output: 35.80 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:50<00:07, 34.20it/s, est. speed input: 36648.50 toks/s, output: 35.79 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:50<00:06, 34.20it/s, est. speed input: 36633.27 toks/s, output: 35.77 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:51<00:06, 34.20it/s, est. speed input: 36618.61 toks/s, output: 35.76 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:51<00:06, 34.20it/s, est. speed input: 36603.95 toks/s, output: 35.75 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:51<00:05, 34.20it/s, est. speed input: 36589.76 toks/s, output: 35.73 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:52<00:04, 34.82it/s, est. speed input: 36595.10 toks/s, output: 35.74 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:52<00:04, 34.63it/s, est. speed input: 36581.23 toks/s, output: 35.72 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:53<00:04, 34.51it/s, est. speed input: 36567.79 toks/s, output: 35.71 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:53<00:03, 34.41it/s, est. speed input: 36554.21 toks/s, output: 35.70 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:54<00:03, 34.35it/s, est. speed input: 36541.12 toks/s, output: 35.68 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:54<00:02, 34.30it/s, est. speed input: 36528.04 toks/s, output: 35.67 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:55<00:02, 34.28it/s, est. speed input: 36515.42 toks/s, output: 35.66 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:55<00:01, 34.24it/s, est. speed input: 36502.57 toks/s, output: 35.65 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:56<00:01, 34.23it/s, est. speed input: 36490.26 toks/s, output: 35.64 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:56<00:00, 34.22it/s, est. speed input: 36478.17 toks/s, output: 35.62 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:57<00:00, 34.92it/s, est. speed input: 36486.17 toks/s, output: 35.63 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:57<00:00, 34.92it/s, est. speed input: 36737.09 toks/s, output: 35.88 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:57<00:00, 35.88it/s, est. speed input: 36737.09 toks/s, output: 35.88 toks/s]
[rank0]:[W128 00:41:50.995097494 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 87.8s

测试结果:
  Requests/s:   34.23
  Tokens/s:     35088.09
  Total Reqs:   2048
  Elapsed:      59.83s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     35053.86

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:42:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3497840) WARNING 01-28 00:42:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.18 requests/s, 35038.21 total tokens/s, 34.18 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 00:42:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:42:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:42:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:42:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:42:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:42:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:42:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:42:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:42:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:42:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:42:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:42:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:42:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:42:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:42:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:42:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:42:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:42:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:42:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:42:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:42:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:42:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:42:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:42:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:42:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:42:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:42:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:42:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3497840) [2026-01-28 00:42:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3497840) [2026-01-28 00:42:11] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3497840) [2026-01-28 00:42:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3497840) [2026-01-28 00:42:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3497840) [2026-01-28 00:42:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3497840) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3497840) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=3497840) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=3497840) 
(EngineCore_DP0 pid=3497840) [rank0]:W0128 00:42:18.296000 3497840 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3497840) [rank0]:W0128 00:42:18.345000 3497840 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3497840) [rank0]:W0128 00:42:18.963000 3497840 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3497840) [rank0]:W0128 00:42:19.038000 3497840 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3497840) 2026-01-28 00:42:21,802 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3497840) 2026-01-28 00:42:21,820 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3497840) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:06,  1.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:01,  6.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 10.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00, 14.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 10.16it/s]
(EngineCore_DP0 pid=3497840) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.18it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 21.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 19.42it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:05, 711.57it/s]
Adding requests:   4%|▎         | 144/4096 [00:00<00:05, 685.53it/s]
Adding requests:   5%|▌         | 213/4096 [00:00<00:05, 674.17it/s]
Adding requests:   7%|▋         | 288/4096 [00:00<00:05, 702.11it/s]
Adding requests:   9%|▉         | 361/4096 [00:00<00:05, 710.49it/s]
Adding requests:  11%|█         | 439/4096 [00:00<00:05, 731.21it/s]
Adding requests:  13%|█▎        | 516/4096 [00:00<00:04, 742.78it/s]
Adding requests:  14%|█▍        | 593/4096 [00:00<00:04, 750.23it/s]
Adding requests:  16%|█▋        | 670/4096 [00:00<00:04, 754.17it/s]
Adding requests:  18%|█▊        | 747/4096 [00:01<00:04, 758.68it/s]
Adding requests:  20%|██        | 823/4096 [00:01<00:04, 747.49it/s]
Adding requests:  22%|██▏       | 901/4096 [00:01<00:04, 754.97it/s]
Adding requests:  24%|██▍       | 979/4096 [00:01<00:04, 760.78it/s]
Adding requests:  26%|██▌       | 1056/4096 [00:01<00:04, 750.59it/s]
Adding requests:  28%|██▊       | 1132/4096 [00:01<00:03, 751.66it/s]
Adding requests:  30%|██▉       | 1211/4096 [00:01<00:03, 760.96it/s]
Adding requests:  31%|███▏      | 1288/4096 [00:01<00:03, 757.98it/s]
Adding requests:  33%|███▎      | 1367/4096 [00:01<00:03, 766.70it/s]
Adding requests:  35%|███▌      | 1444/4096 [00:01<00:03, 764.92it/s]
Adding requests:  37%|███▋      | 1523/4096 [00:02<00:03, 769.85it/s]
Adding requests:  39%|███▉      | 1603/4096 [00:02<00:03, 775.95it/s]
Adding requests:  41%|████      | 1681/4096 [00:02<00:03, 773.66it/s]
Adding requests:  43%|████▎     | 1760/4096 [00:02<00:03, 777.10it/s]
Adding requests:  45%|████▍     | 1839/4096 [00:02<00:02, 780.73it/s]
Adding requests:  47%|████▋     | 1918/4096 [00:02<00:02, 764.91it/s]
Adding requests:  49%|████▊     | 1995/4096 [00:02<00:02, 763.46it/s]
Adding requests:  51%|█████     | 2072/4096 [00:02<00:02, 763.23it/s]
Adding requests:  52%|█████▏    | 2149/4096 [00:02<00:02, 757.25it/s]
Adding requests:  54%|█████▍    | 2225/4096 [00:02<00:02, 741.50it/s]
Adding requests:  56%|█████▌    | 2302/4096 [00:03<00:02, 749.16it/s]
Adding requests:  58%|█████▊    | 2378/4096 [00:03<00:02, 751.65it/s]
Adding requests:  60%|█████▉    | 2454/4096 [00:03<00:02, 747.64it/s]
Adding requests:  62%|██████▏   | 2531/4096 [00:03<00:02, 752.70it/s]
Adding requests:  64%|██████▎   | 2610/4096 [00:03<00:01, 761.54it/s]
Adding requests:  66%|██████▌   | 2689/4096 [00:03<00:01, 768.17it/s]
Adding requests:  68%|██████▊   | 2768/4096 [00:03<00:01, 773.09it/s]
Adding requests:  69%|██████▉   | 2846/4096 [00:03<00:01, 771.60it/s]
Adding requests:  71%|███████▏  | 2924/4096 [00:03<00:01, 768.04it/s]
Adding requests:  73%|███████▎  | 3001/4096 [00:03<00:01, 764.83it/s]
Adding requests:  75%|███████▌  | 3078/4096 [00:04<00:01, 762.26it/s]
Adding requests:  77%|███████▋  | 3155/4096 [00:04<00:01, 761.22it/s]
Adding requests:  79%|███████▉  | 3233/4096 [00:04<00:01, 766.17it/s]
Adding requests:  81%|████████  | 3312/4096 [00:04<00:01, 770.79it/s]
Adding requests:  83%|████████▎ | 3390/4096 [00:04<00:00, 765.03it/s]
Adding requests:  85%|████████▍ | 3467/4096 [00:04<00:00, 757.19it/s]
Adding requests:  86%|████████▋ | 3543/4096 [00:04<00:00, 751.76it/s]
Adding requests:  88%|████████▊ | 3619/4096 [00:04<00:00, 736.44it/s]
Adding requests:  90%|█████████ | 3697/4096 [00:04<00:00, 746.72it/s]
Adding requests:  92%|█████████▏| 3775/4096 [00:04<00:00, 756.08it/s]
Adding requests:  94%|█████████▍| 3854/4096 [00:05<00:00, 764.12it/s]
Adding requests:  96%|█████████▌| 3931/4096 [00:05<00:00, 760.65it/s]
Adding requests:  98%|█████████▊| 4008/4096 [00:05<00:00, 756.71it/s]
Adding requests: 100%|█████████▉| 4085/4096 [00:05<00:00, 759.05it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 755.72it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 162/4096 [00:00<00:07, 544.83it/s, est. speed input: 557936.01 toks/s, output: 544.84 toks/s]
Processed prompts:   5%|▌         | 217/4096 [00:01<00:26, 147.52it/s, est. speed input: 180551.36 toks/s, output: 176.32 toks/s]
Processed prompts:   6%|▌         | 243/4096 [00:02<00:46, 83.03it/s, est. speed input: 114960.09 toks/s, output: 112.27 toks/s] 
Processed prompts:   6%|▋         | 259/4096 [00:03<01:10, 54.36it/s, est. speed input: 85596.08 toks/s, output: 83.59 toks/s]  
Processed prompts:   7%|▋         | 290/4096 [00:04<01:22, 46.25it/s, est. speed input: 73646.96 toks/s, output: 71.92 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:04<01:29, 41.96it/s, est. speed input: 66353.02 toks/s, output: 64.80 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:05<01:35, 39.37it/s, est. speed input: 61400.71 toks/s, output: 59.96 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:06<01:38, 37.71it/s, est. speed input: 57799.58 toks/s, output: 56.44 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:07<01:40, 36.61it/s, est. speed input: 55063.85 toks/s, output: 53.77 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:08<01:41, 35.86it/s, est. speed input: 52912.86 toks/s, output: 51.67 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:09<01:42, 35.35it/s, est. speed input: 51179.24 toks/s, output: 49.98 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:10<01:42, 35.00it/s, est. speed input: 49751.14 toks/s, output: 48.59 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:11<01:42, 34.76it/s, est. speed input: 48553.47 toks/s, output: 47.42 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:12<01:41, 34.59it/s, est. speed input: 47536.93 toks/s, output: 46.42 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:13<01:41, 34.47it/s, est. speed input: 46661.19 toks/s, output: 45.57 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:14<01:40, 34.39it/s, est. speed input: 45900.97 toks/s, output: 44.83 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:15<01:39, 34.33it/s, est. speed input: 45233.24 toks/s, output: 44.17 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:16<01:38, 34.28it/s, est. speed input: 44641.19 toks/s, output: 43.59 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:17<01:38, 34.25it/s, est. speed input: 44114.58 toks/s, output: 43.08 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:18<01:37, 34.23it/s, est. speed input: 43642.36 toks/s, output: 42.62 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:19<01:36, 34.22it/s, est. speed input: 43216.61 toks/s, output: 42.20 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:19<01:35, 34.21it/s, est. speed input: 42830.81 toks/s, output: 41.83 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:20<01:34, 34.20it/s, est. speed input: 42480.80 toks/s, output: 41.49 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:21<01:33, 34.19it/s, est. speed input: 42159.34 toks/s, output: 41.17 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:22<01:32, 34.19it/s, est. speed input: 41864.44 toks/s, output: 40.88 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:23<01:31, 34.19it/s, est. speed input: 41593.23 toks/s, output: 40.62 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:24<01:30, 34.19it/s, est. speed input: 41343.55 toks/s, output: 40.37 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:25<01:29, 34.18it/s, est. speed input: 41110.02 toks/s, output: 40.15 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:26<01:28, 34.18it/s, est. speed input: 40894.21 toks/s, output: 39.94 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:27<01:27, 34.18it/s, est. speed input: 40693.06 toks/s, output: 39.74 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:28<01:27, 34.18it/s, est. speed input: 40504.97 toks/s, output: 39.56 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:29<01:25, 34.45it/s, est. speed input: 40363.46 toks/s, output: 39.42 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:30<01:24, 34.37it/s, est. speed input: 40197.27 toks/s, output: 39.26 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:31<01:23, 34.31it/s, est. speed input: 40040.36 toks/s, output: 39.10 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:32<01:23, 34.27it/s, est. speed input: 39893.49 toks/s, output: 38.96 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:33<01:22, 34.24it/s, est. speed input: 39754.61 toks/s, output: 38.82 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:33<01:21, 34.22it/s, est. speed input: 39623.51 toks/s, output: 38.69 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:34<01:20, 34.21it/s, est. speed input: 39499.38 toks/s, output: 38.57 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:35<01:19, 34.20it/s, est. speed input: 39381.94 toks/s, output: 38.46 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:36<01:18, 34.20it/s, est. speed input: 39270.70 toks/s, output: 38.35 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:37<01:17, 34.19it/s, est. speed input: 39164.70 toks/s, output: 38.25 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:38<01:16, 34.19it/s, est. speed input: 39063.93 toks/s, output: 38.15 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:39<01:15, 34.19it/s, est. speed input: 38967.94 toks/s, output: 38.05 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:40<01:14, 34.19it/s, est. speed input: 38876.27 toks/s, output: 37.97 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:41<01:13, 34.18it/s, est. speed input: 38788.56 toks/s, output: 37.88 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:42<01:12, 34.18it/s, est. speed input: 38704.81 toks/s, output: 37.80 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:43<01:12, 34.18it/s, est. speed input: 38624.55 toks/s, output: 37.72 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:44<01:11, 34.18it/s, est. speed input: 38548.03 toks/s, output: 37.64 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:45<01:10, 34.18it/s, est. speed input: 38474.48 toks/s, output: 37.57 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:46<01:09, 34.18it/s, est. speed input: 38403.76 toks/s, output: 37.50 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:47<01:08, 34.18it/s, est. speed input: 38335.97 toks/s, output: 37.44 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:48<01:07, 34.18it/s, est. speed input: 38270.72 toks/s, output: 37.37 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:48<01:06, 34.18it/s, est. speed input: 38208.37 toks/s, output: 37.31 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:49<01:04, 34.45it/s, est. speed input: 38166.89 toks/s, output: 37.27 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:50<01:04, 34.36it/s, est. speed input: 38108.23 toks/s, output: 37.22 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:51<01:03, 34.31it/s, est. speed input: 38052.16 toks/s, output: 37.16 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:52<01:02, 34.27it/s, est. speed input: 37997.84 toks/s, output: 37.11 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:53<01:01, 34.24it/s, est. speed input: 37945.37 toks/s, output: 37.06 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:54<01:00, 34.22it/s, est. speed input: 37894.81 toks/s, output: 37.01 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:55<00:59, 34.21it/s, est. speed input: 37845.70 toks/s, output: 36.96 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:56<00:58, 34.20it/s, est. speed input: 37798.38 toks/s, output: 36.91 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:57<00:57, 34.19it/s, est. speed input: 37752.74 toks/s, output: 36.87 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:58<00:57, 34.18it/s, est. speed input: 37708.27 toks/s, output: 36.82 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:59<00:56, 34.18it/s, est. speed input: 37665.21 toks/s, output: 36.78 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [01:00<00:55, 34.18it/s, est. speed input: 37623.57 toks/s, output: 36.74 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [01:01<00:54, 34.17it/s, est. speed input: 37583.14 toks/s, output: 36.70 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [01:02<00:53, 34.17it/s, est. speed input: 37544.07 toks/s, output: 36.66 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [01:02<00:52, 34.17it/s, est. speed input: 37506.19 toks/s, output: 36.63 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [01:03<00:51, 34.17it/s, est. speed input: 37469.34 toks/s, output: 36.59 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [01:04<00:50, 34.17it/s, est. speed input: 37433.46 toks/s, output: 36.56 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [01:05<00:49, 34.17it/s, est. speed input: 37398.68 toks/s, output: 36.52 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [01:06<00:48, 34.17it/s, est. speed input: 37365.04 toks/s, output: 36.49 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [01:07<00:47, 34.18it/s, est. speed input: 37332.30 toks/s, output: 36.46 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [01:08<00:46, 34.45it/s, est. speed input: 37313.98 toks/s, output: 36.44 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [01:09<00:45, 34.37it/s, est. speed input: 37282.77 toks/s, output: 36.41 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [01:10<00:44, 34.32it/s, est. speed input: 37252.66 toks/s, output: 36.38 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [01:11<00:43, 34.27it/s, est. speed input: 37222.81 toks/s, output: 36.35 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [01:12<00:42, 34.24it/s, est. speed input: 37193.85 toks/s, output: 36.32 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [01:13<00:42, 34.22it/s, est. speed input: 37165.67 toks/s, output: 36.29 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [01:14<00:41, 34.20it/s, est. speed input: 37138.11 toks/s, output: 36.27 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [01:15<00:40, 34.20it/s, est. speed input: 37111.51 toks/s, output: 36.24 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [01:16<00:39, 34.18it/s, est. speed input: 37085.00 toks/s, output: 36.22 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [01:16<00:38, 34.18it/s, est. speed input: 37059.49 toks/s, output: 36.19 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [01:17<00:37, 34.17it/s, est. speed input: 37034.54 toks/s, output: 36.17 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [01:18<00:36, 34.17it/s, est. speed input: 37010.19 toks/s, output: 36.14 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [01:19<00:35, 34.17it/s, est. speed input: 36986.48 toks/s, output: 36.12 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [01:20<00:34, 34.17it/s, est. speed input: 36963.12 toks/s, output: 36.10 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [01:21<00:33, 34.17it/s, est. speed input: 36940.51 toks/s, output: 36.07 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:22<00:32, 34.17it/s, est. speed input: 36918.35 toks/s, output: 36.05 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:23<00:31, 34.17it/s, est. speed input: 36896.64 toks/s, output: 36.03 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:24<00:30, 34.16it/s, est. speed input: 36875.32 toks/s, output: 36.01 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:25<00:29, 34.17it/s, est. speed input: 36854.67 toks/s, output: 35.99 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:26<00:28, 34.17it/s, est. speed input: 36834.41 toks/s, output: 35.97 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:27<00:28, 34.17it/s, est. speed input: 36814.55 toks/s, output: 35.95 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:28<00:27, 34.16it/s, est. speed input: 36794.92 toks/s, output: 35.93 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:29<00:26, 34.16it/s, est. speed input: 36775.84 toks/s, output: 35.91 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:30<00:25, 34.16it/s, est. speed input: 36757.11 toks/s, output: 35.90 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:31<00:24, 34.16it/s, est. speed input: 36739.02 toks/s, output: 35.88 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:31<00:23, 34.15it/s, est. speed input: 36720.75 toks/s, output: 35.86 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:32<00:22, 34.15it/s, est. speed input: 36703.17 toks/s, output: 35.84 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:33<00:21, 34.16it/s, est. speed input: 36685.97 toks/s, output: 35.83 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:34<00:20, 34.15it/s, est. speed input: 36668.99 toks/s, output: 35.81 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:35<00:19, 34.16it/s, est. speed input: 36652.43 toks/s, output: 35.79 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:36<00:18, 34.16it/s, est. speed input: 36636.34 toks/s, output: 35.78 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:37<00:17, 34.16it/s, est. speed input: 36620.31 toks/s, output: 35.76 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:38<00:16, 34.15it/s, est. speed input: 36604.61 toks/s, output: 35.75 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:39<00:15, 34.16it/s, est. speed input: 36589.29 toks/s, output: 35.73 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:40<00:14, 34.15it/s, est. speed input: 36574.18 toks/s, output: 35.72 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:41<00:13, 34.16it/s, est. speed input: 36559.42 toks/s, output: 35.70 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:42<00:13, 34.16it/s, est. speed input: 36545.12 toks/s, output: 35.69 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:43<00:12, 34.15it/s, est. speed input: 36530.68 toks/s, output: 35.67 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:44<00:11, 34.15it/s, est. speed input: 36516.66 toks/s, output: 35.66 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:45<00:10, 34.15it/s, est. speed input: 36502.86 toks/s, output: 35.65 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:46<00:09, 34.15it/s, est. speed input: 36489.24 toks/s, output: 35.63 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:46<00:08, 34.15it/s, est. speed input: 36475.91 toks/s, output: 35.62 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:47<00:07, 34.15it/s, est. speed input: 36462.77 toks/s, output: 35.61 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:48<00:06, 34.15it/s, est. speed input: 36450.02 toks/s, output: 35.60 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:49<00:05, 34.14it/s, est. speed input: 36437.19 toks/s, output: 35.58 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:50<00:04, 34.14it/s, est. speed input: 36424.63 toks/s, output: 35.57 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:51<00:03, 34.15it/s, est. speed input: 36412.51 toks/s, output: 35.56 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:52<00:02, 34.15it/s, est. speed input: 36400.43 toks/s, output: 35.55 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:53<00:01, 34.42it/s, est. speed input: 36396.67 toks/s, output: 35.54 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:54<00:00, 34.67it/s, est. speed input: 36394.34 toks/s, output: 35.54 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:54<00:00, 34.67it/s, est. speed input: 36662.76 toks/s, output: 35.80 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:54<00:00, 35.80it/s, est. speed input: 36662.76 toks/s, output: 35.80 toks/s]
[rank0]:[W128 00:44:24.069349996 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 154.1s

测试结果:
  Requests/s:   34.18
  Tokens/s:     35038.21
  Total Reqs:   4096
  Elapsed:      119.82s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     35004.02


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,51.1683,26249.3234,2.5016
1024,1024,1,128,128,35.3566,36240.5557,3.6203
2048,1024,2,256,128,35.7961,36690.9931,7.1516
4096,1024,4,512,128,35.7605,36654.5207,14.3175
8192,1024,8,1024,128,34.4634,35324.9866,29.7127
16384,1024,16,2048,128,34.2323,35088.0903,59.8266
32768,1024,32,4096,128,34.1836,35038.2083,119.8235

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:44:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3500088) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3500088) WARNING 01-28 00:44:38 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.42 requests/s, 28943.85 total tokens/s, 56.42 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:44:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:44:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:44:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:44:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:44:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:44:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:44:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:44:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:44:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:44:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:44:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:44:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:44:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:44:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:44:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:44:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:44:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:44:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3500088) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3500088) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]
(EngineCore_DP0 pid=3500088) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]
(EngineCore_DP0 pid=3500088) 
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3500088) [2026-01-28 00:44:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3500088) 2026-01-28 00:44:45,603 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3500088) 2026-01-28 00:44:45,619 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3500088) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.67it/s]
(EngineCore_DP0 pid=3500088) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.16it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  88%|████████▊ | 113/128 [00:00<00:00, 1124.83it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1128.41it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:14,  8.82it/s, est. speed input: 4514.14 toks/s, output: 8.82 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 39.76it/s, est. speed input: 17990.30 toks/s, output: 35.14 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 49.99it/s, est. speed input: 22589.79 toks/s, output: 44.12 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:01, 54.53it/s, est. speed input: 24814.52 toks/s, output: 48.47 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 56.81it/s, est. speed input: 26093.00 toks/s, output: 50.96 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 58.21it/s, est. speed input: 26947.46 toks/s, output: 52.63 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 59.56it/s, est. speed input: 27654.30 toks/s, output: 54.01 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 60.83it/s, est. speed input: 28251.21 toks/s, output: 55.18 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 61.76it/s, est. speed input: 28730.19 toks/s, output: 56.11 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 61.69it/s, est. speed input: 29009.75 toks/s, output: 56.66 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 61.79it/s, est. speed input: 29259.02 toks/s, output: 57.15 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 62.03it/s, est. speed input: 29489.25 toks/s, output: 57.60 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 62.33it/s, est. speed input: 29700.29 toks/s, output: 58.01 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 62.55it/s, est. speed input: 29882.44 toks/s, output: 58.36 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 62.74it/s, est. speed input: 30044.51 toks/s, output: 58.68 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 62.52it/s, est. speed input: 30151.68 toks/s, output: 58.89 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 62.26it/s, est. speed input: 30235.90 toks/s, output: 59.05 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 62.54it/s, est. speed input: 30352.01 toks/s, output: 59.28 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 62.16it/s, est. speed input: 30406.99 toks/s, output: 59.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 62.16it/s, est. speed input: 30417.62 toks/s, output: 59.41 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 59.41it/s, est. speed input: 30417.62 toks/s, output: 59.41 toks/s]
[rank0]:[W128 00:44:49.396515666 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.4s

测试结果:
  Requests/s:   56.42
  Tokens/s:     28943.85
  Total Reqs:   128
  Elapsed:      2.27s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     28887.43

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:44:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3500806) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3500806) WARNING 01-28 00:45:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 44.78 requests/s, 45903.30 total tokens/s, 44.78 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:44:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:44:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:44:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:44:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:44:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:44:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:44:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:44:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:44:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:44:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:44:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:44:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:44:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:44:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:44:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:44:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:44:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:44:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:44:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3500806) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3500806) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.53it/s]
(EngineCore_DP0 pid=3500806) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.52it/s]
(EngineCore_DP0 pid=3500806) 
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3500806) [2026-01-28 00:44:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3500806) 2026-01-28 00:45:10,437 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3500806) 2026-01-28 00:45:10,453 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3500806) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 22.57it/s]
(EngineCore_DP0 pid=3500806) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.07it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  55%|█████▌    | 71/128 [00:00<00:00, 708.76it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 715.18it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 38.27it/s, est. speed input: 39189.27 toks/s, output: 38.27 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:02, 44.08it/s, est. speed input: 44247.40 toks/s, output: 43.21 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:02, 45.97it/s, est. speed input: 45953.91 toks/s, output: 44.88 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 46.82it/s, est. speed input: 46782.33 toks/s, output: 45.68 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 47.40it/s, est. speed input: 47340.14 toks/s, output: 46.23 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 47.66it/s, est. speed input: 47670.99 toks/s, output: 46.55 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 47.76it/s, est. speed input: 47878.56 toks/s, output: 46.76 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 47.86it/s, est. speed input: 48045.94 toks/s, output: 46.92 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 47.93it/s, est. speed input: 48178.23 toks/s, output: 47.05 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:01, 47.98it/s, est. speed input: 48287.38 toks/s, output: 47.16 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 47.98it/s, est. speed input: 48364.38 toks/s, output: 47.23 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 48.14it/s, est. speed input: 48472.96 toks/s, output: 47.34 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 48.18it/s, est. speed input: 48545.83 toks/s, output: 47.41 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 48.23it/s, est. speed input: 48614.37 toks/s, output: 47.47 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 48.22it/s, est. speed input: 48663.35 toks/s, output: 47.52 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:01, 48.19it/s, est. speed input: 48701.95 toks/s, output: 47.56 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:00, 48.22it/s, est. speed input: 48744.99 toks/s, output: 47.60 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 48.19it/s, est. speed input: 48775.73 toks/s, output: 47.63 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:01<00:00, 48.21it/s, est. speed input: 48809.59 toks/s, output: 47.67 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 48.23it/s, est. speed input: 48840.86 toks/s, output: 47.70 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 48.18it/s, est. speed input: 48857.86 toks/s, output: 47.71 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 48.18it/s, est. speed input: 48880.06 toks/s, output: 47.73 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 48.21it/s, est. speed input: 48904.80 toks/s, output: 47.76 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 48.21it/s, est. speed input: 48924.27 toks/s, output: 47.78 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:02<00:00, 48.16it/s, est. speed input: 48934.93 toks/s, output: 47.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.16it/s, est. speed input: 48954.51 toks/s, output: 47.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 47.81it/s, est. speed input: 48954.51 toks/s, output: 47.81 toks/s]
[rank0]:[W128 00:45:14.427371835 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.0s

测试结果:
  Requests/s:   44.78
  Tokens/s:     45903.30
  Total Reqs:   128
  Elapsed:      2.86s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     45858.51

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:45:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3501442) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3501442) WARNING 01-28 00:45:29 [backends.py:609] Failed to read file <frozen os>
Throughput: 48.35 requests/s, 49555.67 total tokens/s, 48.35 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 00:45:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:45:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:45:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:45:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:45:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:45:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:45:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:45:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:45:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:45:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:45:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:45:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:45:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:45:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:45:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:45:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:45:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:45:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3501442) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3501442) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.11it/s]
(EngineCore_DP0 pid=3501442) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.11it/s]
(EngineCore_DP0 pid=3501442) 
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3501442) [2026-01-28 00:45:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3501442) 2026-01-28 00:45:35,983 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3501442) 2026-01-28 00:45:35,998 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3501442) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 13.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 12.80it/s]
(EngineCore_DP0 pid=3501442) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.53it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:36,  6.93it/s]
Adding requests:  29%|██▉       | 74/256 [00:00<00:00, 366.53it/s]
Adding requests:  58%|█████▊    | 149/256 [00:00<00:00, 523.73it/s]
Adding requests:  88%|████████▊ | 225/256 [00:00<00:00, 609.19it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 526.42it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 148.25it/s, est. speed input: 151820.89 toks/s, output: 148.25 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:00<00:02, 77.60it/s, est. speed input: 86536.75 toks/s, output: 84.51 toks/s]   
Processed prompts:  18%|█▊        | 45/256 [00:00<00:03, 65.80it/s, est. speed input: 75163.44 toks/s, output: 73.40 toks/s]
Processed prompts:  21%|██        | 53/256 [00:00<00:03, 60.76it/s, est. speed input: 70347.74 toks/s, output: 68.70 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:00<00:03, 55.45it/s, est. speed input: 66072.95 toks/s, output: 64.52 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:01<00:03, 54.16it/s, est. speed input: 64429.93 toks/s, output: 62.92 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:03, 53.11it/s, est. speed input: 63104.86 toks/s, output: 61.63 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:03, 52.38it/s, est. speed input: 62051.13 toks/s, output: 60.59 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:03, 51.84it/s, est. speed input: 61173.76 toks/s, output: 59.74 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:03, 51.46it/s, est. speed input: 60438.24 toks/s, output: 59.02 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:01<00:03, 51.05it/s, est. speed input: 59769.10 toks/s, output: 58.37 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:01<00:03, 50.74it/s, est. speed input: 59185.60 toks/s, output: 57.80 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:01<00:02, 50.58it/s, est. speed input: 58691.36 toks/s, output: 57.32 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:02, 50.53it/s, est. speed input: 58272.16 toks/s, output: 56.91 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:02<00:02, 50.51it/s, est. speed input: 57901.97 toks/s, output: 56.54 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:02<00:02, 50.42it/s, est. speed input: 57555.47 toks/s, output: 56.21 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:02<00:02, 50.29it/s, est. speed input: 57233.31 toks/s, output: 55.89 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:02<00:02, 50.29it/s, est. speed input: 56957.27 toks/s, output: 55.62 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:02<00:02, 50.25it/s, est. speed input: 56699.32 toks/s, output: 55.37 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:02<00:02, 50.31it/s, est. speed input: 56478.51 toks/s, output: 55.15 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:02<00:01, 50.34it/s, est. speed input: 56275.02 toks/s, output: 54.96 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:02<00:01, 50.35it/s, est. speed input: 56086.50 toks/s, output: 54.77 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:03<00:01, 50.37it/s, est. speed input: 55913.97 toks/s, output: 54.60 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:03<00:01, 50.41it/s, est. speed input: 55758.41 toks/s, output: 54.45 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:03<00:01, 50.42it/s, est. speed input: 55610.17 toks/s, output: 54.31 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:03<00:01, 50.42it/s, est. speed input: 55472.13 toks/s, output: 54.17 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:03<00:01, 50.32it/s, est. speed input: 55331.60 toks/s, output: 54.03 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:03<00:01, 50.35it/s, est. speed input: 55211.52 toks/s, output: 53.92 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:03<00:01, 50.36it/s, est. speed input: 55098.27 toks/s, output: 53.81 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:03<00:00, 50.36it/s, est. speed input: 54990.10 toks/s, output: 53.70 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:04<00:00, 50.29it/s, est. speed input: 54881.84 toks/s, output: 53.60 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:04<00:00, 50.27it/s, est. speed input: 54782.41 toks/s, output: 53.50 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:04<00:00, 50.28it/s, est. speed input: 54690.71 toks/s, output: 53.41 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:04<00:00, 50.22it/s, est. speed input: 54598.05 toks/s, output: 53.32 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:04<00:00, 50.25it/s, est. speed input: 54516.37 toks/s, output: 53.24 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:04<00:00, 50.10it/s, est. speed input: 54423.66 toks/s, output: 53.15 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:04<00:00, 50.18it/s, est. speed input: 54352.54 toks/s, output: 53.08 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 50.18it/s, est. speed input: 54524.23 toks/s, output: 53.25 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 53.25it/s, est. speed input: 54524.23 toks/s, output: 53.25 toks/s]
[rank0]:[W128 00:45:42.603616496 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 28.1s

测试结果:
  Requests/s:   48.35
  Tokens/s:     49555.67
  Total Reqs:   256
  Elapsed:      5.30s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     49507.32

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:45:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3502119) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3502119) WARNING 01-28 00:45:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 49.38 requests/s, 50613.85 total tokens/s, 49.38 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 00:45:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:45:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:45:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:45:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:45:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:45:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:45:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:45:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:45:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:45:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:45:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:45:53] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:45:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:53] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:53] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:45:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:45:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:45:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:45:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:45:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:45:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3502119) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3502119) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.90it/s]
(EngineCore_DP0 pid=3502119) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.89it/s]
(EngineCore_DP0 pid=3502119) 
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3502119) [2026-01-28 00:45:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3502119) 2026-01-28 00:46:04,794 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3502119) 2026-01-28 00:46:04,810 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3502119) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 24.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 23.01it/s]
(EngineCore_DP0 pid=3502119) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.67it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  14%|█▎        | 70/512 [00:00<00:00, 698.87it/s]
Adding requests:  28%|██▊       | 144/512 [00:00<00:00, 718.35it/s]
Adding requests:  42%|████▏     | 217/512 [00:00<00:00, 723.48it/s]
Adding requests:  57%|█████▋    | 293/512 [00:00<00:00, 736.20it/s]
Adding requests:  72%|███████▏  | 369/512 [00:00<00:00, 741.74it/s]
Adding requests:  87%|████████▋ | 444/512 [00:00<00:00, 741.99it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 736.62it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:00<00:02, 207.42it/s, est. speed input: 212418.94 toks/s, output: 207.42 toks/s]
Processed prompts:  11%|█         | 55/512 [00:00<00:05, 86.94it/s, est. speed input: 99776.31 toks/s, output: 97.44 toks/s]   
Processed prompts:  13%|█▎        | 67/512 [00:00<00:06, 72.23it/s, est. speed input: 85324.79 toks/s, output: 83.32 toks/s]
Processed prompts:  15%|█▍        | 76/512 [00:00<00:06, 67.81it/s, est. speed input: 80712.98 toks/s, output: 78.82 toks/s]
Processed prompts:  16%|█▋        | 84/512 [00:01<00:06, 62.72it/s, est. speed input: 76476.95 toks/s, output: 74.68 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:01<00:07, 57.29it/s, est. speed input: 72545.59 toks/s, output: 70.85 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:01<00:07, 53.32it/s, est. speed input: 69459.33 toks/s, output: 67.83 toks/s]
Processed prompts:  21%|██        | 106/512 [00:01<00:07, 52.40it/s, est. speed input: 67660.25 toks/s, output: 66.07 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:01<00:07, 51.71it/s, est. speed input: 66176.68 toks/s, output: 64.63 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:01<00:07, 51.22it/s, est. speed input: 64936.58 toks/s, output: 63.41 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:02<00:07, 50.88it/s, est. speed input: 63890.57 toks/s, output: 62.39 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:02<00:07, 50.63it/s, est. speed input: 62990.14 toks/s, output: 61.51 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:02<00:07, 50.41it/s, est. speed input: 62197.07 toks/s, output: 60.74 toks/s]
Processed prompts:  30%|███       | 154/512 [00:02<00:07, 50.25it/s, est. speed input: 61500.36 toks/s, output: 60.06 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:02<00:06, 50.14it/s, est. speed input: 60887.73 toks/s, output: 59.46 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:02<00:06, 50.07it/s, est. speed input: 60342.56 toks/s, output: 58.93 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:03<00:06, 50.03it/s, est. speed input: 59859.42 toks/s, output: 58.46 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:03<00:06, 50.00it/s, est. speed input: 59422.61 toks/s, output: 58.03 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:03<00:06, 49.99it/s, est. speed input: 59030.33 toks/s, output: 57.65 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:03<00:06, 49.98it/s, est. speed input: 58671.75 toks/s, output: 57.30 toks/s]
Processed prompts:  41%|████      | 210/512 [00:03<00:06, 50.01it/s, est. speed input: 58351.77 toks/s, output: 56.98 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:03<00:05, 50.07it/s, est. speed input: 58063.72 toks/s, output: 56.70 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:04<00:05, 50.06it/s, est. speed input: 57791.80 toks/s, output: 56.44 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:04<00:05, 50.07it/s, est. speed input: 57543.10 toks/s, output: 56.19 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:04<00:05, 50.02it/s, est. speed input: 57304.48 toks/s, output: 55.96 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:04<00:05, 49.99it/s, est. speed input: 57082.51 toks/s, output: 55.74 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:04<00:05, 50.02it/s, est. speed input: 56883.40 toks/s, output: 55.55 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:04<00:04, 49.99it/s, est. speed input: 56691.05 toks/s, output: 55.36 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:04<00:04, 50.01it/s, est. speed input: 56516.57 toks/s, output: 55.19 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:05<00:04, 49.97it/s, est. speed input: 56345.98 toks/s, output: 55.03 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:05<00:04, 49.96it/s, est. speed input: 56188.38 toks/s, output: 54.87 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:05<00:04, 49.94it/s, est. speed input: 56038.30 toks/s, output: 54.72 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:05<00:04, 49.92it/s, est. speed input: 55895.65 toks/s, output: 54.59 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:05<00:03, 50.01it/s, est. speed input: 55772.07 toks/s, output: 54.46 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:05<00:03, 50.03it/s, est. speed input: 55651.09 toks/s, output: 54.35 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:06<00:03, 50.00it/s, est. speed input: 55532.28 toks/s, output: 54.23 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:06<00:03, 50.03it/s, est. speed input: 55424.37 toks/s, output: 54.13 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:06<00:03, 49.96it/s, est. speed input: 55313.08 toks/s, output: 54.02 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:06<00:03, 49.97it/s, est. speed input: 55212.86 toks/s, output: 53.92 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:06<00:02, 50.00it/s, est. speed input: 55119.22 toks/s, output: 53.83 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:06<00:02, 49.98it/s, est. speed input: 55025.95 toks/s, output: 53.74 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:07<00:02, 50.02it/s, est. speed input: 54941.77 toks/s, output: 53.65 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:07<00:02, 49.99it/s, est. speed input: 54857.30 toks/s, output: 53.57 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:07<00:02, 49.99it/s, est. speed input: 54777.14 toks/s, output: 53.49 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:07<00:02, 49.92it/s, est. speed input: 54695.41 toks/s, output: 53.41 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:07<00:02, 50.00it/s, est. speed input: 54626.78 toks/s, output: 53.35 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:07<00:01, 50.05it/s, est. speed input: 54560.43 toks/s, output: 53.28 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:08<00:01, 50.02it/s, est. speed input: 54492.22 toks/s, output: 53.21 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:08<00:01, 50.04it/s, est. speed input: 54429.74 toks/s, output: 53.15 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:08<00:01, 50.04it/s, est. speed input: 54368.44 toks/s, output: 53.09 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:08<00:01, 49.98it/s, est. speed input: 54305.37 toks/s, output: 53.03 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:08<00:01, 49.94it/s, est. speed input: 54244.60 toks/s, output: 52.97 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:08<00:00, 49.99it/s, est. speed input: 54191.66 toks/s, output: 52.92 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:08<00:00, 49.97it/s, est. speed input: 54136.39 toks/s, output: 52.87 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:09<00:00, 50.01it/s, est. speed input: 54087.24 toks/s, output: 52.82 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:09<00:00, 49.98it/s, est. speed input: 54035.76 toks/s, output: 52.77 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:09<00:00, 49.91it/s, est. speed input: 53982.79 toks/s, output: 52.72 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:09<00:00, 49.94it/s, est. speed input: 53936.61 toks/s, output: 52.67 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 49.94it/s, est. speed input: 54202.84 toks/s, output: 52.93 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 52.93it/s, est. speed input: 54202.84 toks/s, output: 52.93 toks/s]
[rank0]:[W128 00:46:16.597041562 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.0s

测试结果:
  Requests/s:   49.38
  Tokens/s:     50613.85
  Total Reqs:   512
  Elapsed:      10.37s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     50564.47

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:46:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3502884) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3502884) WARNING 01-28 00:46:33 [backends.py:609] Failed to read file <frozen os>
Throughput: 46.99 requests/s, 48162.42 total tokens/s, 46.99 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 00:46:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:46:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:46:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:46:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:46:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:46:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:46:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:46:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:46:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:46:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:46:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:46:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:46:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:46:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:46:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:46:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:46:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:46:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:46:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:46:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:46:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:46:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:46:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:46:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:46:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:46:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:46:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:46:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3502884) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3502884) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.68it/s]
(EngineCore_DP0 pid=3502884) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.67it/s]
(EngineCore_DP0 pid=3502884) 
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3502884) [2026-01-28 00:46:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3502884) 2026-01-28 00:46:39,816 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3502884) 2026-01-28 00:46:39,832 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3502884) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  6.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 16.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 15.66it/s]
(EngineCore_DP0 pid=3502884) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.52it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 71/1024 [00:00<00:01, 702.68it/s]
Adding requests:  14%|█▍        | 145/1024 [00:00<00:01, 722.88it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 732.41it/s]
Adding requests:  29%|██▊       | 294/1024 [00:00<00:01, 717.60it/s]
Adding requests:  36%|███▌      | 366/1024 [00:00<00:00, 716.07it/s]
Adding requests:  43%|████▎     | 438/1024 [00:00<00:00, 715.64it/s]
Adding requests:  50%|█████     | 513/1024 [00:00<00:00, 725.72it/s]
Adding requests:  57%|█████▋    | 588/1024 [00:00<00:00, 730.66it/s]
Adding requests:  65%|██████▍   | 662/1024 [00:00<00:00, 727.77it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:01<00:00, 739.43it/s]
Adding requests:  79%|███████▉  | 814/1024 [00:01<00:00, 728.14it/s]
Adding requests:  87%|████████▋ | 888/1024 [00:01<00:00, 730.08it/s]
Adding requests:  94%|█████████▍| 965/1024 [00:01<00:00, 740.82it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 731.81it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:00<00:03, 305.98it/s, est. speed input: 313352.59 toks/s, output: 305.98 toks/s]
Processed prompts:   9%|▉         | 97/1024 [00:00<00:07, 117.01it/s, est. speed input: 137102.67 toks/s, output: 133.89 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:01<00:12, 73.19it/s, est. speed input: 94659.48 toks/s, output: 92.44 toks/s]  
Processed prompts:  12%|█▏        | 125/1024 [00:01<00:12, 71.52it/s, est. speed input: 91234.63 toks/s, output: 89.10 toks/s]
Processed prompts:  13%|█▎        | 134/1024 [00:01<00:13, 67.40it/s, est. speed input: 87241.56 toks/s, output: 85.20 toks/s]
Processed prompts:  14%|█▍        | 142/1024 [00:01<00:14, 62.53it/s, est. speed input: 83450.82 toks/s, output: 81.49 toks/s]
Processed prompts:  15%|█▍        | 149/1024 [00:01<00:15, 57.07it/s, est. speed input: 79787.89 toks/s, output: 77.92 toks/s]
Processed prompts:  15%|█▌        | 155/1024 [00:02<00:16, 51.28it/s, est. speed input: 76244.19 toks/s, output: 74.46 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:02<00:17, 48.50it/s, est. speed input: 73681.62 toks/s, output: 71.95 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:02<00:17, 48.12it/s, est. speed input: 71903.28 toks/s, output: 70.22 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:02<00:17, 47.84it/s, est. speed input: 70355.78 toks/s, output: 68.71 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:02<00:17, 47.61it/s, est. speed input: 68989.91 toks/s, output: 67.37 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:02<00:17, 47.46it/s, est. speed input: 67785.56 toks/s, output: 66.20 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:03<00:17, 47.37it/s, est. speed input: 66718.54 toks/s, output: 65.15 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:03<00:17, 47.31it/s, est. speed input: 65763.85 toks/s, output: 64.22 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:03<00:17, 47.23it/s, est. speed input: 64893.90 toks/s, output: 63.37 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:03<00:16, 47.20it/s, est. speed input: 64110.86 toks/s, output: 62.61 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:03<00:16, 47.19it/s, est. speed input: 63402.33 toks/s, output: 61.92 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:03<00:16, 47.16it/s, est. speed input: 62749.90 toks/s, output: 61.28 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:04<00:16, 47.18it/s, est. speed input: 62157.07 toks/s, output: 60.70 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:04<00:16, 47.19it/s, est. speed input: 61610.92 toks/s, output: 60.17 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:16, 47.20it/s, est. speed input: 61107.09 toks/s, output: 59.67 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:04<00:15, 47.14it/s, est. speed input: 60630.44 toks/s, output: 59.21 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:04<00:15, 47.18it/s, est. speed input: 60198.71 toks/s, output: 58.79 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:04<00:15, 47.18it/s, est. speed input: 59793.43 toks/s, output: 58.39 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:15, 47.15it/s, est. speed input: 59410.23 toks/s, output: 58.02 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:15, 47.17it/s, est. speed input: 59057.13 toks/s, output: 57.67 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:05<00:15, 47.16it/s, est. speed input: 58722.45 toks/s, output: 57.35 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:05<00:14, 47.13it/s, est. speed input: 58404.72 toks/s, output: 57.04 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:05<00:14, 47.13it/s, est. speed input: 58108.97 toks/s, output: 56.75 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:05<00:14, 47.13it/s, est. speed input: 57829.93 toks/s, output: 56.47 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:06<00:14, 47.15it/s, est. speed input: 57568.25 toks/s, output: 56.22 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:06<00:14, 47.17it/s, est. speed input: 57321.34 toks/s, output: 55.98 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:06<00:14, 47.19it/s, est. speed input: 57088.26 toks/s, output: 55.75 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:06<00:13, 47.14it/s, est. speed input: 56860.31 toks/s, output: 55.53 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:06<00:13, 47.16it/s, est. speed input: 56648.92 toks/s, output: 55.32 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:07<00:13, 47.18it/s, est. speed input: 56448.42 toks/s, output: 55.13 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:07<00:13, 47.17it/s, est. speed input: 56254.66 toks/s, output: 54.94 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:07<00:13, 47.17it/s, est. speed input: 56070.84 toks/s, output: 54.76 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:07<00:13, 47.18it/s, est. speed input: 55896.97 toks/s, output: 54.59 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:07<00:12, 47.13it/s, est. speed input: 55724.93 toks/s, output: 54.42 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:07<00:12, 47.16it/s, est. speed input: 55566.42 toks/s, output: 54.26 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:08<00:12, 47.16it/s, est. speed input: 55412.55 toks/s, output: 54.11 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:08<00:12, 47.15it/s, est. speed input: 55264.15 toks/s, output: 53.97 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:08<00:12, 47.14it/s, est. speed input: 55121.52 toks/s, output: 53.83 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:08<00:12, 47.15it/s, est. speed input: 54985.81 toks/s, output: 53.70 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:08<00:11, 47.13it/s, est. speed input: 54853.65 toks/s, output: 53.57 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:08<00:11, 47.12it/s, est. speed input: 54727.08 toks/s, output: 53.44 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:09<00:11, 47.14it/s, est. speed input: 54606.51 toks/s, output: 53.33 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:09<00:11, 47.15it/s, est. speed input: 54490.34 toks/s, output: 53.21 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:09<00:11, 47.14it/s, est. speed input: 54377.16 toks/s, output: 53.10 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:09<00:10, 47.14it/s, est. speed input: 54268.94 toks/s, output: 53.00 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:09<00:10, 47.11it/s, est. speed input: 54162.21 toks/s, output: 52.89 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:09<00:10, 47.13it/s, est. speed input: 54061.97 toks/s, output: 52.79 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:10<00:10, 47.14it/s, est. speed input: 53964.29 toks/s, output: 52.70 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:10<00:10, 47.14it/s, est. speed input: 53869.74 toks/s, output: 52.61 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:10<00:10, 47.14it/s, est. speed input: 53778.50 toks/s, output: 52.52 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:10<00:09, 47.15it/s, est. speed input: 53690.71 toks/s, output: 52.43 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:10<00:09, 47.13it/s, est. speed input: 53604.06 toks/s, output: 52.35 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:10<00:09, 47.13it/s, est. speed input: 53520.87 toks/s, output: 52.27 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:11<00:09, 47.15it/s, est. speed input: 53441.52 toks/s, output: 52.19 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:11<00:09, 47.16it/s, est. speed input: 53364.35 toks/s, output: 52.11 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:11<00:09, 47.14it/s, est. speed input: 53287.71 toks/s, output: 52.04 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:11<00:08, 47.14it/s, est. speed input: 53214.18 toks/s, output: 51.97 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:11<00:08, 47.10it/s, est. speed input: 53140.78 toks/s, output: 51.90 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:11<00:08, 47.11it/s, est. speed input: 53071.51 toks/s, output: 51.83 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:12<00:08, 47.12it/s, est. speed input: 53004.01 toks/s, output: 51.76 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:12<00:08, 47.15it/s, est. speed input: 52939.83 toks/s, output: 51.70 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:12<00:08, 47.17it/s, est. speed input: 52877.10 toks/s, output: 51.64 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:12<00:07, 47.18it/s, est. speed input: 52815.90 toks/s, output: 51.58 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:12<00:07, 47.17it/s, est. speed input: 52755.60 toks/s, output: 51.52 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:12<00:07, 47.18it/s, est. speed input: 52697.86 toks/s, output: 51.46 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:13<00:07, 47.20it/s, est. speed input: 52641.94 toks/s, output: 51.41 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:13<00:07, 47.22it/s, est. speed input: 52588.01 toks/s, output: 51.36 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:13<00:07, 47.19it/s, est. speed input: 52533.22 toks/s, output: 51.30 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:13<00:06, 47.16it/s, est. speed input: 52479.46 toks/s, output: 51.25 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:13<00:06, 47.12it/s, est. speed input: 52425.91 toks/s, output: 51.20 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:13<00:06, 47.11it/s, est. speed input: 52374.85 toks/s, output: 51.15 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:14<00:06, 47.13it/s, est. speed input: 52325.98 toks/s, output: 51.10 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:14<00:06, 47.13it/s, est. speed input: 52277.73 toks/s, output: 51.05 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:14<00:06, 47.13it/s, est. speed input: 52230.81 toks/s, output: 51.01 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:14<00:05, 47.11it/s, est. speed input: 52183.96 toks/s, output: 50.96 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:14<00:05, 47.10it/s, est. speed input: 52138.19 toks/s, output: 50.92 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:14<00:05, 47.12it/s, est. speed input: 52094.72 toks/s, output: 50.87 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:15<00:05, 47.13it/s, est. speed input: 52052.03 toks/s, output: 50.83 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:15<00:05, 47.15it/s, est. speed input: 52010.92 toks/s, output: 50.79 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:15<00:05, 47.16it/s, est. speed input: 51970.56 toks/s, output: 50.75 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:15<00:04, 47.14it/s, est. speed input: 51929.70 toks/s, output: 50.71 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:15<00:04, 47.15it/s, est. speed input: 51890.85 toks/s, output: 50.67 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:15<00:04, 47.14it/s, est. speed input: 51852.36 toks/s, output: 50.64 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:16<00:04, 47.14it/s, est. speed input: 51814.68 toks/s, output: 50.60 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:16<00:04, 47.15it/s, est. speed input: 51778.12 toks/s, output: 50.56 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:16<00:04, 47.17it/s, est. speed input: 51743.18 toks/s, output: 50.53 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:16<00:03, 47.16it/s, est. speed input: 51707.77 toks/s, output: 50.50 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:16<00:03, 47.15it/s, est. speed input: 51672.91 toks/s, output: 50.46 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:17<00:03, 47.16it/s, est. speed input: 51639.33 toks/s, output: 50.43 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:17<00:03, 47.16it/s, est. speed input: 51606.46 toks/s, output: 50.40 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:17<00:03, 47.16it/s, est. speed input: 51573.94 toks/s, output: 50.37 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:17<00:03, 47.17it/s, est. speed input: 51542.40 toks/s, output: 50.33 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:17<00:02, 47.13it/s, est. speed input: 51510.04 toks/s, output: 50.30 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:17<00:02, 47.11it/s, est. speed input: 51478.43 toks/s, output: 50.27 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:18<00:02, 47.12it/s, est. speed input: 51448.34 toks/s, output: 50.24 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:18<00:02, 47.10it/s, est. speed input: 51417.86 toks/s, output: 50.21 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:18<00:02, 47.11it/s, est. speed input: 51388.87 toks/s, output: 50.18 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:18<00:01, 47.14it/s, est. speed input: 51361.08 toks/s, output: 50.16 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:18<00:01, 47.11it/s, est. speed input: 51331.96 toks/s, output: 50.13 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:18<00:01, 47.12it/s, est. speed input: 51304.51 toks/s, output: 50.10 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:19<00:01, 47.13it/s, est. speed input: 51277.67 toks/s, output: 50.08 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:19<00:01, 47.13it/s, est. speed input: 51251.30 toks/s, output: 50.05 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:19<00:01, 47.15it/s, est. speed input: 51225.55 toks/s, output: 50.02 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:19<00:00, 47.17it/s, est. speed input: 51200.46 toks/s, output: 50.00 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:19<00:00, 47.14it/s, est. speed input: 51174.57 toks/s, output: 49.98 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:19<00:00, 47.13it/s, est. speed input: 51149.63 toks/s, output: 49.95 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:20<00:00, 47.15it/s, est. speed input: 51125.68 toks/s, output: 49.93 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:20<00:00, 47.10it/s, est. speed input: 51100.49 toks/s, output: 49.90 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:20<00:00, 48.51it/s, est. speed input: 51117.89 toks/s, output: 49.92 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:20<00:00, 48.51it/s, est. speed input: 51418.87 toks/s, output: 50.21 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:20<00:00, 50.21it/s, est. speed input: 51418.87 toks/s, output: 50.21 toks/s]
[rank0]:[W128 00:47:03.096323898 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 46.6s

测试结果:
  Requests/s:   46.99
  Tokens/s:     48162.42
  Total Reqs:   1024
  Elapsed:      21.79s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     48115.43

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:47:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3503840) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3503840) WARNING 01-28 00:47:23 [backends.py:609] Failed to read file <frozen os>
Throughput: 46.16 requests/s, 47310.48 total tokens/s, 46.16 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 00:47:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:47:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:47:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:47:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:47:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:47:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:47:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:47:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:47:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:47:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:47:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:47:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:47:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:47:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:47:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:47:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:47:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:47:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:47:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:47:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:47:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:47:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:47:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:47:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:47:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:47:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:47:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:47:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:18] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:18] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3503840) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3503840) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.68it/s]
(EngineCore_DP0 pid=3503840) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.67it/s]
(EngineCore_DP0 pid=3503840) 
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3503840) [2026-01-28 00:47:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3503840) 2026-01-28 00:47:29,568 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3503840) 2026-01-28 00:47:29,584 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3503840) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 13.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 14.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.59it/s]
(EngineCore_DP0 pid=3503840) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  5.79it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 15.94it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 73/2048 [00:00<00:02, 722.66it/s]
Adding requests:   7%|▋         | 149/2048 [00:00<00:02, 741.89it/s]
Adding requests:  11%|█         | 227/2048 [00:00<00:02, 755.20it/s]
Adding requests:  15%|█▍        | 303/2048 [00:00<00:02, 756.18it/s]
Adding requests:  19%|█▊        | 380/2048 [00:00<00:02, 759.02it/s]
Adding requests:  22%|██▏       | 457/2048 [00:00<00:02, 760.12it/s]
Adding requests:  26%|██▌       | 534/2048 [00:00<00:02, 745.73it/s]
Adding requests:  30%|██▉       | 610/2048 [00:00<00:01, 749.26it/s]
Adding requests:  33%|███▎      | 686/2048 [00:00<00:01, 752.32it/s]
Adding requests:  37%|███▋      | 762/2048 [00:01<00:01, 747.31it/s]
Adding requests:  41%|████      | 837/2048 [00:01<00:01, 714.64it/s]
Adding requests:  45%|████▍     | 915/2048 [00:01<00:01, 731.82it/s]
Adding requests:  48%|████▊     | 993/2048 [00:01<00:01, 744.84it/s]
Adding requests:  52%|█████▏    | 1071/2048 [00:01<00:01, 753.36it/s]
Adding requests:  56%|█████▌    | 1148/2048 [00:01<00:01, 757.60it/s]
Adding requests:  60%|█████▉    | 1226/2048 [00:01<00:01, 763.01it/s]
Adding requests:  64%|██████▎   | 1303/2048 [00:01<00:00, 754.58it/s]
Adding requests:  67%|██████▋   | 1380/2048 [00:01<00:00, 756.74it/s]
Adding requests:  71%|███████   | 1458/2048 [00:01<00:00, 763.47it/s]
Adding requests:  75%|███████▍  | 1535/2048 [00:02<00:00, 763.43it/s]
Adding requests:  79%|███████▉  | 1614/2048 [00:02<00:00, 769.09it/s]
Adding requests:  83%|████████▎ | 1691/2048 [00:02<00:00, 760.21it/s]
Adding requests:  86%|████████▋ | 1768/2048 [00:02<00:00, 761.47it/s]
Adding requests:  90%|█████████ | 1845/2048 [00:02<00:00, 753.44it/s]
Adding requests:  94%|█████████▍| 1922/2048 [00:02<00:00, 755.48it/s]
Adding requests:  98%|█████████▊| 2001/2048 [00:02<00:00, 764.62it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 754.81it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:00<00:02, 856.32it/s, est. speed input: 876983.94 toks/s, output: 856.37 toks/s]
Processed prompts:  10%|▉         | 200/2048 [00:01<00:20, 91.07it/s, est. speed input: 110074.22 toks/s, output: 107.49 toks/s] 
Processed prompts:  12%|█▏        | 239/2048 [00:02<00:22, 78.88it/s, est. speed input: 95906.44 toks/s, output: 93.66 toks/s]  
Processed prompts:  13%|█▎        | 263/2048 [00:03<00:27, 64.13it/s, est. speed input: 83050.30 toks/s, output: 81.10 toks/s]
Processed prompts:  14%|█▎        | 279/2048 [00:03<00:29, 60.68it/s, est. speed input: 79615.14 toks/s, output: 77.75 toks/s]
Processed prompts:  14%|█▍        | 291/2048 [00:03<00:31, 55.09it/s, est. speed input: 75751.03 toks/s, output: 73.98 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:04<00:33, 52.34it/s, est. speed input: 73223.76 toks/s, output: 71.51 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:04<00:33, 50.83it/s, est. speed input: 71300.31 toks/s, output: 69.63 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:04<00:34, 49.52it/s, est. speed input: 69600.10 toks/s, output: 67.97 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:05<00:34, 48.63it/s, est. speed input: 68155.19 toks/s, output: 66.56 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:05<00:34, 47.97it/s, est. speed input: 66888.22 toks/s, output: 65.32 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:06<00:34, 47.50it/s, est. speed input: 65769.32 toks/s, output: 64.23 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:06<00:34, 47.14it/s, est. speed input: 64768.52 toks/s, output: 63.25 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:06<00:34, 46.90it/s, est. speed input: 63874.95 toks/s, output: 62.38 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:07<00:34, 46.72it/s, est. speed input: 63067.14 toks/s, output: 61.59 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:07<00:34, 46.59it/s, est. speed input: 62333.34 toks/s, output: 60.87 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:07<00:34, 46.50it/s, est. speed input: 61666.74 toks/s, output: 60.22 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:08<00:33, 46.44it/s, est. speed input: 61057.54 toks/s, output: 59.63 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:08<00:33, 46.39it/s, est. speed input: 60496.39 toks/s, output: 59.08 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:08<00:33, 46.37it/s, est. speed input: 59982.56 toks/s, output: 58.58 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:09<00:32, 46.35it/s, est. speed input: 59505.44 toks/s, output: 58.11 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:09<00:32, 46.32it/s, est. speed input: 59062.63 toks/s, output: 57.68 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:09<00:32, 46.32it/s, est. speed input: 58653.23 toks/s, output: 57.28 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:10<00:31, 46.31it/s, est. speed input: 58269.60 toks/s, output: 56.90 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:10<00:31, 46.30it/s, est. speed input: 57910.95 toks/s, output: 56.55 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:10<00:31, 46.30it/s, est. speed input: 57577.50 toks/s, output: 56.23 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:11<00:30, 46.29it/s, est. speed input: 57262.01 toks/s, output: 55.92 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:11<00:30, 46.28it/s, est. speed input: 56965.08 toks/s, output: 55.63 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:11<00:30, 46.28it/s, est. speed input: 56687.14 toks/s, output: 55.36 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:12<00:29, 46.29it/s, est. speed input: 56425.36 toks/s, output: 55.10 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:12<00:29, 46.27it/s, est. speed input: 56175.43 toks/s, output: 54.86 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:12<00:29, 46.27it/s, est. speed input: 55940.27 toks/s, output: 54.63 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:13<00:28, 46.28it/s, est. speed input: 55717.58 toks/s, output: 54.41 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:13<00:28, 46.26it/s, est. speed input: 55504.46 toks/s, output: 54.20 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:13<00:27, 46.26it/s, est. speed input: 55303.11 toks/s, output: 54.01 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:14<00:27, 46.25it/s, est. speed input: 55109.76 toks/s, output: 53.82 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:14<00:27, 46.24it/s, est. speed input: 54926.57 toks/s, output: 53.64 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:14<00:26, 46.27it/s, est. speed input: 54753.92 toks/s, output: 53.47 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:15<00:26, 46.24it/s, est. speed input: 54585.16 toks/s, output: 53.31 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:15<00:26, 46.24it/s, est. speed input: 54426.12 toks/s, output: 53.15 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:16<00:25, 46.25it/s, est. speed input: 54274.36 toks/s, output: 53.00 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:16<00:25, 46.26it/s, est. speed input: 54128.91 toks/s, output: 52.86 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:16<00:25, 46.26it/s, est. speed input: 53989.29 toks/s, output: 52.72 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:17<00:24, 46.27it/s, est. speed input: 53855.90 toks/s, output: 52.59 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:17<00:24, 46.25it/s, est. speed input: 53725.54 toks/s, output: 52.47 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:17<00:24, 46.24it/s, est. speed input: 53601.02 toks/s, output: 52.34 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:18<00:23, 46.22it/s, est. speed input: 53480.14 toks/s, output: 52.23 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:18<00:23, 46.21it/s, est. speed input: 53364.21 toks/s, output: 52.11 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:18<00:23, 46.21it/s, est. speed input: 53252.63 toks/s, output: 52.00 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:19<00:22, 46.21it/s, est. speed input: 53145.46 toks/s, output: 51.90 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:19<00:22, 46.19it/s, est. speed input: 53040.73 toks/s, output: 51.80 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:19<00:22, 46.20it/s, est. speed input: 52940.81 toks/s, output: 51.70 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:20<00:21, 46.19it/s, est. speed input: 52843.72 toks/s, output: 51.61 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:20<00:21, 46.18it/s, est. speed input: 52749.77 toks/s, output: 51.51 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:20<00:21, 46.18it/s, est. speed input: 52659.10 toks/s, output: 51.42 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:21<00:20, 46.19it/s, est. speed input: 52572.27 toks/s, output: 51.34 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:21<00:20, 46.17it/s, est. speed input: 52486.40 toks/s, output: 51.26 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:21<00:20, 46.19it/s, est. speed input: 52404.97 toks/s, output: 51.18 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:22<00:19, 46.19it/s, est. speed input: 52325.40 toks/s, output: 51.10 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:22<00:19, 46.81it/s, est. speed input: 52284.06 toks/s, output: 51.06 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:22<00:18, 46.63it/s, est. speed input: 52209.04 toks/s, output: 50.99 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:23<00:18, 46.48it/s, est. speed input: 52135.21 toks/s, output: 50.91 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:23<00:18, 46.38it/s, est. speed input: 52063.60 toks/s, output: 50.84 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:23<00:17, 46.32it/s, est. speed input: 51994.76 toks/s, output: 50.78 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:24<00:17, 46.27it/s, est. speed input: 51927.09 toks/s, output: 50.71 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:24<00:17, 46.24it/s, est. speed input: 51861.82 toks/s, output: 50.65 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:25<00:16, 46.22it/s, est. speed input: 51798.29 toks/s, output: 50.58 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:25<00:16, 46.20it/s, est. speed input: 51736.25 toks/s, output: 50.52 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:25<00:16, 46.18it/s, est. speed input: 51675.96 toks/s, output: 50.46 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:26<00:15, 46.18it/s, est. speed input: 51617.72 toks/s, output: 50.41 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:26<00:15, 46.17it/s, est. speed input: 51560.34 toks/s, output: 50.35 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:26<00:15, 46.17it/s, est. speed input: 51504.64 toks/s, output: 50.30 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:27<00:14, 46.16it/s, est. speed input: 51450.34 toks/s, output: 50.24 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:27<00:14, 46.16it/s, est. speed input: 51397.63 toks/s, output: 50.19 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:27<00:14, 46.17it/s, est. speed input: 51346.38 toks/s, output: 50.14 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:28<00:13, 46.18it/s, est. speed input: 51296.63 toks/s, output: 50.09 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:28<00:13, 46.17it/s, est. speed input: 51247.36 toks/s, output: 50.05 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:28<00:13, 46.16it/s, est. speed input: 51199.47 toks/s, output: 50.00 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:29<00:12, 46.16it/s, est. speed input: 51152.71 toks/s, output: 49.95 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:29<00:12, 46.15it/s, est. speed input: 51106.74 toks/s, output: 49.91 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:29<00:12, 46.16it/s, est. speed input: 51062.29 toks/s, output: 49.87 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:30<00:11, 46.16it/s, est. speed input: 51018.79 toks/s, output: 49.82 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:30<00:11, 46.16it/s, est. speed input: 50976.48 toks/s, output: 49.78 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:30<00:11, 46.17it/s, est. speed input: 50935.09 toks/s, output: 49.74 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:31<00:10, 46.16it/s, est. speed input: 50894.42 toks/s, output: 49.70 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:31<00:10, 46.16it/s, est. speed input: 50854.58 toks/s, output: 49.66 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:31<00:10, 46.16it/s, est. speed input: 50815.61 toks/s, output: 49.62 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:32<00:09, 46.15it/s, est. speed input: 50777.31 toks/s, output: 49.59 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:32<00:09, 46.16it/s, est. speed input: 50740.04 toks/s, output: 49.55 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:33<00:08, 46.15it/s, est. speed input: 50703.50 toks/s, output: 49.52 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:33<00:08, 46.15it/s, est. speed input: 50667.45 toks/s, output: 49.48 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:33<00:08, 46.15it/s, est. speed input: 50632.29 toks/s, output: 49.45 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:34<00:07, 46.15it/s, est. speed input: 50598.20 toks/s, output: 49.41 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:34<00:07, 46.14it/s, est. speed input: 50563.91 toks/s, output: 49.38 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:34<00:07, 46.15it/s, est. speed input: 50531.30 toks/s, output: 49.35 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:35<00:06, 46.16it/s, est. speed input: 50499.43 toks/s, output: 49.32 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:35<00:06, 46.16it/s, est. speed input: 50467.58 toks/s, output: 49.28 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:35<00:06, 46.14it/s, est. speed input: 50436.09 toks/s, output: 49.25 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:36<00:05, 46.15it/s, est. speed input: 50405.75 toks/s, output: 49.22 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:36<00:05, 46.14it/s, est. speed input: 50375.50 toks/s, output: 49.19 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:36<00:05, 46.16it/s, est. speed input: 50346.65 toks/s, output: 49.17 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:37<00:04, 46.14it/s, est. speed input: 50317.30 toks/s, output: 49.14 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:37<00:04, 46.14it/s, est. speed input: 50288.92 toks/s, output: 49.11 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:37<00:04, 46.15it/s, est. speed input: 50261.27 toks/s, output: 49.08 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:38<00:03, 46.97it/s, est. speed input: 50260.52 toks/s, output: 49.08 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:38<00:03, 46.71it/s, est. speed input: 50233.32 toks/s, output: 49.06 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:38<00:03, 46.55it/s, est. speed input: 50206.96 toks/s, output: 49.03 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:39<00:02, 46.43it/s, est. speed input: 50181.03 toks/s, output: 49.00 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:39<00:02, 46.34it/s, est. speed input: 50155.25 toks/s, output: 48.98 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:39<00:02, 46.29it/s, est. speed input: 50130.26 toks/s, output: 48.96 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:40<00:01, 46.27it/s, est. speed input: 50106.06 toks/s, output: 48.93 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:40<00:01, 46.21it/s, est. speed input: 50081.18 toks/s, output: 48.91 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:40<00:00, 46.19it/s, est. speed input: 50057.19 toks/s, output: 48.88 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:41<00:00, 46.17it/s, est. speed input: 50033.45 toks/s, output: 48.86 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:41<00:00, 45.83it/s, est. speed input: 50000.18 toks/s, output: 48.83 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:41<00:00, 45.83it/s, est. speed input: 50344.11 toks/s, output: 49.16 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:41<00:00, 49.16it/s, est. speed input: 50344.11 toks/s, output: 49.16 toks/s]
[rank0]:[W128 00:48:15.695479321 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 72.5s

测试结果:
  Requests/s:   46.16
  Tokens/s:     47310.48
  Total Reqs:   2048
  Elapsed:      44.37s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     47264.33

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:48:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3505172) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3505172) WARNING 01-28 00:48:41 [backends.py:609] Failed to read file <frozen os>
Throughput: 45.93 requests/s, 47074.79 total tokens/s, 45.93 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 00:48:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:48:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:48:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:48:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:48:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:48:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:48:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:48:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:48:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:48:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:48:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:48:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:48:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:48:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:48:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:48:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:48:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:48:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:48:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:48:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:48:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:48:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:48:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:48:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:48:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:48:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:48:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:48:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3505172) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3505172) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.66it/s]
(EngineCore_DP0 pid=3505172) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.66it/s]
(EngineCore_DP0 pid=3505172) 
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3505172) [2026-01-28 00:48:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3505172) [rank0]:W0128 00:48:44.796000 3505172 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3505172) [rank0]:W0128 00:48:44.846000 3505172 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3505172) [rank0]:W0128 00:48:45.585000 3505172 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3505172) [rank0]:W0128 00:48:45.662000 3505172 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3505172) 2026-01-28 00:48:48,447 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3505172) 2026-01-28 00:48:48,475 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3505172) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:04,  2.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 13.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 17.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 13.49it/s]
(EngineCore_DP0 pid=3505172) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.35it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 22.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 20.25it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:05, 719.48it/s]
Adding requests:   4%|▎         | 148/4096 [00:00<00:05, 738.87it/s]
Adding requests:   5%|▌         | 222/4096 [00:00<00:05, 737.41it/s]
Adding requests:   7%|▋         | 298/4096 [00:00<00:05, 743.26it/s]
Adding requests:   9%|▉         | 373/4096 [00:00<00:05, 740.59it/s]
Adding requests:  11%|█         | 448/4096 [00:00<00:04, 743.03it/s]
Adding requests:  13%|█▎        | 523/4096 [00:00<00:04, 734.94it/s]
Adding requests:  15%|█▍        | 600/4096 [00:00<00:04, 742.29it/s]
Adding requests:  16%|█▋        | 675/4096 [00:00<00:04, 729.84it/s]
Adding requests:  18%|█▊        | 751/4096 [00:01<00:04, 738.77it/s]
Adding requests:  20%|██        | 825/4096 [00:01<00:04, 729.62it/s]
Adding requests:  22%|██▏       | 903/4096 [00:01<00:04, 743.99it/s]
Adding requests:  24%|██▍       | 980/4096 [00:01<00:04, 749.51it/s]
Adding requests:  26%|██▌       | 1058/4096 [00:01<00:04, 758.29it/s]
Adding requests:  28%|██▊       | 1134/4096 [00:01<00:04, 734.02it/s]
Adding requests:  30%|██▉       | 1213/4096 [00:01<00:03, 748.30it/s]
Adding requests:  31%|███▏      | 1288/4096 [00:01<00:03, 740.77it/s]
Adding requests:  33%|███▎      | 1367/4096 [00:01<00:03, 752.48it/s]
Adding requests:  35%|███▌      | 1446/4096 [00:01<00:03, 761.92it/s]
Adding requests:  37%|███▋      | 1527/4096 [00:02<00:03, 774.74it/s]
Adding requests:  39%|███▉      | 1608/4096 [00:02<00:03, 783.91it/s]
Adding requests:  41%|████      | 1687/4096 [00:02<00:03, 772.15it/s]
Adding requests:  43%|████▎     | 1765/4096 [00:02<00:03, 771.08it/s]
Adding requests:  45%|████▌     | 1845/4096 [00:02<00:02, 778.03it/s]
Adding requests:  47%|████▋     | 1923/4096 [00:02<00:02, 774.73it/s]
Adding requests:  49%|████▉     | 2002/4096 [00:02<00:02, 778.75it/s]
Adding requests:  51%|█████     | 2082/4096 [00:02<00:02, 783.18it/s]
Adding requests:  53%|█████▎    | 2161/4096 [00:02<00:02, 769.17it/s]
Adding requests:  55%|█████▍    | 2239/4096 [00:02<00:02, 769.54it/s]
Adding requests:  57%|█████▋    | 2317/4096 [00:03<00:02, 754.74it/s]
Adding requests:  58%|█████▊    | 2393/4096 [00:03<00:02, 754.96it/s]
Adding requests:  60%|██████    | 2470/4096 [00:03<00:02, 757.86it/s]
Adding requests:  62%|██████▏   | 2546/4096 [00:03<00:02, 751.38it/s]
Adding requests:  64%|██████▍   | 2624/4096 [00:03<00:01, 757.70it/s]
Adding requests:  66%|██████▌   | 2700/4096 [00:03<00:01, 757.09it/s]
Adding requests:  68%|██████▊   | 2776/4096 [00:03<00:01, 754.43it/s]
Adding requests:  70%|██████▉   | 2852/4096 [00:03<00:01, 741.54it/s]
Adding requests:  71%|███████▏  | 2928/4096 [00:03<00:01, 744.41it/s]
Adding requests:  73%|███████▎  | 3003/4096 [00:03<00:01, 743.35it/s]
Adding requests:  75%|███████▌  | 3079/4096 [00:04<00:01, 747.77it/s]
Adding requests:  77%|███████▋  | 3157/4096 [00:04<00:01, 756.90it/s]
Adding requests:  79%|███████▉  | 3235/4096 [00:04<00:01, 762.93it/s]
Adding requests:  81%|████████  | 3312/4096 [00:04<00:01, 764.70it/s]
Adding requests:  83%|████████▎ | 3390/4096 [00:04<00:00, 765.08it/s]
Adding requests:  85%|████████▍ | 3467/4096 [00:04<00:00, 762.50it/s]
Adding requests:  87%|████████▋ | 3545/4096 [00:04<00:00, 767.30it/s]
Adding requests:  88%|████████▊ | 3622/4096 [00:04<00:00, 762.97it/s]
Adding requests:  90%|█████████ | 3699/4096 [00:04<00:00, 754.95it/s]
Adding requests:  92%|█████████▏| 3779/4096 [00:04<00:00, 764.81it/s]
Adding requests:  94%|█████████▍| 3858/4096 [00:05<00:00, 770.81it/s]
Adding requests:  96%|█████████▌| 3936/4096 [00:05<00:00, 770.72it/s]
Adding requests:  98%|█████████▊| 4016/4096 [00:05<00:00, 777.27it/s]
Adding requests: 100%|█████████▉| 4094/4096 [00:05<00:00, 772.20it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 757.55it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:00<00:04, 897.09it/s, est. speed input: 918670.45 toks/s, output: 897.11 toks/s]
Processed prompts:   8%|▊         | 316/4096 [00:01<00:23, 158.46it/s, est. speed input: 197079.19 toks/s, output: 192.46 toks/s]
Processed prompts:   9%|▊         | 357/4096 [00:03<00:43, 86.38it/s, est. speed input: 120593.46 toks/s, output: 117.77 toks/s] 
Processed prompts:   9%|▉         | 386/4096 [00:03<00:50, 73.63it/s, est. speed input: 106061.06 toks/s, output: 103.57 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:04<00:56, 65.64it/s, est. speed input: 96799.57 toks/s, output: 94.53 toks/s]  
Processed prompts:  11%|█         | 450/4096 [00:05<01:00, 59.89it/s, est. speed input: 90049.97 toks/s, output: 87.94 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:05<01:04, 55.81it/s, est. speed input: 84917.83 toks/s, output: 82.93 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:06<01:07, 52.91it/s, est. speed input: 80882.38 toks/s, output: 78.99 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:07<01:09, 50.87it/s, est. speed input: 77625.86 toks/s, output: 75.81 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:07<01:11, 49.41it/s, est. speed input: 74934.88 toks/s, output: 73.18 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:08<01:12, 48.39it/s, est. speed input: 72682.35 toks/s, output: 70.98 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:09<01:12, 47.69it/s, est. speed input: 70770.76 toks/s, output: 69.11 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:09<01:12, 47.16it/s, est. speed input: 69115.58 toks/s, output: 67.50 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:10<01:12, 46.80it/s, est. speed input: 67677.98 toks/s, output: 66.09 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:11<01:12, 46.56it/s, est. speed input: 66420.79 toks/s, output: 64.86 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:12<01:11, 46.38it/s, est. speed input: 65303.23 toks/s, output: 63.77 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:12<01:11, 46.26it/s, est. speed input: 64309.47 toks/s, output: 62.80 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:13<01:10, 46.16it/s, est. speed input: 63416.69 toks/s, output: 61.93 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:14<01:10, 46.10it/s, est. speed input: 62612.39 toks/s, output: 61.14 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:14<01:09, 46.06it/s, est. speed input: 61884.02 toks/s, output: 60.43 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:15<01:08, 46.02it/s, est. speed input: 61218.41 toks/s, output: 59.78 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:16<01:08, 45.99it/s, est. speed input: 60610.51 toks/s, output: 59.19 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:16<01:07, 45.97it/s, est. speed input: 60052.00 toks/s, output: 58.64 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:17<01:06, 45.95it/s, est. speed input: 59536.50 toks/s, output: 58.14 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:18<01:06, 45.94it/s, est. speed input: 59060.71 toks/s, output: 57.68 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:19<01:05, 45.94it/s, est. speed input: 58621.44 toks/s, output: 57.25 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:19<01:04, 45.93it/s, est. speed input: 58211.36 toks/s, output: 56.85 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:20<01:03, 46.28it/s, est. speed input: 57880.13 toks/s, output: 56.52 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:21<01:03, 46.18it/s, est. speed input: 57522.23 toks/s, output: 56.17 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:21<01:02, 46.10it/s, est. speed input: 57186.22 toks/s, output: 55.85 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:22<01:01, 46.04it/s, est. speed input: 56870.96 toks/s, output: 55.54 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:23<01:01, 45.96it/s, est. speed input: 56570.38 toks/s, output: 55.24 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:23<01:00, 45.95it/s, est. speed input: 56291.82 toks/s, output: 54.97 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:24<00:59, 45.93it/s, est. speed input: 56028.25 toks/s, output: 54.72 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:25<00:59, 45.94it/s, est. speed input: 55781.31 toks/s, output: 54.47 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:25<00:58, 45.92it/s, est. speed input: 55545.59 toks/s, output: 54.24 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:26<00:57, 45.92it/s, est. speed input: 55323.09 toks/s, output: 54.03 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:27<00:57, 45.92it/s, est. speed input: 55111.72 toks/s, output: 53.82 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:28<00:56, 45.93it/s, est. speed input: 54911.46 toks/s, output: 53.62 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:28<00:55, 45.92it/s, est. speed input: 54720.07 toks/s, output: 53.44 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:29<00:55, 45.92it/s, est. speed input: 54537.72 toks/s, output: 53.26 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:30<00:54, 45.92it/s, est. speed input: 54364.02 toks/s, output: 53.09 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:30<00:53, 45.91it/s, est. speed input: 54197.81 toks/s, output: 52.93 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:31<00:52, 45.91it/s, est. speed input: 54038.66 toks/s, output: 52.77 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:32<00:52, 45.91it/s, est. speed input: 53887.29 toks/s, output: 52.62 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:32<00:51, 45.91it/s, est. speed input: 53741.65 toks/s, output: 52.48 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:33<00:50, 45.91it/s, est. speed input: 53602.52 toks/s, output: 52.35 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:34<00:50, 45.91it/s, est. speed input: 53468.79 toks/s, output: 52.22 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:35<00:49, 45.92it/s, est. speed input: 53341.00 toks/s, output: 52.09 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:35<00:48, 46.31it/s, est. speed input: 53246.89 toks/s, output: 52.00 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:36<00:47, 46.18it/s, est. speed input: 53127.32 toks/s, output: 51.88 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:37<00:47, 46.11it/s, est. speed input: 53013.01 toks/s, output: 51.77 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:37<00:46, 46.05it/s, est. speed input: 52902.49 toks/s, output: 51.66 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:38<00:45, 46.00it/s, est. speed input: 52795.65 toks/s, output: 51.56 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:39<00:45, 45.98it/s, est. speed input: 52692.98 toks/s, output: 51.46 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:39<00:44, 45.96it/s, est. speed input: 52593.78 toks/s, output: 51.36 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:40<00:43, 45.94it/s, est. speed input: 52497.54 toks/s, output: 51.27 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:41<00:43, 45.93it/s, est. speed input: 52404.85 toks/s, output: 51.18 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:42<00:42, 45.92it/s, est. speed input: 52315.11 toks/s, output: 51.09 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:42<00:41, 45.92it/s, est. speed input: 52228.59 toks/s, output: 51.00 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:43<00:41, 45.91it/s, est. speed input: 52144.56 toks/s, output: 50.92 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:44<00:40, 45.91it/s, est. speed input: 52063.55 toks/s, output: 50.84 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:44<00:39, 45.91it/s, est. speed input: 51984.73 toks/s, output: 50.77 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:45<00:38, 45.90it/s, est. speed input: 51908.24 toks/s, output: 50.69 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:46<00:38, 45.91it/s, est. speed input: 51834.65 toks/s, output: 50.62 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:46<00:37, 45.90it/s, est. speed input: 51762.44 toks/s, output: 50.55 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:47<00:36, 45.91it/s, est. speed input: 51693.41 toks/s, output: 50.48 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:48<00:36, 45.90it/s, est. speed input: 51624.89 toks/s, output: 50.41 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:48<00:35, 45.90it/s, est. speed input: 51559.23 toks/s, output: 50.35 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:49<00:34, 46.29it/s, est. speed input: 51515.67 toks/s, output: 50.31 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:50<00:33, 46.17it/s, est. speed input: 51453.24 toks/s, output: 50.25 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:51<00:33, 46.10it/s, est. speed input: 51392.68 toks/s, output: 50.19 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:51<00:32, 46.04it/s, est. speed input: 51333.53 toks/s, output: 50.13 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:52<00:31, 46.00it/s, est. speed input: 51275.95 toks/s, output: 50.07 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:53<00:31, 45.97it/s, est. speed input: 51219.83 toks/s, output: 50.02 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:53<00:30, 45.94it/s, est. speed input: 51165.12 toks/s, output: 49.97 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:54<00:29, 45.93it/s, est. speed input: 51111.93 toks/s, output: 49.91 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:55<00:29, 45.92it/s, est. speed input: 51059.89 toks/s, output: 49.86 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:55<00:28, 45.91it/s, est. speed input: 51009.13 toks/s, output: 49.81 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:56<00:27, 45.90it/s, est. speed input: 50959.59 toks/s, output: 49.77 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:57<00:27, 45.90it/s, est. speed input: 50911.45 toks/s, output: 49.72 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:58<00:26, 45.91it/s, est. speed input: 50864.88 toks/s, output: 49.67 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:58<00:25, 45.90it/s, est. speed input: 50818.64 toks/s, output: 49.63 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:59<00:25, 45.89it/s, est. speed input: 50773.66 toks/s, output: 49.58 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:00<00:24, 45.89it/s, est. speed input: 50729.78 toks/s, output: 49.54 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:00<00:23, 45.89it/s, est. speed input: 50686.88 toks/s, output: 49.50 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:01<00:22, 45.89it/s, est. speed input: 50644.94 toks/s, output: 49.46 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:02<00:22, 45.89it/s, est. speed input: 50604.16 toks/s, output: 49.42 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:02<00:21, 45.90it/s, est. speed input: 50564.23 toks/s, output: 49.38 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:03<00:20, 45.89it/s, est. speed input: 50524.91 toks/s, output: 49.34 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:04<00:20, 45.90it/s, est. speed input: 50487.06 toks/s, output: 49.30 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:04<00:19, 45.90it/s, est. speed input: 50449.56 toks/s, output: 49.27 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:05<00:18, 45.90it/s, est. speed input: 50413.09 toks/s, output: 49.23 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:06<00:18, 45.90it/s, est. speed input: 50377.01 toks/s, output: 49.20 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:07<00:17, 45.90it/s, est. speed input: 50342.19 toks/s, output: 49.16 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:07<00:16, 45.90it/s, est. speed input: 50307.70 toks/s, output: 49.13 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:08<00:15, 45.90it/s, est. speed input: 50273.97 toks/s, output: 49.10 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:09<00:15, 45.90it/s, est. speed input: 50241.18 toks/s, output: 49.06 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:09<00:14, 45.90it/s, est. speed input: 50208.76 toks/s, output: 49.03 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:10<00:13, 45.90it/s, est. speed input: 50177.01 toks/s, output: 49.00 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:11<00:13, 45.90it/s, est. speed input: 50145.97 toks/s, output: 48.97 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:11<00:12, 45.90it/s, est. speed input: 50115.47 toks/s, output: 48.94 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:12<00:11, 45.90it/s, est. speed input: 50085.63 toks/s, output: 48.91 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:13<00:11, 45.90it/s, est. speed input: 50056.34 toks/s, output: 48.88 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:14<00:10, 45.89it/s, est. speed input: 50027.37 toks/s, output: 48.85 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:14<00:09, 45.89it/s, est. speed input: 49999.06 toks/s, output: 48.83 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:15<00:09, 45.90it/s, est. speed input: 49971.46 toks/s, output: 48.80 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:16<00:08, 45.90it/s, est. speed input: 49944.46 toks/s, output: 48.77 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:16<00:07, 45.90it/s, est. speed input: 49917.53 toks/s, output: 48.75 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:17<00:06, 45.90it/s, est. speed input: 49891.29 toks/s, output: 48.72 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:18<00:06, 45.90it/s, est. speed input: 49865.52 toks/s, output: 48.70 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:18<00:05, 45.90it/s, est. speed input: 49840.34 toks/s, output: 48.67 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:19<00:04, 45.89it/s, est. speed input: 49814.99 toks/s, output: 48.65 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:20<00:04, 45.88it/s, est. speed input: 49790.35 toks/s, output: 48.62 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:21<00:03, 45.89it/s, est. speed input: 49766.43 toks/s, output: 48.60 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:21<00:02, 45.89it/s, est. speed input: 49742.67 toks/s, output: 48.58 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:22<00:02, 45.89it/s, est. speed input: 49719.44 toks/s, output: 48.55 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:23<00:01, 46.28it/s, est. speed input: 49708.42 toks/s, output: 48.54 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:23<00:00, 46.59it/s, est. speed input: 49698.23 toks/s, output: 48.53 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:23<00:00, 46.59it/s, est. speed input: 50064.74 toks/s, output: 48.89 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:23<00:00, 48.89it/s, est. speed input: 50064.74 toks/s, output: 48.89 toks/s]
[rank0]:[W128 00:50:20.828987281 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 124.2s

测试结果:
  Requests/s:   45.93
  Tokens/s:     47074.79
  Total Reqs:   4096
  Elapsed:      89.19s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     47028.87


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,56.4208,28943.8530,2.2687
1024,1024,1,128,128,44.7837,45903.2952,2.8582
2048,1024,2,256,128,48.3470,49555.6701,5.2951
4096,1024,4,512,128,49.3794,50613.8532,10.3687
8192,1024,8,1024,128,46.9877,48162.4188,21.7929
16384,1024,16,2048,128,46.1566,47310.4836,44.3707
32768,1024,32,4096,128,45.9266,47074.7920,89.1857

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:50:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3506994) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3506994) WARNING 01-28 00:50:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.13 requests/s, 28792.58 total tokens/s, 56.13 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:50:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:50:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:50:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:50:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:50:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:50:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:50:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:50:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:50:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:50:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:50:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:50:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:50:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:50:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:50:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:50:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:50:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:50:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:29] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:29] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:29] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:29] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:29] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3506994) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3506994) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.29it/s]
(EngineCore_DP0 pid=3506994) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.29it/s]
(EngineCore_DP0 pid=3506994) 
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3506994) [2026-01-28 00:50:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3506994) 2026-01-28 00:50:40,955 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3506994) 2026-01-28 00:50:40,971 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3506994) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.77it/s]
(EngineCore_DP0 pid=3506994) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.25it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  87%|████████▋ | 111/128 [00:00<00:00, 1105.14it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1125.07it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:14,  8.65it/s, est. speed input: 4428.13 toks/s, output: 8.65 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 39.50it/s, est. speed input: 17839.28 toks/s, output: 34.84 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 49.62it/s, est. speed input: 22400.19 toks/s, output: 43.75 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:01, 54.57it/s, est. speed input: 24740.82 toks/s, output: 48.32 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 57.29it/s, est. speed input: 26143.43 toks/s, output: 51.06 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 58.74it/s, est. speed input: 27038.91 toks/s, output: 52.81 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 59.41it/s, est. speed input: 27627.86 toks/s, output: 53.96 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 60.03it/s, est. speed input: 28101.20 toks/s, output: 54.88 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 60.44it/s, est. speed input: 28470.40 toks/s, output: 55.61 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 61.07it/s, est. speed input: 28818.30 toks/s, output: 56.29 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 61.47it/s, est. speed input: 29098.13 toks/s, output: 56.83 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 61.72it/s, est. speed input: 29328.69 toks/s, output: 57.28 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 62.10it/s, est. speed input: 29549.11 toks/s, output: 57.71 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 62.24it/s, est. speed input: 29724.31 toks/s, output: 58.05 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 62.21it/s, est. speed input: 29863.05 toks/s, output: 58.33 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 62.00it/s, est. speed input: 29966.16 toks/s, output: 58.53 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 62.04it/s, est. speed input: 30073.84 toks/s, output: 58.74 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 62.04it/s, est. speed input: 30167.37 toks/s, output: 58.92 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 62.08it/s, est. speed input: 30254.96 toks/s, output: 59.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 62.08it/s, est. speed input: 30269.21 toks/s, output: 59.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 59.12it/s, est. speed input: 30269.21 toks/s, output: 59.12 toks/s]
[rank0]:[W128 00:50:44.722011170 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.9s

测试结果:
  Requests/s:   56.13
  Tokens/s:     28792.58
  Total Reqs:   128
  Elapsed:      2.28s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     28736.45

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:50:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3507680) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3507680) WARNING 01-28 00:50:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.21 requests/s, 39169.80 total tokens/s, 38.21 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:50:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:50:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:50:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:50:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:50:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:50:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:50:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:50:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:50:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:50:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:50:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:50:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:50:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:50:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:50:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:50:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:50:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:50:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:50:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3507680) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3507680) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=3507680) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=3507680) 
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3507680) [2026-01-28 00:50:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3507680) 2026-01-28 00:51:06,097 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3507680) 2026-01-28 00:51:06,122 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3507680) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.44it/s]
(EngineCore_DP0 pid=3507680) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.06it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 629.60it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 678.05it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 35.37it/s, est. speed input: 36227.25 toks/s, output: 35.38 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 38.66it/s, est. speed input: 39102.74 toks/s, output: 38.18 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:02, 39.63it/s, est. speed input: 40004.09 toks/s, output: 39.07 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 40.05it/s, est. speed input: 40427.41 toks/s, output: 39.48 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 40.34it/s, est. speed input: 40711.78 toks/s, output: 39.76 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 40.50it/s, est. speed input: 40891.91 toks/s, output: 39.93 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 40.52it/s, est. speed input: 40984.70 toks/s, output: 40.02 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:02, 40.60it/s, est. speed input: 41083.36 toks/s, output: 40.12 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 40.62it/s, est. speed input: 41144.93 toks/s, output: 40.18 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:01, 40.67it/s, est. speed input: 41207.38 toks/s, output: 40.24 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 40.66it/s, est. speed input: 41243.24 toks/s, output: 40.28 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 40.68it/s, est. speed input: 41282.56 toks/s, output: 40.31 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 40.71it/s, est. speed input: 41320.17 toks/s, output: 40.35 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 40.73it/s, est. speed input: 41352.42 toks/s, output: 40.38 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 40.70it/s, est. speed input: 41369.17 toks/s, output: 40.40 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:01, 40.54it/s, est. speed input: 41353.37 toks/s, output: 40.38 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 40.60it/s, est. speed input: 41374.74 toks/s, output: 40.40 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:00, 40.58it/s, est. speed input: 41383.16 toks/s, output: 40.41 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 40.66it/s, est. speed input: 41405.49 toks/s, output: 40.43 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 40.75it/s, est. speed input: 41432.50 toks/s, output: 40.46 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 40.71it/s, est. speed input: 41439.46 toks/s, output: 40.47 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 40.73it/s, est. speed input: 41454.28 toks/s, output: 40.48 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 40.71it/s, est. speed input: 41462.27 toks/s, output: 40.49 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 40.73it/s, est. speed input: 41475.02 toks/s, output: 40.50 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 40.73it/s, est. speed input: 41483.40 toks/s, output: 40.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.73it/s, est. speed input: 41492.22 toks/s, output: 40.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.52it/s, est. speed input: 41492.22 toks/s, output: 40.52 toks/s]
[rank0]:[W128 00:51:10.700656428 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.0s

测试结果:
  Requests/s:   38.21
  Tokens/s:     39169.80
  Total Reqs:   128
  Elapsed:      3.35s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     39131.58

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:51:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3508341) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3508341) WARNING 01-28 00:51:26 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.00 requests/s, 43046.68 total tokens/s, 42.00 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 00:51:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:51:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:51:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:51:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:51:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:51:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:51:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:51:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:51:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:51:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:51:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:51:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:51:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:51:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:51:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:51:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:51:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:51:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:21] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:21] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:21] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:21] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:21] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3508341) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3508341) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.57it/s]
(EngineCore_DP0 pid=3508341) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.57it/s]
(EngineCore_DP0 pid=3508341) 
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3508341) [2026-01-28 00:51:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3508341) 2026-01-28 00:51:32,251 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3508341) 2026-01-28 00:51:32,266 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3508341) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 21.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 21.89it/s]
(EngineCore_DP0 pid=3508341) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  6.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 10.36it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  25%|██▌       | 64/256 [00:00<00:00, 633.64it/s]
Adding requests:  54%|█████▎    | 137/256 [00:00<00:00, 689.36it/s]
Adding requests:  82%|████████▏ | 211/256 [00:00<00:00, 708.65it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 711.93it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 16/256 [00:00<00:01, 150.93it/s, est. speed input: 154570.64 toks/s, output: 150.94 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:00<00:03, 60.39it/s, est. speed input: 67952.80 toks/s, output: 66.36 toks/s]   
Processed prompts:  16%|█▌        | 41/256 [00:00<00:03, 55.70it/s, est. speed input: 62635.24 toks/s, output: 61.17 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:00<00:04, 49.21it/s, est. speed input: 57220.99 toks/s, output: 55.88 toks/s]
Processed prompts:  21%|██        | 54/256 [00:01<00:04, 47.34it/s, est. speed input: 55256.35 toks/s, output: 53.96 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:01<00:04, 46.02it/s, est. speed input: 53823.12 toks/s, output: 52.56 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:01<00:04, 45.05it/s, est. speed input: 52701.75 toks/s, output: 51.47 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:04, 44.30it/s, est. speed input: 51786.00 toks/s, output: 50.57 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:04, 43.77it/s, est. speed input: 51040.18 toks/s, output: 49.84 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:03, 43.44it/s, est. speed input: 50431.63 toks/s, output: 49.25 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:03, 43.19it/s, est. speed input: 49911.73 toks/s, output: 48.74 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:01<00:03, 42.94it/s, est. speed input: 49446.87 toks/s, output: 48.29 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:03, 42.81it/s, est. speed input: 49053.45 toks/s, output: 47.90 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:02<00:03, 42.71it/s, est. speed input: 48707.59 toks/s, output: 47.57 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:03, 42.68it/s, est. speed input: 48410.66 toks/s, output: 47.28 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:02<00:03, 42.52it/s, est. speed input: 48118.93 toks/s, output: 46.99 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:02<00:03, 42.46it/s, est. speed input: 47866.65 toks/s, output: 46.74 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:02<00:02, 42.51it/s, est. speed input: 47657.93 toks/s, output: 46.54 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:02<00:02, 42.43it/s, est. speed input: 47447.20 toks/s, output: 46.34 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:03<00:02, 42.38it/s, est. speed input: 47258.33 toks/s, output: 46.15 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:02, 42.43it/s, est. speed input: 47097.92 toks/s, output: 45.99 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:03<00:02, 42.43it/s, est. speed input: 46946.55 toks/s, output: 45.85 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:03<00:02, 42.41it/s, est. speed input: 46804.17 toks/s, output: 45.71 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:03<00:02, 42.47it/s, est. speed input: 46682.91 toks/s, output: 45.59 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:03<00:01, 42.51it/s, est. speed input: 46570.88 toks/s, output: 45.48 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:03<00:01, 42.54it/s, est. speed input: 46465.95 toks/s, output: 45.38 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 42.52it/s, est. speed input: 46364.30 toks/s, output: 45.28 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:04<00:01, 42.55it/s, est. speed input: 46273.92 toks/s, output: 45.19 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:04<00:01, 42.60it/s, est. speed input: 46192.32 toks/s, output: 45.11 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:04<00:01, 42.51it/s, est. speed input: 46102.35 toks/s, output: 45.02 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:04<00:01, 42.54it/s, est. speed input: 46027.81 toks/s, output: 44.95 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:04<00:00, 42.49it/s, est. speed input: 45950.15 toks/s, output: 44.87 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:04<00:00, 42.46it/s, est. speed input: 45877.26 toks/s, output: 44.80 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:05<00:00, 42.39it/s, est. speed input: 45804.01 toks/s, output: 44.73 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:05<00:00, 42.41it/s, est. speed input: 45740.79 toks/s, output: 44.67 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:05<00:00, 42.43it/s, est. speed input: 45681.78 toks/s, output: 44.61 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:05<00:00, 42.39it/s, est. speed input: 45620.77 toks/s, output: 44.55 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:05<00:00, 42.39it/s, est. speed input: 45565.26 toks/s, output: 44.50 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 42.39it/s, est. speed input: 45715.44 toks/s, output: 44.64 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 44.64it/s, est. speed input: 45715.44 toks/s, output: 44.64 toks/s]
[rank0]:[W128 00:51:39.602965162 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 28.9s

测试结果:
  Requests/s:   42.00
  Tokens/s:     43046.68
  Total Reqs:   256
  Elapsed:      6.10s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     43004.68

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:51:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3509013) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3509013) WARNING 01-28 00:51:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 41.67 requests/s, 42715.33 total tokens/s, 41.67 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 00:51:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:51:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:51:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:51:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:51:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:51:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:51:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:51:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:51:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:51:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:51:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:51:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:51:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:51:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:51:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:51:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:51:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:51:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:51:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:50] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3509013) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3509013) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.10it/s]
(EngineCore_DP0 pid=3509013) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.10it/s]
(EngineCore_DP0 pid=3509013) 
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3509013) [2026-01-28 00:51:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3509013) 2026-01-28 00:52:01,594 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3509013) 2026-01-28 00:52:01,610 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3509013) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 22.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 21.65it/s]
(EngineCore_DP0 pid=3509013) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.52it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 72/512 [00:00<00:00, 713.37it/s]
Adding requests:  29%|██▊       | 146/512 [00:00<00:00, 726.26it/s]
Adding requests:  43%|████▎     | 219/512 [00:00<00:00, 720.84it/s]
Adding requests:  57%|█████▋    | 294/512 [00:00<00:00, 729.55it/s]
Adding requests:  72%|███████▏  | 370/512 [00:00<00:00, 738.03it/s]
Adding requests:  87%|████████▋ | 445/512 [00:00<00:00, 739.34it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 735.88it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 30/512 [00:00<00:02, 161.02it/s, est. speed input: 164898.04 toks/s, output: 161.02 toks/s]
Processed prompts:   9%|▉         | 47/512 [00:00<00:06, 74.37it/s, est. speed input: 84903.48 toks/s, output: 82.91 toks/s]   
Processed prompts:  11%|█         | 57/512 [00:00<00:06, 66.69it/s, est. speed input: 77007.83 toks/s, output: 75.20 toks/s]
Processed prompts:  13%|█▎        | 65/512 [00:00<00:07, 58.49it/s, est. speed input: 70163.58 toks/s, output: 68.52 toks/s]
Processed prompts:  14%|█▍        | 72/512 [00:01<00:08, 51.54it/s, est. speed input: 64745.76 toks/s, output: 63.23 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:01<00:09, 45.25it/s, est. speed input: 60105.20 toks/s, output: 58.70 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:01<00:09, 44.27it/s, est. speed input: 57974.65 toks/s, output: 56.62 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:01<00:09, 43.55it/s, est. speed input: 56298.57 toks/s, output: 54.98 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:01<00:09, 43.10it/s, est. speed input: 54975.06 toks/s, output: 53.69 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:02<00:09, 42.81it/s, est. speed input: 53902.34 toks/s, output: 52.64 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:02<00:09, 42.56it/s, est. speed input: 52988.20 toks/s, output: 51.75 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:02<00:09, 42.39it/s, est. speed input: 52220.31 toks/s, output: 51.00 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:02<00:08, 42.27it/s, est. speed input: 51560.98 toks/s, output: 50.35 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:02<00:08, 42.17it/s, est. speed input: 50983.15 toks/s, output: 49.79 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:03<00:08, 42.11it/s, est. speed input: 50482.04 toks/s, output: 49.30 toks/s]
Processed prompts:  31%|███       | 158/512 [00:03<00:08, 42.10it/s, est. speed input: 50046.34 toks/s, output: 48.87 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:03<00:08, 42.09it/s, est. speed input: 49659.90 toks/s, output: 48.50 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:03<00:08, 42.08it/s, est. speed input: 49312.35 toks/s, output: 48.16 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:03<00:07, 42.05it/s, est. speed input: 48995.81 toks/s, output: 47.85 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:03<00:07, 42.07it/s, est. speed input: 48716.29 toks/s, output: 47.57 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:04<00:07, 42.11it/s, est. speed input: 48466.44 toks/s, output: 47.33 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:07, 42.10it/s, est. speed input: 48233.45 toks/s, output: 47.10 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:04<00:07, 42.11it/s, est. speed input: 48022.17 toks/s, output: 46.90 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:04<00:06, 42.06it/s, est. speed input: 47817.76 toks/s, output: 46.70 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:04<00:06, 41.99it/s, est. speed input: 47625.39 toks/s, output: 46.51 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:05<00:06, 42.00it/s, est. speed input: 47454.35 toks/s, output: 46.34 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:06, 41.99it/s, est. speed input: 47294.31 toks/s, output: 46.19 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:06, 42.01it/s, est. speed input: 47149.15 toks/s, output: 46.04 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:05<00:05, 42.01it/s, est. speed input: 47011.00 toks/s, output: 45.91 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:05<00:05, 42.00it/s, est. speed input: 46880.28 toks/s, output: 45.78 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:06<00:05, 42.03it/s, est. speed input: 46762.38 toks/s, output: 45.67 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:06<00:05, 42.06it/s, est. speed input: 46653.36 toks/s, output: 45.56 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:06<00:05, 42.07it/s, est. speed input: 46548.83 toks/s, output: 45.46 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:06<00:04, 42.07it/s, est. speed input: 46449.83 toks/s, output: 45.36 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:06<00:04, 42.05it/s, est. speed input: 46353.72 toks/s, output: 45.27 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:07<00:04, 42.02it/s, est. speed input: 46262.03 toks/s, output: 45.18 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:07<00:04, 42.01it/s, est. speed input: 46175.88 toks/s, output: 45.09 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:07<00:04, 42.04it/s, est. speed input: 46097.69 toks/s, output: 45.02 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:07<00:04, 42.04it/s, est. speed input: 46021.87 toks/s, output: 44.94 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:07<00:03, 42.03it/s, est. speed input: 45947.69 toks/s, output: 44.87 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:07<00:03, 42.02it/s, est. speed input: 45877.59 toks/s, output: 44.80 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:08<00:03, 42.02it/s, est. speed input: 45811.83 toks/s, output: 44.74 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:08<00:03, 42.01it/s, est. speed input: 45747.32 toks/s, output: 44.68 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:08<00:03, 42.01it/s, est. speed input: 45686.89 toks/s, output: 44.62 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:08<00:02, 41.99it/s, est. speed input: 45627.48 toks/s, output: 44.56 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:08<00:02, 41.99it/s, est. speed input: 45571.37 toks/s, output: 44.50 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:09<00:02, 42.00it/s, est. speed input: 45518.28 toks/s, output: 44.45 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:09<00:02, 42.01it/s, est. speed input: 45467.59 toks/s, output: 44.40 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:09<00:02, 42.02it/s, est. speed input: 45419.19 toks/s, output: 44.35 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:09<00:01, 42.01it/s, est. speed input: 45371.45 toks/s, output: 44.31 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:09<00:01, 42.03it/s, est. speed input: 45327.96 toks/s, output: 44.27 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:10<00:01, 42.02it/s, est. speed input: 45283.70 toks/s, output: 44.22 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:10<00:01, 42.03it/s, est. speed input: 45242.91 toks/s, output: 44.18 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:10<00:01, 42.05it/s, est. speed input: 45204.10 toks/s, output: 44.14 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:10<00:00, 42.04it/s, est. speed input: 45165.07 toks/s, output: 44.11 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:10<00:00, 42.05it/s, est. speed input: 45128.29 toks/s, output: 44.07 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:11<00:00, 42.03it/s, est. speed input: 45091.69 toks/s, output: 44.03 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:11<00:00, 42.01it/s, est. speed input: 45055.56 toks/s, output: 44.00 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:11<00:00, 42.00it/s, est. speed input: 45021.01 toks/s, output: 43.97 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:11<00:00, 43.30it/s, est. speed input: 45061.90 toks/s, output: 44.01 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 43.30it/s, est. speed input: 45238.31 toks/s, output: 44.18 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 44.18it/s, est. speed input: 45238.31 toks/s, output: 44.18 toks/s]
[rank0]:[W128 00:52:15.248297029 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.6s

测试结果:
  Requests/s:   41.67
  Tokens/s:     42715.33
  Total Reqs:   512
  Elapsed:      12.29s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     42673.66

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:52:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3509814) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3509814) WARNING 01-28 00:52:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.89 requests/s, 40885.40 total tokens/s, 39.89 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 00:52:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:52:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:52:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:52:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:52:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:52:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:52:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:52:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:52:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:52:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:52:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:52:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:52:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:52:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:52:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:52:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:52:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:52:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:52:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:52:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:52:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:52:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:52:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:52:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:52:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:52:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:52:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:52:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:27] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3509814) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3509814) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.07it/s]
(EngineCore_DP0 pid=3509814) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.07it/s]
(EngineCore_DP0 pid=3509814) 
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3509814) [2026-01-28 00:52:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3509814) 2026-01-28 00:52:38,171 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3509814) 2026-01-28 00:52:38,187 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3509814) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 13.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 12.70it/s]
(EngineCore_DP0 pid=3509814) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.25it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.50it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 74/1024 [00:00<00:01, 731.47it/s]
Adding requests:  15%|█▍        | 151/1024 [00:00<00:01, 746.32it/s]
Adding requests:  22%|██▏       | 227/1024 [00:00<00:01, 751.08it/s]
Adding requests:  30%|██▉       | 303/1024 [00:00<00:00, 745.17it/s]
Adding requests:  37%|███▋      | 380/1024 [00:00<00:00, 750.51it/s]
Adding requests:  45%|████▍     | 456/1024 [00:00<00:00, 744.21it/s]
Adding requests:  52%|█████▏    | 531/1024 [00:00<00:00, 727.47it/s]
Adding requests:  59%|█████▉    | 609/1024 [00:00<00:00, 742.38it/s]
Adding requests:  67%|██████▋   | 689/1024 [00:00<00:00, 756.93it/s]
Adding requests:  75%|███████▍  | 767/1024 [00:01<00:00, 763.38it/s]
Adding requests:  82%|████████▏ | 844/1024 [00:01<00:00, 745.58it/s]
Adding requests:  90%|█████████ | 923/1024 [00:01<00:00, 757.63it/s]
Adding requests:  98%|█████████▊| 1000/1024 [00:01<00:00, 759.72it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 751.35it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:00<00:02, 414.78it/s, est. speed input: 424790.06 toks/s, output: 414.80 toks/s]
Processed prompts:   9%|▉         | 92/1024 [00:01<00:13, 71.08it/s, est. speed input: 84155.62 toks/s, output: 82.18 toks/s]   
Processed prompts:  11%|█         | 112/1024 [00:01<00:14, 63.85it/s, est. speed input: 75525.14 toks/s, output: 73.75 toks/s]
Processed prompts:  12%|█▏        | 125/1024 [00:01<00:16, 53.54it/s, est. speed input: 66755.94 toks/s, output: 65.19 toks/s]
Processed prompts:  13%|█▎        | 135/1024 [00:02<00:16, 52.87it/s, est. speed input: 65294.86 toks/s, output: 63.76 toks/s]
Processed prompts:  14%|█▍        | 143/1024 [00:02<00:17, 50.11it/s, est. speed input: 63195.64 toks/s, output: 61.71 toks/s]
Processed prompts:  15%|█▍        | 150/1024 [00:02<00:18, 46.61it/s, est. speed input: 61039.90 toks/s, output: 59.61 toks/s]
Processed prompts:  15%|█▌        | 156/1024 [00:02<00:20, 42.44it/s, est. speed input: 58806.98 toks/s, output: 57.43 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:02<00:21, 39.19it/s, est. speed input: 56893.95 toks/s, output: 55.56 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:03<00:21, 39.41it/s, est. speed input: 55872.67 toks/s, output: 54.56 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:03<00:21, 39.59it/s, est. speed input: 54977.57 toks/s, output: 53.69 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:03<00:21, 39.74it/s, est. speed input: 54186.95 toks/s, output: 52.92 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:03<00:20, 39.82it/s, est. speed input: 53477.19 toks/s, output: 52.22 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:03<00:20, 39.87it/s, est. speed input: 52838.30 toks/s, output: 51.60 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:04<00:20, 39.92it/s, est. speed input: 52262.35 toks/s, output: 51.04 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:04<00:20, 39.93it/s, est. speed input: 51736.38 toks/s, output: 50.52 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:04<00:19, 39.97it/s, est. speed input: 51262.19 toks/s, output: 50.06 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:04<00:19, 39.99it/s, est. speed input: 50827.80 toks/s, output: 49.64 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:04<00:19, 40.02it/s, est. speed input: 50430.44 toks/s, output: 49.25 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:05<00:19, 40.01it/s, est. speed input: 50059.18 toks/s, output: 48.89 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:05<00:19, 40.02it/s, est. speed input: 49719.39 toks/s, output: 48.55 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:05<00:18, 40.03it/s, est. speed input: 49403.90 toks/s, output: 48.25 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:05<00:18, 40.03it/s, est. speed input: 49109.33 toks/s, output: 47.96 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:05<00:18, 40.05it/s, est. speed input: 48837.82 toks/s, output: 47.69 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:06<00:18, 40.03it/s, est. speed input: 48579.35 toks/s, output: 47.44 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:06<00:18, 40.04it/s, est. speed input: 48339.76 toks/s, output: 47.21 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:06<00:17, 40.03it/s, est. speed input: 48114.07 toks/s, output: 46.99 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:06<00:17, 40.06it/s, est. speed input: 47904.81 toks/s, output: 46.78 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:06<00:17, 40.04it/s, est. speed input: 47704.21 toks/s, output: 46.59 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:07<00:17, 40.03it/s, est. speed input: 47515.03 toks/s, output: 46.40 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:07<00:17, 40.03it/s, est. speed input: 47336.71 toks/s, output: 46.23 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:07<00:16, 40.04it/s, est. speed input: 47168.96 toks/s, output: 46.06 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:07<00:16, 40.04it/s, est. speed input: 47008.91 toks/s, output: 45.91 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:07<00:16, 40.06it/s, est. speed input: 46858.90 toks/s, output: 45.76 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:08<00:16, 40.04it/s, est. speed input: 46713.35 toks/s, output: 45.62 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:08<00:16, 40.04it/s, est. speed input: 46576.45 toks/s, output: 45.48 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:08<00:15, 40.04it/s, est. speed input: 46445.19 toks/s, output: 45.36 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:08<00:15, 40.05it/s, est. speed input: 46321.32 toks/s, output: 45.24 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:08<00:15, 40.06it/s, est. speed input: 46203.08 toks/s, output: 45.12 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:09<00:15, 40.04it/s, est. speed input: 46087.59 toks/s, output: 45.01 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:09<00:15, 40.05it/s, est. speed input: 45979.12 toks/s, output: 44.90 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:09<00:14, 40.04it/s, est. speed input: 45874.02 toks/s, output: 44.80 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:09<00:14, 40.05it/s, est. speed input: 45774.30 toks/s, output: 44.70 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:09<00:14, 40.04it/s, est. speed input: 45677.70 toks/s, output: 44.61 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:10<00:14, 40.02it/s, est. speed input: 45583.64 toks/s, output: 44.52 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:10<00:14, 40.02it/s, est. speed input: 45494.42 toks/s, output: 44.43 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:10<00:13, 40.03it/s, est. speed input: 45409.51 toks/s, output: 44.35 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:10<00:13, 40.03it/s, est. speed input: 45326.89 toks/s, output: 44.26 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:10<00:13, 40.04it/s, est. speed input: 45248.24 toks/s, output: 44.19 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:11<00:13, 40.03it/s, est. speed input: 45171.07 toks/s, output: 44.11 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:11<00:13, 40.01it/s, est. speed input: 45096.04 toks/s, output: 44.04 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:11<00:12, 39.99it/s, est. speed input: 45023.05 toks/s, output: 43.97 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:11<00:12, 39.99it/s, est. speed input: 44953.47 toks/s, output: 43.90 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:11<00:12, 39.98it/s, est. speed input: 44885.65 toks/s, output: 43.83 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:12<00:12, 39.95it/s, est. speed input: 44818.23 toks/s, output: 43.77 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:12<00:12, 39.96it/s, est. speed input: 44755.52 toks/s, output: 43.71 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:12<00:11, 39.96it/s, est. speed input: 44694.23 toks/s, output: 43.65 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:12<00:11, 39.99it/s, est. speed input: 44636.29 toks/s, output: 43.59 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:12<00:11, 39.99it/s, est. speed input: 44579.09 toks/s, output: 43.53 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:13<00:11, 39.97it/s, est. speed input: 44522.78 toks/s, output: 43.48 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:13<00:11, 39.96it/s, est. speed input: 44467.95 toks/s, output: 43.43 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:13<00:10, 39.96it/s, est. speed input: 44415.71 toks/s, output: 43.37 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:13<00:10, 39.96it/s, est. speed input: 44364.42 toks/s, output: 43.32 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:13<00:10, 39.95it/s, est. speed input: 44314.43 toks/s, output: 43.28 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:14<00:10, 39.92it/s, est. speed input: 44264.53 toks/s, output: 43.23 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:14<00:10, 39.93it/s, est. speed input: 44217.71 toks/s, output: 43.18 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:14<00:09, 39.95it/s, est. speed input: 44172.70 toks/s, output: 43.14 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:14<00:09, 39.95it/s, est. speed input: 44128.37 toks/s, output: 43.09 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:14<00:09, 39.94it/s, est. speed input: 44084.67 toks/s, output: 43.05 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:15<00:09, 39.94it/s, est. speed input: 44042.30 toks/s, output: 43.01 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:15<00:09, 39.97it/s, est. speed input: 44002.47 toks/s, output: 42.97 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:15<00:08, 39.97it/s, est. speed input: 43963.05 toks/s, output: 42.93 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:15<00:08, 39.97it/s, est. speed input: 43924.28 toks/s, output: 42.89 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:15<00:08, 39.95it/s, est. speed input: 43885.71 toks/s, output: 42.86 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:16<00:08, 39.93it/s, est. speed input: 43847.93 toks/s, output: 42.82 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:16<00:08, 39.91it/s, est. speed input: 43810.72 toks/s, output: 42.78 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:16<00:07, 39.92it/s, est. speed input: 43775.32 toks/s, output: 42.75 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:16<00:07, 39.94it/s, est. speed input: 43741.48 toks/s, output: 42.72 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:16<00:07, 39.92it/s, est. speed input: 43707.15 toks/s, output: 42.68 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:17<00:07, 39.91it/s, est. speed input: 43673.54 toks/s, output: 42.65 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:17<00:07, 39.92it/s, est. speed input: 43641.26 toks/s, output: 42.62 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:17<00:06, 39.93it/s, est. speed input: 43609.99 toks/s, output: 42.59 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:17<00:06, 39.93it/s, est. speed input: 43579.19 toks/s, output: 42.56 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:17<00:06, 39.92it/s, est. speed input: 43548.76 toks/s, output: 42.53 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:18<00:06, 39.90it/s, est. speed input: 43518.60 toks/s, output: 42.50 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:18<00:06, 39.91it/s, est. speed input: 43489.88 toks/s, output: 42.47 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:18<00:05, 39.92it/s, est. speed input: 43461.94 toks/s, output: 42.44 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:18<00:05, 39.94it/s, est. speed input: 43434.93 toks/s, output: 42.42 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:18<00:05, 39.92it/s, est. speed input: 43407.45 toks/s, output: 42.39 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:19<00:05, 39.90it/s, est. speed input: 43380.08 toks/s, output: 42.36 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:19<00:05, 39.92it/s, est. speed input: 43354.56 toks/s, output: 42.34 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:19<00:04, 39.94it/s, est. speed input: 43329.83 toks/s, output: 42.31 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:19<00:04, 39.95it/s, est. speed input: 43305.64 toks/s, output: 42.29 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:19<00:04, 39.95it/s, est. speed input: 43281.38 toks/s, output: 42.27 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:20<00:04, 39.93it/s, est. speed input: 43257.30 toks/s, output: 42.24 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:20<00:04, 39.92it/s, est. speed input: 43233.33 toks/s, output: 42.22 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:20<00:03, 39.93it/s, est. speed input: 43210.74 toks/s, output: 42.20 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:20<00:03, 39.94it/s, est. speed input: 43188.75 toks/s, output: 42.18 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:20<00:03, 39.94it/s, est. speed input: 43166.68 toks/s, output: 42.15 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:21<00:03, 39.92it/s, est. speed input: 43144.52 toks/s, output: 42.13 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:21<00:03, 39.92it/s, est. speed input: 43123.20 toks/s, output: 42.11 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:21<00:02, 39.94it/s, est. speed input: 43103.01 toks/s, output: 42.09 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:21<00:02, 39.95it/s, est. speed input: 43082.94 toks/s, output: 42.07 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:21<00:02, 39.94it/s, est. speed input: 43062.68 toks/s, output: 42.05 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:22<00:02, 39.90it/s, est. speed input: 43042.03 toks/s, output: 42.03 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:22<00:02, 39.91it/s, est. speed input: 43022.59 toks/s, output: 42.01 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:22<00:01, 39.92it/s, est. speed input: 43003.87 toks/s, output: 42.00 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:22<00:01, 39.93it/s, est. speed input: 42985.51 toks/s, output: 41.98 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:22<00:01, 39.95it/s, est. speed input: 42967.76 toks/s, output: 41.96 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:23<00:01, 39.93it/s, est. speed input: 42949.28 toks/s, output: 41.94 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:23<00:01, 39.92it/s, est. speed input: 42931.39 toks/s, output: 41.93 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:23<00:00, 39.94it/s, est. speed input: 42914.38 toks/s, output: 41.91 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:23<00:00, 39.95it/s, est. speed input: 42897.58 toks/s, output: 41.89 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:23<00:00, 39.95it/s, est. speed input: 42880.99 toks/s, output: 41.88 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:24<00:00, 39.91it/s, est. speed input: 42863.32 toks/s, output: 41.86 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:24<00:00, 41.23it/s, est. speed input: 42884.67 toks/s, output: 41.88 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:24<00:00, 41.23it/s, est. speed input: 43137.20 toks/s, output: 42.13 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:24<00:00, 42.13it/s, est. speed input: 43137.20 toks/s, output: 42.13 toks/s]
[rank0]:[W128 00:53:05.611688338 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.4s

测试结果:
  Requests/s:   39.89
  Tokens/s:     40885.40
  Total Reqs:   1024
  Elapsed:      25.67s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     40845.52

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:53:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3510822) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3510822) WARNING 01-28 00:53:25 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.18 requests/s, 40158.33 total tokens/s, 39.18 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 00:53:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:53:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:53:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:53:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:53:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:53:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:53:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:53:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:53:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:53:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:53:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:53:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:53:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:53:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:53:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:53:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:53:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:53:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:53:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:53:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:53:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:53:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:53:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:53:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:53:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:53:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:53:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:53:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3510822) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3510822) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.06it/s]
(EngineCore_DP0 pid=3510822) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.06it/s]
(EngineCore_DP0 pid=3510822) 
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3510822) [2026-01-28 00:53:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3510822) 2026-01-28 00:53:31,558 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3510822) 2026-01-28 00:53:31,573 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3510822) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  4.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 14.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 18.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.94it/s]
(EngineCore_DP0 pid=3510822) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.55it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 72/2048 [00:00<00:02, 715.93it/s]
Adding requests:   7%|▋         | 144/2048 [00:00<00:02, 693.10it/s]
Adding requests:  11%|█         | 217/2048 [00:00<00:02, 709.09it/s]
Adding requests:  14%|█▍        | 293/2048 [00:00<00:02, 726.97it/s]
Adding requests:  18%|█▊        | 368/2048 [00:00<00:02, 733.76it/s]
Adding requests:  22%|██▏       | 443/2048 [00:00<00:02, 737.69it/s]
Adding requests:  25%|██▌       | 519/2048 [00:00<00:02, 741.74it/s]
Adding requests:  29%|██▉       | 597/2048 [00:00<00:01, 751.48it/s]
Adding requests:  33%|███▎      | 673/2048 [00:00<00:01, 744.92it/s]
Adding requests:  37%|███▋      | 750/2048 [00:01<00:01, 752.32it/s]
Adding requests:  40%|████      | 826/2048 [00:01<00:01, 742.53it/s]
Adding requests:  44%|████▍     | 904/2048 [00:01<00:01, 753.16it/s]
Adding requests:  48%|████▊     | 982/2048 [00:01<00:01, 758.70it/s]
Adding requests:  52%|█████▏    | 1060/2048 [00:01<00:01, 764.18it/s]
Adding requests:  56%|█████▌    | 1137/2048 [00:01<00:01, 760.47it/s]
Adding requests:  60%|█████▉    | 1219/2048 [00:01<00:01, 777.13it/s]
Adding requests:  63%|██████▎   | 1297/2048 [00:01<00:00, 763.96it/s]
Adding requests:  67%|██████▋   | 1374/2048 [00:01<00:00, 765.72it/s]
Adding requests:  71%|███████   | 1451/2048 [00:01<00:00, 760.81it/s]
Adding requests:  75%|███████▍  | 1530/2048 [00:02<00:00, 769.14it/s]
Adding requests:  79%|███████▊  | 1608/2048 [00:02<00:00, 769.02it/s]
Adding requests:  82%|████████▏ | 1685/2048 [00:02<00:00, 759.18it/s]
Adding requests:  86%|████████▌ | 1761/2048 [00:02<00:00, 759.21it/s]
Adding requests:  90%|████████▉ | 1837/2048 [00:02<00:00, 743.91it/s]
Adding requests:  93%|█████████▎| 1912/2048 [00:02<00:00, 743.46it/s]
Adding requests:  97%|█████████▋| 1989/2048 [00:02<00:00, 749.16it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 750.69it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:00<00:04, 420.50it/s, est. speed input: 430619.86 toks/s, output: 420.51 toks/s]
Processed prompts:   7%|▋         | 141/2048 [00:01<00:16, 114.25it/s, est. speed input: 137940.75 toks/s, output: 134.71 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:01<00:28, 66.20it/s, est. speed input: 89171.88 toks/s, output: 87.08 toks/s]   
Processed prompts:   9%|▊         | 178/2048 [00:02<00:31, 58.68it/s, est. speed input: 80393.89 toks/s, output: 78.51 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:02<00:34, 53.15it/s, est. speed input: 74286.04 toks/s, output: 72.54 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:03<00:37, 49.15it/s, est. speed input: 69798.47 toks/s, output: 68.16 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:03<00:39, 46.27it/s, est. speed input: 66351.90 toks/s, output: 64.80 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:03<00:40, 44.22it/s, est. speed input: 63624.93 toks/s, output: 62.13 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:04<00:41, 42.77it/s, est. speed input: 61414.64 toks/s, output: 59.98 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:04<00:42, 41.65it/s, est. speed input: 59549.78 toks/s, output: 58.15 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:05<00:42, 40.95it/s, est. speed input: 58014.99 toks/s, output: 56.66 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:05<00:43, 40.46it/s, est. speed input: 56704.00 toks/s, output: 55.37 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:05<00:43, 40.12it/s, est. speed input: 55578.65 toks/s, output: 54.28 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:06<00:42, 39.89it/s, est. speed input: 54596.52 toks/s, output: 53.32 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:06<00:42, 39.70it/s, est. speed input: 53727.09 toks/s, output: 52.47 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:07<00:42, 39.59it/s, est. speed input: 52963.01 toks/s, output: 51.72 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:07<00:42, 39.51it/s, est. speed input: 52279.38 toks/s, output: 51.05 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:07<00:41, 39.45it/s, est. speed input: 51666.04 toks/s, output: 50.46 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:08<00:41, 39.38it/s, est. speed input: 51105.51 toks/s, output: 49.91 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:08<00:41, 39.36it/s, est. speed input: 50602.81 toks/s, output: 49.42 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:09<00:40, 39.32it/s, est. speed input: 50138.78 toks/s, output: 48.96 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:09<00:40, 39.27it/s, est. speed input: 49712.31 toks/s, output: 48.55 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:10<00:39, 39.26it/s, est. speed input: 49323.78 toks/s, output: 48.17 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:10<00:39, 39.24it/s, est. speed input: 48963.08 toks/s, output: 47.82 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:10<00:39, 39.24it/s, est. speed input: 48632.00 toks/s, output: 47.49 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:11<00:38, 39.23it/s, est. speed input: 48323.62 toks/s, output: 47.19 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:11<00:38, 39.22it/s, est. speed input: 48036.69 toks/s, output: 46.91 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:12<00:37, 39.22it/s, est. speed input: 47770.13 toks/s, output: 46.65 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:12<00:37, 39.21it/s, est. speed input: 47519.15 toks/s, output: 46.41 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:12<00:37, 39.21it/s, est. speed input: 47286.04 toks/s, output: 46.18 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:13<00:36, 39.21it/s, est. speed input: 47066.66 toks/s, output: 45.96 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:13<00:36, 39.21it/s, est. speed input: 46860.07 toks/s, output: 45.76 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:14<00:35, 39.21it/s, est. speed input: 46665.62 toks/s, output: 45.57 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:14<00:35, 39.20it/s, est. speed input: 46481.65 toks/s, output: 45.39 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:14<00:35, 39.20it/s, est. speed input: 46307.93 toks/s, output: 45.22 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:15<00:34, 39.21it/s, est. speed input: 46144.06 toks/s, output: 45.06 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:15<00:34, 39.19it/s, est. speed input: 45987.03 toks/s, output: 44.91 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:16<00:33, 39.19it/s, est. speed input: 45838.53 toks/s, output: 44.76 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:16<00:33, 39.18it/s, est. speed input: 45696.80 toks/s, output: 44.63 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:16<00:33, 39.19it/s, est. speed input: 45563.14 toks/s, output: 44.50 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:17<00:32, 39.19it/s, est. speed input: 45435.79 toks/s, output: 44.37 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:17<00:32, 39.18it/s, est. speed input: 45313.07 toks/s, output: 44.25 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:18<00:31, 39.19it/s, est. speed input: 45197.37 toks/s, output: 44.14 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:18<00:31, 39.20it/s, est. speed input: 45086.45 toks/s, output: 44.03 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:18<00:30, 39.20it/s, est. speed input: 44980.27 toks/s, output: 43.93 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:19<00:30, 39.19it/s, est. speed input: 44877.70 toks/s, output: 43.83 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:19<00:30, 39.19it/s, est. speed input: 44780.14 toks/s, output: 43.73 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:20<00:29, 39.19it/s, est. speed input: 44686.21 toks/s, output: 43.64 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:20<00:29, 39.19it/s, est. speed input: 44595.63 toks/s, output: 43.55 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:21<00:28, 39.19it/s, est. speed input: 44509.35 toks/s, output: 43.47 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:21<00:28, 39.18it/s, est. speed input: 44425.36 toks/s, output: 43.38 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:21<00:28, 39.18it/s, est. speed input: 44344.89 toks/s, output: 43.31 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:22<00:27, 39.19it/s, est. speed input: 44267.99 toks/s, output: 43.23 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:22<00:27, 39.18it/s, est. speed input: 44192.78 toks/s, output: 43.16 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:23<00:26, 39.20it/s, est. speed input: 44121.67 toks/s, output: 43.09 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:23<00:26, 39.19it/s, est. speed input: 44051.72 toks/s, output: 43.02 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:23<00:26, 39.19it/s, est. speed input: 43984.93 toks/s, output: 42.95 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:24<00:25, 39.20it/s, est. speed input: 43920.54 toks/s, output: 42.89 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:24<00:25, 39.19it/s, est. speed input: 43857.79 toks/s, output: 42.83 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:25<00:24, 39.20it/s, est. speed input: 43797.48 toks/s, output: 42.77 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:25<00:24, 39.19it/s, est. speed input: 43738.33 toks/s, output: 42.71 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:25<00:24, 39.19it/s, est. speed input: 43681.77 toks/s, output: 42.66 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:26<00:23, 39.20it/s, est. speed input: 43627.05 toks/s, output: 42.60 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:26<00:23, 39.18it/s, est. speed input: 43573.17 toks/s, output: 42.55 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:27<00:22, 39.78it/s, est. speed input: 43554.09 toks/s, output: 42.53 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:27<00:22, 39.60it/s, est. speed input: 43503.07 toks/s, output: 42.48 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:27<00:21, 39.47it/s, est. speed input: 43453.64 toks/s, output: 42.44 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:28<00:21, 39.39it/s, est. speed input: 43405.87 toks/s, output: 42.39 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:28<00:21, 39.32it/s, est. speed input: 43359.05 toks/s, output: 42.34 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:29<00:20, 39.29it/s, est. speed input: 43314.26 toks/s, output: 42.30 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:29<00:20, 39.25it/s, est. speed input: 43269.80 toks/s, output: 42.26 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:29<00:19, 39.25it/s, est. speed input: 43227.73 toks/s, output: 42.21 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:30<00:19, 39.22it/s, est. speed input: 43185.55 toks/s, output: 42.17 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:30<00:19, 39.20it/s, est. speed input: 43144.61 toks/s, output: 42.13 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:31<00:18, 39.20it/s, est. speed input: 43105.48 toks/s, output: 42.10 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:31<00:18, 39.19it/s, est. speed input: 43066.55 toks/s, output: 42.06 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:32<00:17, 39.19it/s, est. speed input: 43029.31 toks/s, output: 42.02 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:32<00:17, 39.19it/s, est. speed input: 42992.55 toks/s, output: 41.98 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:32<00:17, 39.19it/s, est. speed input: 42957.14 toks/s, output: 41.95 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:33<00:16, 39.19it/s, est. speed input: 42922.42 toks/s, output: 41.92 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:33<00:16, 39.19it/s, est. speed input: 42888.69 toks/s, output: 41.88 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:34<00:15, 39.19it/s, est. speed input: 42855.54 toks/s, output: 41.85 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:34<00:15, 39.19it/s, est. speed input: 42823.27 toks/s, output: 41.82 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:34<00:15, 39.19it/s, est. speed input: 42791.86 toks/s, output: 41.79 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:35<00:14, 39.20it/s, est. speed input: 42761.24 toks/s, output: 41.76 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:35<00:14, 39.20it/s, est. speed input: 42731.28 toks/s, output: 41.73 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:36<00:13, 39.20it/s, est. speed input: 42702.22 toks/s, output: 41.70 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:36<00:13, 39.19it/s, est. speed input: 42673.06 toks/s, output: 41.67 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:36<00:13, 39.19it/s, est. speed input: 42645.00 toks/s, output: 41.65 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:37<00:12, 39.18it/s, est. speed input: 42617.29 toks/s, output: 41.62 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:37<00:12, 39.18it/s, est. speed input: 42590.19 toks/s, output: 41.59 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:38<00:11, 39.18it/s, est. speed input: 42563.60 toks/s, output: 41.57 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:38<00:11, 39.18it/s, est. speed input: 42537.70 toks/s, output: 41.54 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:38<00:10, 39.18it/s, est. speed input: 42512.57 toks/s, output: 41.52 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:39<00:10, 39.18it/s, est. speed input: 42487.65 toks/s, output: 41.49 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:39<00:10, 39.17it/s, est. speed input: 42463.11 toks/s, output: 41.47 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:40<00:09, 39.18it/s, est. speed input: 42439.55 toks/s, output: 41.44 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:40<00:09, 39.18it/s, est. speed input: 42416.24 toks/s, output: 41.42 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:41<00:08, 39.18it/s, est. speed input: 42393.47 toks/s, output: 41.40 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:41<00:08, 39.18it/s, est. speed input: 42370.95 toks/s, output: 41.38 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:41<00:08, 39.19it/s, est. speed input: 42349.27 toks/s, output: 41.36 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:42<00:07, 39.19it/s, est. speed input: 42328.04 toks/s, output: 41.34 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:42<00:07, 39.19it/s, est. speed input: 42306.73 toks/s, output: 41.32 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:43<00:06, 39.18it/s, est. speed input: 42285.85 toks/s, output: 41.29 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:43<00:06, 39.17it/s, est. speed input: 42265.33 toks/s, output: 41.27 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:43<00:06, 39.18it/s, est. speed input: 42245.46 toks/s, output: 41.26 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:44<00:05, 39.18it/s, est. speed input: 42226.02 toks/s, output: 41.24 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:44<00:05, 39.18it/s, est. speed input: 42206.69 toks/s, output: 41.22 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:45<00:04, 39.18it/s, est. speed input: 42187.71 toks/s, output: 41.20 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:45<00:04, 39.89it/s, est. speed input: 42191.70 toks/s, output: 41.20 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:45<00:03, 39.67it/s, est. speed input: 42173.07 toks/s, output: 41.18 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:46<00:03, 39.52it/s, est. speed input: 42155.05 toks/s, output: 41.17 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:46<00:03, 39.41it/s, est. speed input: 42137.09 toks/s, output: 41.15 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:47<00:02, 39.34it/s, est. speed input: 42119.54 toks/s, output: 41.13 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:47<00:02, 39.29it/s, est. speed input: 42102.17 toks/s, output: 41.12 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:47<00:01, 39.26it/s, est. speed input: 42085.48 toks/s, output: 41.10 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:48<00:01, 39.24it/s, est. speed input: 42068.90 toks/s, output: 41.08 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:48<00:01, 39.21it/s, est. speed input: 42052.34 toks/s, output: 41.07 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:49<00:00, 39.20it/s, est. speed input: 42036.18 toks/s, output: 41.05 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:49<00:00, 39.88it/s, est. speed input: 42040.17 toks/s, output: 41.05 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:49<00:00, 39.88it/s, est. speed input: 42329.23 toks/s, output: 41.34 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:49<00:00, 41.34it/s, est. speed input: 42329.23 toks/s, output: 41.34 toks/s]
[rank0]:[W128 00:54:25.483542667 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 79.9s

测试结果:
  Requests/s:   39.18
  Tokens/s:     40158.33
  Total Reqs:   2048
  Elapsed:      52.27s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     40119.15

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:54:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3512270) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3512270) WARNING 01-28 00:54:51 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.16 requests/s, 40134.80 total tokens/s, 39.16 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 00:54:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:54:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:54:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:54:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:54:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:54:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:54:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:54:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:54:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:54:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:54:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:54:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:54:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:54:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:54:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:54:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:54:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:54:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:54:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:54:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:54:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:54:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:54:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:54:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:54:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:54:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:54:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:54:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:46] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:46] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3512270) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3512270) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.01it/s]
(EngineCore_DP0 pid=3512270) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=3512270) 
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3512270) [2026-01-28 00:54:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3512270) [rank0]:W0128 00:54:53.985000 3512270 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3512270) [rank0]:W0128 00:54:54.035000 3512270 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3512270) [rank0]:W0128 00:54:54.772000 3512270 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3512270) [rank0]:W0128 00:54:54.848000 3512270 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3512270) 2026-01-28 00:54:57,271 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3512270) 2026-01-28 00:54:57,297 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3512270) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:05,  1.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  7.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 11.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 11.65it/s]
(EngineCore_DP0 pid=3512270) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.06it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 22.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 19.87it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:05, 716.12it/s]
Adding requests:   4%|▎         | 148/4096 [00:00<00:05, 736.69it/s]
Adding requests:   5%|▌         | 225/4096 [00:00<00:05, 747.79it/s]
Adding requests:   7%|▋         | 300/4096 [00:00<00:05, 735.90it/s]
Adding requests:   9%|▉         | 375/4096 [00:00<00:05, 738.32it/s]
Adding requests:  11%|█         | 451/4096 [00:00<00:04, 743.56it/s]
Adding requests:  13%|█▎        | 526/4096 [00:00<00:04, 735.55it/s]
Adding requests:  15%|█▍        | 602/4096 [00:00<00:04, 740.64it/s]
Adding requests:  17%|█▋        | 679/4096 [00:00<00:04, 747.97it/s]
Adding requests:  18%|█▊        | 754/4096 [00:01<00:04, 747.22it/s]
Adding requests:  20%|██        | 829/4096 [00:01<00:04, 736.87it/s]
Adding requests:  22%|██▏       | 907/4096 [00:01<00:04, 748.79it/s]
Adding requests:  24%|██▍       | 985/4096 [00:01<00:04, 757.24it/s]
Adding requests:  26%|██▌       | 1064/4096 [00:01<00:03, 765.86it/s]
Adding requests:  28%|██▊       | 1141/4096 [00:01<00:03, 741.91it/s]
Adding requests:  30%|██▉       | 1219/4096 [00:01<00:03, 752.92it/s]
Adding requests:  32%|███▏      | 1295/4096 [00:01<00:03, 747.70it/s]
Adding requests:  33%|███▎      | 1370/4096 [00:01<00:03, 747.00it/s]
Adding requests:  35%|███▌      | 1446/4096 [00:01<00:03, 750.11it/s]
Adding requests:  37%|███▋      | 1526/4096 [00:02<00:03, 763.78it/s]
Adding requests:  39%|███▉      | 1605/4096 [00:02<00:03, 771.19it/s]
Adding requests:  41%|████      | 1683/4096 [00:02<00:03, 762.90it/s]
Adding requests:  43%|████▎     | 1761/4096 [00:02<00:03, 766.63it/s]
Adding requests:  45%|████▍     | 1839/4096 [00:02<00:02, 768.22it/s]
Adding requests:  47%|████▋     | 1916/4096 [00:02<00:02, 761.04it/s]
Adding requests:  49%|████▊     | 1994/4096 [00:02<00:02, 764.36it/s]
Adding requests:  51%|█████     | 2075/4096 [00:02<00:02, 775.89it/s]
Adding requests:  53%|█████▎    | 2153/4096 [00:02<00:02, 761.17it/s]
Adding requests:  54%|█████▍    | 2230/4096 [00:02<00:02, 762.16it/s]
Adding requests:  56%|█████▋    | 2310/4096 [00:03<00:02, 770.52it/s]
Adding requests:  58%|█████▊    | 2388/4096 [00:03<00:02, 754.57it/s]
Adding requests:  60%|██████    | 2465/4096 [00:03<00:02, 757.08it/s]
Adding requests:  62%|██████▏   | 2541/4096 [00:03<00:02, 754.50it/s]
Adding requests:  64%|██████▍   | 2618/4096 [00:03<00:01, 758.42it/s]
Adding requests:  66%|██████▌   | 2695/4096 [00:03<00:01, 760.20it/s]
Adding requests:  68%|██████▊   | 2772/4096 [00:03<00:01, 756.40it/s]
Adding requests:  70%|██████▉   | 2848/4096 [00:03<00:01, 750.44it/s]
Adding requests:  71%|███████▏  | 2924/4096 [00:03<00:01, 744.11it/s]
Adding requests:  73%|███████▎  | 2999/4096 [00:03<00:01, 737.05it/s]
Adding requests:  75%|███████▌  | 3075/4096 [00:04<00:01, 742.54it/s]
Adding requests:  77%|███████▋  | 3152/4096 [00:04<00:01, 749.62it/s]
Adding requests:  79%|███████▉  | 3230/4096 [00:04<00:01, 757.18it/s]
Adding requests:  81%|████████  | 3308/4096 [00:04<00:01, 763.57it/s]
Adding requests:  83%|████████▎ | 3387/4096 [00:04<00:00, 768.33it/s]
Adding requests:  85%|████████▍ | 3464/4096 [00:04<00:00, 756.42it/s]
Adding requests:  86%|████████▋ | 3542/4096 [00:04<00:00, 761.40it/s]
Adding requests:  88%|████████▊ | 3619/4096 [00:04<00:00, 761.76it/s]
Adding requests:  90%|█████████ | 3696/4096 [00:04<00:00, 750.46it/s]
Adding requests:  92%|█████████▏| 3775/4096 [00:05<00:00, 761.02it/s]
Adding requests:  94%|█████████▍| 3854/4096 [00:05<00:00, 768.95it/s]
Adding requests:  96%|█████████▌| 3932/4096 [00:05<00:00, 771.12it/s]
Adding requests:  98%|█████████▊| 4012/4096 [00:05<00:00, 779.18it/s]
Adding requests: 100%|█████████▉| 4090/4096 [00:05<00:00, 772.49it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 756.49it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:00<00:07, 518.97it/s, est. speed input: 531446.19 toks/s, output: 518.98 toks/s]
Processed prompts:   6%|▌         | 246/4096 [00:01<00:22, 174.34it/s, est. speed input: 211800.21 toks/s, output: 206.84 toks/s]
Processed prompts:   7%|▋         | 272/4096 [00:02<00:38, 100.33it/s, est. speed input: 138935.30 toks/s, output: 135.68 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:02<00:56, 66.99it/s, est. speed input: 105293.63 toks/s, output: 102.83 toks/s] 
Processed prompts:   8%|▊         | 322/4096 [00:03<01:06, 56.48it/s, est. speed input: 90682.42 toks/s, output: 88.56 toks/s]  
Processed prompts:   9%|▊         | 354/4096 [00:04<01:14, 50.42it/s, est. speed input: 81418.21 toks/s, output: 79.51 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:05<01:19, 46.68it/s, est. speed input: 75031.52 toks/s, output: 73.27 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:06<01:23, 44.27it/s, est. speed input: 70353.04 toks/s, output: 68.70 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:06<01:25, 42.67it/s, est. speed input: 66777.35 toks/s, output: 65.21 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:07<01:26, 41.58it/s, est. speed input: 63957.82 toks/s, output: 62.46 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:08<01:27, 40.85it/s, est. speed input: 61676.80 toks/s, output: 60.23 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:09<01:28, 40.34it/s, est. speed input: 59795.19 toks/s, output: 58.39 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:10<01:27, 39.99it/s, est. speed input: 58215.69 toks/s, output: 56.85 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:10<01:27, 39.74it/s, est. speed input: 56870.05 toks/s, output: 55.54 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:11<01:27, 39.58it/s, est. speed input: 55711.73 toks/s, output: 54.41 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:12<01:26, 39.46it/s, est. speed input: 54702.45 toks/s, output: 53.42 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:13<01:26, 39.37it/s, est. speed input: 53815.20 toks/s, output: 52.55 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:14<01:25, 39.32it/s, est. speed input: 53030.96 toks/s, output: 51.79 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:15<01:24, 39.28it/s, est. speed input: 52331.02 toks/s, output: 51.10 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:15<01:23, 39.24it/s, est. speed input: 51702.45 toks/s, output: 50.49 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:16<01:23, 39.22it/s, est. speed input: 51135.40 toks/s, output: 49.94 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:17<01:22, 39.21it/s, est. speed input: 50622.09 toks/s, output: 49.44 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:18<01:21, 39.20it/s, est. speed input: 50154.53 toks/s, output: 48.98 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:19<01:20, 39.20it/s, est. speed input: 49726.61 toks/s, output: 48.56 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:19<01:19, 39.19it/s, est. speed input: 49333.48 toks/s, output: 48.18 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:20<01:19, 39.19it/s, est. speed input: 48971.29 toks/s, output: 47.82 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:21<01:18, 39.18it/s, est. speed input: 48636.55 toks/s, output: 47.50 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:22<01:17, 39.18it/s, est. speed input: 48326.19 toks/s, output: 47.19 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:23<01:16, 39.18it/s, est. speed input: 48037.86 toks/s, output: 46.91 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:24<01:15, 39.14it/s, est. speed input: 47763.53 toks/s, output: 46.64 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:24<01:14, 39.35it/s, est. speed input: 47538.66 toks/s, output: 46.42 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:25<01:14, 39.30it/s, est. speed input: 47303.23 toks/s, output: 46.19 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:26<01:13, 39.26it/s, est. speed input: 47080.43 toks/s, output: 45.98 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:27<01:12, 39.23it/s, est. speed input: 46872.17 toks/s, output: 45.77 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:28<01:11, 39.21it/s, est. speed input: 46674.93 toks/s, output: 45.58 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:28<01:10, 39.19it/s, est. speed input: 46489.50 toks/s, output: 45.40 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:29<01:10, 39.19it/s, est. speed input: 46314.56 toks/s, output: 45.23 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:30<01:09, 39.18it/s, est. speed input: 46148.42 toks/s, output: 45.07 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:31<01:08, 39.18it/s, est. speed input: 45991.54 toks/s, output: 44.91 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:32<01:07, 39.18it/s, est. speed input: 45842.50 toks/s, output: 44.77 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:33<01:06, 39.17it/s, est. speed input: 45700.55 toks/s, output: 44.63 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:33<01:06, 39.18it/s, est. speed input: 45566.23 toks/s, output: 44.50 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:34<01:05, 39.17it/s, est. speed input: 45437.23 toks/s, output: 44.37 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:35<01:04, 39.17it/s, est. speed input: 45314.18 toks/s, output: 44.25 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:36<01:03, 39.17it/s, est. speed input: 45197.31 toks/s, output: 44.14 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:37<01:02, 39.17it/s, est. speed input: 45085.23 toks/s, output: 44.03 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:37<01:02, 39.17it/s, est. speed input: 44978.15 toks/s, output: 43.92 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:38<01:01, 39.16it/s, est. speed input: 44875.05 toks/s, output: 43.82 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:39<01:00, 39.17it/s, est. speed input: 44776.87 toks/s, output: 43.73 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:40<00:59, 39.17it/s, est. speed input: 44682.49 toks/s, output: 43.64 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:41<00:58, 39.17it/s, est. speed input: 44591.55 toks/s, output: 43.55 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:42<00:57, 39.16it/s, est. speed input: 44503.98 toks/s, output: 43.46 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:42<00:56, 39.49it/s, est. speed input: 44443.26 toks/s, output: 43.40 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:43<00:56, 39.39it/s, est. speed input: 44361.81 toks/s, output: 43.32 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:44<00:55, 39.32it/s, est. speed input: 44283.55 toks/s, output: 43.25 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:45<00:54, 39.28it/s, est. speed input: 44208.41 toks/s, output: 43.17 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:46<00:53, 39.24it/s, est. speed input: 44135.16 toks/s, output: 43.10 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:46<00:53, 39.21it/s, est. speed input: 44064.33 toks/s, output: 43.03 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:47<00:52, 39.20it/s, est. speed input: 43996.75 toks/s, output: 42.97 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:48<00:51, 39.18it/s, est. speed input: 43930.54 toks/s, output: 42.90 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:49<00:50, 39.17it/s, est. speed input: 43867.11 toks/s, output: 42.84 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:50<00:49, 39.16it/s, est. speed input: 43805.46 toks/s, output: 42.78 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:50<00:48, 39.16it/s, est. speed input: 43745.67 toks/s, output: 42.72 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:51<00:48, 39.15it/s, est. speed input: 43688.00 toks/s, output: 42.66 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:52<00:47, 39.15it/s, est. speed input: 43631.91 toks/s, output: 42.61 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:53<00:46, 39.14it/s, est. speed input: 43577.34 toks/s, output: 42.56 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:54<00:45, 39.15it/s, est. speed input: 43524.90 toks/s, output: 42.50 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:55<00:44, 39.15it/s, est. speed input: 43473.81 toks/s, output: 42.45 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:55<00:44, 39.14it/s, est. speed input: 43423.80 toks/s, output: 42.41 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:56<00:43, 39.14it/s, est. speed input: 43375.59 toks/s, output: 42.36 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:57<00:42, 39.13it/s, est. speed input: 43328.27 toks/s, output: 42.31 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:58<00:41, 39.14it/s, est. speed input: 43282.84 toks/s, output: 42.27 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:59<00:40, 39.46it/s, est. speed input: 43254.78 toks/s, output: 42.24 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:59<00:39, 39.36it/s, est. speed input: 43211.28 toks/s, output: 42.20 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [01:00<00:39, 39.28it/s, est. speed input: 43168.67 toks/s, output: 42.16 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [01:01<00:38, 39.23it/s, est. speed input: 43127.31 toks/s, output: 42.12 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [01:02<00:37, 39.20it/s, est. speed input: 43087.12 toks/s, output: 42.08 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [01:03<00:36, 39.18it/s, est. speed input: 43048.08 toks/s, output: 42.04 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [01:04<00:35, 39.16it/s, est. speed input: 43009.67 toks/s, output: 42.00 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [01:04<00:35, 39.14it/s, est. speed input: 42972.19 toks/s, output: 41.97 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [01:05<00:34, 39.14it/s, est. speed input: 42935.99 toks/s, output: 41.93 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [01:06<00:33, 39.13it/s, est. speed input: 42900.71 toks/s, output: 41.90 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [01:07<00:32, 39.13it/s, est. speed input: 42866.24 toks/s, output: 41.86 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [01:08<00:31, 39.12it/s, est. speed input: 42832.28 toks/s, output: 41.83 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [01:08<00:31, 39.12it/s, est. speed input: 42799.34 toks/s, output: 41.80 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [01:09<00:30, 39.12it/s, est. speed input: 42767.07 toks/s, output: 41.76 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [01:10<00:29, 39.12it/s, est. speed input: 42735.61 toks/s, output: 41.73 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:11<00:28, 39.11it/s, est. speed input: 42704.60 toks/s, output: 41.70 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:12<00:27, 39.11it/s, est. speed input: 42674.68 toks/s, output: 41.67 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:13<00:26, 39.12it/s, est. speed input: 42645.41 toks/s, output: 41.65 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:13<00:26, 39.11it/s, est. speed input: 42616.49 toks/s, output: 41.62 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:14<00:25, 39.11it/s, est. speed input: 42588.28 toks/s, output: 41.59 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:15<00:24, 39.11it/s, est. speed input: 42560.71 toks/s, output: 41.56 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:16<00:23, 39.11it/s, est. speed input: 42533.79 toks/s, output: 41.54 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:17<00:22, 39.11it/s, est. speed input: 42507.57 toks/s, output: 41.51 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:17<00:22, 39.10it/s, est. speed input: 42481.20 toks/s, output: 41.49 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:18<00:21, 39.10it/s, est. speed input: 42455.88 toks/s, output: 41.46 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:19<00:20, 39.10it/s, est. speed input: 42430.99 toks/s, output: 41.44 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:20<00:19, 39.10it/s, est. speed input: 42406.63 toks/s, output: 41.41 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:21<00:18, 39.11it/s, est. speed input: 42382.99 toks/s, output: 41.39 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:22<00:17, 39.10it/s, est. speed input: 42359.51 toks/s, output: 41.37 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:22<00:17, 39.09it/s, est. speed input: 42336.18 toks/s, output: 41.34 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:23<00:16, 39.09it/s, est. speed input: 42313.73 toks/s, output: 41.32 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:24<00:15, 39.10it/s, est. speed input: 42291.69 toks/s, output: 41.30 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:25<00:14, 39.10it/s, est. speed input: 42270.09 toks/s, output: 41.28 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:26<00:13, 39.10it/s, est. speed input: 42248.79 toks/s, output: 41.26 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:26<00:13, 39.10it/s, est. speed input: 42227.95 toks/s, output: 41.24 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:27<00:12, 39.09it/s, est. speed input: 42207.30 toks/s, output: 41.22 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:28<00:11, 39.09it/s, est. speed input: 42187.14 toks/s, output: 41.20 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:29<00:10, 39.09it/s, est. speed input: 42167.28 toks/s, output: 41.18 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:30<00:09, 39.10it/s, est. speed input: 42148.18 toks/s, output: 41.16 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:31<00:08, 39.09it/s, est. speed input: 42129.10 toks/s, output: 41.14 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:31<00:08, 39.09it/s, est. speed input: 42110.36 toks/s, output: 41.12 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:32<00:07, 39.09it/s, est. speed input: 42091.81 toks/s, output: 41.11 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:33<00:06, 39.08it/s, est. speed input: 42073.55 toks/s, output: 41.09 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:34<00:05, 39.09it/s, est. speed input: 42055.84 toks/s, output: 41.07 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:35<00:04, 39.08it/s, est. speed input: 42038.11 toks/s, output: 41.05 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:35<00:04, 39.08it/s, est. speed input: 42020.98 toks/s, output: 41.04 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:36<00:03, 39.09it/s, est. speed input: 42004.17 toks/s, output: 41.02 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:37<00:02, 39.08it/s, est. speed input: 41987.36 toks/s, output: 41.00 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:38<00:01, 39.41it/s, est. speed input: 41980.76 toks/s, output: 41.00 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:39<00:00, 39.68it/s, est. speed input: 41975.22 toks/s, output: 40.99 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:39<00:00, 39.68it/s, est. speed input: 42284.81 toks/s, output: 41.29 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:39<00:00, 41.29it/s, est. speed input: 42284.81 toks/s, output: 41.29 toks/s]
[rank0]:[W128 00:56:44.174415436 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 138.7s

测试结果:
  Requests/s:   39.16
  Tokens/s:     40134.80
  Total Reqs:   4096
  Elapsed:      104.61s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     40095.64


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,56.1259,28792.5760,2.2806
1024,1024,1,128,128,38.2144,39169.7975,3.3495
2048,1024,2,256,128,41.9968,43046.6792,6.0957
4096,1024,4,512,128,41.6735,42715.3292,12.2860
8192,1024,8,1024,128,39.8882,40885.4049,25.6718
16384,1024,16,2048,128,39.1789,40158.3331,52.2731
32768,1024,32,4096,128,39.1559,40134.7954,104.6075

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:56:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3514304) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3514304) WARNING 01-28 00:56:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.59 requests/s, 29032.48 total tokens/s, 56.59 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:56:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:56:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:56:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:56:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:56:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:56:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:56:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:56:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:56:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:56:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:56:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:56:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:56:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:56:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:56:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:56:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:56:53] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:56:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:56:53] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:56:53] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:56:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:56:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:56:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:56:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:56:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:56:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:56:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:56:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3514304) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3514304) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=3514304) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=3514304) 
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3514304) [2026-01-28 00:56:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3514304) 2026-01-28 00:57:05,398 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3514304) 2026-01-28 00:57:05,413 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3514304) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.29it/s]
(EngineCore_DP0 pid=3514304) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.70it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.69it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  96%|█████████▌| 123/128 [00:00<00:00, 1226.03it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1229.66it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:15,  8.45it/s, est. speed input: 4326.69 toks/s, output: 8.45 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 39.26it/s, est. speed input: 17683.11 toks/s, output: 34.54 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 49.54it/s, est. speed input: 22297.73 toks/s, output: 43.55 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:01, 54.38it/s, est. speed input: 24616.46 toks/s, output: 48.08 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 57.27it/s, est. speed input: 26066.16 toks/s, output: 50.91 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 59.32it/s, est. speed input: 27105.31 toks/s, output: 52.94 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 60.52it/s, est. speed input: 27829.05 toks/s, output: 54.35 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 61.05it/s, est. speed input: 28327.42 toks/s, output: 55.33 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 61.58it/s, est. speed input: 28745.85 toks/s, output: 56.14 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 61.97it/s, est. speed input: 29085.02 toks/s, output: 56.81 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 62.35it/s, est. speed input: 29378.82 toks/s, output: 57.38 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 62.45it/s, est. speed input: 29603.19 toks/s, output: 57.82 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 62.69it/s, est. speed input: 29814.01 toks/s, output: 58.23 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 62.77it/s, est. speed input: 29985.26 toks/s, output: 58.56 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 63.01it/s, est. speed input: 30153.10 toks/s, output: 58.89 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 63.09it/s, est. speed input: 30291.36 toks/s, output: 59.16 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 62.65it/s, est. speed input: 30367.34 toks/s, output: 59.31 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 61.01it/s, est. speed input: 30312.02 toks/s, output: 59.20 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 61.17it/s, est. speed input: 30375.50 toks/s, output: 59.33 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 61.17it/s, est. speed input: 30383.09 toks/s, output: 59.34 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 59.34it/s, est. speed input: 30383.09 toks/s, output: 59.34 toks/s]
[rank0]:[W128 00:57:09.186880562 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.0s

测试结果:
  Requests/s:   56.59
  Tokens/s:     29032.48
  Total Reqs:   128
  Elapsed:      2.26s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     28975.88

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:57:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3514981) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3514981) WARNING 01-28 00:57:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.97 requests/s, 36865.52 total tokens/s, 35.97 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 00:57:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:57:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:57:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:57:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:57:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:57:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:57:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:57:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:57:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:57:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:57:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:57:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:57:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:57:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:57:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:57:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:57:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:57:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:19] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3514981) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3514981) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.37it/s]
(EngineCore_DP0 pid=3514981) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.37it/s]
(EngineCore_DP0 pid=3514981) 
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3514981) [2026-01-28 00:57:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3514981) 2026-01-28 00:57:30,093 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3514981) 2026-01-28 00:57:30,109 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3514981) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 11.55it/s]
(EngineCore_DP0 pid=3514981) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.25it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  52%|█████▏    | 66/128 [00:00<00:00, 653.64it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 697.29it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:04, 28.64it/s, est. speed input: 29329.66 toks/s, output: 28.64 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 34.32it/s, est. speed input: 34272.31 toks/s, output: 33.47 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 36.13it/s, est. speed input: 35913.94 toks/s, output: 35.07 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:03, 36.99it/s, est. speed input: 36742.66 toks/s, output: 35.88 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 37.46it/s, est. speed input: 37234.53 toks/s, output: 36.36 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 37.77it/s, est. speed input: 37580.20 toks/s, output: 36.70 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 37.90it/s, est. speed input: 37794.40 toks/s, output: 36.91 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 37.87it/s, est. speed input: 37910.97 toks/s, output: 37.02 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:02, 37.96it/s, est. speed input: 38040.54 toks/s, output: 37.15 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:02, 37.98it/s, est. speed input: 38130.97 toks/s, output: 37.24 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 37.97it/s, est. speed input: 38196.44 toks/s, output: 37.30 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 38.03it/s, est. speed input: 38271.67 toks/s, output: 37.37 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 38.10it/s, est. speed input: 38340.16 toks/s, output: 37.44 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 38.15it/s, est. speed input: 38400.60 toks/s, output: 37.50 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 38.24it/s, est. speed input: 38465.03 toks/s, output: 37.56 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 38.30it/s, est. speed input: 38520.65 toks/s, output: 37.62 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 38.36it/s, est. speed input: 38572.97 toks/s, output: 37.67 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 38.21it/s, est. speed input: 38585.23 toks/s, output: 37.68 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 38.24it/s, est. speed input: 38617.92 toks/s, output: 37.71 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 38.27it/s, est. speed input: 38650.23 toks/s, output: 37.74 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 38.24it/s, est. speed input: 38671.90 toks/s, output: 37.77 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 38.26it/s, est. speed input: 38696.64 toks/s, output: 37.79 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 38.28it/s, est. speed input: 38719.93 toks/s, output: 37.81 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 38.32it/s, est. speed input: 38745.22 toks/s, output: 37.84 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 38.31it/s, est. speed input: 38764.12 toks/s, output: 37.86 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 38.31it/s, est. speed input: 38782.49 toks/s, output: 37.87 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 38.22it/s, est. speed input: 38786.80 toks/s, output: 37.88 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 38.24it/s, est. speed input: 38802.70 toks/s, output: 37.89 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:03<00:00, 38.26it/s, est. speed input: 38816.88 toks/s, output: 37.91 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 38.21it/s, est. speed input: 38823.67 toks/s, output: 37.91 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 38.16it/s, est. speed input: 38828.18 toks/s, output: 37.92 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 38.18it/s, est. speed input: 38838.06 toks/s, output: 37.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.18it/s, est. speed input: 38841.65 toks/s, output: 37.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.93it/s, est. speed input: 38841.65 toks/s, output: 37.93 toks/s]
[rank0]:[W128 00:57:35.863678040 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.7s

测试结果:
  Requests/s:   35.97
  Tokens/s:     36865.52
  Total Reqs:   128
  Elapsed:      3.56s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     36829.56

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:57:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3515628) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3515628) WARNING 01-28 00:57:50 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.52 requests/s, 40510.70 total tokens/s, 39.52 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 00:57:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:57:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:57:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:57:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:57:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:57:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:57:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:57:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:57:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:57:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:57:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:57:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:57:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:57:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:57:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:57:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:57:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:57:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:57:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3515628) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3515628) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=3515628) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=3515628) 
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3515628) [2026-01-28 00:57:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3515628) 2026-01-28 00:57:55,991 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3515628) 2026-01-28 00:57:56,006 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3515628) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 21.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 21.61it/s]
(EngineCore_DP0 pid=3515628) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.49it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  25%|██▍       | 63/256 [00:00<00:00, 626.36it/s]
Adding requests:  53%|█████▎    | 136/256 [00:00<00:00, 682.12it/s]
Adding requests:  82%|████████▏ | 210/256 [00:00<00:00, 705.21it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 709.48it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:00<00:01, 137.02it/s, est. speed input: 140326.31 toks/s, output: 137.03 toks/s]
Processed prompts:  11%|█         | 28/256 [00:00<00:04, 56.65it/s, est. speed input: 63605.22 toks/s, output: 62.11 toks/s]   
Processed prompts:  14%|█▍        | 37/256 [00:00<00:04, 51.87it/s, est. speed input: 58169.33 toks/s, output: 56.81 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:00<00:04, 45.74it/s, est. speed input: 52952.99 toks/s, output: 51.71 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:01<00:04, 44.16it/s, est. speed input: 51182.23 toks/s, output: 49.98 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:01<00:04, 43.01it/s, est. speed input: 49870.36 toks/s, output: 48.70 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:04, 42.19it/s, est. speed input: 48867.27 toks/s, output: 47.72 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:01<00:04, 41.61it/s, est. speed input: 48074.22 toks/s, output: 46.95 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:04, 41.18it/s, est. speed input: 47424.35 toks/s, output: 46.31 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:04, 40.84it/s, est. speed input: 46874.15 toks/s, output: 45.78 toks/s]
Processed prompts:  33%|███▎      | 85/256 [00:01<00:03, 42.86it/s, est. speed input: 47106.04 toks/s, output: 46.00 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:04, 39.85it/s, est. speed input: 46146.50 toks/s, output: 45.06 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:02<00:04, 39.93it/s, est. speed input: 45792.36 toks/s, output: 44.72 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:03, 39.98it/s, est. speed input: 45483.45 toks/s, output: 44.42 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:02<00:03, 40.04it/s, est. speed input: 45217.99 toks/s, output: 44.16 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:03, 40.08it/s, est. speed input: 44982.74 toks/s, output: 43.93 toks/s]
Processed prompts:  46%|████▋     | 119/256 [00:02<00:03, 42.27it/s, est. speed input: 45205.01 toks/s, output: 44.15 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:02<00:03, 39.31it/s, est. speed input: 44611.85 toks/s, output: 43.57 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:02<00:03, 39.58it/s, est. speed input: 44438.46 toks/s, output: 43.40 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:03<00:03, 39.74it/s, est. speed input: 44277.47 toks/s, output: 43.24 toks/s]
Processed prompts:  55%|█████▌    | 141/256 [00:03<00:02, 42.06it/s, est. speed input: 44486.76 toks/s, output: 43.44 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:02, 39.24it/s, est. speed input: 44036.53 toks/s, output: 43.00 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:03<00:02, 39.54it/s, est. speed input: 43914.79 toks/s, output: 42.89 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:03<00:02, 39.75it/s, est. speed input: 43803.97 toks/s, output: 42.78 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:03<00:02, 39.83it/s, est. speed input: 43694.17 toks/s, output: 42.67 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:03<00:02, 39.90it/s, est. speed input: 43593.76 toks/s, output: 42.57 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:04<00:02, 39.97it/s, est. speed input: 43502.89 toks/s, output: 42.48 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:01, 40.01it/s, est. speed input: 43418.70 toks/s, output: 42.40 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:04<00:01, 40.07it/s, est. speed input: 43342.44 toks/s, output: 42.33 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:04<00:01, 40.10it/s, est. speed input: 43270.99 toks/s, output: 42.26 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:04<00:01, 40.07it/s, est. speed input: 43197.50 toks/s, output: 42.18 toks/s]
Processed prompts:  80%|████████  | 205/256 [00:04<00:01, 42.24it/s, est. speed input: 43358.05 toks/s, output: 42.34 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:04<00:01, 39.42it/s, est. speed input: 43087.15 toks/s, output: 42.08 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:05<00:01, 39.66it/s, est. speed input: 43031.15 toks/s, output: 42.02 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:05<00:00, 39.80it/s, est. speed input: 42975.74 toks/s, output: 41.97 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:05<00:00, 39.88it/s, est. speed input: 42922.18 toks/s, output: 41.92 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:05<00:00, 39.96it/s, est. speed input: 42873.83 toks/s, output: 41.87 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:05<00:00, 39.99it/s, est. speed input: 42825.41 toks/s, output: 41.82 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:05<00:00, 42.22it/s, est. speed input: 42967.40 toks/s, output: 41.96 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:05<00:00, 39.33it/s, est. speed input: 42745.92 toks/s, output: 41.74 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 41.69it/s, est. speed input: 42872.05 toks/s, output: 41.87 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 41.69it/s, est. speed input: 42872.05 toks/s, output: 41.87 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 41.87it/s, est. speed input: 42872.05 toks/s, output: 41.87 toks/s]
[rank0]:[W128 00:58:03.699392890 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 28.8s

测试结果:
  Requests/s:   39.52
  Tokens/s:     40510.70
  Total Reqs:   256
  Elapsed:      6.48s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     40471.18

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:58:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3516314) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3516314) WARNING 01-28 00:58:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.30 requests/s, 40287.61 total tokens/s, 39.30 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 00:58:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:58:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:58:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:58:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:58:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:58:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:58:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:58:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:58:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:58:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:58:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:58:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:58:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:58:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:58:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:58:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:58:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:58:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3516314) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3516314) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=3516314) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=3516314) 
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3516314) [2026-01-28 00:58:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3516314) 2026-01-28 00:58:25,472 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3516314) 2026-01-28 00:58:25,488 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3516314) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 23.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 22.53it/s]
(EngineCore_DP0 pid=3516314) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.57it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 66/512 [00:00<00:00, 656.65it/s]
Adding requests:  28%|██▊       | 141/512 [00:00<00:00, 709.18it/s]
Adding requests:  42%|████▏     | 215/512 [00:00<00:00, 721.16it/s]
Adding requests:  56%|█████▋    | 288/512 [00:00<00:00, 719.87it/s]
Adding requests:  71%|███████   | 363/512 [00:00<00:00, 729.23it/s]
Adding requests:  85%|████████▌ | 437/512 [00:00<00:00, 729.89it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 733.65it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 724.83it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:00<00:02, 223.92it/s, est. speed input: 229317.52 toks/s, output: 223.93 toks/s]
Processed prompts:  10%|▉         | 49/512 [00:00<00:06, 70.28it/s, est. speed input: 80795.51 toks/s, output: 78.90 toks/s]   
Processed prompts:  12%|█▏        | 61/512 [00:00<00:07, 57.26it/s, est. speed input: 67611.12 toks/s, output: 66.03 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:01<00:09, 46.87it/s, est. speed input: 58438.13 toks/s, output: 57.07 toks/s]
Processed prompts:  15%|█▌        | 77/512 [00:01<00:08, 50.32it/s, est. speed input: 59405.29 toks/s, output: 58.01 toks/s]
Processed prompts:  16%|█▋        | 84/512 [00:01<00:09, 45.58it/s, est. speed input: 56256.44 toks/s, output: 54.94 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:01<00:10, 40.81it/s, est. speed input: 53256.01 toks/s, output: 52.01 toks/s]
Processed prompts:  19%|█▊        | 95/512 [00:01<00:09, 42.34it/s, est. speed input: 53111.07 toks/s, output: 51.87 toks/s]
Processed prompts:  20%|█▉        | 100/512 [00:01<00:09, 43.79it/s, est. speed input: 52984.01 toks/s, output: 51.74 toks/s]
Processed prompts:  21%|██        | 105/512 [00:02<00:09, 45.10it/s, est. speed input: 52878.02 toks/s, output: 51.64 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:02<00:10, 37.19it/s, est. speed input: 50403.57 toks/s, output: 49.22 toks/s]
Processed prompts:  22%|██▏       | 115/512 [00:02<00:09, 39.88it/s, est. speed input: 50415.19 toks/s, output: 49.23 toks/s]
Processed prompts:  23%|██▎       | 120/512 [00:02<00:09, 42.15it/s, est. speed input: 50422.52 toks/s, output: 49.24 toks/s]
Processed prompts:  24%|██▍       | 125/512 [00:02<00:08, 44.00it/s, est. speed input: 50430.97 toks/s, output: 49.25 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:02<00:10, 35.91it/s, est. speed input: 48579.87 toks/s, output: 47.44 toks/s]
Processed prompts:  26%|██▋       | 135/512 [00:02<00:09, 39.00it/s, est. speed input: 48645.82 toks/s, output: 47.51 toks/s]
Processed prompts:  27%|██▋       | 140/512 [00:02<00:08, 41.56it/s, est. speed input: 48710.14 toks/s, output: 47.57 toks/s]
Processed prompts:  28%|██▊       | 145/512 [00:03<00:08, 43.65it/s, est. speed input: 48778.05 toks/s, output: 47.63 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:03<00:10, 35.59it/s, est. speed input: 47322.75 toks/s, output: 46.21 toks/s]
Processed prompts:  30%|███       | 154/512 [00:03<00:09, 36.54it/s, est. speed input: 47120.32 toks/s, output: 46.02 toks/s]
Processed prompts:  31%|███       | 158/512 [00:03<00:09, 37.33it/s, est. speed input: 46930.75 toks/s, output: 45.83 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:03<00:09, 37.90it/s, est. speed input: 46745.55 toks/s, output: 45.65 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:03<00:09, 38.38it/s, est. speed input: 46576.42 toks/s, output: 45.48 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:03<00:08, 38.76it/s, est. speed input: 46418.62 toks/s, output: 45.33 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:03<00:08, 38.98it/s, est. speed input: 46262.57 toks/s, output: 45.18 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:03<00:08, 39.17it/s, est. speed input: 46117.71 toks/s, output: 45.04 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:04<00:08, 39.33it/s, est. speed input: 45982.19 toks/s, output: 44.90 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:04<00:08, 39.44it/s, est. speed input: 45852.57 toks/s, output: 44.78 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:04<00:08, 39.53it/s, est. speed input: 45730.61 toks/s, output: 44.66 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:04<00:08, 39.54it/s, est. speed input: 45609.39 toks/s, output: 44.54 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:04<00:07, 39.53it/s, est. speed input: 45492.78 toks/s, output: 44.43 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:04<00:07, 39.55it/s, est. speed input: 45383.33 toks/s, output: 44.32 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:07, 39.52it/s, est. speed input: 45274.37 toks/s, output: 44.21 toks/s]
Processed prompts:  41%|████      | 210/512 [00:04<00:07, 39.52it/s, est. speed input: 45172.17 toks/s, output: 44.11 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:04<00:07, 39.52it/s, est. speed input: 45074.26 toks/s, output: 44.02 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:04<00:07, 39.55it/s, est. speed input: 44982.46 toks/s, output: 43.93 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:05<00:07, 39.59it/s, est. speed input: 44896.21 toks/s, output: 43.84 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:05<00:07, 39.62it/s, est. speed input: 44812.82 toks/s, output: 43.76 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:05<00:07, 39.60it/s, est. speed input: 44730.44 toks/s, output: 43.68 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:05<00:07, 39.59it/s, est. speed input: 44650.63 toks/s, output: 43.60 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:05<00:06, 39.59it/s, est. speed input: 44574.66 toks/s, output: 43.53 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:05<00:06, 39.62it/s, est. speed input: 44503.50 toks/s, output: 43.46 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:06, 39.61it/s, est. speed input: 44432.63 toks/s, output: 43.39 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:05<00:06, 39.63it/s, est. speed input: 44366.10 toks/s, output: 43.33 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:06, 39.60it/s, est. speed input: 44299.45 toks/s, output: 43.26 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:05<00:06, 39.57it/s, est. speed input: 44233.88 toks/s, output: 43.20 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:06<00:06, 39.58it/s, est. speed input: 44172.80 toks/s, output: 43.14 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:06<00:06, 39.58it/s, est. speed input: 44113.36 toks/s, output: 43.08 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:06<00:06, 39.62it/s, est. speed input: 44057.99 toks/s, output: 43.03 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:06<00:06, 39.63it/s, est. speed input: 44003.56 toks/s, output: 42.97 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:06<00:05, 39.61it/s, est. speed input: 43948.92 toks/s, output: 42.92 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:06<00:05, 39.64it/s, est. speed input: 43898.73 toks/s, output: 42.87 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:06<00:05, 39.65it/s, est. speed input: 43849.09 toks/s, output: 42.82 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:06<00:05, 39.59it/s, est. speed input: 43797.59 toks/s, output: 42.77 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:06<00:05, 39.57it/s, est. speed input: 43748.68 toks/s, output: 42.72 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:06<00:05, 39.60it/s, est. speed input: 43703.37 toks/s, output: 42.68 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:07<00:05, 39.63it/s, est. speed input: 43660.00 toks/s, output: 42.64 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:07<00:05, 39.60it/s, est. speed input: 43615.42 toks/s, output: 42.59 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:07<00:05, 39.60it/s, est. speed input: 43572.56 toks/s, output: 42.55 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:07<00:04, 39.62it/s, est. speed input: 43532.26 toks/s, output: 42.51 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:07<00:04, 39.65it/s, est. speed input: 43493.69 toks/s, output: 42.47 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:07<00:04, 39.67it/s, est. speed input: 43456.35 toks/s, output: 42.44 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:07<00:04, 39.67it/s, est. speed input: 43418.95 toks/s, output: 42.40 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:07<00:04, 39.65it/s, est. speed input: 43382.01 toks/s, output: 42.37 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:07<00:04, 39.60it/s, est. speed input: 43343.98 toks/s, output: 42.33 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:07<00:04, 39.61it/s, est. speed input: 43309.39 toks/s, output: 42.29 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:08<00:04, 39.64it/s, est. speed input: 43276.07 toks/s, output: 42.26 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:08<00:04, 39.65it/s, est. speed input: 43243.71 toks/s, output: 42.23 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:08<00:04, 39.65it/s, est. speed input: 43211.35 toks/s, output: 42.20 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:08<00:03, 39.65it/s, est. speed input: 43180.26 toks/s, output: 42.17 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:08<00:03, 39.64it/s, est. speed input: 43149.05 toks/s, output: 42.14 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:08<00:03, 39.62it/s, est. speed input: 43118.33 toks/s, output: 42.11 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:08<00:03, 39.56it/s, est. speed input: 43086.14 toks/s, output: 42.08 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:08<00:03, 39.53it/s, est. speed input: 43055.31 toks/s, output: 42.05 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:08<00:03, 39.49it/s, est. speed input: 43024.52 toks/s, output: 42.02 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:09<00:03, 39.51it/s, est. speed input: 42995.98 toks/s, output: 41.99 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:09<00:03, 39.54it/s, est. speed input: 42969.00 toks/s, output: 41.96 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:09<00:03, 39.56it/s, est. speed input: 42942.51 toks/s, output: 41.94 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:09<00:03, 39.57it/s, est. speed input: 42916.43 toks/s, output: 41.91 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:09<00:02, 39.58it/s, est. speed input: 42891.15 toks/s, output: 41.89 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:09<00:02, 39.59it/s, est. speed input: 42866.56 toks/s, output: 41.86 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:09<00:02, 39.63it/s, est. speed input: 42843.32 toks/s, output: 41.84 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:09<00:02, 39.66it/s, est. speed input: 42820.87 toks/s, output: 41.82 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:09<00:02, 39.61it/s, est. speed input: 42796.33 toks/s, output: 41.79 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:09<00:02, 39.60it/s, est. speed input: 42773.14 toks/s, output: 41.77 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:10<00:02, 39.62it/s, est. speed input: 42751.52 toks/s, output: 41.75 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:10<00:02, 39.63it/s, est. speed input: 42729.97 toks/s, output: 41.73 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:10<00:02, 39.63it/s, est. speed input: 42708.75 toks/s, output: 41.71 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:10<00:02, 39.67it/s, est. speed input: 42689.23 toks/s, output: 41.69 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:10<00:01, 39.67it/s, est. speed input: 42669.44 toks/s, output: 41.67 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:10<00:01, 39.63it/s, est. speed input: 42648.32 toks/s, output: 41.65 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:10<00:01, 39.65it/s, est. speed input: 42629.22 toks/s, output: 41.63 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:10<00:01, 39.64it/s, est. speed input: 42609.99 toks/s, output: 41.61 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:10<00:01, 39.63it/s, est. speed input: 42590.71 toks/s, output: 41.59 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:10<00:01, 39.56it/s, est. speed input: 42569.88 toks/s, output: 41.57 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:11<00:01, 39.57it/s, est. speed input: 42551.32 toks/s, output: 41.55 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:11<00:01, 39.56it/s, est. speed input: 42532.57 toks/s, output: 41.54 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:11<00:01, 39.51it/s, est. speed input: 42512.60 toks/s, output: 41.52 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:11<00:01, 39.52it/s, est. speed input: 42494.71 toks/s, output: 41.50 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:11<00:00, 39.49it/s, est. speed input: 42475.80 toks/s, output: 41.48 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:11<00:00, 39.49it/s, est. speed input: 42457.94 toks/s, output: 41.46 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:11<00:00, 39.53it/s, est. speed input: 42441.48 toks/s, output: 41.45 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:11<00:00, 39.57it/s, est. speed input: 42425.81 toks/s, output: 41.43 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:11<00:00, 39.45it/s, est. speed input: 42405.98 toks/s, output: 41.41 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:11<00:00, 39.44it/s, est. speed input: 42388.40 toks/s, output: 41.39 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:12<00:00, 39.48it/s, est. speed input: 42372.69 toks/s, output: 41.38 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:12<00:00, 39.51it/s, est. speed input: 42357.38 toks/s, output: 41.36 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:12<00:00, 39.49it/s, est. speed input: 42341.17 toks/s, output: 41.35 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 39.49it/s, est. speed input: 42562.44 toks/s, output: 41.56 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 41.56it/s, est. speed input: 42562.44 toks/s, output: 41.56 toks/s]
[rank0]:[W128 00:58:40.821185998 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.2s

测试结果:
  Requests/s:   39.30
  Tokens/s:     40287.61
  Total Reqs:   512
  Elapsed:      13.03s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     40248.31

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:58:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3517107) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3517107) WARNING 01-28 00:58:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.64 requests/s, 38585.45 total tokens/s, 37.64 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 00:58:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:58:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:58:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:58:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:58:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:58:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:58:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:58:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:58:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:58:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:58:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:58:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:58:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:58:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:58:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:58:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:58:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:58:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:58:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3517107) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3517107) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=3517107) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=3517107) 
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3517107) [2026-01-28 00:58:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3517107) 2026-01-28 00:59:02,924 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3517107) 2026-01-28 00:59:02,940 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3517107) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  6.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 10.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 12.21it/s]
(EngineCore_DP0 pid=3517107) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.69it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.72it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 74/1024 [00:00<00:01, 731.39it/s]
Adding requests:  15%|█▍        | 150/1024 [00:00<00:01, 747.45it/s]
Adding requests:  22%|██▏       | 227/1024 [00:00<00:01, 755.35it/s]
Adding requests:  30%|██▉       | 303/1024 [00:00<00:00, 756.35it/s]
Adding requests:  37%|███▋      | 380/1024 [00:00<00:00, 760.88it/s]
Adding requests:  45%|████▍     | 457/1024 [00:00<00:00, 752.74it/s]
Adding requests:  52%|█████▏    | 533/1024 [00:00<00:00, 741.23it/s]
Adding requests:  60%|█████▉    | 611/1024 [00:00<00:00, 752.16it/s]
Adding requests:  67%|██████▋   | 691/1024 [00:00<00:00, 764.54it/s]
Adding requests:  75%|███████▌  | 768/1024 [00:01<00:00, 764.25it/s]
Adding requests:  83%|████████▎ | 845/1024 [00:01<00:00, 748.79it/s]
Adding requests:  90%|█████████ | 922/1024 [00:01<00:00, 752.67it/s]
Adding requests:  97%|█████████▋| 998/1024 [00:01<00:00, 746.79it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 751.75it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:00<00:03, 249.99it/s, est. speed input: 256010.01 toks/s, output: 250.00 toks/s]
Processed prompts:   7%|▋         | 75/1024 [00:00<00:12, 77.42it/s, est. speed input: 91979.64 toks/s, output: 89.82 toks/s]   
Processed prompts:   9%|▊         | 88/1024 [00:01<00:12, 72.90it/s, est. speed input: 86073.74 toks/s, output: 84.06 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:01<00:18, 50.99it/s, est. speed input: 68263.78 toks/s, output: 66.66 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:01<00:19, 47.79it/s, est. speed input: 64536.73 toks/s, output: 63.02 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:01<00:20, 45.26it/s, est. speed input: 61662.23 toks/s, output: 60.22 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:02<00:20, 43.26it/s, est. speed input: 59356.46 toks/s, output: 57.96 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:02<00:21, 41.72it/s, est. speed input: 57457.99 toks/s, output: 56.11 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:02<00:21, 40.61it/s, est. speed input: 55886.09 toks/s, output: 54.58 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:02<00:22, 39.80it/s, est. speed input: 54561.84 toks/s, output: 53.28 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:02<00:22, 39.23it/s, est. speed input: 53428.44 toks/s, output: 52.18 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:03<00:22, 38.80it/s, est. speed input: 52440.37 toks/s, output: 51.21 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:03<00:22, 38.50it/s, est. speed input: 51577.92 toks/s, output: 50.37 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:03<00:22, 38.28it/s, est. speed input: 50815.45 toks/s, output: 49.62 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:03<00:21, 38.13it/s, est. speed input: 50139.18 toks/s, output: 48.96 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:04<00:21, 38.05it/s, est. speed input: 49539.44 toks/s, output: 48.38 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:04<00:21, 37.98it/s, est. speed input: 48997.35 toks/s, output: 47.85 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:04<00:21, 37.90it/s, est. speed input: 48500.72 toks/s, output: 47.36 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:04<00:21, 37.88it/s, est. speed input: 48055.99 toks/s, output: 46.93 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:04<00:21, 37.88it/s, est. speed input: 47652.53 toks/s, output: 46.54 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:05<00:20, 37.86it/s, est. speed input: 47280.71 toks/s, output: 46.17 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:05<00:20, 37.86it/s, est. speed input: 46940.47 toks/s, output: 45.84 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:05<00:20, 37.84it/s, est. speed input: 46622.10 toks/s, output: 45.53 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:05<00:20, 37.84it/s, est. speed input: 46330.55 toks/s, output: 45.24 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:05<00:20, 37.84it/s, est. speed input: 46059.75 toks/s, output: 44.98 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:06<00:19, 37.83it/s, est. speed input: 45805.11 toks/s, output: 44.73 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:06<00:19, 37.79it/s, est. speed input: 45564.55 toks/s, output: 44.50 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:06<00:19, 37.79it/s, est. speed input: 45342.79 toks/s, output: 44.28 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:06<00:19, 37.81it/s, est. speed input: 45136.39 toks/s, output: 44.08 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:06<00:18, 37.81it/s, est. speed input: 44942.18 toks/s, output: 43.89 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:07<00:18, 37.80it/s, est. speed input: 44757.97 toks/s, output: 43.71 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:07<00:18, 37.79it/s, est. speed input: 44583.15 toks/s, output: 43.54 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:07<00:18, 37.80it/s, est. speed input: 44420.68 toks/s, output: 43.38 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:07<00:18, 37.81it/s, est. speed input: 44266.60 toks/s, output: 43.23 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:08<00:17, 37.83it/s, est. speed input: 44122.70 toks/s, output: 43.09 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:08<00:17, 37.82it/s, est. speed input: 43983.76 toks/s, output: 42.95 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:08<00:17, 37.79it/s, est. speed input: 43849.03 toks/s, output: 42.82 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:08<00:17, 37.78it/s, est. speed input: 43722.59 toks/s, output: 42.70 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:08<00:17, 37.77it/s, est. speed input: 43601.22 toks/s, output: 42.58 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:09<00:16, 37.78it/s, est. speed input: 43487.74 toks/s, output: 42.47 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:09<00:16, 37.79it/s, est. speed input: 43378.73 toks/s, output: 42.36 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:09<00:16, 37.76it/s, est. speed input: 43272.66 toks/s, output: 42.26 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:09<00:16, 37.78it/s, est. speed input: 43173.74 toks/s, output: 42.16 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:09<00:16, 37.80it/s, est. speed input: 43079.66 toks/s, output: 42.07 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:10<00:15, 37.81it/s, est. speed input: 42988.89 toks/s, output: 41.98 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:10<00:15, 37.76it/s, est. speed input: 42898.03 toks/s, output: 41.89 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:10<00:15, 37.77it/s, est. speed input: 42814.19 toks/s, output: 41.81 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:10<00:15, 37.78it/s, est. speed input: 42733.16 toks/s, output: 41.73 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:10<00:14, 37.78it/s, est. speed input: 42655.78 toks/s, output: 41.66 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:11<00:14, 37.78it/s, est. speed input: 42580.65 toks/s, output: 41.58 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:11<00:14, 37.77it/s, est. speed input: 42507.39 toks/s, output: 41.51 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:11<00:14, 37.77it/s, est. speed input: 42437.99 toks/s, output: 41.44 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:11<00:14, 37.78it/s, est. speed input: 42371.06 toks/s, output: 41.38 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:12<00:13, 37.77it/s, est. speed input: 42306.03 toks/s, output: 41.31 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:12<00:13, 37.77it/s, est. speed input: 42243.54 toks/s, output: 41.25 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:12<00:13, 37.76it/s, est. speed input: 42181.96 toks/s, output: 41.19 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:12<00:13, 37.75it/s, est. speed input: 42122.60 toks/s, output: 41.14 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:12<00:13, 37.73it/s, est. speed input: 42064.39 toks/s, output: 41.08 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:13<00:12, 37.74it/s, est. speed input: 42009.51 toks/s, output: 41.02 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:13<00:12, 37.71it/s, est. speed input: 41954.42 toks/s, output: 40.97 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:13<00:12, 37.70it/s, est. speed input: 41901.52 toks/s, output: 40.92 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:13<00:12, 37.70it/s, est. speed input: 41850.55 toks/s, output: 40.87 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:13<00:12, 37.70it/s, est. speed input: 41801.02 toks/s, output: 40.82 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:14<00:11, 37.69it/s, est. speed input: 41752.95 toks/s, output: 40.77 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:14<00:11, 37.68it/s, est. speed input: 41705.78 toks/s, output: 40.73 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:14<00:11, 37.67it/s, est. speed input: 41660.02 toks/s, output: 40.68 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:14<00:11, 37.66it/s, est. speed input: 41615.20 toks/s, output: 40.64 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:15<00:10, 37.67it/s, est. speed input: 41572.41 toks/s, output: 40.60 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:15<00:10, 37.66it/s, est. speed input: 41530.25 toks/s, output: 40.56 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:15<00:10, 37.64it/s, est. speed input: 41488.40 toks/s, output: 40.52 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:15<00:10, 37.65it/s, est. speed input: 41448.99 toks/s, output: 40.48 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:15<00:10, 37.67it/s, est. speed input: 41411.17 toks/s, output: 40.44 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:16<00:09, 37.67it/s, est. speed input: 41373.71 toks/s, output: 40.40 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:16<00:09, 37.67it/s, est. speed input: 41337.30 toks/s, output: 40.37 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:16<00:09, 37.65it/s, est. speed input: 41300.96 toks/s, output: 40.33 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:16<00:09, 37.66it/s, est. speed input: 41266.28 toks/s, output: 40.30 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:16<00:09, 37.65it/s, est. speed input: 41231.81 toks/s, output: 40.27 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:17<00:08, 37.66it/s, est. speed input: 41199.07 toks/s, output: 40.23 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:17<00:08, 37.66it/s, est. speed input: 41166.72 toks/s, output: 40.20 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:17<00:08, 37.67it/s, est. speed input: 41136.02 toks/s, output: 40.17 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:17<00:08, 37.69it/s, est. speed input: 41106.11 toks/s, output: 40.14 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:17<00:08, 37.70it/s, est. speed input: 41076.90 toks/s, output: 40.11 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:18<00:07, 37.69it/s, est. speed input: 41047.60 toks/s, output: 40.09 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:18<00:07, 37.67it/s, est. speed input: 41018.65 toks/s, output: 40.06 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:18<00:07, 37.66it/s, est. speed input: 40990.27 toks/s, output: 40.03 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:18<00:07, 37.66it/s, est. speed input: 40963.08 toks/s, output: 40.00 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:19<00:06, 37.65it/s, est. speed input: 40935.67 toks/s, output: 39.98 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:19<00:06, 37.65it/s, est. speed input: 40909.61 toks/s, output: 39.95 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:19<00:06, 37.66it/s, est. speed input: 40884.09 toks/s, output: 39.93 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:19<00:06, 37.67it/s, est. speed input: 40859.60 toks/s, output: 39.90 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:19<00:06, 37.68it/s, est. speed input: 40835.59 toks/s, output: 39.88 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:20<00:05, 37.70it/s, est. speed input: 40812.44 toks/s, output: 39.86 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:20<00:05, 37.67it/s, est. speed input: 40788.52 toks/s, output: 39.83 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:20<00:05, 37.68it/s, est. speed input: 40765.77 toks/s, output: 39.81 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:20<00:05, 37.68it/s, est. speed input: 40743.62 toks/s, output: 39.79 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:20<00:05, 37.69it/s, est. speed input: 40722.21 toks/s, output: 39.77 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:21<00:04, 37.68it/s, est. speed input: 40700.27 toks/s, output: 39.75 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:21<00:04, 37.67it/s, est. speed input: 40679.03 toks/s, output: 39.73 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:21<00:04, 37.66it/s, est. speed input: 40658.09 toks/s, output: 39.71 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:21<00:04, 37.67it/s, est. speed input: 40638.03 toks/s, output: 39.69 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:22<00:03, 37.68it/s, est. speed input: 40618.53 toks/s, output: 39.67 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:22<00:03, 37.68it/s, est. speed input: 40599.12 toks/s, output: 39.65 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:22<00:03, 37.68it/s, est. speed input: 40580.13 toks/s, output: 39.63 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:22<00:03, 37.67it/s, est. speed input: 40561.03 toks/s, output: 39.61 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:22<00:03, 37.68it/s, est. speed input: 40543.05 toks/s, output: 39.59 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:23<00:02, 37.68it/s, est. speed input: 40525.10 toks/s, output: 39.58 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:23<00:02, 37.67it/s, est. speed input: 40506.93 toks/s, output: 39.56 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:23<00:02, 37.67it/s, est. speed input: 40489.45 toks/s, output: 39.54 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:23<00:02, 37.66it/s, est. speed input: 40472.02 toks/s, output: 39.52 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:23<00:02, 37.69it/s, est. speed input: 40455.95 toks/s, output: 39.51 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:24<00:01, 37.67it/s, est. speed input: 40438.92 toks/s, output: 39.49 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:24<00:01, 37.64it/s, est. speed input: 40421.87 toks/s, output: 39.47 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:24<00:01, 37.63it/s, est. speed input: 40405.64 toks/s, output: 39.46 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:24<00:01, 37.66it/s, est. speed input: 40390.15 toks/s, output: 39.44 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:25<00:01, 37.67it/s, est. speed input: 40375.00 toks/s, output: 39.43 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:25<00:00, 37.66it/s, est. speed input: 40359.52 toks/s, output: 39.41 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:25<00:00, 37.64it/s, est. speed input: 40343.83 toks/s, output: 39.40 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:25<00:00, 37.65it/s, est. speed input: 40329.38 toks/s, output: 39.38 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:25<00:00, 38.67it/s, est. speed input: 40343.79 toks/s, output: 39.40 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:25<00:00, 38.67it/s, est. speed input: 40581.38 toks/s, output: 39.63 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:25<00:00, 39.63it/s, est. speed input: 40581.38 toks/s, output: 39.63 toks/s]
[rank0]:[W128 00:59:32.841686794 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.1s

测试结果:
  Requests/s:   37.64
  Tokens/s:     38585.45
  Total Reqs:   1024
  Elapsed:      27.20s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     38547.80

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:59:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3518133) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3518133) WARNING 01-28 00:59:51 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.96 requests/s, 37888.59 total tokens/s, 36.96 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 00:59:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:59:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:59:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:59:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:59:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:59:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:59:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:59:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:59:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:59:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:59:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:59:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:59:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:59:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:59:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:59:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 00:59:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 00:59:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:59:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:59:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:59:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:59:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 00:59:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 00:59:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:59:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:59:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:59:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:59:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3518133) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3518133) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=3518133) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.81it/s]
(EngineCore_DP0 pid=3518133) 
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3518133) [2026-01-28 00:59:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3518133) 2026-01-28 00:59:57,693 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3518133) 2026-01-28 00:59:57,709 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3518133) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  4.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 13.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 18.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.44it/s]
(EngineCore_DP0 pid=3518133) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.36it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.41it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 72/2048 [00:00<00:02, 717.46it/s]
Adding requests:   7%|▋         | 147/2048 [00:00<00:02, 735.21it/s]
Adding requests:  11%|█         | 222/2048 [00:00<00:02, 740.51it/s]
Adding requests:  15%|█▍        | 297/2048 [00:00<00:02, 743.88it/s]
Adding requests:  18%|█▊        | 372/2048 [00:00<00:02, 737.17it/s]
Adding requests:  22%|██▏       | 447/2048 [00:00<00:02, 739.11it/s]
Adding requests:  25%|██▌       | 521/2048 [00:00<00:02, 735.59it/s]
Adding requests:  29%|██▉       | 597/2048 [00:00<00:01, 741.54it/s]
Adding requests:  33%|███▎      | 675/2048 [00:00<00:01, 753.06it/s]
Adding requests:  37%|███▋      | 751/2048 [00:01<00:01, 750.19it/s]
Adding requests:  40%|████      | 827/2048 [00:01<00:01, 730.41it/s]
Adding requests:  44%|████▍     | 904/2048 [00:01<00:01, 741.37it/s]
Adding requests:  48%|████▊     | 979/2048 [00:01<00:01, 740.85it/s]
Adding requests:  52%|█████▏    | 1055/2048 [00:01<00:01, 743.24it/s]
Adding requests:  55%|█████▌    | 1130/2048 [00:01<00:01, 735.77it/s]
Adding requests:  59%|█████▉    | 1208/2048 [00:01<00:01, 746.49it/s]
Adding requests:  63%|██████▎   | 1283/2048 [00:01<00:01, 742.27it/s]
Adding requests:  66%|██████▋   | 1361/2048 [00:01<00:00, 752.05it/s]
Adding requests:  70%|███████   | 1440/2048 [00:01<00:00, 761.23it/s]
Adding requests:  74%|███████▍  | 1519/2048 [00:02<00:00, 768.06it/s]
Adding requests:  78%|███████▊  | 1600/2048 [00:02<00:00, 778.65it/s]
Adding requests:  82%|████████▏ | 1678/2048 [00:02<00:00, 775.80it/s]
Adding requests:  86%|████████▌ | 1756/2048 [00:02<00:00, 766.70it/s]
Adding requests:  90%|████████▉ | 1833/2048 [00:02<00:00, 758.71it/s]
Adding requests:  93%|█████████▎| 1909/2048 [00:02<00:00, 754.07it/s]
Adding requests:  97%|█████████▋| 1985/2048 [00:02<00:00, 740.56it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 748.28it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:00<00:07, 269.68it/s, est. speed input: 276163.82 toks/s, output: 269.68 toks/s]
Processed prompts:   6%|▌         | 125/2048 [00:00<00:13, 139.53it/s, est. speed input: 161177.24 toks/s, output: 157.40 toks/s]
Processed prompts:   7%|▋         | 140/2048 [00:01<00:21, 89.94it/s, est. speed input: 116996.00 toks/s, output: 114.25 toks/s] 
Processed prompts:   7%|▋         | 150/2048 [00:01<00:30, 63.02it/s, est. speed input: 92729.72 toks/s, output: 90.56 toks/s]  
Processed prompts:   8%|▊         | 162/2048 [00:02<00:37, 50.16it/s, est. speed input: 79471.24 toks/s, output: 77.61 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:02<00:40, 45.69it/s, est. speed input: 72380.75 toks/s, output: 70.68 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:02<00:43, 42.87it/s, est. speed input: 67352.85 toks/s, output: 65.77 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:03<00:44, 41.02it/s, est. speed input: 63607.66 toks/s, output: 62.12 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:03<00:45, 39.80it/s, est. speed input: 60714.43 toks/s, output: 59.29 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:04<00:46, 38.96it/s, est. speed input: 58404.12 toks/s, output: 57.04 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:04<00:46, 38.31it/s, est. speed input: 56485.00 toks/s, output: 55.16 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:05<00:46, 37.95it/s, est. speed input: 54924.39 toks/s, output: 53.64 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:05<00:46, 37.68it/s, est. speed input: 53602.05 toks/s, output: 52.35 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:05<00:46, 37.50it/s, est. speed input: 52473.05 toks/s, output: 51.24 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:06<00:46, 37.37it/s, est. speed input: 51494.28 toks/s, output: 50.29 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:06<00:45, 37.28it/s, est. speed input: 50639.57 toks/s, output: 49.45 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:07<00:45, 37.20it/s, est. speed input: 49882.39 toks/s, output: 48.71 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:07<00:45, 37.15it/s, est. speed input: 49212.35 toks/s, output: 48.06 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:08<00:44, 37.13it/s, est. speed input: 48615.89 toks/s, output: 47.48 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:08<00:44, 37.11it/s, est. speed input: 48076.92 toks/s, output: 46.95 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:08<00:43, 37.09it/s, est. speed input: 47591.18 toks/s, output: 46.48 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:09<00:43, 37.07it/s, est. speed input: 47146.80 toks/s, output: 46.04 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:09<00:43, 37.06it/s, est. speed input: 46741.97 toks/s, output: 45.65 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:10<00:42, 37.04it/s, est. speed input: 46369.95 toks/s, output: 45.28 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:10<00:42, 37.03it/s, est. speed input: 46028.02 toks/s, output: 44.95 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:11<00:41, 37.02it/s, est. speed input: 45713.16 toks/s, output: 44.64 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:11<00:41, 37.02it/s, est. speed input: 45421.71 toks/s, output: 44.36 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:12<00:41, 37.02it/s, est. speed input: 45151.53 toks/s, output: 44.09 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:12<00:40, 37.01it/s, est. speed input: 44898.94 toks/s, output: 43.85 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:12<00:40, 37.00it/s, est. speed input: 44663.16 toks/s, output: 43.62 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:13<00:39, 37.00it/s, est. speed input: 44442.83 toks/s, output: 43.40 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:13<00:39, 37.00it/s, est. speed input: 44236.31 toks/s, output: 43.20 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:14<00:38, 36.99it/s, est. speed input: 44041.81 toks/s, output: 43.01 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:14<00:38, 36.99it/s, est. speed input: 43859.60 toks/s, output: 42.83 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:15<00:38, 37.00it/s, est. speed input: 43688.05 toks/s, output: 42.66 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:15<00:37, 36.99it/s, est. speed input: 43525.19 toks/s, output: 42.51 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:15<00:37, 37.00it/s, est. speed input: 43372.82 toks/s, output: 42.36 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:16<00:36, 37.00it/s, est. speed input: 43227.62 toks/s, output: 42.21 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:16<00:36, 37.00it/s, est. speed input: 43089.98 toks/s, output: 42.08 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:17<00:35, 37.01it/s, est. speed input: 42959.98 toks/s, output: 41.95 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:17<00:35, 37.00it/s, est. speed input: 42835.25 toks/s, output: 41.83 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:18<00:34, 37.01it/s, est. speed input: 42717.38 toks/s, output: 41.72 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:18<00:34, 37.00it/s, est. speed input: 42604.30 toks/s, output: 41.61 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:18<00:34, 37.01it/s, est. speed input: 42497.04 toks/s, output: 41.50 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:19<00:33, 36.99it/s, est. speed input: 42393.39 toks/s, output: 41.40 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:19<00:33, 36.99it/s, est. speed input: 42294.86 toks/s, output: 41.30 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:20<00:32, 36.99it/s, est. speed input: 42200.46 toks/s, output: 41.21 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:20<00:32, 37.00it/s, est. speed input: 42110.59 toks/s, output: 41.12 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:21<00:31, 37.00it/s, est. speed input: 42024.42 toks/s, output: 41.04 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:21<00:31, 37.00it/s, est. speed input: 41941.11 toks/s, output: 40.96 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:21<00:31, 37.00it/s, est. speed input: 41861.55 toks/s, output: 40.88 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:22<00:30, 37.00it/s, est. speed input: 41784.33 toks/s, output: 40.80 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:22<00:30, 36.99it/s, est. speed input: 41710.39 toks/s, output: 40.73 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:23<00:29, 36.99it/s, est. speed input: 41638.97 toks/s, output: 40.66 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:23<00:29, 37.00it/s, est. speed input: 41570.64 toks/s, output: 40.60 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:24<00:28, 37.00it/s, est. speed input: 41504.55 toks/s, output: 40.53 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:24<00:28, 36.98it/s, est. speed input: 41440.01 toks/s, output: 40.47 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:24<00:28, 36.99it/s, est. speed input: 41378.53 toks/s, output: 40.41 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:25<00:27, 36.98it/s, est. speed input: 41318.46 toks/s, output: 40.35 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:25<00:27, 36.99it/s, est. speed input: 41261.07 toks/s, output: 40.29 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:26<00:26, 36.99it/s, est. speed input: 41205.60 toks/s, output: 40.24 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:26<00:26, 36.99it/s, est. speed input: 41151.65 toks/s, output: 40.19 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:27<00:25, 36.99it/s, est. speed input: 41099.46 toks/s, output: 40.14 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:27<00:25, 36.99it/s, est. speed input: 41048.73 toks/s, output: 40.09 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:28<00:25, 36.99it/s, est. speed input: 40999.97 toks/s, output: 40.04 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:28<00:24, 36.98it/s, est. speed input: 40952.22 toks/s, output: 39.99 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:28<00:23, 37.55it/s, est. speed input: 40936.93 toks/s, output: 39.98 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:29<00:23, 37.37it/s, est. speed input: 40891.34 toks/s, output: 39.93 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:29<00:23, 37.26it/s, est. speed input: 40847.71 toks/s, output: 39.89 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:30<00:22, 37.19it/s, est. speed input: 40805.64 toks/s, output: 39.85 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:30<00:22, 37.12it/s, est. speed input: 40763.94 toks/s, output: 39.81 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:31<00:21, 37.09it/s, est. speed input: 40723.81 toks/s, output: 39.77 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:31<00:21, 37.05it/s, est. speed input: 40684.27 toks/s, output: 39.73 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:31<00:21, 37.03it/s, est. speed input: 40646.05 toks/s, output: 39.69 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:32<00:20, 37.01it/s, est. speed input: 40608.79 toks/s, output: 39.66 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:32<00:20, 37.00it/s, est. speed input: 40572.38 toks/s, output: 39.62 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:33<00:19, 37.00it/s, est. speed input: 40537.62 toks/s, output: 39.59 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:33<00:19, 37.00it/s, est. speed input: 40503.45 toks/s, output: 39.55 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:34<00:18, 37.00it/s, est. speed input: 40470.35 toks/s, output: 39.52 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:34<00:18, 36.98it/s, est. speed input: 40437.14 toks/s, output: 39.49 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:34<00:18, 36.98it/s, est. speed input: 40405.34 toks/s, output: 39.46 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:35<00:17, 36.98it/s, est. speed input: 40374.18 toks/s, output: 39.43 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:35<00:17, 36.99it/s, est. speed input: 40344.24 toks/s, output: 39.40 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:36<00:16, 36.99it/s, est. speed input: 40314.77 toks/s, output: 39.37 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:36<00:16, 36.98it/s, est. speed input: 40285.73 toks/s, output: 39.34 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:37<00:15, 36.99it/s, est. speed input: 40258.00 toks/s, output: 39.31 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:37<00:15, 36.98it/s, est. speed input: 40230.06 toks/s, output: 39.29 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:37<00:15, 36.98it/s, est. speed input: 40203.26 toks/s, output: 39.26 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:38<00:14, 36.98it/s, est. speed input: 40176.86 toks/s, output: 39.24 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:38<00:14, 36.98it/s, est. speed input: 40151.02 toks/s, output: 39.21 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:39<00:13, 36.98it/s, est. speed input: 40125.97 toks/s, output: 39.19 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:39<00:13, 36.98it/s, est. speed input: 40101.40 toks/s, output: 39.16 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:40<00:12, 36.99it/s, est. speed input: 40077.54 toks/s, output: 39.14 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:40<00:12, 36.98it/s, est. speed input: 40053.87 toks/s, output: 39.12 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:40<00:12, 36.98it/s, est. speed input: 40030.86 toks/s, output: 39.09 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:41<00:11, 36.99it/s, est. speed input: 40008.46 toks/s, output: 39.07 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:41<00:11, 36.99it/s, est. speed input: 39986.41 toks/s, output: 39.05 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:42<00:10, 36.98it/s, est. speed input: 39964.68 toks/s, output: 39.03 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:42<00:10, 36.99it/s, est. speed input: 39943.67 toks/s, output: 39.01 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:43<00:09, 37.00it/s, est. speed input: 39923.20 toks/s, output: 38.99 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:43<00:09, 36.98it/s, est. speed input: 39902.54 toks/s, output: 38.97 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:44<00:09, 37.00it/s, est. speed input: 39882.99 toks/s, output: 38.95 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:44<00:08, 36.99it/s, est. speed input: 39863.20 toks/s, output: 38.93 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:44<00:08, 36.99it/s, est. speed input: 39844.06 toks/s, output: 38.91 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:45<00:07, 36.98it/s, est. speed input: 39824.98 toks/s, output: 38.89 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:45<00:07, 36.99it/s, est. speed input: 39806.68 toks/s, output: 38.87 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:46<00:06, 36.99it/s, est. speed input: 39788.62 toks/s, output: 38.86 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:46<00:06, 36.98it/s, est. speed input: 39770.66 toks/s, output: 38.84 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:47<00:06, 36.98it/s, est. speed input: 39753.17 toks/s, output: 38.82 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:47<00:05, 36.97it/s, est. speed input: 39735.78 toks/s, output: 38.80 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:47<00:05, 36.98it/s, est. speed input: 39719.11 toks/s, output: 38.79 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:48<00:04, 37.64it/s, est. speed input: 39723.31 toks/s, output: 38.79 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:48<00:04, 37.45it/s, est. speed input: 39707.14 toks/s, output: 38.78 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:49<00:03, 37.30it/s, est. speed input: 39690.76 toks/s, output: 38.76 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:49<00:03, 37.20it/s, est. speed input: 39674.73 toks/s, output: 38.74 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:50<00:02, 37.14it/s, est. speed input: 39659.25 toks/s, output: 38.73 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:50<00:02, 37.08it/s, est. speed input: 39643.69 toks/s, output: 38.71 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:50<00:02, 37.06it/s, est. speed input: 39628.68 toks/s, output: 38.70 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:51<00:01, 37.02it/s, est. speed input: 39613.59 toks/s, output: 38.69 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:51<00:01, 37.01it/s, est. speed input: 39599.01 toks/s, output: 38.67 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:52<00:00, 37.01it/s, est. speed input: 39584.97 toks/s, output: 38.66 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:52<00:00, 36.22it/s, est. speed input: 39547.46 toks/s, output: 38.62 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:52<00:00, 36.22it/s, est. speed input: 39819.53 toks/s, output: 38.89 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:52<00:00, 38.89it/s, est. speed input: 39819.53 toks/s, output: 38.89 toks/s]
[rank0]:[W128 01:00:54.778775867 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 82.9s

测试结果:
  Requests/s:   36.96
  Tokens/s:     37888.59
  Total Reqs:   2048
  Elapsed:      55.40s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     37851.63

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:01:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3519614) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3519614) WARNING 01-28 01:01:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.98 requests/s, 37903.58 total tokens/s, 36.98 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 01:01:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:01:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:01:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:01:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:01:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:01:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:01:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:01:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:01:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:01:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:01:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:01:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:01:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:01:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:01:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:01:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:01:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:01:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:01:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:01:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:01:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:01:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:01:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:01:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:01:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:01:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:01:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:01:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3519614) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3519614) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.78it/s]
(EngineCore_DP0 pid=3519614) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.78it/s]
(EngineCore_DP0 pid=3519614) 
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3519614) [2026-01-28 01:01:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3519614) [rank0]:W0128 01:01:23.808000 3519614 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3519614) [rank0]:W0128 01:01:23.857000 3519614 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3519614) [rank0]:W0128 01:01:24.579000 3519614 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3519614) [rank0]:W0128 01:01:24.656000 3519614 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3519614) 2026-01-28 01:01:27,097 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3519614) 2026-01-28 01:01:27,120 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3519614) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:06,  1.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:01,  6.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 10.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 14.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 10.69it/s]
(EngineCore_DP0 pid=3519614) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.26it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 23.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 20.40it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 71/4096 [00:00<00:05, 709.65it/s]
Adding requests:   4%|▎         | 146/4096 [00:00<00:05, 732.12it/s]
Adding requests:   5%|▌         | 221/4096 [00:00<00:05, 737.11it/s]
Adding requests:   7%|▋         | 299/4096 [00:00<00:05, 752.29it/s]
Adding requests:   9%|▉         | 375/4096 [00:00<00:04, 747.32it/s]
Adding requests:  11%|█         | 451/4096 [00:00<00:04, 748.76it/s]
Adding requests:  13%|█▎        | 526/4096 [00:00<00:04, 738.40it/s]
Adding requests:  15%|█▍        | 602/4096 [00:00<00:04, 743.60it/s]
Adding requests:  17%|█▋        | 682/4096 [00:00<00:04, 759.35it/s]
Adding requests:  19%|█▊        | 758/4096 [00:01<00:04, 755.21it/s]
Adding requests:  20%|██        | 834/4096 [00:01<00:04, 737.22it/s]
Adding requests:  22%|██▏       | 914/4096 [00:01<00:04, 752.47it/s]
Adding requests:  24%|██▍       | 993/4096 [00:01<00:04, 761.40it/s]
Adding requests:  26%|██▌       | 1071/4096 [00:01<00:03, 763.65it/s]
Adding requests:  28%|██▊       | 1148/4096 [00:01<00:03, 745.30it/s]
Adding requests:  30%|██▉       | 1225/4096 [00:01<00:03, 750.56it/s]
Adding requests:  32%|███▏      | 1301/4096 [00:01<00:03, 749.76it/s]
Adding requests:  34%|███▎      | 1379/4096 [00:01<00:03, 757.55it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:01<00:03, 761.90it/s]
Adding requests:  38%|███▊      | 1537/4096 [00:02<00:03, 770.73it/s]
Adding requests:  39%|███▉      | 1615/4096 [00:02<00:03, 769.66it/s]
Adding requests:  41%|████▏     | 1692/4096 [00:02<00:03, 766.26it/s]
Adding requests:  43%|████▎     | 1771/4096 [00:02<00:03, 771.02it/s]
Adding requests:  45%|████▌     | 1850/4096 [00:02<00:02, 776.50it/s]
Adding requests:  47%|████▋     | 1928/4096 [00:02<00:02, 775.54it/s]
Adding requests:  49%|████▉     | 2008/4096 [00:02<00:02, 781.72it/s]
Adding requests:  51%|█████     | 2087/4096 [00:02<00:02, 781.54it/s]
Adding requests:  53%|█████▎    | 2166/4096 [00:02<00:02, 768.17it/s]
Adding requests:  55%|█████▍    | 2243/4096 [00:02<00:02, 767.16it/s]
Adding requests:  57%|█████▋    | 2320/4096 [00:03<00:02, 756.50it/s]
Adding requests:  58%|█████▊    | 2396/4096 [00:03<00:02, 757.07it/s]
Adding requests:  60%|██████    | 2475/4096 [00:03<00:02, 764.95it/s]
Adding requests:  62%|██████▏   | 2553/4096 [00:03<00:02, 767.46it/s]
Adding requests:  64%|██████▍   | 2630/4096 [00:03<00:01, 765.89it/s]
Adding requests:  66%|██████▌   | 2707/4096 [00:03<00:01, 761.50it/s]
Adding requests:  68%|██████▊   | 2784/4096 [00:03<00:01, 753.29it/s]
Adding requests:  70%|██████▉   | 2860/4096 [00:03<00:01, 753.94it/s]
Adding requests:  72%|███████▏  | 2936/4096 [00:03<00:01, 754.26it/s]
Adding requests:  74%|███████▎  | 3012/4096 [00:03<00:01, 755.25it/s]
Adding requests:  75%|███████▌  | 3089/4096 [00:04<00:01, 757.52it/s]
Adding requests:  77%|███████▋  | 3165/4096 [00:04<00:01, 751.09it/s]
Adding requests:  79%|███████▉  | 3241/4096 [00:04<00:01, 752.57it/s]
Adding requests:  81%|████████  | 3320/4096 [00:04<00:01, 762.83it/s]
Adding requests:  83%|████████▎ | 3397/4096 [00:04<00:00, 764.40it/s]
Adding requests:  85%|████████▍ | 3474/4096 [00:04<00:00, 762.52it/s]
Adding requests:  87%|████████▋ | 3554/4096 [00:04<00:00, 773.28it/s]
Adding requests:  89%|████████▊ | 3632/4096 [00:04<00:00, 770.99it/s]
Adding requests:  91%|█████████ | 3710/4096 [00:04<00:00, 757.05it/s]
Adding requests:  93%|█████████▎| 3789/4096 [00:04<00:00, 765.87it/s]
Adding requests:  94%|█████████▍| 3869/4096 [00:05<00:00, 774.34it/s]
Adding requests:  96%|█████████▋| 3947/4096 [00:05<00:00, 774.31it/s]
Adding requests:  98%|█████████▊| 4026/4096 [00:05<00:00, 776.50it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 761.19it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:00<00:15, 259.78it/s, est. speed input: 266023.25 toks/s, output: 259.79 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:01<00:31, 121.05it/s, est. speed input: 143718.81 toks/s, output: 140.35 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:02<00:47, 81.51it/s, est. speed input: 106805.41 toks/s, output: 104.30 toks/s] 
Processed prompts:   7%|▋         | 290/4096 [00:03<00:59, 63.62it/s, est. speed input: 88978.25 toks/s, output: 86.89 toks/s]  
Processed prompts:   8%|▊         | 322/4096 [00:04<01:09, 53.92it/s, est. speed input: 78484.83 toks/s, output: 76.65 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:05<01:17, 48.12it/s, est. speed input: 71556.31 toks/s, output: 69.88 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:05<01:23, 44.47it/s, est. speed input: 66653.09 toks/s, output: 65.09 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:06<01:27, 42.09it/s, est. speed input: 62994.17 toks/s, output: 61.52 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:07<01:30, 40.49it/s, est. speed input: 60158.82 toks/s, output: 58.75 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:08<01:31, 39.42it/s, est. speed input: 57902.90 toks/s, output: 56.55 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:09<01:32, 38.68it/s, est. speed input: 56059.90 toks/s, output: 54.75 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:10<01:33, 38.17it/s, est. speed input: 54526.97 toks/s, output: 53.25 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:11<01:33, 37.81it/s, est. speed input: 53231.18 toks/s, output: 51.98 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:11<01:32, 37.56it/s, est. speed input: 52123.29 toks/s, output: 50.90 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:12<01:32, 37.40it/s, est. speed input: 51165.93 toks/s, output: 49.97 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:13<01:31, 37.27it/s, est. speed input: 50327.91 toks/s, output: 49.15 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:14<01:31, 37.19it/s, est. speed input: 49589.68 toks/s, output: 48.43 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:15<01:30, 37.13it/s, est. speed input: 48933.09 toks/s, output: 47.79 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:16<01:29, 37.09it/s, est. speed input: 48346.88 toks/s, output: 47.21 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:17<01:28, 37.07it/s, est. speed input: 47821.36 toks/s, output: 46.70 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:18<01:28, 37.03it/s, est. speed input: 47342.51 toks/s, output: 46.23 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:18<01:27, 37.02it/s, est. speed input: 46909.53 toks/s, output: 45.81 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:19<01:26, 37.01it/s, est. speed input: 46514.49 toks/s, output: 45.42 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:20<01:25, 37.01it/s, est. speed input: 46152.34 toks/s, output: 45.07 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:21<01:24, 37.00it/s, est. speed input: 45819.91 toks/s, output: 44.75 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:22<01:23, 37.00it/s, est. speed input: 45512.64 toks/s, output: 44.45 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:23<01:22, 37.00it/s, est. speed input: 45228.79 toks/s, output: 44.17 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:24<01:22, 37.00it/s, est. speed input: 44964.69 toks/s, output: 43.91 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:24<01:21, 36.99it/s, est. speed input: 44719.07 toks/s, output: 43.67 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:25<01:20, 36.99it/s, est. speed input: 44490.02 toks/s, output: 43.45 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:26<01:18, 37.26it/s, est. speed input: 44310.06 toks/s, output: 43.27 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:27<01:18, 37.19it/s, est. speed input: 44109.02 toks/s, output: 43.08 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:28<01:17, 37.12it/s, est. speed input: 43918.68 toks/s, output: 42.89 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:29<01:16, 37.08it/s, est. speed input: 43740.14 toks/s, output: 42.71 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:30<01:15, 37.06it/s, est. speed input: 43571.83 toks/s, output: 42.55 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:30<01:15, 37.03it/s, est. speed input: 43412.78 toks/s, output: 42.40 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:31<01:14, 37.02it/s, est. speed input: 43262.42 toks/s, output: 42.25 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:32<01:13, 37.01it/s, est. speed input: 43119.62 toks/s, output: 42.11 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:33<01:12, 37.00it/s, est. speed input: 42984.71 toks/s, output: 41.98 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:34<01:11, 37.00it/s, est. speed input: 42856.64 toks/s, output: 41.85 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:35<01:10, 37.00it/s, est. speed input: 42734.74 toks/s, output: 41.73 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:36<01:10, 36.99it/s, est. speed input: 42618.48 toks/s, output: 41.62 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:37<01:09, 36.99it/s, est. speed input: 42507.68 toks/s, output: 41.51 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:37<01:08, 36.99it/s, est. speed input: 42401.99 toks/s, output: 41.41 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:38<01:07, 36.99it/s, est. speed input: 42300.84 toks/s, output: 41.31 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:39<01:06, 36.98it/s, est. speed input: 42204.00 toks/s, output: 41.21 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:40<01:05, 36.99it/s, est. speed input: 42111.91 toks/s, output: 41.12 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:41<01:04, 36.99it/s, est. speed input: 42023.14 toks/s, output: 41.04 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:42<01:03, 36.99it/s, est. speed input: 41938.40 toks/s, output: 40.96 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:43<01:03, 36.99it/s, est. speed input: 41856.65 toks/s, output: 40.88 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:43<01:02, 36.98it/s, est. speed input: 41777.98 toks/s, output: 40.80 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:44<01:01, 36.98it/s, est. speed input: 41702.56 toks/s, output: 40.73 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:45<00:59, 37.30it/s, est. speed input: 41652.53 toks/s, output: 40.68 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:46<00:59, 37.19it/s, est. speed input: 41581.33 toks/s, output: 40.61 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:47<00:58, 37.13it/s, est. speed input: 41513.76 toks/s, output: 40.54 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:48<00:57, 37.09it/s, est. speed input: 41448.93 toks/s, output: 40.48 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:49<00:56, 37.05it/s, est. speed input: 41385.25 toks/s, output: 40.42 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:50<00:56, 37.03it/s, est. speed input: 41324.13 toks/s, output: 40.36 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:50<00:55, 37.01it/s, est. speed input: 41265.06 toks/s, output: 40.30 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:51<00:54, 37.00it/s, est. speed input: 41207.97 toks/s, output: 40.24 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:52<00:53, 36.99it/s, est. speed input: 41152.79 toks/s, output: 40.19 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:53<00:52, 36.98it/s, est. speed input: 41099.53 toks/s, output: 40.14 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:54<00:51, 36.97it/s, est. speed input: 41047.61 toks/s, output: 40.09 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:55<00:51, 36.97it/s, est. speed input: 40997.54 toks/s, output: 40.04 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:56<00:50, 36.97it/s, est. speed input: 40948.95 toks/s, output: 39.99 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:56<00:49, 36.98it/s, est. speed input: 40902.24 toks/s, output: 39.94 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:57<00:48, 36.96it/s, est. speed input: 40856.01 toks/s, output: 39.90 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:58<00:47, 36.97it/s, est. speed input: 40811.80 toks/s, output: 39.86 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:59<00:46, 36.96it/s, est. speed input: 40768.42 toks/s, output: 39.81 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [01:00<00:45, 36.96it/s, est. speed input: 40726.64 toks/s, output: 39.77 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [01:01<00:44, 36.96it/s, est. speed input: 40685.81 toks/s, output: 39.73 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [01:02<00:44, 36.95it/s, est. speed input: 40645.82 toks/s, output: 39.69 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [01:02<00:42, 37.27it/s, est. speed input: 40623.23 toks/s, output: 39.67 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [01:03<00:42, 37.17it/s, est. speed input: 40585.10 toks/s, output: 39.63 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [01:04<00:41, 37.10it/s, est. speed input: 40548.32 toks/s, output: 39.60 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [01:05<00:40, 37.06it/s, est. speed input: 40512.44 toks/s, output: 39.56 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [01:06<00:39, 37.03it/s, est. speed input: 40477.83 toks/s, output: 39.53 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [01:07<00:38, 37.01it/s, est. speed input: 40444.01 toks/s, output: 39.50 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [01:08<00:38, 36.99it/s, est. speed input: 40410.80 toks/s, output: 39.46 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [01:09<00:37, 36.98it/s, est. speed input: 40378.47 toks/s, output: 39.43 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [01:09<00:36, 36.96it/s, est. speed input: 40346.50 toks/s, output: 39.40 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [01:10<00:35, 36.95it/s, est. speed input: 40315.61 toks/s, output: 39.37 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [01:11<00:34, 36.95it/s, est. speed input: 40285.41 toks/s, output: 39.34 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [01:12<00:33, 36.95it/s, est. speed input: 40256.20 toks/s, output: 39.31 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [01:13<00:32, 36.94it/s, est. speed input: 40227.46 toks/s, output: 39.28 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [01:14<00:31, 36.94it/s, est. speed input: 40199.24 toks/s, output: 39.26 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [01:15<00:31, 36.94it/s, est. speed input: 40171.76 toks/s, output: 39.23 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:15<00:30, 36.93it/s, est. speed input: 40144.80 toks/s, output: 39.20 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:16<00:29, 36.93it/s, est. speed input: 40118.66 toks/s, output: 39.18 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:17<00:28, 36.93it/s, est. speed input: 40093.03 toks/s, output: 39.15 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:18<00:27, 36.93it/s, est. speed input: 40067.95 toks/s, output: 39.13 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:19<00:26, 36.94it/s, est. speed input: 40043.49 toks/s, output: 39.10 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:20<00:25, 36.93it/s, est. speed input: 40019.45 toks/s, output: 39.08 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:21<00:25, 36.94it/s, est. speed input: 39996.06 toks/s, output: 39.06 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:22<00:24, 36.93it/s, est. speed input: 39972.79 toks/s, output: 39.04 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:22<00:23, 36.93it/s, est. speed input: 39950.21 toks/s, output: 39.01 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:23<00:22, 36.93it/s, est. speed input: 39928.05 toks/s, output: 38.99 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:24<00:21, 36.92it/s, est. speed input: 39906.17 toks/s, output: 38.97 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:25<00:20, 36.93it/s, est. speed input: 39885.11 toks/s, output: 38.95 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:26<00:19, 36.93it/s, est. speed input: 39864.35 toks/s, output: 38.93 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:27<00:19, 36.92it/s, est. speed input: 39843.84 toks/s, output: 38.91 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:28<00:18, 36.93it/s, est. speed input: 39823.94 toks/s, output: 38.89 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:28<00:17, 36.92it/s, est. speed input: 39804.27 toks/s, output: 38.87 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:29<00:16, 36.92it/s, est. speed input: 39784.88 toks/s, output: 38.85 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:30<00:15, 36.93it/s, est. speed input: 39766.14 toks/s, output: 38.83 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:31<00:14, 36.93it/s, est. speed input: 39747.67 toks/s, output: 38.82 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:32<00:13, 36.93it/s, est. speed input: 39729.54 toks/s, output: 38.80 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:33<00:12, 36.92it/s, est. speed input: 39711.52 toks/s, output: 38.78 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:34<00:12, 36.92it/s, est. speed input: 39693.95 toks/s, output: 38.76 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:35<00:11, 36.91it/s, est. speed input: 39676.48 toks/s, output: 38.75 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:35<00:10, 36.91it/s, est. speed input: 39659.50 toks/s, output: 38.73 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:36<00:09, 36.90it/s, est. speed input: 39642.53 toks/s, output: 38.71 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:37<00:08, 36.90it/s, est. speed input: 39626.08 toks/s, output: 38.70 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:38<00:07, 36.90it/s, est. speed input: 39609.95 toks/s, output: 38.68 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:39<00:06, 36.91it/s, est. speed input: 39594.14 toks/s, output: 38.67 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:40<00:06, 36.91it/s, est. speed input: 39578.76 toks/s, output: 38.65 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:41<00:05, 36.90it/s, est. speed input: 39562.98 toks/s, output: 38.64 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:41<00:04, 36.90it/s, est. speed input: 39547.90 toks/s, output: 38.62 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:42<00:03, 36.90it/s, est. speed input: 39533.06 toks/s, output: 38.61 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:43<00:02, 36.90it/s, est. speed input: 39518.37 toks/s, output: 38.59 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:44<00:01, 37.21it/s, est. speed input: 39513.18 toks/s, output: 38.59 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:45<00:00, 37.47it/s, est. speed input: 39509.19 toks/s, output: 38.58 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:45<00:00, 37.47it/s, est. speed input: 39800.59 toks/s, output: 38.87 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:45<00:00, 38.87it/s, est. speed input: 39800.59 toks/s, output: 38.87 toks/s]
[rank0]:[W128 01:03:20.228009839 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 145.5s

测试结果:
  Requests/s:   36.98
  Tokens/s:     37903.58
  Total Reqs:   4096
  Elapsed:      110.77s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     37866.60


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,56.5935,29032.4758,2.2617
1024,1024,1,128,128,35.9664,36865.5225,3.5589
2048,1024,2,256,128,39.5226,40510.7021,6.4773
4096,1024,4,512,128,39.3050,40287.6105,13.0263
8192,1024,8,1024,128,37.6443,38585.4453,27.2020
16384,1024,16,2048,128,36.9645,37888.5918,55.4045
32768,1024,32,4096,128,36.9791,37903.5782,110.7653

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:03:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3521743) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3521743) WARNING 01-28 01:03:35 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.28 requests/s, 28356.35 total tokens/s, 55.28 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 01:03:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:03:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:03:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:03:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:03:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:03:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:03:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:03:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:03:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:03:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:03:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:03:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:03:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:03:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:03:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:03:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:03:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:03:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:29] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:29] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:29] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:29] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:29] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3521743) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3521743) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.06s/it]
(EngineCore_DP0 pid=3521743) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.06s/it]
(EngineCore_DP0 pid=3521743) 
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3521743) [2026-01-28 01:03:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3521743) 2026-01-28 01:03:41,608 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3521743) 2026-01-28 01:03:41,624 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3521743) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.17it/s]
(EngineCore_DP0 pid=3521743) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.20it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  97%|█████████▋| 124/128 [00:00<00:00, 1235.13it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1237.49it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:16,  7.53it/s, est. speed input: 3853.90 toks/s, output: 7.53 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 37.00it/s, est. speed input: 16517.68 toks/s, output: 32.26 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 47.54it/s, est. speed input: 21163.51 toks/s, output: 41.33 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 52.97it/s, est. speed input: 23647.33 toks/s, output: 46.19 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 55.86it/s, est. speed input: 25125.93 toks/s, output: 49.07 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 57.67it/s, est. speed input: 26134.46 toks/s, output: 51.04 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 58.98it/s, est. speed input: 26893.24 toks/s, output: 52.53 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 59.65it/s, est. speed input: 27430.97 toks/s, output: 53.58 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 60.16it/s, est. speed input: 27862.04 toks/s, output: 54.42 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 60.69it/s, est. speed input: 28234.98 toks/s, output: 55.14 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 61.32it/s, est. speed input: 28576.72 toks/s, output: 55.81 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 61.77it/s, est. speed input: 28865.68 toks/s, output: 56.38 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 62.04it/s, est. speed input: 29106.25 toks/s, output: 56.85 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 61.66it/s, est. speed input: 29250.63 toks/s, output: 57.13 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 58.74it/s, est. speed input: 29087.88 toks/s, output: 56.81 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 59.79it/s, est. speed input: 29259.88 toks/s, output: 57.15 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 60.74it/s, est. speed input: 29431.57 toks/s, output: 57.48 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 60.65it/s, est. speed input: 29515.89 toks/s, output: 57.65 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 61.06it/s, est. speed input: 29631.45 toks/s, output: 57.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 61.06it/s, est. speed input: 29649.04 toks/s, output: 57.91 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 57.91it/s, est. speed input: 29649.04 toks/s, output: 57.91 toks/s]
[rank0]:[W128 01:03:45.515745386 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.4s

测试结果:
  Requests/s:   55.28
  Tokens/s:     28356.35
  Total Reqs:   128
  Elapsed:      2.32s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     28301.07

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:03:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3522424) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3522424) WARNING 01-28 01:04:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 33.69 requests/s, 34532.99 total tokens/s, 33.69 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 01:03:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:03:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:03:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:03:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:03:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:03:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:03:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:03:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:03:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:03:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:03:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:03:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:03:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:03:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:03:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:03:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:03:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:03:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:03:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3522424) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3522424) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.17s/it]
(EngineCore_DP0 pid=3522424) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.17s/it]
(EngineCore_DP0 pid=3522424) 
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3522424) [2026-01-28 01:03:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3522424) 2026-01-28 01:04:07,569 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3522424) 2026-01-28 01:04:07,585 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3522424) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 21.46it/s]
(EngineCore_DP0 pid=3522424) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.08it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 625.22it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 674.62it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:15,  8.17it/s, est. speed input: 8369.16 toks/s, output: 8.17 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 24.10it/s, est. speed input: 22098.34 toks/s, output: 21.58 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 29.65it/s, est. speed input: 27020.85 toks/s, output: 26.39 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 32.23it/s, est. speed input: 29490.10 toks/s, output: 28.80 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 33.70it/s, est. speed input: 31005.00 toks/s, output: 30.28 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 34.65it/s, est. speed input: 32045.67 toks/s, output: 31.29 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 35.24it/s, est. speed input: 32789.05 toks/s, output: 32.02 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 35.62it/s, est. speed input: 33345.99 toks/s, output: 32.56 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 35.90it/s, est. speed input: 33789.91 toks/s, output: 33.00 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 36.12it/s, est. speed input: 34154.76 toks/s, output: 33.35 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 36.23it/s, est. speed input: 34443.05 toks/s, output: 33.64 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 36.32it/s, est. speed input: 34687.19 toks/s, output: 33.87 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 36.35it/s, est. speed input: 34885.48 toks/s, output: 34.07 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 36.39it/s, est. speed input: 35060.76 toks/s, output: 34.24 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 36.42it/s, est. speed input: 35212.95 toks/s, output: 34.39 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 36.36it/s, est. speed input: 35331.11 toks/s, output: 34.50 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 36.41it/s, est. speed input: 35451.32 toks/s, output: 34.62 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 36.41it/s, est. speed input: 35552.32 toks/s, output: 34.72 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 36.40it/s, est. speed input: 35642.10 toks/s, output: 34.81 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 36.43it/s, est. speed input: 35728.51 toks/s, output: 34.89 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 36.45it/s, est. speed input: 35805.69 toks/s, output: 34.97 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 36.41it/s, est. speed input: 35868.12 toks/s, output: 35.03 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 36.44it/s, est. speed input: 35933.36 toks/s, output: 35.09 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 36.42it/s, est. speed input: 35988.78 toks/s, output: 35.15 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 36.42it/s, est. speed input: 36041.05 toks/s, output: 35.20 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 36.43it/s, est. speed input: 36089.73 toks/s, output: 35.24 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 36.36it/s, est. speed input: 36126.73 toks/s, output: 35.28 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 36.40it/s, est. speed input: 36171.12 toks/s, output: 35.32 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 36.45it/s, est. speed input: 36214.16 toks/s, output: 35.37 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 36.49it/s, est. speed input: 36255.40 toks/s, output: 35.41 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 36.46it/s, est. speed input: 36287.55 toks/s, output: 35.44 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 36.35it/s, est. speed input: 36309.02 toks/s, output: 35.46 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.35it/s, est. speed input: 36330.62 toks/s, output: 35.48 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.48it/s, est. speed input: 36330.62 toks/s, output: 35.48 toks/s]
[rank0]:[W128 01:04:12.522339147 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.9s

测试结果:
  Requests/s:   33.69
  Tokens/s:     34532.99
  Total Reqs:   128
  Elapsed:      3.80s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     34499.30

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:04:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3523104) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3523104) WARNING 01-28 01:04:27 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.28 requests/s, 38214.98 total tokens/s, 37.28 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 01:04:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:04:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:04:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:04:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:04:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:04:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:04:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:04:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:04:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:04:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:04:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:04:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:04:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:04:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:04:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:04:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:04:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:04:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:22] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:22] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:22] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:22] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:22] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3523104) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3523104) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.73it/s]
(EngineCore_DP0 pid=3523104) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.73it/s]
(EngineCore_DP0 pid=3523104) 
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3523104) [2026-01-28 01:04:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3523104) 2026-01-28 01:04:33,658 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3523104) 2026-01-28 01:04:33,674 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3523104) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 21.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 21.55it/s]
(EngineCore_DP0 pid=3523104) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.66it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:35,  7.23it/s]
Adding requests:  29%|██▉       | 75/256 [00:00<00:00, 378.27it/s]
Adding requests:  58%|█████▊    | 148/256 [00:00<00:00, 525.46it/s]
Adding requests:  88%|████████▊ | 224/256 [00:00<00:00, 611.76it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 532.11it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:00<00:01, 138.66it/s, est. speed input: 141999.83 toks/s, output: 138.66 toks/s]
Processed prompts:  11%|█         | 28/256 [00:00<00:04, 54.74it/s, est. speed input: 61650.24 toks/s, output: 60.20 toks/s]   
Processed prompts:  14%|█▍        | 36/256 [00:00<00:04, 48.04it/s, est. speed input: 54690.57 toks/s, output: 53.41 toks/s]
Processed prompts:  17%|█▋        | 43/256 [00:00<00:04, 47.01it/s, est. speed input: 53001.17 toks/s, output: 51.76 toks/s]
Processed prompts:  19%|█▉        | 49/256 [00:00<00:04, 44.28it/s, est. speed input: 50755.39 toks/s, output: 49.57 toks/s]
Processed prompts:  21%|██        | 54/256 [00:01<00:04, 40.56it/s, est. speed input: 48286.16 toks/s, output: 47.15 toks/s]
Processed prompts:  23%|██▎       | 59/256 [00:01<00:04, 42.21it/s, est. speed input: 48354.70 toks/s, output: 47.22 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:01<00:04, 38.93it/s, est. speed input: 46631.33 toks/s, output: 45.54 toks/s]
Processed prompts:  27%|██▋       | 69/256 [00:01<00:04, 41.08it/s, est. speed input: 46807.65 toks/s, output: 45.71 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:04, 38.04it/s, est. speed input: 45493.36 toks/s, output: 44.43 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:04, 38.14it/s, est. speed input: 45132.90 toks/s, output: 44.07 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:04, 38.21it/s, est. speed input: 44810.53 toks/s, output: 43.76 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:01<00:04, 38.17it/s, est. speed input: 44501.12 toks/s, output: 43.46 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:02<00:04, 38.25it/s, est. speed input: 44244.50 toks/s, output: 43.21 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:04, 38.31it/s, est. speed input: 44014.55 toks/s, output: 42.98 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 38.36it/s, est. speed input: 43804.72 toks/s, output: 42.78 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:04, 38.40it/s, est. speed input: 43614.89 toks/s, output: 42.59 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:03, 38.44it/s, est. speed input: 43441.48 toks/s, output: 42.42 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:03, 38.38it/s, est. speed input: 43269.74 toks/s, output: 42.26 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:03, 38.39it/s, est. speed input: 43119.04 toks/s, output: 42.11 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 38.41it/s, est. speed input: 42980.72 toks/s, output: 41.97 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:02<00:03, 38.36it/s, est. speed input: 42843.20 toks/s, output: 41.84 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:03<00:03, 38.27it/s, est. speed input: 42708.92 toks/s, output: 41.71 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:03<00:03, 38.27it/s, est. speed input: 42591.65 toks/s, output: 41.59 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 38.31it/s, est. speed input: 42486.21 toks/s, output: 41.49 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 38.31it/s, est. speed input: 42384.53 toks/s, output: 41.39 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:02, 38.37it/s, est. speed input: 42295.24 toks/s, output: 41.30 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:02, 38.34it/s, est. speed input: 42203.36 toks/s, output: 41.21 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:02, 38.37it/s, est. speed input: 42122.53 toks/s, output: 41.14 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 38.40it/s, est. speed input: 42046.32 toks/s, output: 41.06 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:03<00:02, 38.39it/s, est. speed input: 41972.05 toks/s, output: 40.99 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:03<00:02, 38.38it/s, est. speed input: 41901.50 toks/s, output: 40.92 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:02, 38.33it/s, est. speed input: 41829.81 toks/s, output: 40.85 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 38.30it/s, est. speed input: 41762.54 toks/s, output: 40.78 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 38.35it/s, est. speed input: 41704.79 toks/s, output: 40.73 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:02, 38.37it/s, est. speed input: 41648.81 toks/s, output: 40.67 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:01, 38.40it/s, est. speed input: 41596.37 toks/s, output: 40.62 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 38.41it/s, est. speed input: 41545.75 toks/s, output: 40.57 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 38.36it/s, est. speed input: 41492.03 toks/s, output: 40.52 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:04<00:01, 38.40it/s, est. speed input: 41447.45 toks/s, output: 40.48 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:04<00:01, 38.42it/s, est. speed input: 41403.50 toks/s, output: 40.43 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:05<00:01, 38.32it/s, est. speed input: 41353.22 toks/s, output: 40.38 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:05<00:01, 38.29it/s, est. speed input: 41307.72 toks/s, output: 40.34 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:05<00:01, 38.33it/s, est. speed input: 41268.45 toks/s, output: 40.30 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:05<00:01, 38.36it/s, est. speed input: 41231.14 toks/s, output: 40.26 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:05<00:00, 38.38it/s, est. speed input: 41194.87 toks/s, output: 40.23 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:05<00:00, 38.41it/s, est. speed input: 41160.88 toks/s, output: 40.20 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:05<00:00, 38.43it/s, est. speed input: 41128.32 toks/s, output: 40.16 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:05<00:00, 38.43it/s, est. speed input: 41096.44 toks/s, output: 40.13 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:05<00:00, 38.44it/s, est. speed input: 41065.85 toks/s, output: 40.10 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:05<00:00, 38.39it/s, est. speed input: 41032.68 toks/s, output: 40.07 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:06<00:00, 38.37it/s, est. speed input: 41001.96 toks/s, output: 40.04 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:06<00:00, 38.32it/s, est. speed input: 40969.70 toks/s, output: 40.01 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:06<00:00, 38.32it/s, est. speed input: 40941.17 toks/s, output: 39.98 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:06<00:00, 38.36it/s, est. speed input: 40915.16 toks/s, output: 39.96 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 38.36it/s, est. speed input: 41059.01 toks/s, output: 40.10 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 40.10it/s, est. speed input: 41059.01 toks/s, output: 40.10 toks/s]
[rank0]:[W128 01:04:41.751786095 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 29.3s

测试结果:
  Requests/s:   37.28
  Tokens/s:     38214.98
  Total Reqs:   256
  Elapsed:      6.87s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     38177.70

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:04:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3523796) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3523796) WARNING 01-28 01:04:57 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.83 requests/s, 38775.37 total tokens/s, 37.83 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 01:04:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:04:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:04:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:04:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:04:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:04:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:04:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:04:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:04:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:04:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:04:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:04:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:04:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:04:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:04:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:04:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:04:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:04:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:04:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:52] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:52] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:52] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:52] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:52] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3523796) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3523796) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.64it/s]
(EngineCore_DP0 pid=3523796) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.64it/s]
(EngineCore_DP0 pid=3523796) 
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3523796) [2026-01-28 01:04:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3523796) 2026-01-28 01:05:03,742 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3523796) 2026-01-28 01:05:03,758 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3523796) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 24.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 22.26it/s]
(EngineCore_DP0 pid=3523796) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.33it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 73/512 [00:00<00:00, 726.90it/s]
Adding requests:  29%|██▉       | 148/512 [00:00<00:00, 738.37it/s]
Adding requests:  44%|████▎     | 223/512 [00:00<00:00, 742.84it/s]
Adding requests:  58%|█████▊    | 299/512 [00:00<00:00, 745.66it/s]
Adding requests:  73%|███████▎  | 375/512 [00:00<00:00, 747.04it/s]
Adding requests:  88%|████████▊ | 451/512 [00:00<00:00, 749.24it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 742.00it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:00<00:02, 200.66it/s, est. speed input: 205496.67 toks/s, output: 200.67 toks/s]
Processed prompts:   9%|▉         | 47/512 [00:00<00:07, 63.49it/s, est. speed input: 73335.75 toks/s, output: 71.62 toks/s]   
Processed prompts:  11%|█▏        | 58/512 [00:00<00:08, 51.44it/s, est. speed input: 61146.01 toks/s, output: 59.71 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:01<00:09, 47.63it/s, est. speed input: 57216.90 toks/s, output: 55.88 toks/s]
Processed prompts:  14%|█▍        | 72/512 [00:01<00:08, 49.25it/s, est. speed input: 57325.20 toks/s, output: 55.98 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:01<00:10, 42.42it/s, est. speed input: 53349.31 toks/s, output: 52.10 toks/s]
Processed prompts:  16%|█▌        | 83/512 [00:01<00:09, 43.41it/s, est. speed input: 53049.25 toks/s, output: 51.81 toks/s]
Processed prompts:  17%|█▋        | 88/512 [00:01<00:09, 44.31it/s, est. speed input: 52788.93 toks/s, output: 51.55 toks/s]
Processed prompts:  18%|█▊        | 93/512 [00:01<00:09, 45.09it/s, est. speed input: 52556.88 toks/s, output: 51.32 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:02<00:11, 36.60it/s, est. speed input: 49637.65 toks/s, output: 48.47 toks/s]
Processed prompts:  20%|██        | 103/512 [00:02<00:10, 39.05it/s, est. speed input: 49594.84 toks/s, output: 48.43 toks/s]
Processed prompts:  21%|██        | 108/512 [00:02<00:09, 41.13it/s, est. speed input: 49559.80 toks/s, output: 48.40 toks/s]
Processed prompts:  22%|██▏       | 113/512 [00:02<00:09, 42.75it/s, est. speed input: 49514.18 toks/s, output: 48.35 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:02<00:11, 34.71it/s, est. speed input: 47432.72 toks/s, output: 46.32 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:02<00:10, 35.51it/s, est. speed input: 47102.56 toks/s, output: 46.00 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:02<00:10, 36.14it/s, est. speed input: 46792.25 toks/s, output: 45.70 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:02<00:10, 36.62it/s, est. speed input: 46500.76 toks/s, output: 45.41 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:02<00:10, 37.04it/s, est. speed input: 46238.99 toks/s, output: 45.16 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:03<00:10, 37.30it/s, est. speed input: 45986.75 toks/s, output: 44.91 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:03<00:09, 37.52it/s, est. speed input: 45755.03 toks/s, output: 44.68 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:03<00:09, 37.70it/s, est. speed input: 45540.33 toks/s, output: 44.47 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:03<00:09, 37.78it/s, est. speed input: 45334.15 toks/s, output: 44.27 toks/s]
Processed prompts:  30%|███       | 154/512 [00:03<00:09, 37.80it/s, est. speed input: 45134.99 toks/s, output: 44.08 toks/s]
Processed prompts:  31%|███       | 158/512 [00:03<00:09, 37.87it/s, est. speed input: 44953.63 toks/s, output: 43.90 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:03<00:09, 37.91it/s, est. speed input: 44781.93 toks/s, output: 43.73 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:03<00:09, 37.96it/s, est. speed input: 44622.44 toks/s, output: 43.58 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:03<00:09, 38.00it/s, est. speed input: 44471.68 toks/s, output: 43.43 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:04<00:08, 38.03it/s, est. speed input: 44329.76 toks/s, output: 43.29 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:04<00:08, 38.06it/s, est. speed input: 44195.11 toks/s, output: 43.16 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:04<00:08, 38.08it/s, est. speed input: 44066.95 toks/s, output: 43.03 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:04<00:08, 38.05it/s, est. speed input: 43941.56 toks/s, output: 42.91 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:04<00:08, 37.97it/s, est. speed input: 43815.85 toks/s, output: 42.79 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:04<00:08, 37.98it/s, est. speed input: 43703.04 toks/s, output: 42.68 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:04<00:08, 37.97it/s, est. speed input: 43592.70 toks/s, output: 42.57 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:04<00:08, 37.99it/s, est. speed input: 43489.83 toks/s, output: 42.47 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:08, 38.03it/s, est. speed input: 43393.68 toks/s, output: 42.38 toks/s]
Processed prompts:  41%|████      | 210/512 [00:04<00:07, 37.98it/s, est. speed input: 43296.01 toks/s, output: 42.28 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:05<00:07, 37.99it/s, est. speed input: 43205.32 toks/s, output: 42.19 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:05<00:07, 38.02it/s, est. speed input: 43119.98 toks/s, output: 42.11 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:05<00:07, 38.07it/s, est. speed input: 43040.01 toks/s, output: 42.03 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:05<00:07, 38.06it/s, est. speed input: 42960.59 toks/s, output: 41.95 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:05<00:07, 38.01it/s, est. speed input: 42880.79 toks/s, output: 41.88 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:05<00:07, 38.02it/s, est. speed input: 42807.13 toks/s, output: 41.80 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:05<00:07, 38.06it/s, est. speed input: 42737.95 toks/s, output: 41.74 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:05<00:07, 38.05it/s, est. speed input: 42669.42 toks/s, output: 41.67 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:06, 38.02it/s, est. speed input: 42601.78 toks/s, output: 41.60 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:06<00:06, 38.02it/s, est. speed input: 42537.28 toks/s, output: 41.54 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:06<00:06, 38.03it/s, est. speed input: 42475.89 toks/s, output: 41.48 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:06<00:06, 38.05it/s, est. speed input: 42417.55 toks/s, output: 41.42 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:06<00:06, 38.06it/s, est. speed input: 42360.66 toks/s, output: 41.37 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:06<00:06, 38.00it/s, est. speed input: 42301.87 toks/s, output: 41.31 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:06<00:06, 37.98it/s, est. speed input: 42246.32 toks/s, output: 41.26 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:06<00:06, 37.97it/s, est. speed input: 42192.67 toks/s, output: 41.20 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:06<00:06, 38.01it/s, est. speed input: 42143.02 toks/s, output: 41.16 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:06<00:06, 37.99it/s, est. speed input: 42092.34 toks/s, output: 41.11 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:06<00:05, 37.97it/s, est. speed input: 42043.19 toks/s, output: 41.06 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:07<00:05, 37.97it/s, est. speed input: 41995.81 toks/s, output: 41.01 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:07<00:05, 37.97it/s, est. speed input: 41950.52 toks/s, output: 40.97 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:07<00:05, 37.96it/s, est. speed input: 41905.58 toks/s, output: 40.92 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:07<00:05, 37.94it/s, est. speed input: 41861.33 toks/s, output: 40.88 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:07<00:05, 37.92it/s, est. speed input: 41817.91 toks/s, output: 40.84 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:07<00:05, 37.92it/s, est. speed input: 41776.10 toks/s, output: 40.80 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:07<00:05, 37.93it/s, est. speed input: 41736.19 toks/s, output: 40.76 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:07<00:05, 37.93it/s, est. speed input: 41697.48 toks/s, output: 40.72 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:07<00:05, 37.97it/s, est. speed input: 41661.25 toks/s, output: 40.68 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:08<00:04, 38.00it/s, est. speed input: 41625.96 toks/s, output: 40.65 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:08<00:04, 38.04it/s, est. speed input: 41592.67 toks/s, output: 40.62 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:08<00:04, 38.07it/s, est. speed input: 41560.51 toks/s, output: 40.59 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:08<00:04, 38.08it/s, est. speed input: 41528.39 toks/s, output: 40.56 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:08<00:04, 38.05it/s, est. speed input: 41495.71 toks/s, output: 40.52 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:08<00:04, 38.00it/s, est. speed input: 41462.37 toks/s, output: 40.49 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:08<00:04, 38.03it/s, est. speed input: 41432.57 toks/s, output: 40.46 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:08<00:04, 38.05it/s, est. speed input: 41403.56 toks/s, output: 40.43 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:08<00:04, 38.08it/s, est. speed input: 41375.72 toks/s, output: 40.41 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:08<00:03, 38.06it/s, est. speed input: 41347.16 toks/s, output: 40.38 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:09<00:03, 38.03it/s, est. speed input: 41318.42 toks/s, output: 40.35 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:09<00:03, 38.06it/s, est. speed input: 41292.16 toks/s, output: 40.32 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:09<00:03, 38.07it/s, est. speed input: 41266.37 toks/s, output: 40.30 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:09<00:03, 38.10it/s, est. speed input: 41241.90 toks/s, output: 40.28 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:09<00:03, 38.04it/s, est. speed input: 41214.83 toks/s, output: 40.25 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:09<00:03, 38.00it/s, est. speed input: 41188.51 toks/s, output: 40.22 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:09<00:03, 38.00it/s, est. speed input: 41163.75 toks/s, output: 40.20 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:09<00:03, 38.00it/s, est. speed input: 41139.74 toks/s, output: 40.18 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:09<00:03, 37.98it/s, est. speed input: 41115.38 toks/s, output: 40.15 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:10<00:02, 37.99it/s, est. speed input: 41092.22 toks/s, output: 40.13 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:10<00:02, 38.00it/s, est. speed input: 41069.91 toks/s, output: 40.11 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:10<00:02, 37.98it/s, est. speed input: 41046.95 toks/s, output: 40.08 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:10<00:02, 38.03it/s, est. speed input: 41026.80 toks/s, output: 40.07 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:10<00:02, 38.01it/s, est. speed input: 41005.02 toks/s, output: 40.04 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:10<00:02, 38.05it/s, est. speed input: 40985.65 toks/s, output: 40.03 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:10<00:02, 37.99it/s, est. speed input: 40963.60 toks/s, output: 40.00 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:10<00:02, 38.00it/s, est. speed input: 40943.68 toks/s, output: 39.98 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:10<00:02, 37.98it/s, est. speed input: 40923.30 toks/s, output: 39.96 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:10<00:01, 37.96it/s, est. speed input: 40902.99 toks/s, output: 39.94 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:11<00:01, 37.93it/s, est. speed input: 40882.74 toks/s, output: 39.92 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:11<00:01, 37.92it/s, est. speed input: 40863.25 toks/s, output: 39.91 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:11<00:01, 37.95it/s, est. speed input: 40845.04 toks/s, output: 39.89 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:11<00:01, 37.97it/s, est. speed input: 40827.28 toks/s, output: 39.87 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:11<00:01, 37.96it/s, est. speed input: 40809.10 toks/s, output: 39.85 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:11<00:01, 37.98it/s, est. speed input: 40792.08 toks/s, output: 39.84 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:11<00:01, 38.00it/s, est. speed input: 40775.61 toks/s, output: 39.82 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:11<00:01, 38.01it/s, est. speed input: 40759.28 toks/s, output: 39.80 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:11<00:01, 37.96it/s, est. speed input: 40741.62 toks/s, output: 39.79 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:12<00:00, 37.94it/s, est. speed input: 40724.50 toks/s, output: 39.77 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:12<00:00, 37.92it/s, est. speed input: 40707.55 toks/s, output: 39.75 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:12<00:00, 37.92it/s, est. speed input: 40691.41 toks/s, output: 39.74 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:12<00:00, 37.92it/s, est. speed input: 40675.61 toks/s, output: 39.72 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:12<00:00, 37.88it/s, est. speed input: 40658.55 toks/s, output: 39.71 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:12<00:00, 37.85it/s, est. speed input: 40641.99 toks/s, output: 39.69 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:12<00:00, 37.84it/s, est. speed input: 40626.08 toks/s, output: 39.67 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:12<00:00, 37.85it/s, est. speed input: 40610.58 toks/s, output: 39.66 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 37.85it/s, est. speed input: 40821.86 toks/s, output: 39.87 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 39.86it/s, est. speed input: 40821.86 toks/s, output: 39.87 toks/s]
[rank0]:[W128 01:05:18.732596529 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 37.0s

测试结果:
  Requests/s:   37.83
  Tokens/s:     38775.37
  Total Reqs:   512
  Elapsed:      13.53s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     38737.54

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:05:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3524606) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3524606) WARNING 01-28 01:05:36 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.16 requests/s, 37060.88 total tokens/s, 36.16 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 01:05:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:05:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:05:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:05:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:05:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:05:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:05:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:05:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:05:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:05:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:05:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:05:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:05:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:05:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:05:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:05:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:05:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:05:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:05:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:05:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:05:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:05:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:05:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:05:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:05:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:05:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:05:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:05:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3524606) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3524606) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=3524606) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=3524606) 
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3524606) [2026-01-28 01:05:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3524606) 2026-01-28 01:05:41,943 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3524606) 2026-01-28 01:05:41,959 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3524606) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  3.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 10.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 10.40it/s]
(EngineCore_DP0 pid=3524606) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.94it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 70/1024 [00:00<00:01, 699.19it/s]
Adding requests:  14%|█▍        | 146/1024 [00:00<00:01, 734.90it/s]
Adding requests:  22%|██▏       | 223/1024 [00:00<00:01, 748.80it/s]
Adding requests:  29%|██▉       | 300/1024 [00:00<00:00, 751.41it/s]
Adding requests:  37%|███▋      | 378/1024 [00:00<00:00, 761.55it/s]
Adding requests:  44%|████▍     | 455/1024 [00:00<00:00, 753.35it/s]
Adding requests:  52%|█████▏    | 531/1024 [00:00<00:00, 736.32it/s]
Adding requests:  59%|█████▉    | 608/1024 [00:00<00:00, 743.77it/s]
Adding requests:  67%|██████▋   | 686/1024 [00:00<00:00, 753.57it/s]
Adding requests:  75%|███████▍  | 763/1024 [00:01<00:00, 756.88it/s]
Adding requests:  82%|████████▏ | 839/1024 [00:01<00:00, 741.09it/s]
Adding requests:  90%|████████▉ | 917/1024 [00:01<00:00, 750.68it/s]
Adding requests:  97%|█████████▋| 993/1024 [00:01<00:00, 744.16it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 747.22it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:00<00:05, 193.54it/s, est. speed input: 198197.49 toks/s, output: 193.55 toks/s]
Processed prompts:   7%|▋         | 70/1024 [00:00<00:10, 88.46it/s, est. speed input: 102512.18 toks/s, output: 100.11 toks/s] 
Processed prompts:   8%|▊         | 81/1024 [00:00<00:12, 75.53it/s, est. speed input: 90214.82 toks/s, output: 88.10 toks/s]  
Processed prompts:   9%|▉         | 90/1024 [00:01<00:19, 48.54it/s, est. speed input: 67771.60 toks/s, output: 66.18 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:01<00:20, 45.38it/s, est. speed input: 63513.24 toks/s, output: 62.02 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:01<00:21, 42.94it/s, est. speed input: 60296.01 toks/s, output: 58.88 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:02<00:22, 41.09it/s, est. speed input: 57778.07 toks/s, output: 56.42 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:02<00:22, 39.74it/s, est. speed input: 55755.79 toks/s, output: 54.45 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:02<00:23, 38.73it/s, est. speed input: 54084.86 toks/s, output: 52.82 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:02<00:23, 38.03it/s, est. speed input: 52698.92 toks/s, output: 51.46 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:02<00:23, 37.53it/s, est. speed input: 51522.39 toks/s, output: 50.31 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:03<00:23, 37.16it/s, est. speed input: 50509.65 toks/s, output: 49.33 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:03<00:23, 36.90it/s, est. speed input: 49627.22 toks/s, output: 48.46 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:03<00:23, 36.71it/s, est. speed input: 48853.52 toks/s, output: 47.71 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:03<00:23, 36.60it/s, est. speed input: 48176.99 toks/s, output: 47.05 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:04<00:22, 36.52it/s, est. speed input: 47572.71 toks/s, output: 46.46 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:04<00:22, 36.45it/s, est. speed input: 47029.04 toks/s, output: 45.93 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:04<00:22, 36.39it/s, est. speed input: 46536.97 toks/s, output: 45.45 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:04<00:22, 36.37it/s, est. speed input: 46097.02 toks/s, output: 45.02 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:04<00:22, 36.35it/s, est. speed input: 45694.18 toks/s, output: 44.62 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:05<00:21, 36.35it/s, est. speed input: 45329.19 toks/s, output: 44.27 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:05<00:21, 36.35it/s, est. speed input: 44993.72 toks/s, output: 43.94 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:05<00:21, 36.33it/s, est. speed input: 44682.03 toks/s, output: 43.63 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:05<00:21, 36.32it/s, est. speed input: 44394.37 toks/s, output: 43.35 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:05<00:21, 36.32it/s, est. speed input: 44130.09 toks/s, output: 43.10 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:06<00:20, 36.31it/s, est. speed input: 43882.87 toks/s, output: 42.85 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:06<00:20, 36.30it/s, est. speed input: 43651.15 toks/s, output: 42.63 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:06<00:20, 36.29it/s, est. speed input: 43434.89 toks/s, output: 42.42 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:06<00:20, 36.30it/s, est. speed input: 43235.53 toks/s, output: 42.22 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:07<00:19, 36.31it/s, est. speed input: 43047.75 toks/s, output: 42.04 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:07<00:19, 36.32it/s, est. speed input: 42872.49 toks/s, output: 41.87 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:07<00:19, 36.31it/s, est. speed input: 42705.48 toks/s, output: 41.70 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:07<00:19, 36.31it/s, est. speed input: 42547.82 toks/s, output: 41.55 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:07<00:19, 36.31it/s, est. speed input: 42399.91 toks/s, output: 41.41 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:08<00:18, 36.32it/s, est. speed input: 42260.24 toks/s, output: 41.27 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:08<00:18, 36.29it/s, est. speed input: 42124.19 toks/s, output: 41.14 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:08<00:18, 36.30it/s, est. speed input: 41998.43 toks/s, output: 41.01 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:08<00:18, 36.27it/s, est. speed input: 41875.53 toks/s, output: 40.89 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:09<00:18, 36.27it/s, est. speed input: 41760.44 toks/s, output: 40.78 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:09<00:17, 36.27it/s, est. speed input: 41651.18 toks/s, output: 40.67 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:09<00:17, 36.27it/s, est. speed input: 41546.12 toks/s, output: 40.57 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:09<00:17, 36.26it/s, est. speed input: 41445.80 toks/s, output: 40.47 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:09<00:17, 36.28it/s, est. speed input: 41352.02 toks/s, output: 40.38 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:10<00:16, 36.30it/s, est. speed input: 41262.18 toks/s, output: 40.30 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:10<00:16, 36.30it/s, est. speed input: 41175.29 toks/s, output: 40.21 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:10<00:16, 36.29it/s, est. speed input: 41091.36 toks/s, output: 40.13 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:10<00:16, 36.29it/s, est. speed input: 41011.52 toks/s, output: 40.05 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:11<00:16, 36.29it/s, est. speed input: 40934.98 toks/s, output: 39.98 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:11<00:15, 36.27it/s, est. speed input: 40859.89 toks/s, output: 39.90 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:11<00:15, 36.27it/s, est. speed input: 40788.10 toks/s, output: 39.83 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:11<00:15, 36.26it/s, est. speed input: 40718.65 toks/s, output: 39.76 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:11<00:15, 36.23it/s, est. speed input: 40650.61 toks/s, output: 39.70 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:12<00:14, 36.22it/s, est. speed input: 40585.50 toks/s, output: 39.63 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:12<00:14, 36.20it/s, est. speed input: 40521.95 toks/s, output: 39.57 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:12<00:14, 36.20it/s, est. speed input: 40461.34 toks/s, output: 39.51 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:12<00:14, 36.22it/s, est. speed input: 40403.80 toks/s, output: 39.46 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:13<00:14, 36.21it/s, est. speed input: 40347.49 toks/s, output: 39.40 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:13<00:13, 36.21it/s, est. speed input: 40293.18 toks/s, output: 39.35 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:13<00:13, 36.19it/s, est. speed input: 40239.33 toks/s, output: 39.30 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:13<00:13, 36.20it/s, est. speed input: 40188.72 toks/s, output: 39.25 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:13<00:13, 36.21it/s, est. speed input: 40139.48 toks/s, output: 39.20 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:14<00:12, 36.21it/s, est. speed input: 40092.08 toks/s, output: 39.15 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:14<00:12, 36.18it/s, est. speed input: 40043.92 toks/s, output: 39.11 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:14<00:12, 36.18it/s, est. speed input: 39998.40 toks/s, output: 39.06 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:14<00:12, 36.19it/s, est. speed input: 39955.22 toks/s, output: 39.02 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:15<00:12, 36.19it/s, est. speed input: 39912.74 toks/s, output: 38.98 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:15<00:11, 36.18it/s, est. speed input: 39870.77 toks/s, output: 38.94 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:15<00:11, 36.16it/s, est. speed input: 39829.48 toks/s, output: 38.90 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:15<00:11, 36.17it/s, est. speed input: 39790.50 toks/s, output: 38.86 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:15<00:11, 36.18it/s, est. speed input: 39752.46 toks/s, output: 38.82 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:16<00:11, 36.18it/s, est. speed input: 39715.40 toks/s, output: 38.78 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:16<00:10, 36.17it/s, est. speed input: 39678.93 toks/s, output: 38.75 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:16<00:10, 36.19it/s, est. speed input: 39644.58 toks/s, output: 38.72 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:16<00:10, 36.20it/s, est. speed input: 39611.10 toks/s, output: 38.68 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:17<00:10, 36.22it/s, est. speed input: 39578.85 toks/s, output: 38.65 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:17<00:09, 36.19it/s, est. speed input: 39545.85 toks/s, output: 38.62 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:17<00:09, 36.17it/s, est. speed input: 39513.53 toks/s, output: 38.59 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:17<00:09, 36.19it/s, est. speed input: 39483.20 toks/s, output: 38.56 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:17<00:09, 36.19it/s, est. speed input: 39453.45 toks/s, output: 38.53 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:18<00:09, 36.19it/s, est. speed input: 39424.24 toks/s, output: 38.50 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:18<00:08, 36.17it/s, est. speed input: 39395.04 toks/s, output: 38.47 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:18<00:08, 36.17it/s, est. speed input: 39366.88 toks/s, output: 38.44 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:18<00:08, 36.17it/s, est. speed input: 39339.47 toks/s, output: 38.42 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:19<00:08, 36.20it/s, est. speed input: 39313.88 toks/s, output: 38.39 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:19<00:07, 36.21it/s, est. speed input: 39288.52 toks/s, output: 38.37 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:19<00:07, 36.18it/s, est. speed input: 39262.34 toks/s, output: 38.34 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:19<00:07, 36.18it/s, est. speed input: 39237.46 toks/s, output: 38.32 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:19<00:07, 36.20it/s, est. speed input: 39213.81 toks/s, output: 38.29 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:20<00:07, 36.19it/s, est. speed input: 39189.99 toks/s, output: 38.27 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:20<00:06, 36.17it/s, est. speed input: 39165.96 toks/s, output: 38.25 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:20<00:06, 36.17it/s, est. speed input: 39143.13 toks/s, output: 38.23 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:20<00:06, 36.19it/s, est. speed input: 39121.48 toks/s, output: 38.20 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:21<00:06, 36.19it/s, est. speed input: 39099.71 toks/s, output: 38.18 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:21<00:05, 36.18it/s, est. speed input: 39078.02 toks/s, output: 38.16 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:21<00:05, 36.16it/s, est. speed input: 39056.48 toks/s, output: 38.14 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:21<00:05, 36.17it/s, est. speed input: 39035.98 toks/s, output: 38.12 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:21<00:05, 36.18it/s, est. speed input: 39016.14 toks/s, output: 38.10 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:22<00:05, 36.19it/s, est. speed input: 38996.86 toks/s, output: 38.08 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:22<00:04, 36.17it/s, est. speed input: 38976.90 toks/s, output: 38.06 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:22<00:04, 36.18it/s, est. speed input: 38958.16 toks/s, output: 38.05 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:22<00:04, 36.19it/s, est. speed input: 38939.93 toks/s, output: 38.03 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:22<00:04, 36.19it/s, est. speed input: 38922.06 toks/s, output: 38.01 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:23<00:03, 36.22it/s, est. speed input: 38905.05 toks/s, output: 37.99 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:23<00:03, 36.20it/s, est. speed input: 38887.52 toks/s, output: 37.98 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:23<00:03, 36.19it/s, est. speed input: 38869.93 toks/s, output: 37.96 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:23<00:03, 36.18it/s, est. speed input: 38853.02 toks/s, output: 37.94 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:24<00:03, 36.17it/s, est. speed input: 38836.22 toks/s, output: 37.93 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:24<00:02, 36.18it/s, est. speed input: 38820.04 toks/s, output: 37.91 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:24<00:02, 36.16it/s, est. speed input: 38803.52 toks/s, output: 37.89 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:24<00:02, 36.16it/s, est. speed input: 38787.66 toks/s, output: 37.88 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:24<00:02, 36.17it/s, est. speed input: 38772.52 toks/s, output: 37.86 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:25<00:01, 36.19it/s, est. speed input: 38757.79 toks/s, output: 37.85 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:25<00:01, 36.17it/s, est. speed input: 38742.37 toks/s, output: 37.83 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:25<00:01, 36.18it/s, est. speed input: 38728.12 toks/s, output: 37.82 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:25<00:01, 36.18it/s, est. speed input: 38713.79 toks/s, output: 37.81 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:26<00:01, 36.19it/s, est. speed input: 38699.98 toks/s, output: 37.79 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:26<00:00, 36.19it/s, est. speed input: 38686.20 toks/s, output: 37.78 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:26<00:00, 36.18it/s, est. speed input: 38672.19 toks/s, output: 37.77 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:26<00:00, 36.19it/s, est. speed input: 38659.24 toks/s, output: 37.75 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:26<00:00, 37.42it/s, est. speed input: 38680.76 toks/s, output: 37.77 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:26<00:00, 37.42it/s, est. speed input: 38908.56 toks/s, output: 38.00 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:26<00:00, 38.00it/s, est. speed input: 38908.56 toks/s, output: 38.00 toks/s]
[rank0]:[W128 01:06:12.931113187 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.3s

测试结果:
  Requests/s:   36.16
  Tokens/s:     37060.88
  Total Reqs:   1024
  Elapsed:      28.32s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     37024.72

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:06:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3525665) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3525665) WARNING 01-28 01:06:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.64 requests/s, 36527.30 total tokens/s, 35.64 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 01:06:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:06:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:06:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:06:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:06:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:06:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:06:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:06:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:06:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:06:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:06:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:06:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:06:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:06:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:06:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:06:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:06:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:06:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:06:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:06:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:06:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:06:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:06:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:06:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:06:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:06:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:06:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:06:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:27] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3525665) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3525665) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.52it/s]
(EngineCore_DP0 pid=3525665) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.52it/s]
(EngineCore_DP0 pid=3525665) 
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3525665) [2026-01-28 01:06:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3525665) 2026-01-28 01:06:38,287 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3525665) 2026-01-28 01:06:38,303 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3525665) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  4.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 12.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 17.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.63it/s]
(EngineCore_DP0 pid=3525665) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.01it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 14.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 13.83it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 73/2048 [00:00<00:02, 724.04it/s]
Adding requests:   7%|▋         | 146/2048 [00:00<00:02, 724.35it/s]
Adding requests:  11%|█         | 221/2048 [00:00<00:02, 732.91it/s]
Adding requests:  14%|█▍        | 296/2048 [00:00<00:02, 738.97it/s]
Adding requests:  18%|█▊        | 371/2048 [00:00<00:02, 741.56it/s]
Adding requests:  22%|██▏       | 448/2048 [00:00<00:02, 748.88it/s]
Adding requests:  26%|██▌       | 523/2048 [00:00<00:02, 745.14it/s]
Adding requests:  29%|██▉       | 602/2048 [00:00<00:01, 757.02it/s]
Adding requests:  33%|███▎      | 681/2048 [00:00<00:01, 766.94it/s]
Adding requests:  37%|███▋      | 758/2048 [00:01<00:01, 758.38it/s]
Adding requests:  41%|████      | 834/2048 [00:01<00:01, 742.14it/s]
Adding requests:  45%|████▍     | 912/2048 [00:01<00:01, 750.45it/s]
Adding requests:  48%|████▊     | 990/2048 [00:01<00:01, 757.05it/s]
Adding requests:  52%|█████▏    | 1068/2048 [00:01<00:01, 762.97it/s]
Adding requests:  56%|█████▌    | 1145/2048 [00:01<00:01, 756.68it/s]
Adding requests:  60%|█████▉    | 1221/2048 [00:01<00:01, 745.92it/s]
Adding requests:  63%|██████▎   | 1296/2048 [00:01<00:01, 737.79it/s]
Adding requests:  67%|██████▋   | 1372/2048 [00:01<00:00, 743.69it/s]
Adding requests:  71%|███████   | 1449/2048 [00:01<00:00, 749.25it/s]
Adding requests:  75%|███████▍  | 1528/2048 [00:02<00:00, 759.04it/s]
Adding requests:  79%|███████▊  | 1608/2048 [00:02<00:00, 769.49it/s]
Adding requests:  82%|████████▏ | 1685/2048 [00:02<00:00, 746.40it/s]
Adding requests:  86%|████████▌ | 1760/2048 [00:02<00:00, 734.32it/s]
Adding requests:  90%|████████▉ | 1834/2048 [00:02<00:00, 735.81it/s]
Adding requests:  93%|█████████▎| 1909/2048 [00:02<00:00, 739.65it/s]
Adding requests:  97%|█████████▋| 1984/2048 [00:02<00:00, 733.99it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 747.42it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:00<00:09, 206.06it/s, est. speed input: 211011.63 toks/s, output: 206.06 toks/s]
Processed prompts:   6%|▌         | 119/2048 [00:00<00:16, 114.78it/s, est. speed input: 131982.45 toks/s, output: 128.89 toks/s]
Processed prompts:   6%|▋         | 131/2048 [00:01<00:25, 75.20it/s, est. speed input: 97885.49 toks/s, output: 95.59 toks/s]   
Processed prompts:   7%|▋         | 146/2048 [00:01<00:32, 58.89it/s, est. speed input: 82241.15 toks/s, output: 80.31 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:02<00:37, 50.60it/s, est. speed input: 73235.72 toks/s, output: 71.52 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:02<00:41, 45.56it/s, est. speed input: 67185.14 toks/s, output: 65.61 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:03<00:43, 42.36it/s, est. speed input: 62856.78 toks/s, output: 61.38 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:03<00:45, 40.25it/s, est. speed input: 59595.79 toks/s, output: 58.20 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:04<00:46, 38.85it/s, est. speed input: 57060.60 toks/s, output: 55.72 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:04<00:47, 37.81it/s, est. speed input: 54990.25 toks/s, output: 53.70 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:04<00:48, 37.19it/s, est. speed input: 53334.80 toks/s, output: 52.08 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:05<00:48, 36.74it/s, est. speed input: 51944.29 toks/s, output: 50.73 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:05<00:48, 36.42it/s, est. speed input: 50764.16 toks/s, output: 49.57 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:06<00:48, 36.21it/s, est. speed input: 49754.76 toks/s, output: 48.59 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:06<00:47, 36.06it/s, est. speed input: 48879.02 toks/s, output: 47.73 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:07<00:47, 35.94it/s, est. speed input: 48107.48 toks/s, output: 46.98 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:07<00:47, 35.85it/s, est. speed input: 47425.89 toks/s, output: 46.31 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:08<00:46, 35.79it/s, est. speed input: 46820.31 toks/s, output: 45.72 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:08<00:46, 35.75it/s, est. speed input: 46278.80 toks/s, output: 45.19 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:08<00:46, 35.72it/s, est. speed input: 45790.56 toks/s, output: 44.72 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:09<00:45, 35.70it/s, est. speed input: 45348.51 toks/s, output: 44.29 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:09<00:45, 35.68it/s, est. speed input: 44946.17 toks/s, output: 43.89 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:10<00:44, 35.67it/s, est. speed input: 44579.20 toks/s, output: 43.53 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:10<00:44, 35.65it/s, est. speed input: 44240.79 toks/s, output: 43.20 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:11<00:43, 35.64it/s, est. speed input: 43931.20 toks/s, output: 42.90 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:11<00:43, 35.64it/s, est. speed input: 43644.80 toks/s, output: 42.62 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:12<00:43, 35.64it/s, est. speed input: 43380.59 toks/s, output: 42.36 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:12<00:42, 35.64it/s, est. speed input: 43134.46 toks/s, output: 42.12 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:13<00:42, 35.64it/s, est. speed input: 42905.87 toks/s, output: 41.90 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:13<00:41, 35.63it/s, est. speed input: 42691.75 toks/s, output: 41.69 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:13<00:41, 35.64it/s, est. speed input: 42491.92 toks/s, output: 41.50 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:14<00:40, 35.64it/s, est. speed input: 42304.81 toks/s, output: 41.31 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:14<00:40, 35.64it/s, est. speed input: 42129.30 toks/s, output: 41.14 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:15<00:39, 35.64it/s, est. speed input: 41963.80 toks/s, output: 40.98 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:15<00:39, 35.64it/s, est. speed input: 41807.61 toks/s, output: 40.83 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:16<00:39, 35.64it/s, est. speed input: 41660.21 toks/s, output: 40.68 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:16<00:38, 35.63it/s, est. speed input: 41519.66 toks/s, output: 40.55 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:17<00:38, 35.64it/s, est. speed input: 41387.93 toks/s, output: 40.42 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:17<00:37, 35.63it/s, est. speed input: 41261.84 toks/s, output: 40.29 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:17<00:37, 35.61it/s, est. speed input: 41140.36 toks/s, output: 40.18 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:18<00:36, 35.61it/s, est. speed input: 41025.96 toks/s, output: 40.06 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:18<00:36, 35.61it/s, est. speed input: 40917.48 toks/s, output: 39.96 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:19<00:35, 35.62it/s, est. speed input: 40814.50 toks/s, output: 39.86 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:19<00:35, 35.62it/s, est. speed input: 40715.86 toks/s, output: 39.76 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:20<00:34, 35.63it/s, est. speed input: 40622.38 toks/s, output: 39.67 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:20<00:34, 35.62it/s, est. speed input: 40531.82 toks/s, output: 39.58 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:21<00:34, 35.62it/s, est. speed input: 40445.92 toks/s, output: 39.50 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:21<00:33, 35.63it/s, est. speed input: 40363.52 toks/s, output: 39.42 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:22<00:33, 35.62it/s, est. speed input: 40283.87 toks/s, output: 39.34 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:22<00:32, 35.62it/s, est. speed input: 40207.42 toks/s, output: 39.27 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:22<00:32, 35.62it/s, est. speed input: 40134.57 toks/s, output: 39.19 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:23<00:31, 35.63it/s, est. speed input: 40064.57 toks/s, output: 39.13 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:23<00:31, 35.63it/s, est. speed input: 39996.97 toks/s, output: 39.06 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:24<00:30, 35.63it/s, est. speed input: 39932.09 toks/s, output: 39.00 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:24<00:30, 35.63it/s, est. speed input: 39869.34 toks/s, output: 38.93 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:25<00:30, 35.63it/s, est. speed input: 39808.81 toks/s, output: 38.88 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:25<00:29, 35.62it/s, est. speed input: 39750.20 toks/s, output: 38.82 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:26<00:29, 35.63it/s, est. speed input: 39694.10 toks/s, output: 38.76 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:26<00:28, 35.63it/s, est. speed input: 39639.66 toks/s, output: 38.71 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:26<00:28, 35.63it/s, est. speed input: 39587.00 toks/s, output: 38.66 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:27<00:27, 35.63it/s, est. speed input: 39536.24 toks/s, output: 38.61 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:27<00:27, 35.62it/s, est. speed input: 39486.78 toks/s, output: 38.56 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:28<00:26, 35.63it/s, est. speed input: 39439.12 toks/s, output: 38.51 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:28<00:26, 35.62it/s, est. speed input: 39392.80 toks/s, output: 38.47 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:29<00:25, 35.63it/s, est. speed input: 39348.31 toks/s, output: 38.43 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:29<00:25, 35.62it/s, est. speed input: 39304.42 toks/s, output: 38.38 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:30<00:24, 36.18it/s, est. speed input: 39292.23 toks/s, output: 38.37 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:30<00:24, 36.00it/s, est. speed input: 39250.57 toks/s, output: 38.33 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:30<00:24, 35.90it/s, est. speed input: 39210.82 toks/s, output: 38.29 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:31<00:23, 35.82it/s, est. speed input: 39171.97 toks/s, output: 38.25 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:31<00:23, 35.76it/s, est. speed input: 39134.08 toks/s, output: 38.22 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:32<00:22, 35.72it/s, est. speed input: 39097.31 toks/s, output: 38.18 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:32<00:22, 35.69it/s, est. speed input: 39061.14 toks/s, output: 38.15 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:33<00:21, 35.67it/s, est. speed input: 39026.48 toks/s, output: 38.11 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:33<00:21, 35.65it/s, est. speed input: 38992.17 toks/s, output: 38.08 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:34<00:21, 35.65it/s, est. speed input: 38959.25 toks/s, output: 38.05 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:34<00:20, 35.64it/s, est. speed input: 38926.88 toks/s, output: 38.01 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:35<00:20, 35.63it/s, est. speed input: 38895.43 toks/s, output: 37.98 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:35<00:19, 35.62it/s, est. speed input: 38864.59 toks/s, output: 37.95 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:35<00:19, 35.63it/s, est. speed input: 38835.00 toks/s, output: 37.92 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:36<00:18, 35.63it/s, est. speed input: 38805.91 toks/s, output: 37.90 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:36<00:18, 35.63it/s, est. speed input: 38777.57 toks/s, output: 37.87 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:37<00:17, 35.62it/s, est. speed input: 38749.77 toks/s, output: 37.84 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:37<00:17, 35.62it/s, est. speed input: 38722.45 toks/s, output: 37.81 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:38<00:17, 35.63it/s, est. speed input: 38696.30 toks/s, output: 37.79 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:38<00:16, 35.62it/s, est. speed input: 38670.23 toks/s, output: 37.76 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:39<00:16, 35.63it/s, est. speed input: 38645.35 toks/s, output: 37.74 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:39<00:15, 35.62it/s, est. speed input: 38620.34 toks/s, output: 37.72 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:39<00:15, 35.62it/s, est. speed input: 38596.14 toks/s, output: 37.69 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:40<00:14, 35.62it/s, est. speed input: 38572.66 toks/s, output: 37.67 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:40<00:14, 35.62it/s, est. speed input: 38549.44 toks/s, output: 37.65 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:41<00:13, 35.62it/s, est. speed input: 38527.12 toks/s, output: 37.62 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:41<00:13, 35.62it/s, est. speed input: 38504.96 toks/s, output: 37.60 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:42<00:12, 35.62it/s, est. speed input: 38483.43 toks/s, output: 37.58 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:42<00:12, 35.62it/s, est. speed input: 38462.12 toks/s, output: 37.56 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:43<00:12, 35.62it/s, est. speed input: 38441.47 toks/s, output: 37.54 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:43<00:11, 35.62it/s, est. speed input: 38421.03 toks/s, output: 37.52 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:43<00:11, 35.62it/s, est. speed input: 38401.17 toks/s, output: 37.50 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:44<00:10, 35.62it/s, est. speed input: 38381.69 toks/s, output: 37.48 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:44<00:10, 35.62it/s, est. speed input: 38362.74 toks/s, output: 37.46 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:45<00:09, 35.62it/s, est. speed input: 38344.16 toks/s, output: 37.45 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:45<00:09, 35.62it/s, est. speed input: 38325.58 toks/s, output: 37.43 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:46<00:08, 35.61it/s, est. speed input: 38307.33 toks/s, output: 37.41 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:46<00:08, 35.61it/s, est. speed input: 38289.47 toks/s, output: 37.39 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:47<00:08, 35.62it/s, est. speed input: 38272.44 toks/s, output: 37.38 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:47<00:07, 35.62it/s, est. speed input: 38255.39 toks/s, output: 37.36 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:48<00:07, 35.62it/s, est. speed input: 38238.84 toks/s, output: 37.34 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:48<00:06, 35.62it/s, est. speed input: 38222.37 toks/s, output: 37.33 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:48<00:06, 35.62it/s, est. speed input: 38206.55 toks/s, output: 37.31 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:49<00:05, 35.63it/s, est. speed input: 38190.97 toks/s, output: 37.30 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:49<00:05, 35.62it/s, est. speed input: 38175.32 toks/s, output: 37.28 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:50<00:04, 36.26it/s, est. speed input: 38180.34 toks/s, output: 37.29 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:50<00:04, 36.07it/s, est. speed input: 38165.16 toks/s, output: 37.27 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:51<00:03, 35.92it/s, est. speed input: 38150.12 toks/s, output: 37.26 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:51<00:03, 35.83it/s, est. speed input: 38135.46 toks/s, output: 37.24 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:52<00:03, 35.77it/s, est. speed input: 38121.22 toks/s, output: 37.23 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:52<00:02, 35.72it/s, est. speed input: 38107.00 toks/s, output: 37.21 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:52<00:02, 35.69it/s, est. speed input: 38093.11 toks/s, output: 37.20 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:53<00:01, 35.67it/s, est. speed input: 38079.70 toks/s, output: 37.19 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:53<00:01, 35.65it/s, est. speed input: 38066.17 toks/s, output: 37.17 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:54<00:00, 35.64it/s, est. speed input: 38052.95 toks/s, output: 37.16 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:54<00:00, 36.25it/s, est. speed input: 38057.59 toks/s, output: 37.17 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:54<00:00, 36.25it/s, est. speed input: 38319.41 toks/s, output: 37.42 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:54<00:00, 37.42it/s, est. speed input: 38319.41 toks/s, output: 37.42 toks/s]
[rank0]:[W128 01:07:37.610549671 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 85.6s

测试结果:
  Requests/s:   35.64
  Tokens/s:     36527.30
  Total Reqs:   2048
  Elapsed:      57.47s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     36491.66

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:07:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3527190) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3527190) WARNING 01-28 01:08:03 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.58 requests/s, 36471.88 total tokens/s, 35.58 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 01:07:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:07:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:07:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:07:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:07:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:07:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:07:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:07:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:07:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:07:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:07:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:07:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:07:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:07:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:07:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:07:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:07:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:07:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:07:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:07:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:07:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:07:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:07:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:07:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:07:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:07:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:07:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:07:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:58] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:58] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:58] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:58] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:58] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:58] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3527190) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3527190) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.48it/s]
(EngineCore_DP0 pid=3527190) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.48it/s]
(EngineCore_DP0 pid=3527190) 
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3527190) [2026-01-28 01:07:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3527190) [rank0]:W0128 01:08:06.691000 3527190 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3527190) [rank0]:W0128 01:08:06.742000 3527190 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3527190) [rank0]:W0128 01:08:07.410000 3527190 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3527190) [rank0]:W0128 01:08:07.487000 3527190 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3527190) 2026-01-28 01:08:09,944 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3527190) 2026-01-28 01:08:09,961 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3527190) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:06,  1.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:01,  6.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 10.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00, 14.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 10.39it/s]
(EngineCore_DP0 pid=3527190) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.27it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 22.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 19.90it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 70/4096 [00:00<00:05, 698.81it/s]
Adding requests:   4%|▎         | 146/4096 [00:00<00:05, 729.64it/s]
Adding requests:   5%|▌         | 222/4096 [00:00<00:05, 741.69it/s]
Adding requests:   7%|▋         | 299/4096 [00:00<00:05, 751.40it/s]
Adding requests:   9%|▉         | 375/4096 [00:00<00:05, 743.09it/s]
Adding requests:  11%|█         | 450/4096 [00:00<00:04, 742.86it/s]
Adding requests:  13%|█▎        | 525/4096 [00:00<00:04, 736.15it/s]
Adding requests:  15%|█▍        | 601/4096 [00:00<00:04, 743.29it/s]
Adding requests:  17%|█▋        | 679/4096 [00:00<00:04, 753.57it/s]
Adding requests:  18%|█▊        | 755/4096 [00:01<00:04, 755.28it/s]
Adding requests:  20%|██        | 831/4096 [00:01<00:04, 738.31it/s]
Adding requests:  22%|██▏       | 909/4096 [00:01<00:04, 750.42it/s]
Adding requests:  24%|██▍       | 987/4096 [00:01<00:04, 756.65it/s]
Adding requests:  26%|██▌       | 1066/4096 [00:01<00:03, 764.11it/s]
Adding requests:  28%|██▊       | 1143/4096 [00:01<00:03, 742.90it/s]
Adding requests:  30%|██▉       | 1223/4096 [00:01<00:03, 758.03it/s]
Adding requests:  32%|███▏      | 1299/4096 [00:01<00:03, 755.91it/s]
Adding requests:  34%|███▎      | 1378/4096 [00:01<00:03, 765.12it/s]
Adding requests:  36%|███▌      | 1455/4096 [00:01<00:03, 766.18it/s]
Adding requests:  37%|███▋      | 1532/4096 [00:02<00:03, 767.24it/s]
Adding requests:  39%|███▉      | 1611/4096 [00:02<00:03, 770.08it/s]
Adding requests:  41%|████      | 1689/4096 [00:02<00:03, 763.34it/s]
Adding requests:  43%|████▎     | 1766/4096 [00:02<00:03, 762.37it/s]
Adding requests:  45%|████▍     | 1843/4096 [00:02<00:02, 753.65it/s]
Adding requests:  47%|████▋     | 1919/4096 [00:02<00:02, 736.17it/s]
Adding requests:  49%|████▊     | 1993/4096 [00:02<00:02, 727.79it/s]
Adding requests:  50%|█████     | 2066/4096 [00:02<00:02, 728.03it/s]
Adding requests:  52%|█████▏    | 2139/4096 [00:02<00:02, 718.40it/s]
Adding requests:  54%|█████▍    | 2211/4096 [00:02<00:02, 711.00it/s]
Adding requests:  56%|█████▌    | 2283/4096 [00:03<00:02, 692.96it/s]
Adding requests:  57%|█████▋    | 2353/4096 [00:03<00:02, 685.68it/s]
Adding requests:  59%|█████▉    | 2428/4096 [00:03<00:02, 703.40it/s]
Adding requests:  61%|██████    | 2503/4096 [00:03<00:02, 715.90it/s]
Adding requests:  63%|██████▎   | 2580/4096 [00:03<00:02, 730.84it/s]
Adding requests:  65%|██████▍   | 2658/4096 [00:03<00:01, 742.91it/s]
Adding requests:  67%|██████▋   | 2733/4096 [00:03<00:01, 742.77it/s]
Adding requests:  69%|██████▊   | 2808/4096 [00:03<00:01, 744.78it/s]
Adding requests:  70%|███████   | 2885/4096 [00:03<00:01, 751.04it/s]
Adding requests:  72%|███████▏  | 2961/4096 [00:03<00:01, 747.85it/s]
Adding requests:  74%|███████▍  | 3037/4096 [00:04<00:01, 749.72it/s]
Adding requests:  76%|███████▌  | 3114/4096 [00:04<00:01, 755.34it/s]
Adding requests:  78%|███████▊  | 3191/4096 [00:04<00:01, 757.62it/s]
Adding requests:  80%|███████▉  | 3268/4096 [00:04<00:01, 760.26it/s]
Adding requests:  82%|████████▏ | 3345/4096 [00:04<00:00, 762.63it/s]
Adding requests:  84%|████████▎ | 3422/4096 [00:04<00:00, 764.13it/s]
Adding requests:  85%|████████▌ | 3499/4096 [00:04<00:00, 748.78it/s]
Adding requests:  87%|████████▋ | 3576/4096 [00:04<00:00, 754.42it/s]
Adding requests:  89%|████████▉ | 3653/4096 [00:04<00:00, 755.44it/s]
Adding requests:  91%|█████████ | 3729/4096 [00:05<00:00, 744.40it/s]
Adding requests:  93%|█████████▎| 3808/4096 [00:05<00:00, 755.89it/s]
Adding requests:  95%|█████████▍| 3886/4096 [00:05<00:00, 762.08it/s]
Adding requests:  97%|█████████▋| 3963/4096 [00:05<00:00, 761.71it/s]
Adding requests:  99%|█████████▊| 4040/4096 [00:05<00:00, 755.93it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 746.25it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:00<00:18, 215.04it/s, est. speed input: 220203.92 toks/s, output: 215.04 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:01<00:35, 109.79it/s, est. speed input: 128638.60 toks/s, output: 125.62 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:02<00:50, 75.98it/s, est. speed input: 97983.94 toks/s, output: 95.69 toks/s]   
Processed prompts:   7%|▋         | 290/4096 [00:03<01:03, 60.07it/s, est. speed input: 82630.60 toks/s, output: 80.69 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:04<01:13, 51.27it/s, est. speed input: 73414.02 toks/s, output: 71.69 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:05<01:21, 45.95it/s, est. speed input: 67261.92 toks/s, output: 65.69 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:06<01:27, 42.58it/s, est. speed input: 62868.48 toks/s, output: 61.39 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:07<01:31, 40.37it/s, est. speed input: 59574.95 toks/s, output: 58.18 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:08<01:33, 38.89it/s, est. speed input: 57010.45 toks/s, output: 55.67 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:08<01:35, 37.88it/s, est. speed input: 54954.08 toks/s, output: 53.67 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:09<01:36, 37.19it/s, est. speed input: 53275.10 toks/s, output: 52.03 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:10<01:36, 36.72it/s, est. speed input: 51876.22 toks/s, output: 50.66 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:11<01:36, 36.39it/s, est. speed input: 50692.03 toks/s, output: 49.50 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:12<01:36, 36.15it/s, est. speed input: 49675.27 toks/s, output: 48.51 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:13<01:35, 36.00it/s, est. speed input: 48796.64 toks/s, output: 47.65 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:14<01:35, 35.89it/s, est. speed input: 48027.23 toks/s, output: 46.90 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:15<01:34, 35.81it/s, est. speed input: 47347.41 toks/s, output: 46.24 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:16<01:33, 35.75it/s, est. speed input: 46743.24 toks/s, output: 45.65 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:17<01:33, 35.71it/s, est. speed input: 46203.03 toks/s, output: 45.12 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:17<01:32, 35.69it/s, est. speed input: 45716.73 toks/s, output: 44.65 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:18<01:31, 35.67it/s, est. speed input: 45276.55 toks/s, output: 44.22 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:19<01:30, 35.65it/s, est. speed input: 44876.17 toks/s, output: 43.82 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:20<01:29, 35.64it/s, est. speed input: 44511.10 toks/s, output: 43.47 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:21<01:28, 35.64it/s, est. speed input: 44176.04 toks/s, output: 43.14 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:22<01:27, 35.63it/s, est. speed input: 43868.32 toks/s, output: 42.84 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:23<01:27, 35.63it/s, est. speed input: 43583.48 toks/s, output: 42.56 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:24<01:26, 35.62it/s, est. speed input: 43320.11 toks/s, output: 42.30 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:25<01:25, 35.62it/s, est. speed input: 43075.08 toks/s, output: 42.07 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:26<01:24, 35.59it/s, est. speed input: 42843.69 toks/s, output: 41.84 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:26<01:23, 35.60it/s, est. speed input: 42631.34 toks/s, output: 41.63 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:27<01:22, 35.84it/s, est. speed input: 42462.38 toks/s, output: 41.47 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:28<01:21, 35.77it/s, est. speed input: 42274.95 toks/s, output: 41.28 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:29<01:20, 35.73it/s, est. speed input: 42099.17 toks/s, output: 41.11 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:30<01:19, 35.69it/s, est. speed input: 41933.37 toks/s, output: 40.95 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:31<01:18, 35.67it/s, est. speed input: 41776.92 toks/s, output: 40.80 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:32<01:18, 35.65it/s, est. speed input: 41629.14 toks/s, output: 40.65 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:33<01:17, 35.64it/s, est. speed input: 41489.95 toks/s, output: 40.52 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:34<01:16, 35.63it/s, est. speed input: 41357.20 toks/s, output: 40.39 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:35<01:15, 35.63it/s, est. speed input: 41232.39 toks/s, output: 40.27 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:35<01:14, 35.62it/s, est. speed input: 41112.23 toks/s, output: 40.15 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:36<01:13, 35.61it/s, est. speed input: 40998.66 toks/s, output: 40.04 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:37<01:12, 35.61it/s, est. speed input: 40890.20 toks/s, output: 39.93 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:38<01:11, 35.61it/s, est. speed input: 40787.14 toks/s, output: 39.83 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:39<01:10, 35.62it/s, est. speed input: 40689.69 toks/s, output: 39.74 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:40<01:10, 35.60it/s, est. speed input: 40594.72 toks/s, output: 39.64 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:41<01:09, 35.60it/s, est. speed input: 40504.79 toks/s, output: 39.56 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:42<01:08, 35.61it/s, est. speed input: 40418.78 toks/s, output: 39.47 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:43<01:07, 35.60it/s, est. speed input: 40335.89 toks/s, output: 39.39 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:44<01:06, 35.60it/s, est. speed input: 40256.77 toks/s, output: 39.31 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:44<01:05, 35.59it/s, est. speed input: 40179.81 toks/s, output: 39.24 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:45<01:04, 35.59it/s, est. speed input: 40106.17 toks/s, output: 39.17 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:46<01:03, 35.59it/s, est. speed input: 40035.94 toks/s, output: 39.10 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:47<01:02, 35.87it/s, est. speed input: 39987.93 toks/s, output: 39.05 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:48<01:01, 35.79it/s, est. speed input: 39922.58 toks/s, output: 38.99 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:49<01:00, 35.73it/s, est. speed input: 39859.15 toks/s, output: 38.92 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:50<01:00, 35.69it/s, est. speed input: 39797.88 toks/s, output: 38.87 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:51<00:59, 35.65it/s, est. speed input: 39738.74 toks/s, output: 38.81 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:52<00:58, 35.62it/s, est. speed input: 39681.08 toks/s, output: 38.75 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:52<00:57, 35.60it/s, est. speed input: 39625.49 toks/s, output: 38.70 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:53<00:56, 35.59it/s, est. speed input: 39571.95 toks/s, output: 38.64 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:54<00:55, 35.58it/s, est. speed input: 39519.89 toks/s, output: 38.59 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:55<00:54, 35.58it/s, est. speed input: 39469.79 toks/s, output: 38.54 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:56<00:53, 35.58it/s, est. speed input: 39421.43 toks/s, output: 38.50 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:57<00:53, 35.57it/s, est. speed input: 39374.21 toks/s, output: 38.45 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:58<00:52, 35.56it/s, est. speed input: 39328.38 toks/s, output: 38.41 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:59<00:51, 35.56it/s, est. speed input: 39283.88 toks/s, output: 38.36 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [01:00<00:50, 35.56it/s, est. speed input: 39241.01 toks/s, output: 38.32 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [01:01<00:49, 35.56it/s, est. speed input: 39199.22 toks/s, output: 38.28 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [01:01<00:48, 35.55it/s, est. speed input: 39158.55 toks/s, output: 38.24 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [01:02<00:47, 35.55it/s, est. speed input: 39118.88 toks/s, output: 38.20 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [01:03<00:46, 35.54it/s, est. speed input: 39080.20 toks/s, output: 38.16 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [01:04<00:45, 35.54it/s, est. speed input: 39042.93 toks/s, output: 38.13 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [01:05<00:44, 35.85it/s, est. speed input: 39021.78 toks/s, output: 38.11 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [01:06<00:43, 35.75it/s, est. speed input: 38985.69 toks/s, output: 38.07 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [01:07<00:42, 35.69it/s, est. speed input: 38951.17 toks/s, output: 38.04 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [01:08<00:42, 35.65it/s, est. speed input: 38917.72 toks/s, output: 38.01 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [01:09<00:41, 35.62it/s, est. speed input: 38885.22 toks/s, output: 37.97 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [01:10<00:40, 35.60it/s, est. speed input: 38853.47 toks/s, output: 37.94 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [01:10<00:39, 35.59it/s, est. speed input: 38822.51 toks/s, output: 37.91 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [01:11<00:38, 35.59it/s, est. speed input: 38792.54 toks/s, output: 37.88 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [01:12<00:37, 35.57it/s, est. speed input: 38762.93 toks/s, output: 37.85 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [01:13<00:36, 35.57it/s, est. speed input: 38734.11 toks/s, output: 37.83 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [01:14<00:35, 35.56it/s, est. speed input: 38705.99 toks/s, output: 37.80 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [01:15<00:35, 35.57it/s, est. speed input: 38679.07 toks/s, output: 37.77 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [01:16<00:34, 35.56it/s, est. speed input: 38651.85 toks/s, output: 37.75 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [01:17<00:33, 35.55it/s, est. speed input: 38625.52 toks/s, output: 37.72 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [01:18<00:32, 35.55it/s, est. speed input: 38600.08 toks/s, output: 37.70 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:19<00:31, 35.55it/s, est. speed input: 38575.09 toks/s, output: 37.67 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:19<00:30, 35.55it/s, est. speed input: 38550.53 toks/s, output: 37.65 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:20<00:29, 35.55it/s, est. speed input: 38526.48 toks/s, output: 37.62 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:21<00:28, 35.54it/s, est. speed input: 38502.95 toks/s, output: 37.60 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:22<00:27, 35.54it/s, est. speed input: 38480.02 toks/s, output: 37.58 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:23<00:26, 35.53it/s, est. speed input: 38457.22 toks/s, output: 37.56 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:24<00:26, 35.53it/s, est. speed input: 38435.13 toks/s, output: 37.53 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:25<00:25, 35.53it/s, est. speed input: 38413.21 toks/s, output: 37.51 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:26<00:24, 35.53it/s, est. speed input: 38392.02 toks/s, output: 37.49 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:27<00:23, 35.53it/s, est. speed input: 38371.46 toks/s, output: 37.47 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:28<00:22, 35.53it/s, est. speed input: 38350.87 toks/s, output: 37.45 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:28<00:21, 35.53it/s, est. speed input: 38330.99 toks/s, output: 37.43 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:29<00:20, 35.53it/s, est. speed input: 38311.64 toks/s, output: 37.41 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:30<00:19, 35.53it/s, est. speed input: 38292.58 toks/s, output: 37.40 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:31<00:18, 35.54it/s, est. speed input: 38274.03 toks/s, output: 37.38 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:32<00:17, 35.54it/s, est. speed input: 38255.89 toks/s, output: 37.36 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:33<00:17, 35.55it/s, est. speed input: 38238.07 toks/s, output: 37.34 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:34<00:16, 35.55it/s, est. speed input: 38220.54 toks/s, output: 37.32 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:35<00:15, 35.55it/s, est. speed input: 38203.42 toks/s, output: 37.31 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:36<00:14, 35.55it/s, est. speed input: 38186.49 toks/s, output: 37.29 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:37<00:13, 35.54it/s, est. speed input: 38169.63 toks/s, output: 37.28 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:37<00:12, 35.54it/s, est. speed input: 38153.29 toks/s, output: 37.26 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:38<00:11, 35.53it/s, est. speed input: 38137.02 toks/s, output: 37.24 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:39<00:10, 35.52it/s, est. speed input: 38120.81 toks/s, output: 37.23 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:40<00:09, 35.51it/s, est. speed input: 38104.78 toks/s, output: 37.21 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:41<00:08, 35.51it/s, est. speed input: 38089.32 toks/s, output: 37.20 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:42<00:08, 35.51it/s, est. speed input: 38074.28 toks/s, output: 37.18 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:43<00:07, 35.52it/s, est. speed input: 38059.54 toks/s, output: 37.17 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:44<00:06, 35.52it/s, est. speed input: 38045.10 toks/s, output: 37.15 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:45<00:05, 35.53it/s, est. speed input: 38030.92 toks/s, output: 37.14 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:46<00:04, 35.53it/s, est. speed input: 38016.90 toks/s, output: 37.13 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:46<00:03, 35.52it/s, est. speed input: 38003.04 toks/s, output: 37.11 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:47<00:02, 35.53it/s, est. speed input: 37989.48 toks/s, output: 37.10 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:48<00:01, 35.82it/s, est. speed input: 37984.79 toks/s, output: 37.09 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:49<00:00, 36.06it/s, est. speed input: 37980.96 toks/s, output: 37.09 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:49<00:00, 36.06it/s, est. speed input: 38261.09 toks/s, output: 37.36 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:49<00:00, 37.36it/s, est. speed input: 38261.09 toks/s, output: 37.36 toks/s]
[rank0]:[W128 01:10:07.463533415 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 149.9s

测试结果:
  Requests/s:   35.58
  Tokens/s:     36471.88
  Total Reqs:   4096
  Elapsed:      115.11s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     36436.30


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,55.2755,28356.3460,2.3157
1024,1024,1,128,128,33.6907,34532.9890,3.7993
2048,1024,2,256,128,37.2829,38214.9781,6.8664
4096,1024,4,512,128,37.8296,38775.3694,13.5344
8192,1024,8,1024,128,36.1570,37060.8782,28.3210
16384,1024,16,2048,128,35.6364,36527.3002,57.4693
32768,1024,32,4096,128,35.5823,36471.8819,115.1133

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_003731.log
[SUCCESS] bitnet1.58-2b-fp8 Prefill 完成 (1960.5s)

[INFO] Prefill 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 4: 完整 Prefill Benchmark - SUCCESS
Duration: 3732.6 seconds (62.2 minutes)
----------------------------------------------------------------------


======================================================================
TASK 5: 完整 Decode Benchmark
Started: 2026-01-28 01:10:09
======================================================================


------------------------------------------------------------
  Decode Benchmark: bitnet1.58-2b-int8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-int8 --backend cublaslt,cusparselt --stage decode --sparsity 2_4,2_6,2_8,2_10 --M 64,128,256,512


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA GeForce RTX 5080                   ││
│ GPU (short):      RTX5080                                   │
│ Memory:           15.5 GB                                    │
│ CC:               cc120 (Blackwell)                            │
│ SM Code:          sm_120                                    │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.95.05                                 │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-int8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['decode']
  M_prefill:        [64, 128, 256, 512]
  M_decode:         [64, 128, 256, 512]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_011012.log


============================================================
  BitNet-2B-INT8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:10:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3529479) WARNING 01-28 01:10:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.78 requests/s, 8372.23 total tokens/s, 7879.75 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 01:10:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:10:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:10:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:10:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:10:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:10:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:10:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:10:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:10:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:10:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:10:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:10:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:10:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:10:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:10:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:10:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:10:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:10:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3529479) [2026-01-28 01:10:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3529479) [2026-01-28 01:10:20] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3529479) [2026-01-28 01:10:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3529479) [2026-01-28 01:10:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3529479) [2026-01-28 01:10:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3529479) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3529479) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.60it/s]
(EngineCore_DP0 pid=3529479) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.60it/s]
(EngineCore_DP0 pid=3529479) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3529479) 2026-01-28 01:10:31,250 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3529479) 2026-01-28 01:10:31,267 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3529479) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:00, 21.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:00, 23.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:00, 23.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:00<00:00, 23.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:00<00:00, 24.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:00<00:00, 24.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 23.65it/s]
(EngineCore_DP0 pid=3529479) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.04it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 17.31it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 21.39it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 23.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 21.39it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3075.81it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:07,  2.02s/it, est. speed input: 7.92 toks/s, output: 126.65 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.02s/it, est. speed input: 497.88 toks/s, output: 7966.02 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 31.12it/s, est. speed input: 497.88 toks/s, output: 7966.02 toks/s]
[rank0]:[W128 01:10:35.616862108 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.1s

测试结果:
  Requests/s:   30.78
  Tokens/s:     8372.23
  Total Reqs:   64
  Elapsed:      2.08s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      7879.75

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:10:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3530109) WARNING 01-28 01:10:50 [backends.py:609] Failed to read file <frozen os>
Throughput: 48.83 requests/s, 13283.11 total tokens/s, 12501.75 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 01:10:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:10:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:10:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:10:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:10:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:10:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:10:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:10:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:10:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:10:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:10:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:10:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:10:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:10:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:10:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:10:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:10:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:10:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:10:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3530109) [2026-01-28 01:10:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3530109) [2026-01-28 01:10:45] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3530109) [2026-01-28 01:10:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3530109) [2026-01-28 01:10:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3530109) [2026-01-28 01:10:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3530109) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3530109) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.68it/s]
(EngineCore_DP0 pid=3530109) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.67it/s]
(EngineCore_DP0 pid=3530109) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3530109) 2026-01-28 01:10:54,257 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3530109) 2026-01-28 01:10:54,286 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3530109) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:01, 23.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:01, 23.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:01, 24.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 25.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 25.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:00<00:00, 25.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:00<00:00, 25.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:00<00:00, 25.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:01<00:00, 26.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:01<00:00, 26.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:01<00:00, 25.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 25.16it/s]
(EngineCore_DP0 pid=3530109) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.08it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 17.58it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 21.51it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:00<00:00, 23.63it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 24.82it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 25.53it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 26.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 23.53it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 6480.27it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<05:18,  2.51s/it, est. speed input: 6.38 toks/s, output: 102.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.51s/it, est. speed input: 787.76 toks/s, output: 12604.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 49.23it/s, est. speed input: 787.76 toks/s, output: 12604.12 toks/s]
[rank0]:[W128 01:11:00.957824454 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.2s

测试结果:
  Requests/s:   48.83
  Tokens/s:     13283.11
  Total Reqs:   128
  Elapsed:      2.62s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      12501.75

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:11:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3530709) WARNING 01-28 01:11:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.83 requests/s, 15458.67 total tokens/s, 14549.34 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 01:11:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:11:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:11:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:11:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:11:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:11:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:11:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:11:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:11:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:11:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:11:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:11:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:11:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:11:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:11:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:11:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:11:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:11:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3530709) [2026-01-28 01:11:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3530709) [2026-01-28 01:11:09] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3530709) [2026-01-28 01:11:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3530709) [2026-01-28 01:11:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3530709) [2026-01-28 01:11:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3530709) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3530709) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.67it/s]
(EngineCore_DP0 pid=3530709) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.66it/s]
(EngineCore_DP0 pid=3530709) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3530709) 2026-01-28 01:11:18,367 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3530709) 2026-01-28 01:11:18,399 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3530709) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 23.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 24.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:01, 24.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 24.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 24.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 25.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:00<00:00, 24.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 24.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:01<00:00, 24.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:01<00:00, 25.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:01<00:00, 25.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 24.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 24.76it/s]
(EngineCore_DP0 pid=3530709) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.22it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:01, 17.83it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:01, 21.64it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:01, 23.63it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 23.02it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 24.30it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:00<00:00, 25.09it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:00<00:00, 25.48it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:01<00:00, 23.32it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:01<00:00, 20.72it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:01<00:00, 19.07it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:01<00:00, 20.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 21.59it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:34,  7.37it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 1570.00it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<16:32,  3.89s/it, est. speed input: 4.11 toks/s, output: 65.75 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:03<00:05, 30.62it/s, est. speed input: 348.04 toks/s, output: 5568.61 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:04<00:01, 69.72it/s, est. speed input: 670.05 toks/s, output: 10720.77 toks/s]
Processed prompts:  92%|█████████▏| 235/256 [00:04<00:00, 103.91it/s, est. speed input: 888.42 toks/s, output: 14214.65 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 103.91it/s, est. speed input: 943.68 toks/s, output: 15098.90 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 58.98it/s, est. speed input: 943.68 toks/s, output: 15098.90 toks/s] 
[rank0]:[W128 01:11:27.809410543 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.8s

测试结果:
  Requests/s:   56.83
  Tokens/s:     15458.67
  Total Reqs:   256
  Elapsed:      4.50s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      14549.34

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:11:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3531346) WARNING 01-28 01:11:41 [backends.py:609] Failed to read file <frozen os>
Throughput: 52.13 requests/s, 14178.67 total tokens/s, 13344.63 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 01:11:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:11:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:11:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:11:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:11:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:11:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:11:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:11:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:11:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:11:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:11:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:11:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:11:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:11:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:11:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:11:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:11:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:11:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:11:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3531346) [2026-01-28 01:11:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3531346) [2026-01-28 01:11:36] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3531346) [2026-01-28 01:11:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3531346) [2026-01-28 01:11:36] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3531346) [2026-01-28 01:11:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3531346) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3531346) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.69it/s]
(EngineCore_DP0 pid=3531346) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.69it/s]
(EngineCore_DP0 pid=3531346) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3531346) 2026-01-28 01:11:47,304 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3531346) 2026-01-28 01:11:47,321 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3531346) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:02, 19.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:01, 23.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:01, 25.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:00<00:01, 25.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:00<00:01, 22.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:00<00:01, 17.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:00<00:01, 17.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:01<00:01, 19.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:01<00:01, 21.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:01<00:01, 22.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:01<00:00, 23.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:01<00:00, 24.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:01<00:00, 24.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:01<00:00, 24.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:01<00:00, 25.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:02<00:00, 25.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:02<00:00, 25.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 22.87it/s]
(EngineCore_DP0 pid=3531346) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:06,  7.20it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:02, 17.67it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:02, 21.50it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:00<00:01, 23.51it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:00<00:01, 24.80it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:00<00:01, 25.37it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:00<00:01, 25.90it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:00<00:01, 26.19it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:01<00:01, 24.65it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:01<00:00, 25.33it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:01<00:00, 23.15it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:01<00:00, 21.46it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:01<00:00, 17.62it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:01<00:00, 19.64it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:01<00:00, 21.37it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:02<00:00, 22.46it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:02<00:00, 23.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 22.55it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 8757.84it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<58:26,  6.86s/it, est. speed input: 2.33 toks/s, output: 37.31 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:06<00:16, 23.75it/s, est. speed input: 268.46 toks/s, output: 4295.38 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:07<00:07, 42.24it/s, est. speed input: 418.29 toks/s, output: 6692.62 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:07<00:03, 68.69it/s, est. speed input: 574.61 toks/s, output: 9193.72 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:07<00:01, 106.69it/s, est. speed input: 741.98 toks/s, output: 11871.69 toks/s]
Processed prompts:  80%|███████▉  | 408/512 [00:08<00:01, 101.49it/s, est. speed input: 811.23 toks/s, output: 12979.68 toks/s]
Processed prompts:  89%|████████▉ | 457/512 [00:08<00:00, 100.15it/s, est. speed input: 854.60 toks/s, output: 13673.61 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:08<00:00, 100.06it/s, est. speed input: 885.40 toks/s, output: 14166.46 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 100.06it/s, est. speed input: 839.10 toks/s, output: 13425.60 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 52.44it/s, est. speed input: 839.10 toks/s, output: 13425.60 toks/s] 
[rank0]:[W128 01:12:02.550494139 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.7s

测试结果:
  Requests/s:   52.13
  Tokens/s:     14178.67
  Total Reqs:   512
  Elapsed:      9.82s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      13344.63


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,30.7803,8372.2341,2.0793
128,16,128,128,256,256,48.8349,13283.1061,2.6211
256,16,256,256,256,256,56.8333,15458.6707,4.5044
512,16,512,512,256,256,52.1275,14178.6704,9.8221

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:12:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3532114) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3532114) WARNING 01-28 01:12:17 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.06 requests/s, 9263.57 total tokens/s, 8718.66 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 01:12:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:12:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:12:07] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:12:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:07] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:07] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:12:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:12:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:12:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:12:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:12:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:12:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:12:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:12:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:12:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:12:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:12:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:12:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:12:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:12:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3532114) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3532114) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.55it/s]
(EngineCore_DP0 pid=3532114) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.55it/s]
(EngineCore_DP0 pid=3532114) 
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3532114) [2026-01-28 01:12:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3532114) 2026-01-28 01:12:23,192 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3532114) 2026-01-28 01:12:23,208 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3532114) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  4.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  4.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  9.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01, 11.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:00, 11.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00, 11.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 14.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00, 18.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 13.32it/s]
(EngineCore_DP0 pid=3532114) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.40it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 18.64it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 24.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 26.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.18it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3546.18it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:01<01:55,  1.83s/it, est. speed input: 8.74 toks/s, output: 139.78 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00,  1.83s/it, est. speed input: 550.68 toks/s, output: 8810.80 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00, 34.42it/s, est. speed input: 550.68 toks/s, output: 8810.80 toks/s]
[rank0]:[W128 01:12:28.888346291 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.3s

测试结果:
  Requests/s:   34.06
  Tokens/s:     9263.57
  Total Reqs:   64
  Elapsed:      1.88s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      8718.66

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:12:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3532786) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3532786) WARNING 01-28 01:12:42 [backends.py:609] Failed to read file <frozen os>
Throughput: 57.55 requests/s, 15653.65 total tokens/s, 14732.85 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 01:12:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:12:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:12:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:12:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:12:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:12:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:12:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:12:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:12:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:12:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:12:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:12:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:12:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:12:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:12:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:12:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:12:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:12:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3532786) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3532786) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.58it/s]
(EngineCore_DP0 pid=3532786) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.58it/s]
(EngineCore_DP0 pid=3532786) 
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3532786) [2026-01-28 01:12:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3532786) 2026-01-28 01:12:46,475 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3532786) 2026-01-28 01:12:46,491 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3532786) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:04,  6.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:04,  6.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:02, 14.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:01, 18.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:01, 21.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:00<00:00, 22.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:00<00:00, 23.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:00<00:00, 23.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:01<00:00, 24.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:01<00:00, 24.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:01<00:00, 22.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:01<00:00, 20.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 18.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 19.97it/s]
(EngineCore_DP0 pid=3532786) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.62it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 17.74it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 22.33it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:00<00:00, 24.91it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 26.42it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 26.69it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 27.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 24.50it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4505.08it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<04:29,  2.12s/it, est. speed input: 7.54 toks/s, output: 120.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.12s/it, est. speed input: 933.47 toks/s, output: 14935.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 58.34it/s, est. speed input: 933.47 toks/s, output: 14935.51 toks/s]
[rank0]:[W128 01:12:52.076407758 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.2s

测试结果:
  Requests/s:   57.55
  Tokens/s:     15653.65
  Total Reqs:   128
  Elapsed:      2.22s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      14732.85

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:12:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3533406) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3533406) WARNING 01-28 01:13:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 66.10 requests/s, 17977.86 total tokens/s, 16920.33 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 01:12:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:12:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:12:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:12:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:12:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:12:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:12:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:12:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:12:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:12:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:13:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:13:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:13:01] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:13:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:01] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:01] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:13:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:13:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:13:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:13:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:13:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3533406) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3533406) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.54it/s]
(EngineCore_DP0 pid=3533406) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.54it/s]
(EngineCore_DP0 pid=3533406) 
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3533406) [2026-01-28 01:13:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3533406) 2026-01-28 01:13:10,578 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3533406) 2026-01-28 01:13:10,594 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3533406) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 23.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 24.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:01, 25.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 25.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 25.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 25.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:00<00:00, 25.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 25.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:01<00:00, 24.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:01<00:00, 24.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:01<00:00, 23.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 19.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 23.36it/s]
(EngineCore_DP0 pid=3533406) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:05,  6.47it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:01, 17.02it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:01, 21.94it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:01, 23.74it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 25.41it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 26.61it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:00<00:00, 27.21it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:00<00:00, 27.75it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:01<00:00, 27.81it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:01<00:00, 28.07it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:01<00:00, 28.39it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:01<00:00, 28.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 26.00it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 8167.64it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<14:44,  3.47s/it, est. speed input: 4.61 toks/s, output: 73.80 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:03<00:04, 38.52it/s, est. speed input: 438.40 toks/s, output: 7014.38 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:03<00:00, 84.38it/s, est. speed input: 817.72 toks/s, output: 13083.44 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 84.38it/s, est. speed input: 1066.42 toks/s, output: 17062.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 66.65it/s, est. speed input: 1066.42 toks/s, output: 17062.62 toks/s]
[rank0]:[W128 01:13:18.175429848 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.1s

测试结果:
  Requests/s:   66.10
  Tokens/s:     17977.86
  Total Reqs:   256
  Elapsed:      3.87s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      16920.33

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:13:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3534044) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3534044) WARNING 01-28 01:13:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 60.03 requests/s, 16327.98 total tokens/s, 15367.51 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 01:13:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:13:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:13:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:13:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:13:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:13:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:13:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:13:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:13:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:13:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:13:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:13:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:13:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:13:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:13:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:13:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:13:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:13:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3534044) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3534044) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.59it/s]
(EngineCore_DP0 pid=3534044) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.59it/s]
(EngineCore_DP0 pid=3534044) 
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3534044) [2026-01-28 01:13:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3534044) 2026-01-28 01:13:38,545 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3534044) 2026-01-28 01:13:38,561 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3534044) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:01, 24.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:01, 24.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:02, 16.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:00<00:02, 13.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:00<00:02, 16.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:00<00:01, 18.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:01<00:01, 20.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:01<00:01, 21.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:01<00:01, 22.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:01<00:00, 23.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:01<00:00, 24.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:01<00:00, 24.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:01<00:00, 24.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:01<00:00, 24.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:02<00:00, 25.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:02<00:00, 24.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:02<00:00, 25.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 22.07it/s]
(EngineCore_DP0 pid=3534044) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:06,  7.35it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:02, 20.63it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:00<00:01, 24.07it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:00<00:01, 25.86it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:00<00:01, 26.84it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:00<00:01, 28.03it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:00<00:01, 28.25it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:00<00:00, 27.05it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:01<00:00, 24.06it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:01<00:00, 21.66it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:01<00:00, 19.42it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:01<00:00, 21.58it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:01<00:00, 23.40it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:01<00:00, 24.78it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:01<00:00, 25.96it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:01<00:00, 26.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 27.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 24.46it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 8169.19it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<53:21,  6.27s/it, est. speed input: 2.55 toks/s, output: 40.86 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:06<00:15, 25.98it/s, est. speed input: 293.80 toks/s, output: 4700.81 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:06<00:07, 46.10it/s, est. speed input: 457.17 toks/s, output: 7314.76 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:06<00:03, 74.64it/s, est. speed input: 627.06 toks/s, output: 10032.93 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:06<00:01, 115.11it/s, est. speed input: 808.13 toks/s, output: 12930.06 toks/s]
Processed prompts:  79%|███████▉  | 407/512 [00:06<00:00, 148.84it/s, est. speed input: 947.36 toks/s, output: 15157.74 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:07<00:00, 124.26it/s, est. speed input: 988.08 toks/s, output: 15809.18 toks/s]
Processed prompts:  99%|█████████▉| 509/512 [00:08<00:00, 90.19it/s, est. speed input: 962.15 toks/s, output: 15394.36 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 90.19it/s, est. speed input: 967.80 toks/s, output: 15484.82 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 60.49it/s, est. speed input: 967.80 toks/s, output: 15484.82 toks/s]
[rank0]:[W128 01:13:52.397140474 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.2s

测试结果:
  Requests/s:   60.03
  Tokens/s:     16327.98
  Total Reqs:   512
  Elapsed:      8.53s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      15367.51


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,34.0573,9263.5746,1.8792
128,16,128,128,256,256,57.5502,15653.6533,2.2241
256,16,256,256,256,256,66.0951,17977.8553,3.8732
512,16,512,512,256,256,60.0293,16327.9816,8.5292

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:13:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3534772) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3534772) WARNING 01-28 01:14:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 33.52 requests/s, 9118.24 total tokens/s, 8581.87 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 01:13:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:13:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:13:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:13:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:13:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:13:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:13:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:13:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:13:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:13:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:14:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:14:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:14:01] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:14:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:01] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:01] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:14:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:14:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:14:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:14:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:14:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3534772) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3534772) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=3534772) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=3534772) 
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3534772) [2026-01-28 01:14:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3534772) 2026-01-28 01:14:12,670 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3534772) 2026-01-28 01:14:12,686 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3534772) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  3.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  3.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  9.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 13.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 16.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 19.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00, 21.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 15.67it/s]
(EngineCore_DP0 pid=3534772) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.35it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 18.80it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 22.40it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 24.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 22.42it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 4379.62it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:01<01:57,  1.86s/it, est. speed input: 8.60 toks/s, output: 137.55 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00,  1.86s/it, est. speed input: 541.23 toks/s, output: 8659.64 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00, 33.82it/s, est. speed input: 541.23 toks/s, output: 8659.64 toks/s]
[rank0]:[W128 01:14:17.187355378 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.9s

测试结果:
  Requests/s:   33.52
  Tokens/s:     9118.24
  Total Reqs:   64
  Elapsed:      1.91s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      8581.87

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:14:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3535438) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3535438) WARNING 01-28 01:14:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.39 requests/s, 14521.19 total tokens/s, 13667.00 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 01:14:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:14:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:14:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:14:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:14:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:14:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:14:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:14:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:14:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:14:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:14:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:14:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:14:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:14:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:14:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:14:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:14:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:14:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3535438) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3535438) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=3535438) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=3535438) 
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3535438) [2026-01-28 01:14:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3535438) 2026-01-28 01:14:35,778 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3535438) 2026-01-28 01:14:35,794 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3535438) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:05,  6.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:07,  4.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:03,  9.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:01, 14.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:01, 17.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:00<00:01, 20.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:01<00:00, 21.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:01<00:00, 22.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:01<00:00, 22.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:01<00:00, 23.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:01<00:00, 24.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:01<00:00, 25.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 24.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 19.66it/s]
(EngineCore_DP0 pid=3535438) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.32it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 18.38it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 22.14it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:00<00:00, 23.82it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 25.49it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 26.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 26.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 24.16it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 8595.44it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<04:52,  2.30s/it, est. speed input: 6.95 toks/s, output: 111.22 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.30s/it, est. speed input: 859.77 toks/s, output: 13756.32 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 53.73it/s, est. speed input: 859.77 toks/s, output: 13756.32 toks/s]
[rank0]:[W128 01:14:41.583996599 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.4s

测试结果:
  Requests/s:   53.39
  Tokens/s:     14521.19
  Total Reqs:   128
  Elapsed:      2.40s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      13667.00

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:14:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3536067) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3536067) WARNING 01-28 01:14:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 61.32 requests/s, 16679.74 total tokens/s, 15698.58 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 01:14:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:14:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:14:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:14:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:14:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:14:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:14:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:14:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:14:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:14:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:14:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:14:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:14:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:14:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:14:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:14:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:14:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:14:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:14:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:51] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3536067) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3536067) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.86it/s]
(EngineCore_DP0 pid=3536067) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.86it/s]
(EngineCore_DP0 pid=3536067) 
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3536067) [2026-01-28 01:14:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3536067) 2026-01-28 01:15:00,473 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3536067) 2026-01-28 01:15:00,489 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3536067) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 25.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 24.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:01, 25.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 25.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 26.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 26.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:00<00:00, 25.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 25.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:01<00:00, 26.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:01<00:00, 26.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:01<00:00, 25.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 25.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 25.74it/s]
(EngineCore_DP0 pid=3536067) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.39it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:01, 18.81it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:01, 24.33it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:00<00:00, 25.88it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:00<00:00, 26.85it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 27.51it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:00<00:00, 28.10it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:00<00:00, 27.31it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:01<00:00, 25.29it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:01<00:00, 22.60it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:01<00:00, 20.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 22.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 23.47it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 7705.30it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<15:52,  3.73s/it, est. speed input: 4.28 toks/s, output: 68.54 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:03<00:05, 31.92it/s, est. speed input: 362.80 toks/s, output: 5804.80 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:03<00:01, 72.78it/s, est. speed input: 698.98 toks/s, output: 11183.64 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:04<00:00, 110.28it/s, est. speed input: 938.11 toks/s, output: 15009.75 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 110.28it/s, est. speed input: 989.30 toks/s, output: 15828.74 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 61.83it/s, est. speed input: 989.30 toks/s, output: 15828.74 toks/s] 
[rank0]:[W128 01:15:08.392778329 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.7s

测试结果:
  Requests/s:   61.32
  Tokens/s:     16679.74
  Total Reqs:   256
  Elapsed:      4.17s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      15698.58

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:15:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3536711) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3536711) WARNING 01-28 01:15:23 [backends.py:609] Failed to read file <frozen os>
Throughput: 54.76 requests/s, 14894.31 total tokens/s, 14018.17 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 01:15:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:15:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:15:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:15:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:15:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:15:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:15:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:15:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:15:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:15:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:15:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:15:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:15:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:15:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:15:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:15:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:15:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:15:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3536711) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3536711) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=3536711) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=3536711) 
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3536711) [2026-01-28 01:15:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3536711) 2026-01-28 01:15:28,995 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3536711) 2026-01-28 01:15:29,010 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3536711) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:01, 24.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:01, 25.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:01, 25.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:00<00:01, 25.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:00<00:01, 25.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:00<00:01, 24.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:00<00:01, 21.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:01<00:01, 18.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:01<00:01, 16.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:01<00:01, 18.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:01<00:00, 20.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:01<00:00, 21.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:01<00:00, 23.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:01<00:00, 22.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:01<00:00, 23.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:02<00:00, 24.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:02<00:00, 25.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 22.58it/s]
(EngineCore_DP0 pid=3536711) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:06,  7.36it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:02, 18.68it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:01, 23.42it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:00<00:01, 26.56it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:00<00:01, 27.51it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:00<00:01, 27.70it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:00<00:01, 27.99it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:00<00:00, 28.48it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:01<00:00, 28.52it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:01<00:00, 28.71it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:01<00:00, 29.00it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:01<00:00, 29.25it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:01<00:00, 29.35it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:01<00:00, 28.19it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:01<00:00, 24.82it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:01<00:00, 21.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 21.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 25.25it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 9502.27it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<56:36,  6.65s/it, est. speed input: 2.41 toks/s, output: 38.52 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:06<00:16, 24.42it/s, est. speed input: 276.39 toks/s, output: 4422.30 toks/s]
Processed prompts:  40%|████      | 205/512 [00:06<00:06, 48.86it/s, est. speed input: 475.30 toks/s, output: 7604.77 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [00:07<00:02, 79.33it/s, est. speed input: 657.03 toks/s, output: 10512.52 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [00:07<00:01, 121.17it/s, est. speed input: 844.37 toks/s, output: 13509.98 toks/s]
Processed prompts:  87%|████████▋ | 447/512 [00:08<00:00, 96.67it/s, est. speed input: 872.75 toks/s, output: 13963.93 toks/s] 
Processed prompts:  97%|█████████▋| 496/512 [00:08<00:00, 101.04it/s, est. speed input: 922.00 toks/s, output: 14751.94 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 101.04it/s, est. speed input: 881.28 toks/s, output: 14100.53 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 55.08it/s, est. speed input: 881.28 toks/s, output: 14100.53 toks/s] 
[rank0]:[W128 01:15:43.549731068 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.1s

测试结果:
  Requests/s:   54.76
  Tokens/s:     14894.31
  Total Reqs:   512
  Elapsed:      9.35s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      14018.17


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,33.5229,9118.2386,1.9091
128,16,128,128,256,256,53.3867,14521.1892,2.3976
256,16,256,256,256,256,61.3226,16679.7441,4.1746
512,16,512,512,256,256,54.7585,14894.3103,9.3501

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_8) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:15:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3537462) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3537462) WARNING 01-28 01:15:57 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.12 requests/s, 8735.43 total tokens/s, 8221.58 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 01:15:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:15:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:15:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:15:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:15:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:15:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:15:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:15:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:15:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:15:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:15:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:15:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:15:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:15:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:15:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:15:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:15:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:15:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:15:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3537462) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3537462) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=3537462) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=3537462) 
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3537462) [2026-01-28 01:15:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3537462) 2026-01-28 01:16:03,722 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3537462) 2026-01-28 01:16:03,738 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3537462) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  4.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  4.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01, 10.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 14.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:00<00:00, 16.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:00<00:00, 19.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00, 21.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 22.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 16.32it/s]
(EngineCore_DP0 pid=3537462) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.31it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 18.59it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 23.22it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 25.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.01it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3265.08it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:01<02:02,  1.94s/it, est. speed input: 8.25 toks/s, output: 132.03 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00,  1.94s/it, est. speed input: 519.42 toks/s, output: 8310.68 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00, 32.46it/s, est. speed input: 519.42 toks/s, output: 8310.68 toks/s]
[rank0]:[W128 01:16:08.258160076 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.9s

测试结果:
  Requests/s:   32.12
  Tokens/s:     8735.43
  Total Reqs:   64
  Elapsed:      1.99s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      8221.58

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:16:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3538122) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3538122) WARNING 01-28 01:16:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 52.20 requests/s, 14198.76 total tokens/s, 13363.54 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 01:16:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:16:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:16:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:16:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:16:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:16:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:16:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:16:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:16:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:16:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:16:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:16:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:16:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:16:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:16:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:16:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:16:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:16:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:17] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3538122) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3538122) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.60it/s]
(EngineCore_DP0 pid=3538122) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.60it/s]
(EngineCore_DP0 pid=3538122) 
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3538122) [2026-01-28 01:16:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3538122) 2026-01-28 01:16:26,949 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3538122) 2026-01-28 01:16:26,965 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3538122) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:07,  4.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:05,  5.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:02, 12.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:01, 17.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:01, 19.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:00<00:00, 22.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:00<00:00, 23.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:01<00:00, 23.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:01<00:00, 24.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:01<00:00, 23.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:01<00:00, 24.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:01<00:00, 25.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 25.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 21.11it/s]
(EngineCore_DP0 pid=3538122) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.21it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 18.11it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 22.73it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:00<00:00, 25.12it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 26.46it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 27.57it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 27.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 24.88it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 6364.72it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<04:58,  2.35s/it, est. speed input: 6.81 toks/s, output: 109.01 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.35s/it, est. speed input: 842.44 toks/s, output: 13478.96 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 52.65it/s, est. speed input: 842.44 toks/s, output: 13478.96 toks/s]
[rank0]:[W128 01:16:32.662693002 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.4s

测试结果:
  Requests/s:   52.20
  Tokens/s:     14198.76
  Total Reqs:   128
  Elapsed:      2.45s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      13363.54

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:16:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3538744) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3538744) WARNING 01-28 01:16:47 [backends.py:609] Failed to read file <frozen os>
Throughput: 58.52 requests/s, 15918.33 total tokens/s, 14981.96 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 01:16:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:16:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:16:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:16:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:16:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:16:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:16:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:16:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:16:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:16:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:16:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:16:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:16:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:16:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:16:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:16:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:16:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:16:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:16:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3538744) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3538744) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.68it/s]
(EngineCore_DP0 pid=3538744) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.68it/s]
(EngineCore_DP0 pid=3538744) 
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3538744) [2026-01-28 01:16:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3538744) 2026-01-28 01:16:51,482 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3538744) 2026-01-28 01:16:51,498 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3538744) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 25.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 26.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:00, 27.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 26.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 25.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 25.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:00<00:00, 25.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 26.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:01<00:00, 26.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:01<00:00, 26.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:01<00:00, 26.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 26.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 26.22it/s]
(EngineCore_DP0 pid=3538744) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.40it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:01, 17.78it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:01, 23.78it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:00<00:00, 25.81it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:00<00:00, 27.17it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 27.97it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:00<00:00, 28.53it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:00<00:00, 29.13it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:01<00:00, 26.12it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:01<00:00, 23.74it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 21.71it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 24.06it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:39,  6.40it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 1396.17it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<16:02,  3.77s/it, est. speed input: 4.24 toks/s, output: 67.83 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:03<00:05, 31.96it/s, est. speed input: 363.24 toks/s, output: 5811.78 toks/s]
Processed prompts:  68%|██████▊   | 173/256 [00:03<00:01, 72.39it/s, est. speed input: 695.91 toks/s, output: 11134.47 toks/s]
Processed prompts:  93%|█████████▎| 239/256 [00:04<00:00, 109.28it/s, est. speed input: 931.84 toks/s, output: 14909.50 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 109.28it/s, est. speed input: 977.79 toks/s, output: 15644.58 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 61.11it/s, est. speed input: 977.79 toks/s, output: 15644.58 toks/s] 
[rank0]:[W128 01:16:59.579559896 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.8s

测试结果:
  Requests/s:   58.52
  Tokens/s:     15918.33
  Total Reqs:   256
  Elapsed:      4.37s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      14981.96

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:17:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3539387) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3539387) WARNING 01-28 01:17:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.74 requests/s, 14616.10 total tokens/s, 13756.33 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 01:17:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:17:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:17:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:17:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:17:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:17:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:17:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:17:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:17:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:17:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:17:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:17:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:17:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:17:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:17:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:17:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:17:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:17:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3539387) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3539387) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.64it/s]
(EngineCore_DP0 pid=3539387) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.64it/s]
(EngineCore_DP0 pid=3539387) 
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3539387) [2026-01-28 01:17:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3539387) 2026-01-28 01:17:20,059 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3539387) 2026-01-28 01:17:20,075 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3539387) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:02, 21.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:02, 22.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:01, 23.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:00<00:01, 24.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:00<00:01, 24.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:00<00:01, 21.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:01<00:01, 18.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:01<00:01, 16.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:01<00:01, 18.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:01<00:01, 19.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:01<00:00, 21.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:01<00:00, 21.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:01<00:00, 22.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:01<00:00, 22.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:02<00:00, 22.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:02<00:00, 23.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:02<00:00, 23.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 21.62it/s]
(EngineCore_DP0 pid=3539387) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:06,  7.32it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:02, 18.51it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:01, 22.71it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:00<00:01, 25.11it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:00<00:01, 26.44it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:00<00:01, 27.34it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:00<00:01, 27.84it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:00<00:01, 28.17it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:00<00:00, 26.59it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:01<00:00, 26.41it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:01<00:00, 26.82it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:01<00:00, 27.25it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:01<00:00, 25.32it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:01<00:00, 23.27it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:01<00:00, 21.69it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:01<00:00, 20.52it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:02<00:00, 21.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 23.89it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 9581.76it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<56:43,  6.66s/it, est. speed input: 2.40 toks/s, output: 38.43 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:06<00:16, 24.38it/s, est. speed input: 275.93 toks/s, output: 4414.85 toks/s]
Processed prompts:  40%|████      | 205/512 [00:06<00:06, 48.84it/s, est. speed input: 474.78 toks/s, output: 7596.49 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [00:07<00:02, 79.52it/s, est. speed input: 656.99 toks/s, output: 10511.87 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:07<00:01, 114.08it/s, est. speed input: 819.17 toks/s, output: 13106.76 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:08<00:00, 90.61it/s, est. speed input: 837.63 toks/s, output: 13402.03 toks/s] 
Processed prompts:  93%|█████████▎| 474/512 [00:08<00:00, 96.02it/s, est. speed input: 884.19 toks/s, output: 14147.07 toks/s]
Processed prompts:  99%|█████████▉| 508/512 [00:09<00:00, 73.63it/s, est. speed input: 857.95 toks/s, output: 13727.22 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 73.63it/s, est. speed input: 864.70 toks/s, output: 13835.18 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 54.04it/s, est. speed input: 864.70 toks/s, output: 13835.18 toks/s]
[rank0]:[W128 01:17:35.145952554 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.6s

测试结果:
  Requests/s:   53.74
  Tokens/s:     14616.10
  Total Reqs:   512
  Elapsed:      9.53s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      13756.33


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,32.1155,8735.4257,1.9928
128,16,128,128,256,256,52.2013,14198.7614,2.4520
256,16,256,256,256,256,58.5233,15918.3276,4.3743
512,16,512,512,256,256,53.7356,14616.0953,9.5281

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:17:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3540150) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3540150) WARNING 01-28 01:17:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.48 requests/s, 8563.76 total tokens/s, 8060.01 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 01:17:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:17:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:17:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:17:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:17:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:17:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:17:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:17:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:17:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:17:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:17:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:17:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:17:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:17:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:17:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:17:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:17:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:17:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:17:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:44] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3540150) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3540150) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.47it/s]
(EngineCore_DP0 pid=3540150) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.47it/s]
(EngineCore_DP0 pid=3540150) 
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3540150) [2026-01-28 01:17:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3540150) 2026-01-28 01:17:55,669 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3540150) 2026-01-28 01:17:55,684 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3540150) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  3.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  4.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01, 10.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 14.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 17.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 20.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00, 21.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 15.78it/s]
(EngineCore_DP0 pid=3540150) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  6.58it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 11.15it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00, 13.87it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 19.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 18.39it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2562.68it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:01<02:04,  1.97s/it, est. speed input: 8.11 toks/s, output: 129.74 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  1.97s/it, est. speed input: 510.52 toks/s, output: 8168.28 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 31.91it/s, est. speed input: 510.52 toks/s, output: 8168.28 toks/s]
[rank0]:[W128 01:18:00.421515407 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.2s

测试结果:
  Requests/s:   31.48
  Tokens/s:     8563.76
  Total Reqs:   64
  Elapsed:      2.03s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      8060.01

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:18:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3540816) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3540816) WARNING 01-28 01:18:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.02 requests/s, 13877.27 total tokens/s, 13060.96 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 01:18:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:18:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:18:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:18:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:18:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:18:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:18:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:18:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:18:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:18:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:18:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:18:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:18:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:18:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:18:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:18:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:18:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:18:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3540816) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3540816) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=3540816) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=3540816) 
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3540816) [2026-01-28 01:18:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3540816) 2026-01-28 01:18:19,184 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3540816) 2026-01-28 01:18:19,201 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3540816) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:04,  6.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:06,  5.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:02, 11.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:01, 16.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:01, 19.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:00<00:00, 21.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:00<00:00, 22.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:01<00:00, 23.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:01<00:00, 24.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:01<00:00, 24.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:01<00:00, 23.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:01<00:00, 24.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 22.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 20.45it/s]
(EngineCore_DP0 pid=3540816) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:03,  5.93it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01, 10.40it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01, 13.36it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:00, 18.68it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:00<00:00, 21.99it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:00<00:00, 23.60it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:00<00:00, 25.48it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 20.98it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3861.30it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<05:03,  2.39s/it, est. speed input: 6.70 toks/s, output: 107.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.39s/it, est. speed input: 827.86 toks/s, output: 13245.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 51.74it/s, est. speed input: 827.86 toks/s, output: 13245.81 toks/s]
[rank0]:[W128 01:18:25.152191407 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.8s

测试结果:
  Requests/s:   51.02
  Tokens/s:     13877.27
  Total Reqs:   128
  Elapsed:      2.51s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      13060.96

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:18:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3541451) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3541451) WARNING 01-28 01:18:39 [backends.py:609] Failed to read file <frozen os>
Throughput: 60.08 requests/s, 16342.35 total tokens/s, 15381.04 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 01:18:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:18:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:18:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:18:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:18:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:18:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:18:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:18:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:18:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:18:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:18:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:18:34] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:18:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:34] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:34] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:18:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:18:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:18:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:18:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:18:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:34] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3541451) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3541451) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.54it/s]
(EngineCore_DP0 pid=3541451) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.53it/s]
(EngineCore_DP0 pid=3541451) 
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3541451) [2026-01-28 01:18:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3541451) 2026-01-28 01:18:43,845 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3541451) 2026-01-28 01:18:43,861 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3541451) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 25.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 25.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:01, 26.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 26.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 24.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 24.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:00<00:00, 24.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 25.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:01<00:00, 20.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:01<00:00, 16.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:01<00:00, 18.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 19.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 21.63it/s]
(EngineCore_DP0 pid=3541451) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.23it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:01, 18.50it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:01, 23.21it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:00, 25.45it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 26.73it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 27.54it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:00<00:00, 27.90it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:00<00:00, 28.25it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 28.44it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:01<00:00, 27.54it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:01<00:00, 27.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 28.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 26.57it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 9143.21it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<16:11,  3.81s/it, est. speed input: 4.20 toks/s, output: 67.22 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:03<00:05, 31.30it/s, est. speed input: 355.76 toks/s, output: 5692.15 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:04<00:01, 71.33it/s, est. speed input: 685.26 toks/s, output: 10964.14 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:04<00:00, 106.91it/s, est. speed input: 912.33 toks/s, output: 14597.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 106.91it/s, est. speed input: 967.88 toks/s, output: 15486.10 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 60.49it/s, est. speed input: 967.88 toks/s, output: 15486.10 toks/s] 
[rank0]:[W128 01:18:52.967094984 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.8s

测试结果:
  Requests/s:   60.08
  Tokens/s:     16342.35
  Total Reqs:   256
  Elapsed:      4.26s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      15381.04

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:18:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3542082) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3542082) WARNING 01-28 01:19:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.15 requests/s, 14456.45 total tokens/s, 13606.07 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 01:18:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:18:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:18:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:18:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:18:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:18:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:18:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:18:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:18:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:18:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:19:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:19:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 01:19:01] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 01:19:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:19:01] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:19:01] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:19:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:19:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 01:19:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 01:19:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:19:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:19:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:19:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:19:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3542082) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3542082) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.35it/s]
(EngineCore_DP0 pid=3542082) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.35it/s]
(EngineCore_DP0 pid=3542082) 
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3542082) [2026-01-28 01:19:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3542082) 2026-01-28 01:19:12,550 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3542082) 2026-01-28 01:19:12,566 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3542082) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:02, 19.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:02, 22.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:01, 24.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:00<00:01, 24.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:00<00:01, 24.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:00<00:01, 25.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:00<00:01, 25.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:00<00:01, 25.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:01<00:00, 26.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:01<00:00, 25.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:01<00:00, 25.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:01<00:00, 25.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:01<00:00, 25.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:01<00:00, 25.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:01<00:00, 25.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:01<00:00, 26.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:01<00:00, 26.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 25.40it/s]
(EngineCore_DP0 pid=3542082) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:08,  6.19it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:03, 14.69it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:02, 16.51it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:00<00:02, 16.44it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:00<00:02, 18.88it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:00<00:01, 21.54it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:00<00:01, 23.73it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:01<00:01, 25.44it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:01<00:00, 26.58it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:01<00:00, 26.03it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:01<00:00, 26.96it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:01<00:00, 27.58it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:01<00:00, 28.13it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:01<00:00, 28.49it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:01<00:00, 28.83it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:01<00:00, 29.02it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:02<00:00, 29.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 24.58it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 9408.97it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<56:34,  6.64s/it, est. speed input: 2.41 toks/s, output: 38.54 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:06<00:16, 24.45it/s, est. speed input: 276.72 toks/s, output: 4427.56 toks/s]
Processed prompts:  40%|████      | 205/512 [00:06<00:06, 49.02it/s, est. speed input: 476.31 toks/s, output: 7620.97 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [00:06<00:02, 79.95it/s, est. speed input: 659.52 toks/s, output: 10552.32 toks/s]
Processed prompts:  70%|███████   | 359/512 [00:07<00:01, 110.69it/s, est. speed input: 804.75 toks/s, output: 12876.00 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:08<00:01, 86.94it/s, est. speed input: 820.53 toks/s, output: 13128.46 toks/s] 
Processed prompts:  91%|█████████ | 466/512 [00:08<00:00, 92.31it/s, est. speed input: 866.32 toks/s, output: 13861.13 toks/s]
Processed prompts:  97%|█████████▋| 499/512 [00:09<00:00, 68.98it/s, est. speed input: 833.61 toks/s, output: 13337.76 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 68.98it/s, est. speed input: 855.30 toks/s, output: 13684.79 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 53.46it/s, est. speed input: 855.30 toks/s, output: 13684.79 toks/s]
[rank0]:[W128 01:19:27.322303989 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.3s

测试结果:
  Requests/s:   53.15
  Tokens/s:     14456.45
  Total Reqs:   512
  Elapsed:      9.63s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      13606.07


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,31.4844,8563.7555,2.0328
128,16,128,128,256,256,51.0194,13877.2690,2.5089
256,16,256,256,256,256,60.0822,16342.3544,4.2608
512,16,512,512,256,256,53.1487,14456.4512,9.6333

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 20 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_011012.log
[SUCCESS] bitnet1.58-2b-int8 Decode 完成 (559.9s)

------------------------------------------------------------
  Decode Benchmark: bitnet1.58-2b-fp8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-fp8 --backend cublaslt,cusparselt --stage decode --sparsity 2_4,2_6,2_8,2_10 --M 64,128,256,512


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA GeForce RTX 5080                   ││
│ GPU (short):      RTX5080                                   │
│ Memory:           15.5 GB                                    │
│ CC:               cc120 (Blackwell)                            │
│ SM Code:          sm_120                                    │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.95.05                                 │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-fp8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['decode']
  M_prefill:        [64, 128, 256, 512]
  M_decode:         [64, 128, 256, 512]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_011931.log


============================================================
  BitNet-2B-FP8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:19:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3542945) WARNING 01-28 01:19:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.75 requests/s, 9452.64 total tokens/s, 8896.60 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 01:19:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:19:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:19:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:19:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:19:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:19:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:19:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:19:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:19:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:19:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:19:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:19:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:19:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:19:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:19:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:19:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:19:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:19:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3542945) [2026-01-28 01:19:40] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3542945) [2026-01-28 01:19:40] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3542945) [2026-01-28 01:19:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3542945) [2026-01-28 01:19:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3542945) [2026-01-28 01:19:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3542945) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3542945) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.65it/s]
(EngineCore_DP0 pid=3542945) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=3542945) 
(EngineCore_DP0 pid=3542945) 2026-01-28 01:19:50,534 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3542945) 2026-01-28 01:19:50,551 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3542945) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:00, 22.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:00, 23.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:00, 20.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:00<00:00, 16.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:00<00:00, 18.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:00<00:00, 20.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 19.54it/s]
(EngineCore_DP0 pid=3542945) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.16it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 17.78it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 21.90it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 23.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 21.75it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2876.97it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:01<01:52,  1.79s/it, est. speed input: 8.96 toks/s, output: 143.39 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00,  1.79s/it, est. speed input: 563.27 toks/s, output: 9012.25 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00, 35.20it/s, est. speed input: 563.27 toks/s, output: 9012.25 toks/s]
[rank0]:[W128 01:19:54.752490318 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.2s

测试结果:
  Requests/s:   34.75
  Tokens/s:     9452.64
  Total Reqs:   64
  Elapsed:      1.84s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      8896.60

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:19:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3543552) WARNING 01-28 01:20:08 [backends.py:609] Failed to read file <frozen os>
Throughput: 50.58 requests/s, 13757.06 total tokens/s, 12947.82 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 01:19:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:19:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:19:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:19:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:19:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:19:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:19:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:19:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:19:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:19:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:20:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:20:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:20:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:20:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:20:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:20:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:20:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:20:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:20:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3543552) [2026-01-28 01:20:04] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3543552) [2026-01-28 01:20:04] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3543552) [2026-01-28 01:20:04] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3543552) [2026-01-28 01:20:04] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3543552) [2026-01-28 01:20:04] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3543552) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3543552) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=3543552) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=3543552) 
(EngineCore_DP0 pid=3543552) 2026-01-28 01:20:12,988 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3543552) 2026-01-28 01:20:13,005 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3543552) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:01, 24.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:01, 24.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:01, 25.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 25.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 25.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:00<00:00, 25.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:00<00:00, 25.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:00<00:00, 26.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:01<00:00, 26.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:01<00:00, 26.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:01<00:00, 26.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 25.53it/s]
(EngineCore_DP0 pid=3543552) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.22it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 17.66it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 21.67it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:00<00:00, 22.15it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 19.59it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 19.46it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:00<00:00, 18.58it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 19.17it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 5869.05it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<05:07,  2.42s/it, est. speed input: 6.61 toks/s, output: 105.68 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.42s/it, est. speed input: 816.84 toks/s, output: 13069.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 51.05it/s, est. speed input: 816.84 toks/s, output: 13069.44 toks/s]
[rank0]:[W128 01:20:18.779906427 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.2s

测试结果:
  Requests/s:   50.58
  Tokens/s:     13757.06
  Total Reqs:   128
  Elapsed:      2.53s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      12947.82

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:20:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3544158) WARNING 01-28 01:20:33 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.71 requests/s, 14608.13 total tokens/s, 13748.83 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 01:20:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:20:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:20:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:20:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:20:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:20:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:20:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:20:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:20:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:20:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:20:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:20:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:20:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:20:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:20:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:20:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:20:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:20:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3544158) [2026-01-28 01:20:28] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3544158) [2026-01-28 01:20:28] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3544158) [2026-01-28 01:20:28] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3544158) [2026-01-28 01:20:28] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3544158) [2026-01-28 01:20:28] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3544158) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3544158) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.66it/s]
(EngineCore_DP0 pid=3544158) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.65it/s]
(EngineCore_DP0 pid=3544158) 
(EngineCore_DP0 pid=3544158) 2026-01-28 01:20:37,310 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3544158) 2026-01-28 01:20:37,328 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3544158) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 24.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 25.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:01, 25.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 25.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 23.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 24.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:00<00:00, 24.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 24.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:01<00:00, 25.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:01<00:00, 25.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:01<00:00, 25.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 24.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 24.87it/s]
(EngineCore_DP0 pid=3544158) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.90it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:02, 11.54it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:02, 13.17it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:02, 13.90it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:01, 17.55it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:01, 20.41it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 22.39it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:00<00:00, 23.70it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:01<00:00, 24.56it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:01<00:00, 25.29it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:01<00:00, 24.81it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:01<00:00, 25.25it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:01<00:00, 25.67it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 21.69it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 7991.83it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:04<18:15,  4.30s/it, est. speed input: 3.72 toks/s, output: 59.58 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:04<00:06, 27.78it/s, est. speed input: 315.65 toks/s, output: 5050.44 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:04<00:01, 63.56it/s, est. speed input: 609.19 toks/s, output: 9747.01 toks/s]
Processed prompts:  91%|█████████ | 233/256 [00:04<00:00, 95.02it/s, est. speed input: 805.48 toks/s, output: 12887.75 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 95.02it/s, est. speed input: 865.32 toks/s, output: 13845.15 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 54.08it/s, est. speed input: 865.32 toks/s, output: 13845.15 toks/s]
[rank0]:[W128 01:20:46.003188271 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 27.1s

测试结果:
  Requests/s:   53.71
  Tokens/s:     14608.13
  Total Reqs:   256
  Elapsed:      4.77s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      13748.83

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:20:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3544789) WARNING 01-28 01:21:00 [backends.py:609] Failed to read file <frozen os>
Throughput: 47.73 requests/s, 12981.25 total tokens/s, 12217.65 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 01:20:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:20:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:20:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:20:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:20:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:20:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:20:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:20:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:20:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:20:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:20:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:20:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:20:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:20:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:20:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:20:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:20:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:20:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:20:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3544789) [2026-01-28 01:20:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3544789) [2026-01-28 01:20:55] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3544789) [2026-01-28 01:20:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3544789) [2026-01-28 01:20:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3544789) [2026-01-28 01:20:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3544789) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3544789) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.66it/s]
(EngineCore_DP0 pid=3544789) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.66it/s]
(EngineCore_DP0 pid=3544789) 
(EngineCore_DP0 pid=3544789) 2026-01-28 01:21:06,099 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3544789) 2026-01-28 01:21:06,122 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3544789) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:07,  6.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:02, 15.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:02, 20.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:00<00:01, 22.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:00<00:01, 23.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:00<00:01, 24.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:00<00:01, 24.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:00<00:01, 25.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:01<00:01, 25.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:01<00:00, 24.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:01<00:00, 24.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:01<00:00, 24.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:01<00:00, 25.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:01<00:00, 25.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:01<00:00, 25.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:01<00:00, 25.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:02<00:00, 25.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 23.82it/s]
(EngineCore_DP0 pid=3544789) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:07,  7.07it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:02, 17.28it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:02, 21.48it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:00<00:01, 23.77it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:00<00:01, 22.34it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:00<00:01, 21.05it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:00<00:01, 17.87it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:01<00:01, 19.38it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:01<00:01, 21.19it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:01<00:01, 22.68it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:01<00:00, 23.93it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:01<00:00, 24.82it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:01<00:00, 25.46it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:01<00:00, 25.88it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:01<00:00, 26.00it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:02<00:00, 26.41it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:02<00:00, 26.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 23.08it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 9242.09it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:07<1:05:25,  7.68s/it, est. speed input: 2.08 toks/s, output: 33.32 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:07<00:25, 16.55it/s, est. speed input: 186.99 toks/s, output: 2991.82 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:07<00:10, 34.66it/s, est. speed input: 332.70 toks/s, output: 5323.20 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:08<00:04, 60.20it/s, est. speed input: 483.19 toks/s, output: 7731.00 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:08<00:02, 91.15it/s, est. speed input: 620.00 toks/s, output: 9919.98 toks/s]
Processed prompts:  74%|███████▎  | 377/512 [00:08<00:01, 124.06it/s, est. speed input: 732.70 toks/s, output: 11723.16 toks/s]
Processed prompts:  86%|████████▌ | 439/512 [00:09<00:00, 87.57it/s, est. speed input: 746.73 toks/s, output: 11947.60 toks/s] 
Processed prompts:  94%|█████████▍| 482/512 [00:09<00:00, 92.57it/s, est. speed input: 788.20 toks/s, output: 12611.15 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 92.57it/s, est. speed input: 767.64 toks/s, output: 12282.21 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 47.98it/s, est. speed input: 767.64 toks/s, output: 12282.21 toks/s]
[rank0]:[W128 01:21:22.243370169 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.3s

测试结果:
  Requests/s:   47.73
  Tokens/s:     12981.25
  Total Reqs:   512
  Elapsed:      10.73s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      12217.65


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,34.7524,9452.6401,1.8416
128,16,128,128,256,256,50.5774,13757.0600,2.5308
256,16,256,256,256,256,53.7064,14608.1331,4.7667
512,16,512,512,256,256,47.7252,12981.2479,10.7281

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:21:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3545563) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3545563) WARNING 01-28 01:21:36 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.11 requests/s, 9278.76 total tokens/s, 8732.95 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 01:21:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:21:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:21:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:21:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:21:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:21:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:21:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:21:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:21:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:21:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:21:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:21:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:21:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:21:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:21:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:21:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:21:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:21:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3545563) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3545563) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.63it/s]
(EngineCore_DP0 pid=3545563) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.62it/s]
(EngineCore_DP0 pid=3545563) 
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3545563) [2026-01-28 01:21:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3545563) 2026-01-28 01:21:42,594 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3545563) 2026-01-28 01:21:42,609 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3545563) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  4.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:03,  4.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01, 10.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 15.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 17.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 19.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00, 21.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 16.78it/s]
(EngineCore_DP0 pid=3545563) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  6.83it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 17.90it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 22.50it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 24.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 22.33it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 7407.57it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:01<01:55,  1.84s/it, est. speed input: 8.72 toks/s, output: 139.50 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00,  1.84s/it, est. speed input: 548.56 toks/s, output: 8776.93 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00, 34.28it/s, est. speed input: 548.56 toks/s, output: 8776.93 toks/s]
[rank0]:[W128 01:21:47.998185867 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.7s

测试结果:
  Requests/s:   34.11
  Tokens/s:     9278.76
  Total Reqs:   64
  Elapsed:      1.88s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      8732.95

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:21:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3546224) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3546224) WARNING 01-28 01:22:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.22 requests/s, 14476.36 total tokens/s, 13624.81 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 01:21:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:21:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:21:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:21:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:21:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:21:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:21:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:21:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:21:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:21:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:21:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:21:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:21:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:21:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:21:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:21:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:21:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:21:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:21:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3546224) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3546224) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.49it/s]
(EngineCore_DP0 pid=3546224) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.49it/s]
(EngineCore_DP0 pid=3546224) 
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3546224) [2026-01-28 01:21:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3546224) 2026-01-28 01:22:05,668 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3546224) 2026-01-28 01:22:05,684 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3546224) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:05,  6.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:04,  7.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:04,  7.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:03,  9.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:02, 12.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:00<00:01, 16.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:00<00:01, 18.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:01<00:00, 21.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:01<00:00, 21.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:01<00:00, 23.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:01<00:00, 23.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:01<00:00, 24.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:01<00:00, 24.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:01<00:00, 25.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 19.61it/s]
(EngineCore_DP0 pid=3546224) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.05it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 18.26it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 22.87it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:00<00:00, 24.81it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 25.97it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 27.00it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 27.70it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 24.74it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 5325.26it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<04:51,  2.30s/it, est. speed input: 6.96 toks/s, output: 111.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.30s/it, est. speed input: 860.51 toks/s, output: 13768.19 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 53.78it/s, est. speed input: 860.51 toks/s, output: 13768.19 toks/s]
[rank0]:[W128 01:22:11.468627108 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.4s

测试结果:
  Requests/s:   53.22
  Tokens/s:     14476.36
  Total Reqs:   128
  Elapsed:      2.41s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      13624.81

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:22:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3546847) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3546847) WARNING 01-28 01:22:25 [backends.py:609] Failed to read file <frozen os>
Throughput: 60.47 requests/s, 16446.66 total tokens/s, 15479.21 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 01:22:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:22:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:22:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:22:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:22:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:22:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:22:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:22:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:22:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:22:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:22:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:22:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:22:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:22:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:22:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:22:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:22:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:22:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3546847) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3546847) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.62it/s]
(EngineCore_DP0 pid=3546847) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.62it/s]
(EngineCore_DP0 pid=3546847) 
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3546847) [2026-01-28 01:22:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3546847) 2026-01-28 01:22:29,854 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3546847) 2026-01-28 01:22:29,869 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3546847) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:02, 16.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:02, 14.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:02, 12.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:01, 16.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:01, 19.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 21.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 22.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:01<00:00, 23.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:01<00:00, 23.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:01<00:00, 24.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:01<00:00, 24.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:01<00:00, 24.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 24.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 21.85it/s]
(EngineCore_DP0 pid=3546847) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.30it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 20.60it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:01, 24.18it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:00<00:00, 26.07it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:00<00:00, 27.22it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 27.98it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:00<00:00, 28.31it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:00<00:00, 26.50it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:01<00:00, 26.71it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:01<00:00, 27.59it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:01<00:00, 27.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 27.62it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 26.24it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 6829.68it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<16:05,  3.79s/it, est. speed input: 4.22 toks/s, output: 67.59 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:03<00:05, 31.48it/s, est. speed input: 357.82 toks/s, output: 5725.18 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:03<00:01, 74.53it/s, est. speed input: 712.40 toks/s, output: 11398.32 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:04<00:00, 110.27it/s, est. speed input: 944.25 toks/s, output: 15107.97 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 110.27it/s, est. speed input: 976.40 toks/s, output: 15622.39 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 61.02it/s, est. speed input: 976.40 toks/s, output: 15622.39 toks/s] 
[rank0]:[W128 01:22:38.912766143 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.5s

测试结果:
  Requests/s:   60.47
  Tokens/s:     16446.66
  Total Reqs:   256
  Elapsed:      4.23s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      15479.21

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:22:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3547482) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3547482) WARNING 01-28 01:22:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.48 requests/s, 15091.05 total tokens/s, 14203.34 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 01:22:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:22:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:22:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:22:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:22:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:22:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:22:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:22:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:22:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:22:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:22:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:22:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:22:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:22:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:22:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:22:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:22:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:22:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:22:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:47] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:47] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:47] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:47] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:47] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3547482) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3547482) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.56it/s]
(EngineCore_DP0 pid=3547482) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.55it/s]
(EngineCore_DP0 pid=3547482) 
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3547482) [2026-01-28 01:22:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3547482) 2026-01-28 01:22:58,435 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3547482) 2026-01-28 01:22:58,450 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3547482) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:02, 23.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:01, 23.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:01, 23.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:00<00:01, 23.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:00<00:01, 23.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:00<00:01, 23.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:00<00:01, 22.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:01<00:01, 23.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:01<00:01, 23.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:01<00:00, 24.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:01<00:00, 20.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:01<00:00, 17.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:01<00:00, 15.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:02<00:00, 17.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:02<00:00, 19.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:02<00:00, 20.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:02<00:00, 22.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 21.21it/s]
(EngineCore_DP0 pid=3547482) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:06,  7.31it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:02, 18.29it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:01, 22.77it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:00<00:01, 25.06it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:00<00:01, 26.45it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:00<00:01, 25.89it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:00<00:01, 26.81it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:00<00:01, 27.63it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:00<00:00, 28.09it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:01<00:00, 28.31it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:01<00:00, 28.62it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:01<00:00, 28.73it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:01<00:00, 28.54it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:01<00:00, 28.58it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:01<00:00, 28.68it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:01<00:00, 26.62it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:01<00:00, 27.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 26.68it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 9418.79it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<57:56,  6.80s/it, est. speed input: 2.35 toks/s, output: 37.63 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:06<00:16, 24.12it/s, est. speed input: 272.78 toks/s, output: 4364.40 toks/s]
Processed prompts:  40%|████      | 206/512 [00:07<00:06, 48.13it/s, est. speed input: 467.80 toks/s, output: 7484.77 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:07<00:03, 72.29it/s, est. speed input: 612.46 toks/s, output: 9799.43 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:07<00:01, 101.71it/s, est. speed input: 744.62 toks/s, output: 11913.94 toks/s]
Processed prompts:  78%|███████▊  | 401/512 [00:07<00:00, 137.85it/s, est. speed input: 870.83 toks/s, output: 13933.29 toks/s]
Processed prompts:  90%|█████████ | 463/512 [00:08<00:00, 117.11it/s, est. speed input: 917.22 toks/s, output: 14675.50 toks/s]
Processed prompts:  99%|█████████▉| 508/512 [00:09<00:00, 80.88it/s, est. speed input: 886.07 toks/s, output: 14177.11 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 80.88it/s, est. speed input: 893.04 toks/s, output: 14288.60 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 55.81it/s, est. speed input: 893.04 toks/s, output: 14288.60 toks/s]
[rank0]:[W128 01:23:13.917085312 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.0s

测试结果:
  Requests/s:   55.48
  Tokens/s:     15091.05
  Total Reqs:   512
  Elapsed:      9.23s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      14203.34


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,34.1131,9278.7552,1.8761
128,16,128,128,256,256,53.2219,14476.3604,2.4050
256,16,256,256,256,256,60.4657,16446.6603,4.2338
512,16,512,512,256,256,55.4818,15091.0534,9.2282

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:23:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3548230) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3548230) WARNING 01-28 01:23:27 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.79 requests/s, 8374.73 total tokens/s, 7882.10 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 01:23:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:23:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:23:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:23:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:23:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:23:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:23:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:23:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:23:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:23:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:23:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:23:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:23:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:23:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:23:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:23:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:23:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:23:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:22] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:22] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:22] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:22] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:22] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3548230) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3548230) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.95it/s]
(EngineCore_DP0 pid=3548230) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.95it/s]
(EngineCore_DP0 pid=3548230) 
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3548230) [2026-01-28 01:23:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3548230) 2026-01-28 01:23:33,095 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3548230) 2026-01-28 01:23:33,110 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3548230) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  4.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:03,  4.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01, 10.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 15.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:00<00:00, 16.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:00<00:00, 19.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00, 21.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 22.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 16.63it/s]
(EngineCore_DP0 pid=3548230) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.32it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 18.68it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 23.18it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 25.45it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 22.99it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 6347.04it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:08,  2.03s/it, est. speed input: 7.87 toks/s, output: 125.94 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.03s/it, est. speed input: 495.24 toks/s, output: 7923.81 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 30.95it/s, est. speed input: 495.24 toks/s, output: 7923.81 toks/s]
[rank0]:[W128 01:23:37.683105400 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.8s

测试结果:
  Requests/s:   30.79
  Tokens/s:     8374.73
  Total Reqs:   64
  Elapsed:      2.08s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      7882.10

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:23:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3548890) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3548890) WARNING 01-28 01:23:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 49.41 requests/s, 13440.72 total tokens/s, 12650.09 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 01:23:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:23:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:23:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:23:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:23:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:23:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:23:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:23:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:23:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:23:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:23:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:23:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:23:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:23:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:23:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:23:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:23:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:23:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:23:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3548890) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3548890) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.93it/s]
(EngineCore_DP0 pid=3548890) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.92it/s]
(EngineCore_DP0 pid=3548890) 
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3548890) [2026-01-28 01:23:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3548890) 2026-01-28 01:23:56,228 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3548890) 2026-01-28 01:23:56,243 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3548890) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:04,  7.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:06,  5.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:02, 11.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:01, 15.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:00<00:01, 15.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:01, 14.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:01, 15.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:01<00:01, 17.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:01<00:00, 19.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:01<00:00, 20.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:01<00:00, 21.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:01<00:00, 22.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:01<00:00, 23.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 23.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 18.33it/s]
(EngineCore_DP0 pid=3548890) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.35it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 18.64it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 22.84it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:00<00:00, 25.13it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 25.29it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 26.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 27.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 24.49it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4285.57it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<05:13,  2.47s/it, est. speed input: 6.48 toks/s, output: 103.69 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.47s/it, est. speed input: 800.50 toks/s, output: 12807.95 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 50.03it/s, est. speed input: 800.50 toks/s, output: 12807.95 toks/s]
[rank0]:[W128 01:24:02.355888903 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.6s

测试结果:
  Requests/s:   49.41
  Tokens/s:     13440.72
  Total Reqs:   128
  Elapsed:      2.59s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      12650.09

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:24:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3549525) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3549525) WARNING 01-28 01:24:16 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.96 requests/s, 15221.91 total tokens/s, 14326.51 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 01:24:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:24:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:24:07] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:24:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:07] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:07] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:24:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:24:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:24:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:24:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:24:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:24:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:24:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:24:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:24:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:24:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:24:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:24:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:24:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:24:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:11] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3549525) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3549525) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.81it/s]
(EngineCore_DP0 pid=3549525) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.81it/s]
(EngineCore_DP0 pid=3549525) 
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3549525) [2026-01-28 01:24:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3549525) 2026-01-28 01:24:20,828 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3549525) 2026-01-28 01:24:20,844 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3549525) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:02, 15.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:02, 13.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:02, 12.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:01, 16.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:01, 19.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:01, 20.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 22.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:01<00:00, 22.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:01<00:00, 23.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:01<00:00, 23.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:01<00:00, 23.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:01<00:00, 24.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 23.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 21.19it/s]
(EngineCore_DP0 pid=3549525) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.26it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:01, 18.56it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:01, 23.22it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:00, 25.50it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 26.83it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 27.50it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:00<00:00, 27.87it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:00<00:00, 28.27it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 28.39it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:01<00:00, 26.52it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:01<00:00, 27.11it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:01<00:00, 25.84it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 25.65it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 7913.55it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:04<17:21,  4.09s/it, est. speed input: 3.92 toks/s, output: 62.66 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:04<00:05, 29.18it/s, est. speed input: 331.68 toks/s, output: 5306.85 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:01, 64.04it/s, est. speed input: 617.92 toks/s, output: 9886.65 toks/s]
Processed prompts:  89%|████████▊ | 227/256 [00:04<00:00, 97.12it/s, est. speed input: 824.26 toks/s, output: 13188.15 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 97.12it/s, est. speed input: 902.15 toks/s, output: 14434.34 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 56.38it/s, est. speed input: 902.15 toks/s, output: 14434.34 toks/s]
[rank0]:[W128 01:24:29.339093196 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 27.1s

测试结果:
  Requests/s:   55.96
  Tokens/s:     15221.91
  Total Reqs:   256
  Elapsed:      4.57s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      14326.51

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:24:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3550173) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3550173) WARNING 01-28 01:24:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 50.02 requests/s, 13606.49 total tokens/s, 12806.11 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 01:24:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:24:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:24:34] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:24:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:34] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:34] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:24:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:24:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:24:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:24:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:24:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:24:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:24:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:24:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:24:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:24:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:24:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:24:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:24:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:24:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:24:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3550173) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3550173) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3550173) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3550173) 
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3550173) [2026-01-28 01:24:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3550173) 2026-01-28 01:24:49,757 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3550173) 2026-01-28 01:24:49,773 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3550173) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:01, 24.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:01, 24.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:01, 23.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:00<00:01, 23.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:00<00:01, 23.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:00<00:01, 22.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:00<00:01, 22.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:01<00:01, 23.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:01<00:01, 16.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:01<00:01, 14.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:01<00:01, 17.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:01<00:00, 18.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:01<00:00, 19.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:02<00:00, 21.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:02<00:00, 22.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:02<00:00, 22.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:02<00:00, 24.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 21.19it/s]
(EngineCore_DP0 pid=3550173) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:06,  7.32it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:02, 18.59it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:01, 23.14it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:00<00:01, 25.38it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:00<00:01, 26.39it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:00<00:01, 27.41it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:00<00:01, 28.06it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:00<00:01, 28.58it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:00<00:00, 28.84it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:01<00:00, 28.94it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:01<00:00, 29.11it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:01<00:00, 28.98it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:01<00:00, 28.89it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:01<00:00, 28.62it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:01<00:00, 27.34it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:01<00:00, 25.22it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:01<00:00, 24.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 25.49it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 7840.54it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:07<1:02:15,  7.31s/it, est. speed input: 2.19 toks/s, output: 35.02 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:07<00:24, 17.38it/s, est. speed input: 196.45 toks/s, output: 3143.24 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:07<00:07, 41.58it/s, est. speed input: 392.26 toks/s, output: 6276.11 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:07<00:03, 66.07it/s, est. speed input: 538.31 toks/s, output: 8612.90 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:07<00:01, 96.33it/s, est. speed input: 672.94 toks/s, output: 10766.99 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:07<00:00, 131.41it/s, est. speed input: 795.25 toks/s, output: 12724.02 toks/s]
Processed prompts:  88%|████████▊ | 453/512 [00:08<00:00, 97.40it/s, est. speed input: 815.23 toks/s, output: 13043.66 toks/s] 
Processed prompts:  97%|█████████▋| 496/512 [00:09<00:00, 97.96it/s, est. speed input: 851.38 toks/s, output: 13622.05 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 97.96it/s, est. speed input: 805.70 toks/s, output: 12891.11 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 50.36it/s, est. speed input: 805.70 toks/s, output: 12891.11 toks/s]
[rank0]:[W128 01:25:05.346012607 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.0s

测试结果:
  Requests/s:   50.02
  Tokens/s:     13606.49
  Total Reqs:   512
  Elapsed:      10.24s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      12806.11


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,30.7894,8374.7263,2.0786
128,16,128,128,256,256,49.4144,13440.7187,2.5903
256,16,256,256,256,256,55.9629,15221.9123,4.5745
512,16,512,512,256,256,50.0239,13606.4900,10.2351

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_8) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:25:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3550929) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3550929) WARNING 01-28 01:25:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 29.74 requests/s, 8089.55 total tokens/s, 7613.69 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 01:25:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:25:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:25:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:25:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:25:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:25:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:25:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:25:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:25:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:25:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:25:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:25:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:25:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:25:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:25:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:25:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:25:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:25:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3550929) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3550929) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.64it/s]
(EngineCore_DP0 pid=3550929) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.64it/s]
(EngineCore_DP0 pid=3550929) 
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3550929) [2026-01-28 01:25:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3550929) 2026-01-28 01:25:25,888 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3550929) 2026-01-28 01:25:25,903 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3550929) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:03,  4.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:03,  4.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:00, 13.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:00<00:00, 16.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:00<00:00, 18.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00, 18.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00, 18.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 16.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 14.47it/s]
(EngineCore_DP0 pid=3550929) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  6.92it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 16.99it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 21.35it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 23.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 21.47it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2753.98it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:11,  2.09s/it, est. speed input: 7.65 toks/s, output: 122.46 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.09s/it, est. speed input: 481.60 toks/s, output: 7705.58 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 30.10it/s, est. speed input: 481.60 toks/s, output: 7705.58 toks/s]
[rank0]:[W128 01:25:31.817655354 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.4s

测试结果:
  Requests/s:   29.74
  Tokens/s:     8089.55
  Total Reqs:   64
  Elapsed:      2.15s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      7613.69

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:25:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3551594) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3551594) WARNING 01-28 01:25:45 [backends.py:609] Failed to read file <frozen os>
Throughput: 47.72 requests/s, 12980.53 total tokens/s, 12216.97 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 01:25:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:25:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:25:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:25:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:25:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:25:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:25:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:25:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:25:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:25:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:25:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:25:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:25:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:25:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:25:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:25:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:25:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:25:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:25:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:40] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:40] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3551594) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3551594) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=3551594) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=3551594) 
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3551594) [2026-01-28 01:25:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3551594) 2026-01-28 01:25:49,327 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3551594) 2026-01-28 01:25:49,342 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3551594) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:04,  7.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:04,  7.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:01, 15.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:01, 19.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:01, 21.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:00<00:00, 21.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:00<00:00, 22.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:01<00:00, 22.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:01<00:00, 23.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:01<00:00, 23.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:01<00:00, 24.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:01<00:00, 24.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 21.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 20.97it/s]
(EngineCore_DP0 pid=3551594) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:03,  5.10it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01, 14.36it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 19.63it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:00<00:00, 22.78it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 24.83it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 26.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 26.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 22.73it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4415.09it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<05:24,  2.56s/it, est. speed input: 6.25 toks/s, output: 100.05 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.56s/it, est. speed input: 772.39 toks/s, output: 12358.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.27it/s, est. speed input: 772.39 toks/s, output: 12358.25 toks/s]
[rank0]:[W128 01:25:55.364947435 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.7s

测试结果:
  Requests/s:   47.72
  Tokens/s:     12980.53
  Total Reqs:   128
  Elapsed:      2.68s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      12216.97

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:26:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3552221) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3552221) WARNING 01-28 01:26:09 [backends.py:609] Failed to read file <frozen os>
Throughput: 54.58 requests/s, 14846.25 total tokens/s, 13972.94 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 01:26:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:26:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:26:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:26:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:26:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:26:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:26:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:26:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:26:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:26:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:26:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:26:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:26:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:26:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:26:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:26:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:26:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:26:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3552221) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3552221) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.72it/s]
(EngineCore_DP0 pid=3552221) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.72it/s]
(EngineCore_DP0 pid=3552221) 
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3552221) [2026-01-28 01:26:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3552221) 2026-01-28 01:26:14,088 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3552221) 2026-01-28 01:26:14,103 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3552221) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 25.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 24.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:01, 25.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 25.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 25.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 21.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:01<00:00, 16.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:01<00:00, 15.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:01<00:00, 17.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:01<00:00, 19.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:01<00:00, 20.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:01<00:00, 22.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 20.73it/s]
(EngineCore_DP0 pid=3552221) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.24it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 20.55it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:01, 24.89it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:00<00:00, 26.10it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:00<00:00, 26.56it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:00<00:00, 27.23it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 27.77it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:00<00:00, 28.03it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:01<00:00, 28.01it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:01<00:00, 26.66it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 27.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 26.18it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 6906.51it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:04<17:47,  4.18s/it, est. speed input: 3.82 toks/s, output: 61.18 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:04<00:05, 28.49it/s, est. speed input: 323.84 toks/s, output: 5181.44 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:01, 62.47it/s, est. speed input: 603.04 toks/s, output: 9648.61 toks/s]
Processed prompts:  88%|████████▊ | 225/256 [00:04<00:00, 93.58it/s, est. speed input: 797.42 toks/s, output: 12758.69 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 93.58it/s, est. speed input: 880.53 toks/s, output: 14088.49 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 55.03it/s, est. speed input: 880.53 toks/s, output: 14088.49 toks/s]
[rank0]:[W128 01:26:22.725864388 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 27.3s

测试结果:
  Requests/s:   54.58
  Tokens/s:     14846.25
  Total Reqs:   256
  Elapsed:      4.69s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      13972.94

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:26:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3552863) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3552863) WARNING 01-28 01:26:37 [backends.py:609] Failed to read file <frozen os>
Throughput: 48.97 requests/s, 13319.37 total tokens/s, 12535.87 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 01:26:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:26:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:26:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:26:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:26:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:26:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:26:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:26:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:26:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:26:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:26:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:26:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:26:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:26:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:26:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:26:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:26:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:26:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:26:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3552863) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3552863) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.73it/s]
(EngineCore_DP0 pid=3552863) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.73it/s]
(EngineCore_DP0 pid=3552863) 
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3552863) [2026-01-28 01:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3552863) 2026-01-28 01:26:43,073 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3552863) 2026-01-28 01:26:43,088 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3552863) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:01, 24.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:01, 23.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:01, 23.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:00<00:01, 23.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:00<00:01, 23.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:00<00:01, 24.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:00<00:01, 24.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:01<00:01, 23.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:01<00:01, 23.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:01<00:00, 24.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:01<00:00, 24.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:01<00:00, 24.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:01<00:00, 24.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:01<00:00, 23.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:02<00:00, 16.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:02<00:00, 15.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:02<00:00, 17.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 21.40it/s]
(EngineCore_DP0 pid=3552863) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:06,  7.31it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:02, 18.40it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:01, 23.08it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:00<00:01, 25.38it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:00<00:01, 26.90it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:00<00:01, 27.89it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:00<00:01, 28.28it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:00<00:01, 27.94it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:00<00:00, 28.51it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:01<00:00, 28.74it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:01<00:00, 28.81it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:01<00:00, 28.69it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:01<00:00, 28.70it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:01<00:00, 28.53it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:01<00:00, 28.48it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:01<00:00, 28.60it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:01<00:00, 28.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 27.31it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 9348.63it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:07<1:02:54,  7.39s/it, est. speed input: 2.17 toks/s, output: 34.66 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:07<00:24, 17.21it/s, est. speed input: 194.47 toks/s, output: 3111.57 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:07<00:07, 41.21it/s, est. speed input: 388.51 toks/s, output: 6216.09 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:07<00:03, 65.67it/s, est. speed input: 533.74 toks/s, output: 8539.86 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:07<00:01, 95.99it/s, est. speed input: 667.70 toks/s, output: 10683.13 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:08<00:01, 111.13it/s, est. speed input: 762.56 toks/s, output: 12200.96 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [00:08<00:00, 93.02it/s, est. speed input: 783.10 toks/s, output: 12529.60 toks/s] 
Processed prompts:  93%|█████████▎| 475/512 [00:09<00:00, 95.46it/s, est. speed input: 815.45 toks/s, output: 13047.24 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:10<00:00, 62.56it/s, est. speed input: 772.32 toks/s, output: 12357.08 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 62.56it/s, est. speed input: 787.69 toks/s, output: 12603.07 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 49.23it/s, est. speed input: 787.69 toks/s, output: 12603.07 toks/s]
[rank0]:[W128 01:26:59.857944228 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.1s

测试结果:
  Requests/s:   48.97
  Tokens/s:     13319.37
  Total Reqs:   512
  Elapsed:      10.46s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      12535.87


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,29.7410,8089.5497,2.1519
128,16,128,128,256,256,47.7226,12980.5346,2.6822
256,16,256,256,256,256,54.5818,14846.2533,4.6902
512,16,512,512,256,256,48.9683,13319.3661,10.4558

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:27:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3553629) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3553629) WARNING 01-28 01:27:13 [backends.py:609] Failed to read file <frozen os>
Throughput: 28.55 requests/s, 7764.61 total tokens/s, 7307.87 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 01:27:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:27:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:27:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:27:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:27:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:27:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:27:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:27:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:27:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:27:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:27:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:27:07] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:27:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:07] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:07] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:27:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:27:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:27:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:27:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:27:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:08] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:08] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:08] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:08] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:08] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3553629) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3553629) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=3553629) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=3553629) 
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3553629) [2026-01-28 01:27:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3553629) 2026-01-28 01:27:19,453 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3553629) 2026-01-28 01:27:19,468 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3553629) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  4.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:03,  4.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01, 10.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 14.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 17.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 19.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00, 21.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 16.30it/s]
(EngineCore_DP0 pid=3553629) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  6.74it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 12.39it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00, 15.31it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 16.73it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 16.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 15.74it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2933.27it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:17,  2.18s/it, est. speed input: 7.34 toks/s, output: 117.38 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.18s/it, est. speed input: 461.60 toks/s, output: 7385.53 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 28.85it/s, est. speed input: 461.60 toks/s, output: 7385.53 toks/s]
[rank0]:[W128 01:27:24.470774205 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.6s

测试结果:
  Requests/s:   28.55
  Tokens/s:     7764.61
  Total Reqs:   64
  Elapsed:      2.24s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      7307.87

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:27:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3554307) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3554307) WARNING 01-28 01:27:38 [backends.py:609] Failed to read file <frozen os>
Throughput: 46.92 requests/s, 12761.05 total tokens/s, 12010.40 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 01:27:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:27:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:27:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:27:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:27:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:27:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:27:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:27:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:27:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:27:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:27:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:27:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:27:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:27:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:27:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:27:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:27:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:27:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3554307) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3554307) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=3554307) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=3554307) 
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3554307) [2026-01-28 01:27:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3554307) 2026-01-28 01:27:42,962 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3554307) 2026-01-28 01:27:42,978 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3554307) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:04,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:06,  5.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:02, 12.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:01, 16.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:01, 19.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:00<00:00, 21.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:00<00:00, 22.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:01<00:00, 22.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:01<00:00, 23.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:01<00:00, 23.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:01<00:00, 23.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:01<00:00, 23.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 23.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 20.50it/s]
(EngineCore_DP0 pid=3554307) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.84it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01, 12.98it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01, 13.67it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 14.70it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 14.45it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:00<00:00, 18.54it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:00<00:00, 21.56it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:00<00:00, 22.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:01<00:00, 18.75it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4487.01it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<05:30,  2.60s/it, est. speed input: 6.15 toks/s, output: 98.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.60s/it, est. speed input: 759.06 toks/s, output: 12144.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 47.44it/s, est. speed input: 759.06 toks/s, output: 12144.93 toks/s]
[rank0]:[W128 01:27:49.263104504 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.8s

测试结果:
  Requests/s:   46.92
  Tokens/s:     12761.05
  Total Reqs:   128
  Elapsed:      2.73s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      12010.40

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:27:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3554942) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3554942) WARNING 01-28 01:28:03 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.28 requests/s, 14491.97 total tokens/s, 13639.50 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 01:27:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:27:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:27:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:27:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:27:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:27:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:27:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:27:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:27:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:27:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:27:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:27:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:27:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:27:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:27:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:27:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:27:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:27:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:27:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3554942) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3554942) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=3554942) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=3554942) 
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3554942) [2026-01-28 01:27:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3554942) 2026-01-28 01:28:08,133 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3554942) 2026-01-28 01:28:08,148 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3554942) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 24.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 21.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:01, 23.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:01, 23.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 24.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 21.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:01<00:01, 14.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:01<00:00, 14.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:01<00:00, 17.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:01<00:00, 18.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:01<00:00, 20.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:01<00:00, 21.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 19.99it/s]
(EngineCore_DP0 pid=3554942) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.30it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:01, 18.59it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:01, 22.80it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:01, 24.90it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 24.95it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 26.23it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:00<00:00, 26.78it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:00<00:00, 27.10it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 27.37it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:01<00:00, 27.41it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:01<00:00, 27.63it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:01<00:00, 27.78it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 25.79it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 6888.75it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:04<18:13,  4.29s/it, est. speed input: 3.73 toks/s, output: 59.71 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:04<00:07, 24.08it/s, est. speed input: 273.44 toks/s, output: 4375.07 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:04<00:01, 59.75it/s, est. speed input: 565.58 toks/s, output: 9049.21 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:04<00:00, 90.88it/s, est. speed input: 758.22 toks/s, output: 12131.58 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 90.88it/s, est. speed input: 859.50 toks/s, output: 13751.93 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 53.72it/s, est. speed input: 859.50 toks/s, output: 13751.93 toks/s]
[rank0]:[W128 01:28:17.956517394 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 27.7s

测试结果:
  Requests/s:   53.28
  Tokens/s:     14491.97
  Total Reqs:   256
  Elapsed:      4.80s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      13639.50

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 01:28:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3555590) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3555590) WARNING 01-28 01:28:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 47.33 requests/s, 12872.87 total tokens/s, 12115.64 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 01:28:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:28:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:28:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:28:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:28:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:28:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:28:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:28:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:28:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:28:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:28:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:28:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:28:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:28:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 01:28:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 01:28:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 01:28:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 01:28:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:28:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:28:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:28:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:28:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 01:28:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 01:28:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 01:28:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 01:28:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 01:28:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 01:28:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3555590) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3555590) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.48it/s]
(EngineCore_DP0 pid=3555590) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.48it/s]
(EngineCore_DP0 pid=3555590) 
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3555590) [2026-01-28 01:28:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3555590) 2026-01-28 01:28:37,444 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3555590) 2026-01-28 01:28:37,459 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3555590) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:02, 23.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:01, 23.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:01, 23.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:00<00:01, 23.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:00<00:01, 23.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:00<00:01, 23.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:00<00:01, 23.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:01<00:01, 23.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:01<00:01, 23.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:01<00:00, 24.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:01<00:00, 21.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:01<00:00, 16.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:01<00:00, 15.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:01<00:00, 17.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:02<00:00, 18.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:02<00:00, 20.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:02<00:00, 21.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 21.09it/s]
(EngineCore_DP0 pid=3555590) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:07,  7.12it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:02, 17.85it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:01, 22.33it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:00<00:01, 24.77it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:00<00:01, 26.47it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:00<00:01, 27.34it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:00<00:01, 27.18it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:00<00:00, 28.20it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:01<00:00, 28.54it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:01<00:00, 28.90it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:01<00:00, 29.06it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:01<00:00, 28.92it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:01<00:00, 28.69it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:01<00:00, 28.58it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:01<00:00, 28.41it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:01<00:00, 26.82it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:01<00:00, 27.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 26.78it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 8435.80it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:07<1:04:06,  7.53s/it, est. speed input: 2.13 toks/s, output: 34.00 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:07<00:24, 16.89it/s, est. speed input: 190.86 toks/s, output: 3053.81 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:07<00:09, 35.20it/s, est. speed input: 338.69 toks/s, output: 5419.08 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:07<00:04, 61.12it/s, est. speed input: 491.86 toks/s, output: 7869.68 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:07<00:02, 92.63it/s, est. speed input: 631.25 toks/s, output: 10100.04 toks/s]
Processed prompts:  74%|███████▍  | 379/512 [00:08<00:01, 121.60it/s, est. speed input: 743.11 toks/s, output: 11889.68 toks/s]
Processed prompts:  84%|████████▍ | 432/512 [00:09<00:00, 87.26it/s, est. speed input: 749.58 toks/s, output: 11993.34 toks/s] 
Processed prompts:  92%|█████████▏| 470/512 [00:09<00:00, 90.03it/s, est. speed input: 783.70 toks/s, output: 12539.20 toks/s]
Processed prompts:  97%|█████████▋| 499/512 [00:10<00:00, 60.09it/s, est. speed input: 742.76 toks/s, output: 11884.14 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 60.09it/s, est. speed input: 761.63 toks/s, output: 12186.01 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 47.60it/s, est. speed input: 761.63 toks/s, output: 12186.01 toks/s]
[rank0]:[W128 01:28:53.546280651 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.6s

测试结果:
  Requests/s:   47.33
  Tokens/s:     12872.87
  Total Reqs:   512
  Elapsed:      10.82s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      12115.64


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,28.5464,7764.6138,2.2420
128,16,128,128,256,256,46.9156,12761.0513,2.7283
256,16,256,256,256,256,53.2793,14491.9698,4.8049
512,16,512,512,256,256,47.3267,12872.8652,10.8184

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 20 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_011931.log
[SUCCESS] bitnet1.58-2b-fp8 Decode 完成 (566.2s)

[INFO] Decode 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 5: 完整 Decode Benchmark - SUCCESS
Duration: 1126.1 seconds (18.8 minutes)
----------------------------------------------------------------------


======================================================================
TASK 6: Kernel: cuBLASLt
Started: 2026-01-28 01:28:55
======================================================================


------------------------------------------------------------
  cuBLASLt Kernel: BitNet-2B
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cublaslt --model BitNet-2B

============================================================
cuBLASLt Dense GEMM 算法搜索
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuBLASLt 可用

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 8, 有效: 8
    NK 2/4: (2560, 2560)
      → 算法数: 8, 有效: 8
    NK 3/4: (13824, 2560)
      → 算法数: 8, 有效: 8
    NK 4/4: (2560, 6912)
      → 算法数: 8, 有效: 8

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
============================================================
cuBLASLt Dense GEMM 算法搜索
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuBLASLt 可用

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 3, 有效: 3
    NK 2/4: (2560, 2560)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 2560)
      → 算法数: 1, 有效: 1
    NK 4/4: (2560, 6912)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
============================================================
cuBLASLt Dense GEMM 算法搜索
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuBLASLt 可用

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 3, 有效: 3
    NK 2/4: (2560, 2560)
      → 算法数: 3, 有效: 3
    NK 3/4: (13824, 2560)
      → 算法数: 3, 有效: 3
    NK 4/4: (2560, 6912)
      → 算法数: 3, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
============================================================
cuBLASLt Dense GEMM 算法搜索
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuBLASLt 可用

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 8, 有效: 8
    NK 2/4: (2560, 2560)
      → 算法数: 8, 有效: 8
    NK 3/4: (13824, 2560)
      → 算法数: 8, 有效: 8
    NK 4/4: (2560, 6912)
      → 算法数: 8, 有效: 8

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
============================================================
cuBLASLt Dense GEMM 算法搜索
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuBLASLt 可用

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 6, 有效: 6
Traceback (most recent call last):
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 641, in <module>
    main()
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 616, in main
    ret = run_search(
          ^^^^^^^^^^^
  File "/root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py", line 407, in run_search
    torch.cuda.empty_cache()
  File "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py", line 224, in empty_cache
    torch._C._cuda_emptyCache()
torch.AcceleratorError: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[INFO] dtype=all, 将测试: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp4e2m1 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384
[ERROR] cuBLASLt search failed with code 1

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[SUCCESS] cuBLASLt Kernel 测试完成 (75.9s)

----------------------------------------------------------------------
TASK 6: Kernel: cuBLASLt - SUCCESS
Duration: 75.9 seconds (1.3 minutes)
----------------------------------------------------------------------


======================================================================
TASK 7: Kernel: cuSPARSELt 高稀疏 (2_4~2_10)
Started: 2026-01-28 01:30:11
======================================================================


------------------------------------------------------------
  cuSPARSELt 高稀疏 Kernel: BitNet-2B
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cusparselt --model BitNet-2B --sparsity 2_4,2_6,2_8,2_10

[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3840, 2560), (2560, 2560), (13824, 2560), (2560, 6912)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_4
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 2560)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 2560)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 6912)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_4.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3840, 3424), (2560, 3424), (13824, 3424), (2560, 9216)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_6
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3424)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 3424)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 3424)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 9216)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_6.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3840, 3840), (2560, 3840), (13824, 3840), (2560, 10368)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_8
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3840)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 3840)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 3840)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 10368)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3840, 4096), (2560, 4096), (13824, 4096), (2560, 11072)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_10
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4096)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 4096)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 4096)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 11072)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_10.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3840, 2560), (2560, 2560), (13824, 2560), (2560, 6912)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_4
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 2560)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 2560)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 6912)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_4.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3840, 3424), (2560, 3424), (13824, 3424), (2560, 9216)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_6
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3424)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 3424)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 3424)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 9216)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_6.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3840, 3840), (2560, 3840), (13824, 3840), (2560, 10368)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_8
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3840)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 3840)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 3840)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 10368)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3840, 4096), (2560, 4096), (13824, 4096), (2560, 11072)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_10
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4096)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 4096)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 4096)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 11072)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_10.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3840, 2560), (2560, 2560), (13824, 2560), (2560, 6912)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_4
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 2560)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 2560)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 6912)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_4.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3840, 3424), (2560, 3424), (13824, 3424), (2560, 9216)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_6
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3424)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 3424)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 3424)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 9216)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_6.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3840, 3840), (2560, 3840), (13824, 3840), (2560, 10368)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_8
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3840)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 3840)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 3840)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 10368)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3840, 4096), (2560, 4096), (13824, 4096), (2560, 11072)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_10
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4096)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 4096)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 4096)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 11072)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_10.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3840, 2560), (2560, 2560), (13824, 2560), (2560, 6912)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_4
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 4, 有效: 15
    NK 2/4: (2560, 2560)
      → 算法数: 4, 有效: 18
    NK 3/4: (13824, 2560)
      → 算法数: 4, 有效: 14
    NK 4/4: (2560, 6912)
      → 算法数: 4, 有效: 16

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_4.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3840, 3424), (2560, 3424), (13824, 3424), (2560, 9216)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_6
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3424)
      → 算法数: 4, 有效: 16
    NK 2/4: (2560, 3424)
      → 算法数: 4, 有效: 14
    NK 3/4: (13824, 3424)
      → 算法数: 4, 有效: 17
    NK 4/4: (2560, 9216)
      → 算法数: 4, 有效: 17

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_6.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3840, 3840), (2560, 3840), (13824, 3840), (2560, 10368)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_8
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3840)
      → 算法数: 4, 有效: 16
    NK 2/4: (2560, 3840)
      → 算法数: 4, 有效: 16
    NK 3/4: (13824, 3840)
      → 算法数: 4, 有效: 15
    NK 4/4: (2560, 10368)
      → 算法数: 4, 有效: 17

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3840, 4096), (2560, 4096), (13824, 4096), (2560, 11072)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_10
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4096)
      → 算法数: 4, 有效: 14
    NK 2/4: (2560, 4096)
      → 算法数: 4, 有效: 19
    NK 3/4: (13824, 4096)
      → 算法数: 4, 有效: 13
    NK 4/4: (2560, 11072)
      → 算法数: 4, 有效: 21

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_10.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3840, 2560), (2560, 2560), (13824, 2560), (2560, 6912)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_4
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 2, 有效: 8
    NK 2/4: (2560, 2560)
      → 算法数: 2, 有效: 6
    NK 3/4: (13824, 2560)
      → 算法数: 2, 有效: 8
    NK 4/4: (2560, 6912)
      → 算法数: 2, 有效: 8

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_4.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[INFO] dtype=all, 将测试: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_4

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_6

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_8

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_10

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_4

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_6

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_8

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_10

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_4

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_6

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_8

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_10

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_4

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_6

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_8

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_10

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_4

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3840, 3456), (2560, 3456), (13824, 3456), (2560, 9216)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_6
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3456)
      → 算法数: 2, 有效: 8
    NK 2/4: (2560, 3456)
      → 算法数: 2, 有效: 7
    NK 3/4: (13824, 3456)
      → 算法数: 2, 有效: 9
    NK 4/4: (2560, 9216)
      → 算法数: 2, 有效: 9

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_6.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3840, 3840), (2560, 3840), (13824, 3840), (2560, 10368)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_8
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3840)
      → 算法数: 2, 有效: 10
    NK 2/4: (2560, 3840)
      → 算法数: 2, 有效: 6
    NK 3/4: (13824, 3840)
      → 算法数: 2, 有效: 6
    NK 4/4: (2560, 10368)
      → 算法数: 2, 有效: 9

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3840, 4096), (2560, 4096), (13824, 4096), (2560, 11072)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_10
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4096)
      → 算法数: 2, 有效: 10
    NK 2/4: (2560, 4096)
      → 算法数: 2, 有效: 6
    NK 3/4: (13824, 4096)
      → 算法数: 2, 有效: 6
    NK 4/4: (2560, 11072)
      → 算法数: 2, 有效: 10

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_10.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================

[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_6

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_8

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[SUCCESS] cuSPARSELt 高稀疏 Kernel 测试完成 (243.7s)

----------------------------------------------------------------------
TASK 7: Kernel: cuSPARSELt 高稀疏 (2_4~2_10) - SUCCESS
Duration: 243.7 seconds (4.1 minutes)
----------------------------------------------------------------------


======================================================================
TASK 8: Kernel: cuSPARSELt 低稀疏 (2_12~2_inf)
Started: 2026-01-28 01:34:15
======================================================================


------------------------------------------------------------
  cuSPARSELt 低稀疏 Kernel: BitNet-2B
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype all --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cusparselt --model BitNet-2B --sparsity 2_12,2_14,2_16,2_inf

[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3840, 4288), (2560, 4288), (13824, 4288), (2560, 11520)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_12
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4288)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 4288)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 4288)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 11520)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_12.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3840, 4416), (2560, 4416), (13824, 4416), (2560, 11872)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_14
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4416)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 4416)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 4416)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 11872)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_14.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3840, 4480), (2560, 4480), (13824, 4480), (2560, 12096)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_16
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4480)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 4480)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 4480)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 12096)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_16.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3840, 5120), (2560, 5120), (13824, 5120), (2560, 13824)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp16 -> fp16 (same input/output)
Sparsity: 2_inf
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 5120)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 5120)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 5120)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 13824)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16/alg_search_BitNet-2B-INT8_2_inf.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP16
============================================================
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3840, 4288), (2560, 4288), (13824, 4288), (2560, 11520)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_12
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4288)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 4288)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 4288)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 11520)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_12.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3840, 4416), (2560, 4416), (13824, 4416), (2560, 11872)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_14
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4416)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 4416)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 4416)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 11872)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_14.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3840, 4480), (2560, 4480), (13824, 4480), (2560, 12096)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_16
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4480)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 4480)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 4480)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 12096)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_16.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3840, 5120), (2560, 5120), (13824, 5120), (2560, 13824)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: bf16 -> bf16 (same input/output)
Sparsity: 2_inf
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 5120)
      → 算法数: 2, 有效: 3
    NK 2/4: (2560, 5120)
      → 算法数: 2, 有效: 3
    NK 3/4: (13824, 5120)
      → 算法数: 2, 有效: 3
    NK 4/4: (2560, 13824)
      → 算法数: 2, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16/alg_search_BitNet-2B-INT8_2_inf.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/BF16
============================================================
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3840, 4288), (2560, 4288), (13824, 4288), (2560, 11520)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_12
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4288)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 4288)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 4288)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 11520)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_12.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3840, 4416), (2560, 4416), (13824, 4416), (2560, 11872)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_14
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4416)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 4416)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 4416)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 11872)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_14.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3840, 4480), (2560, 4480), (13824, 4480), (2560, 12096)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_16
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4480)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 4480)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 4480)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 12096)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_16.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3840, 5120), (2560, 5120), (13824, 5120), (2560, 13824)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_inf
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 5120)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 5120)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 5120)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 13824)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_inf.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3840, 4288), (2560, 4288), (13824, 4288), (2560, 11520)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_12
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4288)
      → 算法数: 4, 有效: 17
    NK 2/4: (2560, 4288)
      → 算法数: 4, 有效: 18
    NK 3/4: (13824, 4288)
      → 算法数: 4, 有效: 16
    NK 4/4: (2560, 11520)
      → 算法数: 4, 有效: 17

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_12.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3840, 4416), (2560, 4416), (13824, 4416), (2560, 11872)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_14
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4416)
      → 算法数: 4, 有效: 14
    NK 2/4: (2560, 4416)
      → 算法数: 4, 有效: 18
    NK 3/4: (13824, 4416)
      → 算法数: 4, 有效: 19
    NK 4/4: (2560, 11872)
      → 算法数: 4, 有效: 17

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_14.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3840, 4480), (2560, 4480), (13824, 4480), (2560, 12096)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_16
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4480)
      → 算法数: 4, 有效: 14
    NK 2/4: (2560, 4480)
      → 算法数: 4, 有效: 13
    NK 3/4: (13824, 4480)
      → 算法数: 4, 有效: 16
    NK 4/4: (2560, 12096)
      → 算法数: 4, 有效: 18

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_16.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3840, 5120), (2560, 5120), (13824, 5120), (2560, 13824)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_inf
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 5120)
      → 算法数: 4, 有效: 14
    NK 2/4: (2560, 5120)
      → 算法数: 4, 有效: 14
    NK 3/4: (13824, 5120)
      → 算法数: 4, 有效: 15
    NK 4/4: (2560, 13824)
      → 算法数: 4, 有效: 16

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_inf.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP8
============================================================
[INFO] dtype=all, 将测试: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['fp16', 'bf16', 'int8', 'fp8e4m3', 'fp4e2m1']
Backend: cusparselt
Sparsity: ['2_12', '2_14', '2_16', '2_inf']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP16
============================================================

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_12

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_14

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_16

[fp16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_inf

============================================================
Benchmark dtype=BF16
============================================================

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_12

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_14

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_16

[bf16] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_inf

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_12

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_14

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_16

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_inf

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_12

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_14

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_16

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_inf

============================================================
Benchmark dtype=FP4E2M1
============================================================

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_12[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3840, 4288), (2560, 4288), (13824, 4288), (2560, 11520)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_12
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4288)
      → 算法数: 2, 有效: 7
    NK 2/4: (2560, 4288)
      → 算法数: 2, 有效: 9
    NK 3/4: (13824, 4288)
      → 算法数: 2, 有效: 7
    NK 4/4: (2560, 11520)
      → 算法数: 2, 有效: 7

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_12.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3840, 4416), (2560, 4416), (13824, 4416), (2560, 11904)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_14
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4416)
      → 算法数: 2, 有效: 6
    NK 2/4: (2560, 4416)
      → 算法数: 2, 有效: 9
    NK 3/4: (13824, 4416)
      → 算法数: 2, 有效: 9
    NK 4/4: (2560, 11904)
      → 算法数: 2, 有效: 9

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_14.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3840, 4480), (2560, 4480), (13824, 4480), (2560, 12096)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_16
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4480)
      → 算法数: 2, 有效: 7
    NK 2/4: (2560, 4480)
      → 算法数: 2, 有效: 9
    NK 3/4: (13824, 4480)
      → 算法数: 2, 有效: 8
    NK 4/4: (2560, 12096)
      → 算法数: 2, 有效: 9

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_16.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3840, 5120), (2560, 5120), (13824, 5120), (2560, 13824)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 5080 (cc120)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp4e2m1 -> fp4e2m1 (same input/output)
Sparsity: 2_inf
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 5120)
      → 算法数: 2, 有效: 8
    NK 2/4: (2560, 5120)
      → 算法数: 2, 有效: 6
    NK 3/4: (13824, 5120)
      → 算法数: 2, 有效: 8
    NK 4/4: (2560, 13824)
      → 算法数: 2, 有效: 7

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4/alg_search_BitNet-2B-INT8_2_inf.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX5080_cc120_py312_cu129_x86_64/FP4
============================================================


[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_14

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_16

[fp4e2m1] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp4e2m1 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_inf

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[SUCCESS] cuSPARSELt 低稀疏 Kernel 测试完成 (294.6s)

----------------------------------------------------------------------
TASK 8: Kernel: cuSPARSELt 低稀疏 (2_12~2_inf) - SUCCESS
Duration: 294.6 seconds (4.9 minutes)
----------------------------------------------------------------------



============================================================
  最终总结
============================================================


  Task 1: 基础模型准备 (下载 + 量化) - SUCCESS (74.3s)
  Task 2: SlideSparse 转换 (prune + slide) - SUCCESS (413.3s)
  Task 3: 离线调优 (粗调优 + 细调优) - SUCCESS (583.8s)
  Task 4: 完整 Prefill Benchmark - SUCCESS (3732.6s)
  Task 5: 完整 Decode Benchmark - SUCCESS (1126.1s)
  Task 6: Kernel: cuBLASLt - SUCCESS (75.9s)
  Task 7: Kernel: cuSPARSELt 高稀疏 (2_4~2_10) - SUCCESS (243.7s)
  Task 8: Kernel: cuSPARSELt 低稀疏 (2_12~2_inf) - SUCCESS (294.6s)

  总计: 8 成功, 0 失败, 0 跳过
  总耗时: 6544.3 秒 (1.82 小时)

[INFO] 日志文件: /root/vllmbench/slidesparse/tools/bitnet_bench_20260127_235005.log
[INFO] 状态文件: /root/vllmbench/slidesparse/tools/bitnet_bench_20260127_235005_status.json
