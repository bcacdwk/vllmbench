======================================================================
BitNet Benchmark Log
Started: 2026-01-28 07:04:55
======================================================================

Hardware:
  GPU: NVIDIA GB10 (cc121)
  Python: py312
  CUDA: cu129
  Arch: aarch64

[INFO] 日志文件: /root/vllmbench/slidesparse/tools/bitnet_bench_20260128_070455.log

======================================================================
TASK 1: 基础模型准备 (下载 + 量化)
Started: 2026-01-28 07:04:55
======================================================================


------------------------------------------------------------
  Step 1: 下载 BitNet BF16 模型
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/model_download.py --model bitnet1.58-2b-bf16

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(

Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]Still waiting to acquire lock on /root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)
Downloading '.gitattributes' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'
Downloading 'model.safetensors' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/xGOKKLRSlIhH692hSVvI1-gpoa8=.529637ff6dab1f5890767356928693f69ffe61d3b6040a43de9306b37bfd5ae1.incomplete'
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/.gitattributes

Fetching 10 files:  10%|█         | 1/10 [00:00<00:04,  1.83it/s]Downloading 'special_tokens_map.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.d8cd5076496dbe4be2320312abc10adc43097b81.incomplete'
Downloading 'LICENSE' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/DhCjcNQuMpl4FL346qr3tvNUCgY=.48ea6616b5b8581df3401872996cecf1f8b08a0d.incomplete'
Downloading 'config.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.8cb95b54570aababf062b4b8d95e78dc74a7ba51.incomplete'
Downloading 'README.md' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.cfe176334831acea2ecdaef02b5ae25416b14941.incomplete'
Downloading 'data_summary_card.md' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/rO2cXQjMJqsORRCCpgXS1A8CgMk=.156c5705d7cf1e2f11a27e62f673c4576af7aa19.incomplete'
Downloading 'generation_config.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.650ab2390d65b8182f2599fe7a0f2014eec3f38b.incomplete'
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/special_tokens_map.json
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/LICENSE

Fetching 10 files:  20%|██        | 2/10 [00:00<00:02,  2.76it/s]Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/data_summary_card.md
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/config.json
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/README.md
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/generation_config.json

Fetching 10 files:  60%|██████    | 6/10 [00:00<00:00,  8.83it/s]Downloading 'tokenizer.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.b197f72effb9d5ed16ee0f5663e11e4cfac2ba62.incomplete'
Downloading 'tokenizer_config.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.d54415c89774937967b9baac420d12455ff6e267.incomplete'
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/tokenizer_config.json
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/tokenizer.json

Fetching 10 files:  60%|██████    | 6/10 [00:20<00:00,  8.83it/s]Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/model.safetensors

Fetching 10 files:  70%|███████   | 7/10 [02:07<01:22, 27.48s/it]
Fetching 10 files: 100%|██████████| 10/10 [02:07<00:00, 12.79s/it]
/root/vllmbench/checkpoints/BitNet-2B-BF16

============================================================
  准备下载 1 个模型 (~4.8 GB)
============================================================

  - BitNet-2B-BF16 (4.8 GB)


============================================================
  下载: BitNet-2B-BF16
============================================================

[INFO] HuggingFace: microsoft/bitnet-b1.58-2B-4T-bf16
[INFO] 本地目录: /root/vllmbench/checkpoints/BitNet-2B-BF16

[INFO] 下载命令: hf download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir /root/vllmbench/checkpoints/BitNet-2B-BF16
[SUCCESS] 下载成功: /root/vllmbench/checkpoints/BitNet-2B-BF16


============================================================
  下载完成
============================================================

成功: 1/1

============================================================
  模型下载状态
============================================================


INT8 模型:
----------------------------------------
  ✓ Qwen2.5-0.5B-INT8 - 0.9 GB
  ✓ Llama3.2-1B-INT8 - 1.9 GB
  ✓ Qwen2.5-1.5B-INT8 - 2.1 GB
  ✗ BitNet-2B-INT8 - not downloaded
  ✓ Qwen2.5-3B-INT8 - 3.8 GB
  ✓ Llama3.2-3B-INT8 - 4.1 GB
  ✓ Qwen2.5-7B-INT8 - 8.1 GB
  ✓ Qwen2.5-14B-INT8 - 15.2 GB

FP8 模型:
----------------------------------------
  ✓ Qwen2.5-0.5B-FP8 - 0.9 GB
  ✓ Llama3.2-1B-FP8 - 1.9 GB
  ✓ Qwen2.5-1.5B-FP8 - 2.1 GB
  ✗ BitNet-2B-FP8 - not downloaded
  ✓ Qwen2.5-3B-FP8 - 3.8 GB
  ✓ Llama3.2-3B-FP8 - 4.1 GB
  ✓ Qwen2.5-7B-FP8 - 8.1 GB
  ✓ Qwen2.5-14B-FP8 - 15.2 GB

BF16 模型:
----------------------------------------
  ✓ BitNet-2B-BF16 - 4.5 GB

----------------------------------------
总计: 15 已下载, 2 缺失
[INFO] Checkpoints 目录大小: 77G
[SUCCESS] 下载完成 (131.6s)

------------------------------------------------------------
  Step 2: 量化为 BitNet-2B-INT8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-BF16 --bitnet --output-dtype int8 --Z 2 --L 2 --skip-slide
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
/root/vllmbench/slidesparse/utils.py:2928: UserWarning: L=2 < 4，这是纯量化模式（无稀疏），slide 操作将被跳过
  warnings.warn(
======================================================================
Processing: BitNet-2B-BF16
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=2, N=1, expand=0.000)
[INFO] Mode: magnitude
[INFO] BitNet Mode: enabled (output_dtype=int8)
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2

[INFO] Copying non-weight files...
[INFO]   Copied: README.md, .gitattributes, data_summary_card.md, tokenizer_config.json, special_tokens_map.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.20%)
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.11%)
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.86%)
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.70%)
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.95%)
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (30.11%)
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.73%)
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (34.84%)
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (48.35%)
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (48.39%)
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.94%)
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.03%)
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.16%)
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.12%)
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.94%)
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.50%)
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.07%)
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (30.23%)
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.13%)
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.83%)
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.66%)
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.92%)
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.62%)
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.08%)
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (24.26%)
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.88%)
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.42%)
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.01%)
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.84%)
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.71%)
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.80%)
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (28.71%)
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.19%)
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.88%)
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.76%)
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.23%)
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.39%)
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.59%)
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.83%)
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.56%)
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.50%)
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.27%)
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.43%)
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.34%)
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.94%)
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (32.82%)
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.05%)
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.77%)
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (11.51%)
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.69%)
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.65%)
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.42%)
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.66%)
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.11%)
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (9.15%)
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.99%)
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.84%)
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.18%)
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.87%)
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.35%)
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.09%)
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (10.66%)
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.45%)
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.72%)
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.49%)
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (31.51%)
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (11.87%)
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (24.63%)
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.37%)
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.68%)
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.87%)
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.57%)
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.39%)
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.39%)
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.62%)
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (9.69%)
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.53%)
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.64%)
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.42%)
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.76%)
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.46%)
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.77%)
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.25%)
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.50%)
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (35.59%)
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (35.75%)
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.00%)
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.57%)
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.80%)
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.10%)
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.53%)
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.29%)
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.71%)
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (31.97%)
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.26%)
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.80%)
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.48%)
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.43%)
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.31%)
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.73%)
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.93%)
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.33%)
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.75%)
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.57%)
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.56%)
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.30%)
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.90%)
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.96%)
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.52%)
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.68%)
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (10.90%)
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.93%)
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.02%)
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.00%)
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (26.12%)
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.42%)
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.67%)
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.05%)
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.28%)
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.27%)
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.51%)
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.09%)
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.74%)
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (26.75%)
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.67%)
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.08%)
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.17%)
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.72%)
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.73%)
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.89%)
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.86%)
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.68%)
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (13.43%)
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.59%)
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.71%)
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.87%)
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.93%)
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.90%)
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (13.77%)
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (13.64%)
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.03%)
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.46%)
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.20%)
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.79%)
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (11.23%)
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.35%)
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.77%)
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.66%)
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.38%)
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.80%)
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.11%)
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.23%)
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.79%)
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.63%)
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.30%)
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.55%)
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (31.95%)
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.35%)
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (24.72%)
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.33%)
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.01%)
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.91%)
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.39%)
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.90%)
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.76%)
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.22%)
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.48%)
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.07%)
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.66%)
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.86%)
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.77%)
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.19%)
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.28%)
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.58%)
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.12%)
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.42%)
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.40%)
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.35%)
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.08%)
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.80%)
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.22%)
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.38%)
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.52%)
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.81%)
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.55%)
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.32%)
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.18%)
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.32%)
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.95%)
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.81%)
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.26%)
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.76%)
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.84%)
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (13.07%)
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.28%)
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.65%)
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.41%)
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.82%)
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.87%)
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.70%)
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.07%)
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.12%)
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.52%)
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.18%)
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.47%)
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.99%)
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.45%)

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 122 layers
[INFO] Time: 13.13s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2/conversion_report.json
[SUCCESS] BitNet-2B-INT8 量化完成并移动到 /root/vllmbench/checkpoints/BitNet-2B-INT8 (16.3s)
[SUCCESS]   ✓ 已修正 BitNet-2B-INT8/config.json (vLLM 兼容)
[SUCCESS]   ✓ 已删除 60 个不兼容权重 (ffn_sub_norm, attn_sub_norm)

------------------------------------------------------------
  Step 2: 量化为 BitNet-2B-FP8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-BF16 --bitnet --output-dtype fp8_e4m3 --Z 2 --L 2 --skip-slide
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
/root/vllmbench/slidesparse/utils.py:2928: UserWarning: L=2 < 4，这是纯量化模式（无稀疏），slide 操作将被跳过
  warnings.warn(
======================================================================
Processing: BitNet-2B-BF16
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=2, N=1, expand=0.000)
[INFO] Mode: magnitude
[INFO] BitNet Mode: enabled (output_dtype=fp8_e4m3)
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2

[INFO] Copying non-weight files...
[INFO]   Copied: README.md, .gitattributes, data_summary_card.md, tokenizer_config.json, special_tokens_map.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.20%)
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.11%)
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.86%)
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.70%)
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.95%)
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (30.11%)
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.73%)
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (34.84%)
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (48.35%)
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (48.39%)
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.94%)
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.03%)
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.16%)
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.12%)
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.94%)
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.50%)
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.07%)
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (30.23%)
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.13%)
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.83%)
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.66%)
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.92%)
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.62%)
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.08%)
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (24.26%)
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.88%)
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.42%)
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.01%)
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.84%)
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.71%)
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.80%)
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (28.71%)
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.19%)
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.88%)
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.76%)
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.23%)
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.39%)
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.59%)
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.83%)
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.56%)
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.50%)
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.27%)
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.43%)
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.34%)
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.94%)
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (32.82%)
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.05%)
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.77%)
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (11.51%)
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.69%)
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.65%)
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.42%)
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.66%)
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.11%)
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (9.15%)
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.99%)
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.84%)
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.18%)
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.87%)
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.35%)
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.09%)
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (10.66%)
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.45%)
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.72%)
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.49%)
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (31.51%)
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (11.87%)
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (24.63%)
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.37%)
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.68%)
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.87%)
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.57%)
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.39%)
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.39%)
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.62%)
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (9.69%)
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.53%)
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.64%)
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.42%)
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.76%)
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.46%)
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.77%)
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.25%)
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.50%)
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (35.59%)
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (35.75%)
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.00%)
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.57%)
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.80%)
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.10%)
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.53%)
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.29%)
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.71%)
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (31.97%)
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.26%)
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.80%)
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.48%)
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.43%)
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.31%)
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.73%)
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.93%)
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.33%)
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.75%)
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.57%)
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.56%)
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.30%)
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.90%)
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.96%)
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.52%)
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.68%)
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (10.90%)
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.93%)
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.02%)
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.00%)
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (26.12%)
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.42%)
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.67%)
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.05%)
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.28%)
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.27%)
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.51%)
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.09%)
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.74%)
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (26.75%)
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.67%)
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.08%)
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.17%)
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.72%)
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.73%)
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.89%)
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.86%)
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.68%)
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (13.43%)
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.59%)
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.71%)
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.87%)
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.93%)
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.90%)
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (13.77%)
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (13.64%)
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.03%)
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.46%)
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.20%)
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.79%)
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (11.23%)
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.35%)
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.77%)
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.66%)
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.38%)
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.80%)
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.11%)
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.23%)
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.79%)
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.63%)
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.30%)
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.55%)
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (31.95%)
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.35%)
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (24.72%)
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.33%)
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.01%)
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.91%)
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.39%)
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.90%)
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.76%)
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.22%)
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.48%)
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.07%)
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.66%)
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.86%)
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.77%)
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.19%)
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.28%)
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.58%)
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.12%)
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.42%)
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.40%)
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.35%)
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.08%)
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.80%)
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.22%)
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.38%)
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.52%)
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.81%)
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.55%)
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.32%)
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.18%)
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.32%)
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.95%)
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.81%)
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.26%)
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.76%)
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.84%)
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (13.07%)
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.28%)
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.65%)
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.41%)
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.82%)
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.87%)
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.70%)
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.07%)
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.12%)
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.52%)
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.18%)
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.47%)
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.99%)
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.45%)

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 122 layers
[INFO] Time: 13.40s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2/conversion_report.json
[SUCCESS] BitNet-2B-FP8 量化完成并移动到 /root/vllmbench/checkpoints/BitNet-2B-FP8 (16.4s)
[SUCCESS]   ✓ 已修正 BitNet-2B-FP8/config.json (vLLM 兼容)
[SUCCESS]   ✓ 已删除 60 个不兼容权重 (ffn_sub_norm, attn_sub_norm)

[INFO] 基础模型准备统计: 成功 2, 失败 0

------------------------------------------------------------
  验证基础模型
------------------------------------------------------------
[SUCCESS]   ✓ BitNet-2B-INT8
[SUCCESS]   ✓ BitNet-2B-FP8

----------------------------------------------------------------------
TASK 1: 基础模型准备 (下载 + 量化) - SUCCESS
Duration: 170.2 seconds (2.8 minutes)
----------------------------------------------------------------------


======================================================================
TASK 2: SlideSparse 转换 (prune + slide)
Started: 2026-01-28 07:07:45
======================================================================


------------------------------------------------------------
  转换: BitNet-2B-INT8 -> SlideSparse-2_4
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-INT8 --Z 2 --L 4
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
======================================================================
Processing: BitNet-2B-INT8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=4, N=2, expand=1.000)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4

[INFO] Copying non-weight files...
[INFO]   Copied: README.md, .gitattributes, data_summary_card.md, tokenizer_config.json, conversion_report.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 19.10s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4/conversion_report.json
[SUCCESS] BitNet-2B-INT8-SlideSparse-2_4 转换完成 (22.2s)

------------------------------------------------------------
  转换: BitNet-2B-INT8 -> SlideSparse-2_6
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-INT8 --Z 2 --L 6
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
======================================================================
Processing: BitNet-2B-INT8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=6, N=3, expand=1.333)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6

[INFO] Copying non-weight files...
[INFO]   Copied: README.md, .gitattributes, data_summary_card.md, tokenizer_config.json, conversion_report.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 26.48s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6/conversion_report.json
[SUCCESS] BitNet-2B-INT8-SlideSparse-2_6 转换完成 (29.6s)

------------------------------------------------------------
  转换: BitNet-2B-INT8 -> SlideSparse-2_8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-INT8 --Z 2 --L 8
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
======================================================================
Processing: BitNet-2B-INT8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=8, N=4, expand=1.500)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8

[INFO] Copying non-weight files...
[INFO]   Copied: README.md, .gitattributes, data_summary_card.md, tokenizer_config.json, conversion_report.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 26.86s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8/conversion_report.json
[SUCCESS] BitNet-2B-INT8-SlideSparse-2_8 转换完成 (29.9s)

------------------------------------------------------------
  转换: BitNet-2B-INT8 -> SlideSparse-2_10
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-INT8 --Z 2 --L 10
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
======================================================================
Processing: BitNet-2B-INT8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=10, N=5, expand=1.600)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10

[INFO] Copying non-weight files...
[INFO]   Copied: README.md, .gitattributes, data_summary_card.md, tokenizer_config.json, conversion_report.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=int8
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 26.22s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10/conversion_report.json
[SUCCESS] BitNet-2B-INT8-SlideSparse-2_10 转换完成 (29.3s)

------------------------------------------------------------
  转换: BitNet-2B-FP8 -> SlideSparse-2_4
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-FP8 --Z 2 --L 4
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
======================================================================
Processing: BitNet-2B-FP8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=4, N=2, expand=1.000)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4

[INFO] Copying non-weight files...
[INFO]   Copied: README.md, .gitattributes, data_summary_card.md, tokenizer_config.json, conversion_report.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 6912] -> [2560, 6912]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [6912, 2560] -> [6912, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [2560, 2560] -> [2560, 2560]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:4, mode=magnitude)
[INFO]     2:4 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.000)
[INFO]     Shape: [640, 2560] -> [640, 2560]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 20.18s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4/conversion_report.json
[SUCCESS] BitNet-2B-FP8-SlideSparse-2_4 转换完成 (23.3s)

------------------------------------------------------------
  转换: BitNet-2B-FP8 -> SlideSparse-2_6
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-FP8 --Z 2 --L 6
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
======================================================================
Processing: BitNet-2B-FP8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=6, N=3, expand=1.333)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6

[INFO] Copying non-weight files...
[INFO]   Copied: README.md, .gitattributes, data_summary_card.md, tokenizer_config.json, conversion_report.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 6912] -> [2560, 9216]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [6912, 2560] -> [6912, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [2560, 2560] -> [2560, 3424]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:6, mode=magnitude)
[INFO]     2:6 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.333)
[INFO]     Shape: [640, 2560] -> [640, 3424]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 25.47s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6/conversion_report.json
[SUCCESS] BitNet-2B-FP8-SlideSparse-2_6 转换完成 (28.6s)

------------------------------------------------------------
  转换: BitNet-2B-FP8 -> SlideSparse-2_8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-FP8 --Z 2 --L 8
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
======================================================================
Processing: BitNet-2B-FP8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=8, N=4, expand=1.500)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8

[INFO] Copying non-weight files...
[INFO]   Copied: README.md, .gitattributes, data_summary_card.md, tokenizer_config.json, conversion_report.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 6912] -> [2560, 10368]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [6912, 2560] -> [6912, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [2560, 2560] -> [2560, 3840]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:8, mode=magnitude)
[INFO]     2:8 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.500)
[INFO]     Shape: [640, 2560] -> [640, 3840]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 23.59s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8/conversion_report.json
[SUCCESS] BitNet-2B-FP8-SlideSparse-2_8 转换完成 (26.8s)

------------------------------------------------------------
  转换: BitNet-2B-FP8 -> SlideSparse-2_10
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-FP8 --Z 2 --L 10
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
======================================================================
Processing: BitNet-2B-FP8
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=10, N=5, expand=1.600)
[INFO] Mode: magnitude
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10

[INFO] Copying non-weight files...
[INFO]   Copied: README.md, .gitattributes, data_summary_card.md, tokenizer_config.json, conversion_report.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 6912] -> [2560, 11072]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [6912, 2560] -> [6912, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [2560, 2560] -> [2560, 4096]
[INFO]     2:4 validation: ✓
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=fp8_e4m3
[INFO]   Stage 1: Pruning (2:10, mode=magnitude)
[INFO]     2:10 validation: ✓
[INFO]   Stage 2: Sliding (expand_ratio=1.600)
[INFO]     Shape: [640, 2560] -> [640, 4096]
[INFO]     2:4 validation: ✓

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 272 layers
[INFO] Time: 24.26s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10/conversion_report.json
[SUCCESS] BitNet-2B-FP8-SlideSparse-2_10 转换完成 (27.3s)

[INFO] SlideSparse 转换统计: 成功 8, 跳过 0, 失败 0

------------------------------------------------------------
  验证 SlideSparse 模型
------------------------------------------------------------
[SUCCESS]   ✓ BitNet-2B-INT8-SlideSparse-2_4
[SUCCESS]   ✓ BitNet-2B-INT8-SlideSparse-2_6
[SUCCESS]   ✓ BitNet-2B-INT8-SlideSparse-2_8
[SUCCESS]   ✓ BitNet-2B-INT8-SlideSparse-2_10
[SUCCESS]   ✓ BitNet-2B-FP8-SlideSparse-2_4
[SUCCESS]   ✓ BitNet-2B-FP8-SlideSparse-2_6
[SUCCESS]   ✓ BitNet-2B-FP8-SlideSparse-2_8
[SUCCESS]   ✓ BitNet-2B-FP8-SlideSparse-2_10

----------------------------------------------------------------------
TASK 2: SlideSparse 转换 (prune + slide) - SUCCESS
Duration: 217.1 seconds (3.6 minutes)
----------------------------------------------------------------------


======================================================================
TASK 3: 离线调优 (粗调优 + 细调优)
Started: 2026-01-28 07:11:22
======================================================================


------------------------------------------------------------
  粗调优: cuBLASLt + Triton quant_only
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/offline_autotune_algsearch.py --model BitNet-2B --dtype all --m_list 256,1024,4096,16384,32768 --Lmax 10 --warmup 25 --repeat 50 --kernels 1,0,0,0,1

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(

============================================================
  SlideSparse 统一离线调优
============================================================

  GPU:           NVIDIA GB10 (cc121)
  Python:        py312
  CUDA:          cu129
  Arch:          aarch64

  数据类型:      ['int8', 'fp8']
  输出类型:      bf16
  高精度累加:    否
  模型 (base):   ['BitNet-2B']
  Lmax:          10
  M-quick:       否
  M 列表:        [256, 1024, 4096, 16384, 32768]
  Warmup/Repeat: 25/50

  Kernel 调优:
    ✓ cuBLASLt GEMM
    ✗ cuSPARSELt GEMM
    ✗ Triton Dequant + Bias
    ✗ Triton Quant + Slide
    ✓ Triton Quant Only

============================================================
  Step 0: 编译 CUDA 扩展
============================================================


------------------------------------------------------------
  编译 cublaslt
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/cublaslt_gemm/build_cublaslt.py build --force
[SUCCESS] cublaslt 编译成功

------------------------------------------------------------
  编译 cusparselt
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/cusparselt_gemm/build_cusparselt.py build --force
[SUCCESS] cusparselt 编译成功

------------------------------------------------------------
  编译 compress
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/build_compress.py build --force
[SUCCESS] compress 编译成功

============================================================
  Step 1: cuBLASLt GEMM
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] dtype=int8, outdtype=int32
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/search/cuBLASLt_AlgSearch/alg_search.py --dtype int8 --outdtype int32 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --compile --Lmax 10 --m_list 256,1024,4096,16384,32768
[SUCCESS] cuBLASLt GEMM (int8) 完成
[INFO] dtype=fp8e4m3, outdtype=bf16
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/search/cuBLASLt_AlgSearch/alg_search.py --dtype fp8e4m3 --outdtype bf16 --model BitNet-2B-FP8 --warmup 25 --repeat 50 --compile --Lmax 10 --m_list 256,1024,4096,16384,32768
[SUCCESS] cuBLASLt GEMM (fp8) 完成

============================================================
  Step 2: cuSPARSELt GEMM [跳过]
============================================================


============================================================
  Step 3: Triton Dequant + Bias [跳过]
============================================================


============================================================
  Step 4: Triton Quant + Slide [跳过]
============================================================


============================================================
  Step 5: Triton Quant Only
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/quant_only_triton/autotune_autogen_quant_only.py --model BitNet-2B-INT8 --warmup 25 --repeat 50 --Lmax 10 --m_list 256,1024,4096,16384,32768
[SUCCESS] Triton Quant Only 完成

============================================================
  调优总结
============================================================

  cuBLASLt GEMM: [全部成功] (2/2)
  cuSPARSELt GEMM: [跳过]
  Triton Dequant + Bias: [跳过]
  Triton Quant + Slide: [跳过]
  Triton Quant Only: [全部成功] (1/1)

总计: 成功 3, 失败 0, 跳过 3
[SUCCESS] 粗调优完成 (463.3s)

------------------------------------------------------------
  细调优: cuSPARSELt + Triton Dequant/QuantSlide
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/offline_autotune_algsearch.py --model BitNet-2B --dtype all --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768 --Lmax 10 --warmup 25 --repeat 50 --kernels 0,1,1,1,0

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(

============================================================
  SlideSparse 统一离线调优
============================================================

  GPU:           NVIDIA GB10 (cc121)
  Python:        py312
  CUDA:          cu129
  Arch:          aarch64

  数据类型:      ['int8', 'fp8']
  输出类型:      bf16
  高精度累加:    否
  模型 (base):   ['BitNet-2B']
  Lmax:          10
  M-quick:       否
  M 列表:        [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]
  Warmup/Repeat: 25/50

  Kernel 调优:
    ✗ cuBLASLt GEMM
    ✓ cuSPARSELt GEMM
    ✓ Triton Dequant + Bias
    ✓ Triton Quant + Slide
    ✗ Triton Quant Only

============================================================
  Step 0: 编译 CUDA 扩展
============================================================


------------------------------------------------------------
  编译 cublaslt
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/cublaslt_gemm/build_cublaslt.py build --force
[SUCCESS] cublaslt 编译成功

------------------------------------------------------------
  编译 cusparselt
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/cusparselt_gemm/build_cusparselt.py build --force
[SUCCESS] cusparselt 编译成功

------------------------------------------------------------
  编译 compress
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/build_compress.py build --force
[SUCCESS] compress 编译成功

============================================================
  Step 1: cuBLASLt GEMM [跳过]
============================================================


============================================================
  Step 2: cuSPARSELt GEMM
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] dtype=int8, outdtype=bf16
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/search/cuSPARSELt_AlgSearch/alg_search.py --dtype int8 --outdtype bf16 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --compile --Lmax 10 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768
[SUCCESS] cuSPARSELt GEMM (int8) 完成
[INFO] dtype=fp8e4m3, outdtype=bf16
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/search/cuSPARSELt_AlgSearch/alg_search.py --dtype fp8e4m3 --outdtype bf16 --model BitNet-2B-FP8 --warmup 25 --repeat 50 --compile --Lmax 10 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768
[SUCCESS] cuSPARSELt GEMM (fp8) 完成

============================================================
  Step 3: Triton Dequant + Bias
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/autotune_autogen_dequant_bias.py --model BitNet-2B-INT8 --warmup 25 --repeat 50 --Lmax 10 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768
[SUCCESS] Triton Dequant + Bias 完成

============================================================
  Step 4: Triton Quant + Slide
============================================================


------------------------------------------------------------
  模型: BitNet-2B
------------------------------------------------------------
[INFO] NK 组合数: 16 (from BitNet-2B-INT8)
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/autotune_autogen_quant_slide.py --model BitNet-2B-INT8 --warmup 25 --repeat 50 --Lmax 10 --m_list 64,128,256,512,1024,2048,4096,8192,16384,32768
[SUCCESS] Triton Quant + Slide 完成

============================================================
  Step 5: Triton Quant Only [跳过]
============================================================


============================================================
  调优总结
============================================================

  cuBLASLt GEMM: [跳过]
  cuSPARSELt GEMM: [全部成功] (2/2)
  Triton Dequant + Bias: [全部成功] (1/1)
  Triton Quant + Slide: [全部成功] (1/1)
  Triton Quant Only: [跳过]

总计: 成功 4, 失败 0, 跳过 2
[SUCCESS] 细调优完成 (734.8s)

----------------------------------------------------------------------
TASK 3: 离线调优 (粗调优 + 细调优) - SUCCESS
Duration: 1198.1 seconds (20.0 minutes)
----------------------------------------------------------------------


======================================================================
TASK 4: 完整 Prefill Benchmark
Started: 2026-01-28 07:31:20
======================================================================


------------------------------------------------------------
  Prefill Benchmark: bitnet1.58-2b-int8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-int8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[WARNING] GPU 架构不被 Triton 支持: GB10 (sm_121a) is not yet supported by Triton/ptxas
[WARNING] 将使用 eager mode (禁用 torch.compile)
[WARNING] 检测到不支持 torch.compile 的 GPU 架构
[WARNING] 自动启用 eager mode

============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA GB10                               ││
│ GPU (short):      GB10                                      │
│ Memory:           119.7 GB                                    │
│ CC:               cc121 (Blackwell)                            │
│ SM Code:          sm_121                                    │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.95.05                                 │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✗ GB10 (sm_121a) is not yet supp          ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-int8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['prefill']
  M_prefill:        [512, 1024, 2048, 4096, 8192, 16384, 32768]
  M_decode:         [512, 1024, 2048, 4096, 8192, 16384, 32768]
  GPU 内存利用率:   0.8
  编译模式:         Eager

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_073123.log


============================================================
  BitNet-2B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:31:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:31:27 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3637834) WARNING 01-28 07:31:56 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 21.83 requests/s, 11196.59 total tokens/s, 21.83 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 07:31:27] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:31:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:31:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:31:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:31:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:31:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:31:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:31:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:31:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:31:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:31:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:31:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:31:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:31:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:31:31] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:31:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:31:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:31:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:31:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:31:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:31:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:31:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:31:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:31:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:31:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:31:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:31:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:31:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3637834) [2026-01-28 07:31:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3637834) [2026-01-28 07:31:32] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3637834) [2026-01-28 07:31:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3637834) [2026-01-28 07:31:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3637834) [2026-01-28 07:31:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3637834) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3637834) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.64s/it]
(EngineCore_DP0 pid=3637834) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.64s/it]
(EngineCore_DP0 pid=3637834) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3637834) 2026-01-28 07:31:55,447 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3637834) 2026-01-28 07:31:55,484 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  67%|██████▋   | 86/128 [00:00<00:00, 859.88it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 966.06it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:42,  2.97it/s, est. speed input: 1518.83 toks/s, output: 2.97 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:12, 10.11it/s, est. speed input: 4384.63 toks/s, output: 8.56 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:08, 14.64it/s, est. speed input: 6047.23 toks/s, output: 11.81 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:06, 17.53it/s, est. speed input: 7114.29 toks/s, output: 13.89 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:05, 19.43it/s, est. speed input: 7856.14 toks/s, output: 15.34 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:05, 20.49it/s, est. speed input: 8367.21 toks/s, output: 16.34 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:05, 21.49it/s, est. speed input: 8803.16 toks/s, output: 17.19 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:04, 22.20it/s, est. speed input: 9150.82 toks/s, output: 17.87 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:04, 22.71it/s, est. speed input: 9435.96 toks/s, output: 18.43 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:04, 22.96it/s, est. speed input: 9660.48 toks/s, output: 18.87 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:04, 23.14it/s, est. speed input: 9850.54 toks/s, output: 19.24 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:04, 23.12it/s, est. speed input: 9997.32 toks/s, output: 19.53 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:03, 23.34it/s, est. speed input: 10146.62 toks/s, output: 19.82 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:03, 23.16it/s, est. speed input: 10245.43 toks/s, output: 20.01 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:03, 23.34it/s, est. speed input: 10360.25 toks/s, output: 20.23 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:03, 23.51it/s, est. speed input: 10464.68 toks/s, output: 20.44 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:03, 23.53it/s, est. speed input: 10550.64 toks/s, output: 20.61 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:03, 23.56it/s, est. speed input: 10629.24 toks/s, output: 20.76 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:02<00:03, 23.43it/s, est. speed input: 10689.36 toks/s, output: 20.88 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:02<00:02, 23.53it/s, est. speed input: 10756.75 toks/s, output: 21.01 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:02, 23.54it/s, est. speed input: 10814.70 toks/s, output: 21.12 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:02, 23.56it/s, est. speed input: 10867.94 toks/s, output: 21.23 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:02, 23.15it/s, est. speed input: 10890.40 toks/s, output: 21.27 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:03<00:02, 23.28it/s, est. speed input: 10936.28 toks/s, output: 21.36 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:03<00:02, 23.39it/s, est. speed input: 10980.31 toks/s, output: 21.45 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:03<00:02, 23.52it/s, est. speed input: 11023.89 toks/s, output: 21.53 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:03<00:02, 23.62it/s, est. speed input: 11064.54 toks/s, output: 21.61 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:03<00:01, 23.74it/s, est. speed input: 11105.28 toks/s, output: 21.69 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:03<00:01, 23.68it/s, est. speed input: 11136.28 toks/s, output: 21.75 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:04<00:01, 23.71it/s, est. speed input: 11168.83 toks/s, output: 21.81 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:04<00:01, 23.38it/s, est. speed input: 11182.18 toks/s, output: 21.84 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:04<00:01, 23.42it/s, est. speed input: 11207.78 toks/s, output: 21.89 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:04<00:01, 23.43it/s, est. speed input: 11231.10 toks/s, output: 21.94 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:04<00:01, 23.58it/s, est. speed input: 11259.02 toks/s, output: 21.99 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:04<00:01, 23.66it/s, est. speed input: 11284.93 toks/s, output: 22.04 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:04<00:00, 23.74it/s, est. speed input: 11310.18 toks/s, output: 22.09 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:04<00:00, 23.73it/s, est. speed input: 11331.50 toks/s, output: 22.13 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:05<00:00, 23.80it/s, est. speed input: 11354.51 toks/s, output: 22.18 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:05<00:00, 23.50it/s, est. speed input: 11363.11 toks/s, output: 22.19 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:05<00:00, 23.36it/s, est. speed input: 11373.54 toks/s, output: 22.21 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:05<00:00, 23.47it/s, est. speed input: 11391.77 toks/s, output: 22.25 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:05<00:00, 23.60it/s, est. speed input: 11411.03 toks/s, output: 22.29 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:05<00:00, 23.69it/s, est. speed input: 11429.17 toks/s, output: 22.32 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.69it/s, est. speed input: 11435.29 toks/s, output: 22.33 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 22.33it/s, est. speed input: 11435.29 toks/s, output: 22.33 toks/s]
[rank0]:[W128 07:32:02.412696534 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.5s

测试结果:
  Requests/s:   21.83
  Tokens/s:     11196.59
  Total Reqs:   128
  Elapsed:      5.86s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     11174.76

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:32:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:32:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3638620) WARNING 01-28 07:32:32 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.85 requests/s, 13171.38 total tokens/s, 12.85 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 07:32:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:32:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:32:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:32:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:32:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:32:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:32:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:32:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:32:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:32:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:32:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:32:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:32:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:32:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:32:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:32:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:32:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:32:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3638620) [2026-01-28 07:32:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3638620) [2026-01-28 07:32:14] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3638620) [2026-01-28 07:32:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3638620) [2026-01-28 07:32:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3638620) [2026-01-28 07:32:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3638620) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3638620) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.59s/it]
(EngineCore_DP0 pid=3638620) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.59s/it]
(EngineCore_DP0 pid=3638620) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3638620) 2026-01-28 07:32:31,914 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3638620) 2026-01-28 07:32:31,956 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 635.31it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 663.22it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:14,  8.58it/s, est. speed input: 8790.41 toks/s, output: 8.58 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:10, 11.42it/s, est. speed input: 11318.15 toks/s, output: 11.05 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:10, 12.27it/s, est. speed input: 12105.37 toks/s, output: 11.82 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:09, 12.48it/s, est. speed input: 12368.71 toks/s, output: 12.08 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:09, 12.76it/s, est. speed input: 12618.99 toks/s, output: 12.32 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:09, 12.91it/s, est. speed input: 12770.01 toks/s, output: 12.47 toks/s]
Processed prompts:  10%|█         | 13/128 [00:01<00:08, 13.00it/s, est. speed input: 12876.42 toks/s, output: 12.57 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:08, 13.11it/s, est. speed input: 12979.25 toks/s, output: 12.67 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:08, 13.14it/s, est. speed input: 13041.00 toks/s, output: 12.74 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:08, 13.19it/s, est. speed input: 13099.31 toks/s, output: 12.79 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:08, 13.01it/s, est. speed input: 13083.49 toks/s, output: 12.78 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:08, 13.11it/s, est. speed input: 13132.08 toks/s, output: 12.82 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:07, 13.18it/s, est. speed input: 13171.18 toks/s, output: 12.86 toks/s]
Processed prompts:  21%|██        | 27/128 [00:02<00:07, 13.14it/s, est. speed input: 13185.97 toks/s, output: 12.88 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:02<00:07, 13.13it/s, est. speed input: 13202.55 toks/s, output: 12.89 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:07, 13.19it/s, est. speed input: 13230.96 toks/s, output: 12.92 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:07, 13.28it/s, est. speed input: 13263.96 toks/s, output: 12.95 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:07, 13.10it/s, est. speed input: 13250.11 toks/s, output: 12.94 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:06, 13.18it/s, est. speed input: 13272.43 toks/s, output: 12.96 toks/s]
Processed prompts:  30%|███       | 39/128 [00:03<00:06, 13.22it/s, est. speed input: 13290.84 toks/s, output: 12.98 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:03<00:06, 13.24it/s, est. speed input: 13305.73 toks/s, output: 12.99 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:03<00:06, 13.26it/s, est. speed input: 13320.78 toks/s, output: 13.01 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:03<00:06, 13.29it/s, est. speed input: 13335.54 toks/s, output: 13.02 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:06, 13.26it/s, est. speed input: 13342.55 toks/s, output: 13.03 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:06, 13.04it/s, est. speed input: 13322.49 toks/s, output: 13.01 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:05, 13.09it/s, est. speed input: 13330.20 toks/s, output: 13.02 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:04<00:05, 13.16it/s, est. speed input: 13341.49 toks/s, output: 13.03 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:04<00:05, 13.20it/s, est. speed input: 13350.93 toks/s, output: 13.04 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:04<00:05, 13.23it/s, est. speed input: 13360.86 toks/s, output: 13.05 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:04<00:05, 13.24it/s, est. speed input: 13368.34 toks/s, output: 13.05 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:04<00:05, 13.22it/s, est. speed input: 13371.69 toks/s, output: 13.06 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:04<00:04, 13.08it/s, est. speed input: 13362.27 toks/s, output: 13.05 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:04, 13.07it/s, est. speed input: 13362.48 toks/s, output: 13.05 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:05<00:04, 13.12it/s, est. speed input: 13367.95 toks/s, output: 13.05 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:05<00:04, 13.15it/s, est. speed input: 13372.37 toks/s, output: 13.06 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:05<00:04, 13.17it/s, est. speed input: 13377.10 toks/s, output: 13.06 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:05<00:04, 13.18it/s, est. speed input: 13381.21 toks/s, output: 13.07 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:05<00:04, 13.09it/s, est. speed input: 13376.45 toks/s, output: 13.06 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:05<00:03, 13.04it/s, est. speed input: 13372.44 toks/s, output: 13.06 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:06<00:03, 13.06it/s, est. speed input: 13373.32 toks/s, output: 13.06 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:06<00:03, 13.14it/s, est. speed input: 13380.24 toks/s, output: 13.07 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:06<00:03, 13.13it/s, est. speed input: 13381.35 toks/s, output: 13.07 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:06<00:03, 13.23it/s, est. speed input: 13390.44 toks/s, output: 13.08 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:06<00:03, 13.20it/s, est. speed input: 13392.24 toks/s, output: 13.08 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:06<00:02, 13.14it/s, est. speed input: 13389.85 toks/s, output: 13.08 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:06<00:02, 13.08it/s, est. speed input: 13386.98 toks/s, output: 13.07 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:07<00:02, 13.20it/s, est. speed input: 13395.74 toks/s, output: 13.08 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:07<00:02, 13.24it/s, est. speed input: 13401.25 toks/s, output: 13.09 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:07<00:02, 13.24it/s, est. speed input: 13404.66 toks/s, output: 13.09 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:07<00:02, 13.19it/s, est. speed input: 13404.33 toks/s, output: 13.09 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:07<00:02, 13.19it/s, est. speed input: 13406.42 toks/s, output: 13.09 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:07<00:01, 13.13it/s, est. speed input: 13404.22 toks/s, output: 13.09 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:08<00:01, 13.08it/s, est. speed input: 13402.08 toks/s, output: 13.09 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:08<00:01, 13.19it/s, est. speed input: 13408.64 toks/s, output: 13.09 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:08<00:01, 13.21it/s, est. speed input: 13411.74 toks/s, output: 13.10 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:08<00:01, 13.22it/s, est. speed input: 13414.51 toks/s, output: 13.10 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:08<00:01, 13.18it/s, est. speed input: 13414.06 toks/s, output: 13.10 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:08<00:00, 13.15it/s, est. speed input: 13413.60 toks/s, output: 13.10 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:08<00:00, 13.04it/s, est. speed input: 13408.03 toks/s, output: 13.09 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:09<00:00, 13.09it/s, est. speed input: 13410.12 toks/s, output: 13.10 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:09<00:00, 13.11it/s, est. speed input: 13411.11 toks/s, output: 13.10 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:09<00:00, 13.16it/s, est. speed input: 13414.15 toks/s, output: 13.10 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:09<00:00, 13.21it/s, est. speed input: 13417.64 toks/s, output: 13.10 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:09<00:00, 13.23it/s, est. speed input: 13420.56 toks/s, output: 13.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 13.23it/s, est. speed input: 13419.83 toks/s, output: 13.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 13.11it/s, est. speed input: 13419.83 toks/s, output: 13.11 toks/s]
[rank0]:[W128 07:32:42.652354690 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.0s

测试结果:
  Requests/s:   12.85
  Tokens/s:     13171.38
  Total Reqs:   128
  Elapsed:      9.96s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     13158.53

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:32:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:32:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3639368) WARNING 01-28 07:33:12 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 13.35 requests/s, 13682.91 total tokens/s, 13.35 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 07:32:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:32:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:32:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:32:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:32:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:32:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:32:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:32:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:32:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:32:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:32:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:32:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:32:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:32:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:32:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:32:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:32:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:32:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:32:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3639368) [2026-01-28 07:32:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3639368) [2026-01-28 07:32:53] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3639368) [2026-01-28 07:32:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3639368) [2026-01-28 07:32:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3639368) [2026-01-28 07:32:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3639368) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3639368) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.36s/it]
(EngineCore_DP0 pid=3639368) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.36s/it]
(EngineCore_DP0 pid=3639368) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3639368) 2026-01-28 07:33:11,473 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3639368) 2026-01-28 07:33:11,565 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  25%|██▌       | 65/256 [00:00<00:00, 648.10it/s]
Adding requests:  51%|█████     | 130/256 [00:00<00:00, 604.20it/s]
Adding requests:  75%|███████▍  | 191/256 [00:00<00:00, 543.46it/s]
Adding requests:  96%|█████████▌| 246/256 [00:00<00:00, 526.63it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 544.64it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 6/256 [00:00<00:08, 30.85it/s, est. speed input: 31600.00 toks/s, output: 30.86 toks/s]
Processed prompts:   4%|▍         | 10/256 [00:00<00:12, 18.99it/s, est. speed input: 20891.44 toks/s, output: 20.40 toks/s]
Processed prompts:   5%|▌         | 13/256 [00:00<00:12, 19.10it/s, est. speed input: 20626.94 toks/s, output: 20.14 toks/s]
Processed prompts:   6%|▋         | 16/256 [00:00<00:16, 14.86it/s, est. speed input: 17452.21 toks/s, output: 17.04 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:01<00:16, 14.48it/s, est. speed input: 16941.65 toks/s, output: 16.54 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:01<00:16, 14.19it/s, est. speed input: 16553.25 toks/s, output: 16.16 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:01<00:16, 14.05it/s, est. speed input: 16283.43 toks/s, output: 15.90 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:01<00:16, 13.91it/s, est. speed input: 16052.83 toks/s, output: 15.68 toks/s]
Processed prompts:  10%|█         | 26/256 [00:01<00:16, 13.67it/s, est. speed input: 15810.99 toks/s, output: 15.44 toks/s]
Processed prompts:  11%|█         | 28/256 [00:01<00:16, 13.64it/s, est. speed input: 15658.08 toks/s, output: 15.29 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:01<00:16, 13.60it/s, est. speed input: 15520.17 toks/s, output: 15.16 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:02<00:16, 13.56it/s, est. speed input: 15400.19 toks/s, output: 15.04 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:02<00:16, 13.56it/s, est. speed input: 15300.98 toks/s, output: 14.94 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:02<00:16, 13.56it/s, est. speed input: 15216.00 toks/s, output: 14.86 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:02<00:16, 13.49it/s, est. speed input: 15124.30 toks/s, output: 14.77 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:02<00:16, 13.34it/s, est. speed input: 15022.18 toks/s, output: 14.67 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:02<00:15, 13.39it/s, est. speed input: 14961.58 toks/s, output: 14.61 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:03<00:15, 13.38it/s, est. speed input: 14898.28 toks/s, output: 14.55 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:03<00:15, 13.41it/s, est. speed input: 14846.89 toks/s, output: 14.50 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:03<00:15, 13.46it/s, est. speed input: 14804.61 toks/s, output: 14.46 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:03<00:15, 13.47it/s, est. speed input: 14762.18 toks/s, output: 14.42 toks/s]
Processed prompts:  20%|██        | 52/256 [00:03<00:15, 13.51it/s, est. speed input: 14728.93 toks/s, output: 14.38 toks/s]
Processed prompts:  21%|██        | 54/256 [00:03<00:15, 13.42it/s, est. speed input: 14681.22 toks/s, output: 14.34 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:03<00:14, 13.47it/s, est. speed input: 14651.40 toks/s, output: 14.31 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:04<00:14, 13.48it/s, est. speed input: 14621.34 toks/s, output: 14.28 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:04<00:14, 13.50it/s, est. speed input: 14595.33 toks/s, output: 14.25 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:04<00:14, 13.50it/s, est. speed input: 14568.99 toks/s, output: 14.23 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:04<00:14, 13.45it/s, est. speed input: 14538.62 toks/s, output: 14.20 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:04<00:14, 13.49it/s, est. speed input: 14518.30 toks/s, output: 14.18 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:04<00:13, 13.55it/s, est. speed input: 14503.41 toks/s, output: 14.16 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:04<00:13, 13.50it/s, est. speed input: 14478.90 toks/s, output: 14.14 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:05<00:13, 13.52it/s, est. speed input: 14462.48 toks/s, output: 14.12 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:05<00:13, 13.52it/s, est. speed input: 14444.45 toks/s, output: 14.11 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:05<00:13, 13.57it/s, est. speed input: 14432.77 toks/s, output: 14.09 toks/s]
Processed prompts:  30%|███       | 78/256 [00:05<00:13, 13.50it/s, est. speed input: 14412.21 toks/s, output: 14.07 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:05<00:12, 13.55it/s, est. speed input: 14401.32 toks/s, output: 14.06 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:05<00:12, 13.52it/s, est. speed input: 14385.31 toks/s, output: 14.05 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:05<00:12, 13.42it/s, est. speed input: 14363.11 toks/s, output: 14.03 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:06<00:12, 13.43it/s, est. speed input: 14348.73 toks/s, output: 14.01 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:06<00:12, 13.49it/s, est. speed input: 14340.14 toks/s, output: 14.00 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:06<00:12, 13.56it/s, est. speed input: 14333.21 toks/s, output: 14.00 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:06<00:12, 13.59it/s, est. speed input: 14325.45 toks/s, output: 13.99 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:06<00:11, 13.60it/s, est. speed input: 14317.13 toks/s, output: 13.98 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:06<00:11, 13.60it/s, est. speed input: 14308.67 toks/s, output: 13.97 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:07<00:11, 13.44it/s, est. speed input: 14288.80 toks/s, output: 13.95 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:07<00:11, 13.47it/s, est. speed input: 14280.20 toks/s, output: 13.95 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:07<00:11, 13.46it/s, est. speed input: 14269.36 toks/s, output: 13.93 toks/s]
Processed prompts:  41%|████      | 104/256 [00:07<00:11, 13.44it/s, est. speed input: 14258.19 toks/s, output: 13.92 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:07<00:11, 13.45it/s, est. speed input: 14249.39 toks/s, output: 13.92 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:07<00:10, 13.46it/s, est. speed input: 14240.78 toks/s, output: 13.91 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:07<00:10, 13.42it/s, est. speed input: 14229.81 toks/s, output: 13.90 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:08<00:10, 13.27it/s, est. speed input: 14211.08 toks/s, output: 13.88 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:08<00:10, 13.34it/s, est. speed input: 14203.76 toks/s, output: 13.87 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:08<00:10, 13.41it/s, est. speed input: 14198.50 toks/s, output: 13.87 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:08<00:10, 13.47it/s, est. speed input: 14193.92 toks/s, output: 13.86 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:08<00:10, 13.47it/s, est. speed input: 14187.43 toks/s, output: 13.85 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:08<00:09, 13.49it/s, est. speed input: 14181.54 toks/s, output: 13.85 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:08<00:09, 13.56it/s, est. speed input: 14179.69 toks/s, output: 13.85 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:09<00:09, 13.41it/s, est. speed input: 14166.44 toks/s, output: 13.83 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:09<00:09, 13.49it/s, est. speed input: 14164.03 toks/s, output: 13.83 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:09<00:09, 13.51it/s, est. speed input: 14159.75 toks/s, output: 13.83 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:09<00:09, 13.53it/s, est. speed input: 14155.51 toks/s, output: 13.82 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:09<00:08, 13.57it/s, est. speed input: 14153.31 toks/s, output: 13.82 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:09<00:08, 13.58it/s, est. speed input: 14149.96 toks/s, output: 13.82 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:09<00:08, 13.59it/s, est. speed input: 14146.80 toks/s, output: 13.82 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:10<00:08, 13.43it/s, est. speed input: 14135.13 toks/s, output: 13.80 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:10<00:08, 13.48it/s, est. speed input: 14131.99 toks/s, output: 13.80 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:10<00:08, 13.46it/s, est. speed input: 14126.54 toks/s, output: 13.80 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:10<00:08, 13.43it/s, est. speed input: 14119.98 toks/s, output: 13.79 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:10<00:08, 13.47it/s, est. speed input: 14116.76 toks/s, output: 13.79 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:10<00:07, 13.47it/s, est. speed input: 14112.28 toks/s, output: 13.78 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:11<00:07, 13.47it/s, est. speed input: 14108.02 toks/s, output: 13.78 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:11<00:07, 13.36it/s, est. speed input: 14098.74 toks/s, output: 13.77 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:11<00:07, 13.33it/s, est. speed input: 14091.88 toks/s, output: 13.76 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:11<00:07, 13.30it/s, est. speed input: 14084.92 toks/s, output: 13.75 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:11<00:07, 13.39it/s, est. speed input: 14082.62 toks/s, output: 13.75 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:11<00:07, 13.35it/s, est. speed input: 14076.25 toks/s, output: 13.75 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:11<00:06, 13.41it/s, est. speed input: 14073.68 toks/s, output: 13.74 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:12<00:06, 13.40it/s, est. speed input: 14069.31 toks/s, output: 13.74 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:12<00:06, 13.39it/s, est. speed input: 14064.52 toks/s, output: 13.73 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:12<00:06, 13.29it/s, est. speed input: 14056.13 toks/s, output: 13.73 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:12<00:06, 13.39it/s, est. speed input: 14055.07 toks/s, output: 13.73 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:12<00:06, 13.42it/s, est. speed input: 14052.04 toks/s, output: 13.72 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:12<00:05, 13.48it/s, est. speed input: 14050.97 toks/s, output: 13.72 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:12<00:05, 13.51it/s, est. speed input: 14049.18 toks/s, output: 13.72 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:13<00:05, 13.44it/s, est. speed input: 14044.17 toks/s, output: 13.71 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:13<00:05, 13.46it/s, est. speed input: 14041.70 toks/s, output: 13.71 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:13<00:05, 13.35it/s, est. speed input: 14034.67 toks/s, output: 13.71 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:13<00:05, 13.46it/s, est. speed input: 14034.95 toks/s, output: 13.71 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:13<00:05, 13.44it/s, est. speed input: 14031.19 toks/s, output: 13.70 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:13<00:04, 13.51it/s, est. speed input: 14030.87 toks/s, output: 13.70 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:14<00:04, 13.47it/s, est. speed input: 14027.35 toks/s, output: 13.70 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:14<00:04, 13.47it/s, est. speed input: 14024.93 toks/s, output: 13.70 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:14<00:04, 13.51it/s, est. speed input: 14024.01 toks/s, output: 13.70 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:14<00:04, 13.39it/s, est. speed input: 14017.68 toks/s, output: 13.69 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:14<00:04, 13.41it/s, est. speed input: 14015.34 toks/s, output: 13.69 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:14<00:04, 13.39it/s, est. speed input: 14011.81 toks/s, output: 13.68 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:14<00:03, 13.48it/s, est. speed input: 14012.06 toks/s, output: 13.68 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:15<00:03, 13.45it/s, est. speed input: 14009.03 toks/s, output: 13.68 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:15<00:03, 13.46it/s, est. speed input: 14006.93 toks/s, output: 13.68 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:15<00:03, 13.48it/s, est. speed input: 14005.41 toks/s, output: 13.68 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:15<00:03, 13.35it/s, est. speed input: 13998.98 toks/s, output: 13.67 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:15<00:03, 13.37it/s, est. speed input: 13996.79 toks/s, output: 13.67 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:15<00:02, 13.40it/s, est. speed input: 13994.88 toks/s, output: 13.67 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:15<00:02, 13.43it/s, est. speed input: 13993.06 toks/s, output: 13.67 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:16<00:02, 13.39it/s, est. speed input: 13989.63 toks/s, output: 13.66 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:16<00:02, 13.47it/s, est. speed input: 13989.59 toks/s, output: 13.66 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:16<00:02, 13.43it/s, est. speed input: 13986.74 toks/s, output: 13.66 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:16<00:02, 13.31it/s, est. speed input: 13980.75 toks/s, output: 13.65 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:16<00:02, 13.38it/s, est. speed input: 13979.69 toks/s, output: 13.65 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:16<00:01, 13.38it/s, est. speed input: 13977.23 toks/s, output: 13.65 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:16<00:01, 13.49it/s, est. speed input: 13978.04 toks/s, output: 13.65 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:17<00:01, 13.50it/s, est. speed input: 13977.06 toks/s, output: 13.65 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:17<00:01, 13.48it/s, est. speed input: 13975.15 toks/s, output: 13.65 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:17<00:01, 13.47it/s, est. speed input: 13973.23 toks/s, output: 13.65 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:17<00:01, 13.39it/s, est. speed input: 13969.49 toks/s, output: 13.64 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:17<00:01, 13.44it/s, est. speed input: 13968.86 toks/s, output: 13.64 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:17<00:00, 13.48it/s, est. speed input: 13968.12 toks/s, output: 13.64 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:18<00:00, 13.50it/s, est. speed input: 13967.39 toks/s, output: 13.64 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:18<00:00, 13.56it/s, est. speed input: 13967.83 toks/s, output: 13.64 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:18<00:00, 13.59it/s, est. speed input: 13967.98 toks/s, output: 13.64 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:18<00:00, 13.59it/s, est. speed input: 13967.55 toks/s, output: 13.64 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:18<00:00, 13.40it/s, est. speed input: 13962.03 toks/s, output: 13.63 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:18<00:00, 13.40it/s, est. speed input: 14014.38 toks/s, output: 13.69 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:18<00:00, 13.69it/s, est. speed input: 14014.38 toks/s, output: 13.69 toks/s]
[rank0]:[W128 07:33:31.755528709 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.1s

测试结果:
  Requests/s:   13.35
  Tokens/s:     13682.91
  Total Reqs:   256
  Elapsed:      19.18s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     13669.56

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:33:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:33:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3640254) WARNING 01-28 07:34:02 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.66 requests/s, 12973.02 total tokens/s, 12.66 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 07:33:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:33:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:33:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:33:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:33:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:33:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:33:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:33:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:33:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:33:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:33:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:33:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:33:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:33:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:33:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:33:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:33:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:33:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:33:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:33:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:33:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:33:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:33:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:33:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:33:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:33:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:33:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:33:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3640254) [2026-01-28 07:33:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3640254) [2026-01-28 07:33:43] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3640254) [2026-01-28 07:33:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3640254) [2026-01-28 07:33:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3640254) [2026-01-28 07:33:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3640254) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3640254) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.49s/it]
(EngineCore_DP0 pid=3640254) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.49s/it]
(EngineCore_DP0 pid=3640254) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3640254) 2026-01-28 07:34:01,509 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3640254) 2026-01-28 07:34:01,626 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 69/512 [00:00<00:00, 680.88it/s]
Adding requests:  27%|██▋       | 138/512 [00:00<00:00, 616.46it/s]
Adding requests:  39%|███▉      | 201/512 [00:00<00:00, 536.20it/s]
Adding requests:  50%|█████     | 256/512 [00:00<00:00, 531.08it/s]
Adding requests:  61%|██████    | 310/512 [00:00<00:00, 516.97it/s]
Adding requests:  71%|███████   | 363/512 [00:00<00:00, 518.92it/s]
Adding requests:  81%|████████▏ | 416/512 [00:00<00:00, 516.46it/s]
Adding requests:  92%|█████████▏| 469/512 [00:00<00:00, 517.59it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 528.03it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 10/512 [00:00<00:09, 51.36it/s, est. speed input: 52604.56 toks/s, output: 51.37 toks/s]
Processed prompts:   3%|▎         | 16/512 [00:00<00:17, 28.81it/s, est. speed input: 32147.79 toks/s, output: 31.39 toks/s]
Processed prompts:   4%|▍         | 20/512 [00:00<00:23, 20.60it/s, est. speed input: 24684.19 toks/s, output: 24.11 toks/s]
Processed prompts:   4%|▍         | 23/512 [00:01<00:30, 16.06it/s, est. speed input: 20614.60 toks/s, output: 20.13 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:01<00:35, 13.64it/s, est. speed input: 18284.81 toks/s, output: 17.86 toks/s]
Processed prompts:   6%|▌         | 30/512 [00:01<00:36, 13.31it/s, est. speed input: 17341.06 toks/s, output: 16.93 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:02<00:36, 13.05it/s, est. speed input: 16652.97 toks/s, output: 16.26 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:02<00:36, 12.99it/s, est. speed input: 16202.72 toks/s, output: 15.82 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:02<00:36, 12.90it/s, est. speed input: 15833.60 toks/s, output: 15.46 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:03<00:36, 12.78it/s, est. speed input: 15516.27 toks/s, output: 15.15 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:03<00:36, 12.77it/s, est. speed input: 15285.46 toks/s, output: 14.93 toks/s]
Processed prompts:  11%|█         | 54/512 [00:03<00:35, 12.75it/s, est. speed input: 15090.93 toks/s, output: 14.74 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:03<00:35, 12.72it/s, est. speed input: 14919.28 toks/s, output: 14.57 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:04<00:35, 12.67it/s, est. speed input: 14766.38 toks/s, output: 14.42 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:04<00:35, 12.70it/s, est. speed input: 14653.32 toks/s, output: 14.31 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:04<00:34, 12.71it/s, est. speed input: 14548.75 toks/s, output: 14.21 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:05<00:34, 12.63it/s, est. speed input: 14439.36 toks/s, output: 14.10 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:05<00:34, 12.65it/s, est. speed input: 14356.69 toks/s, output: 14.02 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:05<00:33, 12.66it/s, est. speed input: 14283.27 toks/s, output: 13.95 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:06<00:33, 12.60it/s, est. speed input: 14204.98 toks/s, output: 13.87 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:06<00:33, 12.65it/s, est. speed input: 14150.18 toks/s, output: 13.82 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:06<00:32, 12.69it/s, est. speed input: 14102.53 toks/s, output: 13.77 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:07<00:32, 12.72it/s, est. speed input: 14058.47 toks/s, output: 13.73 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:07<00:32, 12.66it/s, est. speed input: 14004.78 toks/s, output: 13.68 toks/s]
Processed prompts:  21%|██        | 106/512 [00:07<00:32, 12.69it/s, est. speed input: 13966.82 toks/s, output: 13.64 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:08<00:31, 12.69it/s, est. speed input: 13928.59 toks/s, output: 13.60 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:08<00:31, 12.66it/s, est. speed input: 13889.89 toks/s, output: 13.56 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:08<00:30, 12.71it/s, est. speed input: 13862.95 toks/s, output: 13.54 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:09<00:30, 12.75it/s, est. speed input: 13837.89 toks/s, output: 13.51 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:09<00:30, 12.67it/s, est. speed input: 13802.55 toks/s, output: 13.48 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:09<00:30, 12.64it/s, est. speed input: 13772.09 toks/s, output: 13.45 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:09<00:29, 12.68it/s, est. speed input: 13749.75 toks/s, output: 13.43 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:10<00:29, 12.71it/s, est. speed input: 13729.40 toks/s, output: 13.41 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:10<00:29, 12.63it/s, est. speed input: 13699.43 toks/s, output: 13.38 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:10<00:28, 12.63it/s, est. speed input: 13677.81 toks/s, output: 13.36 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:11<00:28, 12.64it/s, est. speed input: 13657.51 toks/s, output: 13.34 toks/s]
Processed prompts:  30%|███       | 154/512 [00:11<00:28, 12.56it/s, est. speed input: 13630.26 toks/s, output: 13.31 toks/s]
Processed prompts:  31%|███       | 158/512 [00:11<00:28, 12.63it/s, est. speed input: 13616.04 toks/s, output: 13.30 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:12<00:27, 12.62it/s, est. speed input: 13597.41 toks/s, output: 13.28 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:12<00:27, 12.63it/s, est. speed input: 13581.12 toks/s, output: 13.26 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:12<00:27, 12.62it/s, est. speed input: 13564.19 toks/s, output: 13.25 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:13<00:26, 12.64it/s, est. speed input: 13550.97 toks/s, output: 13.23 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:13<00:26, 12.67it/s, est. speed input: 13538.55 toks/s, output: 13.22 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:13<00:26, 12.62it/s, est. speed input: 13521.37 toks/s, output: 13.20 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:14<00:25, 12.61it/s, est. speed input: 13507.04 toks/s, output: 13.19 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:14<00:25, 12.63it/s, est. speed input: 13495.83 toks/s, output: 13.18 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:14<00:25, 12.60it/s, est. speed input: 13481.59 toks/s, output: 13.17 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:15<00:24, 12.64it/s, est. speed input: 13472.05 toks/s, output: 13.16 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:15<00:24, 12.66it/s, est. speed input: 13462.81 toks/s, output: 13.15 toks/s]
Processed prompts:  40%|████      | 206/512 [00:15<00:24, 12.68it/s, est. speed input: 13454.43 toks/s, output: 13.14 toks/s]
Processed prompts:  41%|████      | 210/512 [00:15<00:23, 12.66it/s, est. speed input: 13443.27 toks/s, output: 13.13 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:16<00:23, 12.64it/s, est. speed input: 13433.15 toks/s, output: 13.12 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:16<00:23, 12.67it/s, est. speed input: 13425.78 toks/s, output: 13.11 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:16<00:22, 12.64it/s, est. speed input: 13415.45 toks/s, output: 13.10 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:17<00:22, 12.67it/s, est. speed input: 13408.43 toks/s, output: 13.09 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:17<00:22, 12.70it/s, est. speed input: 13402.45 toks/s, output: 13.09 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:17<00:22, 12.64it/s, est. speed input: 13391.55 toks/s, output: 13.08 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:18<00:21, 12.65it/s, est. speed input: 13384.80 toks/s, output: 13.07 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:18<00:21, 12.69it/s, est. speed input: 13379.71 toks/s, output: 13.07 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:18<00:20, 12.69it/s, est. speed input: 13373.19 toks/s, output: 13.06 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:19<00:20, 12.65it/s, est. speed input: 13364.39 toks/s, output: 13.05 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:19<00:20, 12.67it/s, est. speed input: 13359.34 toks/s, output: 13.05 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:19<00:20, 12.69it/s, est. speed input: 13353.99 toks/s, output: 13.04 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:20<00:19, 12.63it/s, est. speed input: 13345.28 toks/s, output: 13.03 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:20<00:19, 12.63it/s, est. speed input: 13339.02 toks/s, output: 13.03 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:20<00:19, 12.61it/s, est. speed input: 13331.59 toks/s, output: 13.02 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:21<00:18, 12.58it/s, est. speed input: 13323.83 toks/s, output: 13.01 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:21<00:18, 12.61it/s, est. speed input: 13318.63 toks/s, output: 13.01 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:21<00:18, 12.64it/s, est. speed input: 13314.08 toks/s, output: 13.00 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:22<00:17, 12.65it/s, est. speed input: 13309.48 toks/s, output: 13.00 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:22<00:17, 12.62it/s, est. speed input: 13302.86 toks/s, output: 12.99 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:22<00:17, 12.66it/s, est. speed input: 13299.28 toks/s, output: 12.99 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:22<00:16, 12.69it/s, est. speed input: 13296.32 toks/s, output: 12.98 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:23<00:16, 12.62it/s, est. speed input: 13289.11 toks/s, output: 12.98 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:23<00:16, 12.68it/s, est. speed input: 13286.76 toks/s, output: 12.98 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:23<00:15, 12.69it/s, est. speed input: 13283.38 toks/s, output: 12.97 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:24<00:15, 12.70it/s, est. speed input: 13279.82 toks/s, output: 12.97 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:24<00:15, 12.63it/s, est. speed input: 13273.12 toks/s, output: 12.96 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:24<00:15, 12.63it/s, est. speed input: 13268.95 toks/s, output: 12.96 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:25<00:14, 12.65it/s, est. speed input: 13265.57 toks/s, output: 12.95 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:25<00:14, 12.59it/s, est. speed input: 13259.24 toks/s, output: 12.95 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:25<00:14, 12.67it/s, est. speed input: 13258.06 toks/s, output: 12.95 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:26<00:13, 12.70it/s, est. speed input: 13255.85 toks/s, output: 12.95 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:26<00:12, 13.37it/s, est. speed input: 13279.35 toks/s, output: 12.97 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:26<00:12, 13.12it/s, est. speed input: 13274.53 toks/s, output: 12.96 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:27<00:12, 13.01it/s, est. speed input: 13272.09 toks/s, output: 12.96 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:27<00:12, 12.91it/s, est. speed input: 13268.98 toks/s, output: 12.96 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:27<00:12, 12.81it/s, est. speed input: 13264.47 toks/s, output: 12.95 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:27<00:11, 12.82it/s, est. speed input: 13263.33 toks/s, output: 12.95 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:28<00:11, 12.80it/s, est. speed input: 13260.87 toks/s, output: 12.95 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:28<00:11, 12.72it/s, est. speed input: 13256.32 toks/s, output: 12.95 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:28<00:10, 12.70it/s, est. speed input: 13253.15 toks/s, output: 12.94 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:29<00:10, 12.71it/s, est. speed input: 13250.71 toks/s, output: 12.94 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:29<00:10, 12.71it/s, est. speed input: 13248.23 toks/s, output: 12.94 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:29<00:09, 12.66it/s, est. speed input: 13243.98 toks/s, output: 12.93 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:30<00:09, 12.68it/s, est. speed input: 13241.81 toks/s, output: 12.93 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:30<00:09, 12.71it/s, est. speed input: 13240.27 toks/s, output: 12.93 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:30<00:09, 12.63it/s, est. speed input: 13235.12 toks/s, output: 12.92 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:31<00:08, 12.67it/s, est. speed input: 13233.33 toks/s, output: 12.92 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:31<00:08, 12.71it/s, est. speed input: 13232.21 toks/s, output: 12.92 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:31<00:08, 12.66it/s, est. speed input: 13228.22 toks/s, output: 12.92 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:32<00:07, 12.65it/s, est. speed input: 13225.33 toks/s, output: 12.92 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:32<00:07, 12.70it/s, est. speed input: 13224.24 toks/s, output: 12.91 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:32<00:07, 12.73it/s, est. speed input: 13223.21 toks/s, output: 12.91 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:33<00:06, 12.62it/s, est. speed input: 13217.81 toks/s, output: 12.91 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:33<00:06, 12.65it/s, est. speed input: 13216.07 toks/s, output: 12.91 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:33<00:06, 12.66it/s, est. speed input: 13213.70 toks/s, output: 12.90 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:33<00:05, 12.63it/s, est. speed input: 13210.47 toks/s, output: 12.90 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:34<00:05, 12.65it/s, est. speed input: 13208.57 toks/s, output: 12.90 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:34<00:05, 12.69it/s, est. speed input: 13207.64 toks/s, output: 12.90 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:34<00:04, 13.74it/s, est. speed input: 13236.20 toks/s, output: 12.93 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:35<00:04, 13.31it/s, est. speed input: 13231.11 toks/s, output: 12.92 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:35<00:04, 13.12it/s, est. speed input: 13229.19 toks/s, output: 12.92 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:35<00:03, 13.02it/s, est. speed input: 13228.06 toks/s, output: 12.92 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:36<00:03, 12.84it/s, est. speed input: 13223.65 toks/s, output: 12.91 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:36<00:03, 12.81it/s, est. speed input: 13222.17 toks/s, output: 12.91 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:36<00:02, 12.79it/s, est. speed input: 13220.52 toks/s, output: 12.91 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:37<00:02, 12.79it/s, est. speed input: 13219.58 toks/s, output: 12.91 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:37<00:02, 12.70it/s, est. speed input: 13215.90 toks/s, output: 12.91 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:37<00:02, 12.67it/s, est. speed input: 13213.30 toks/s, output: 12.90 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:37<00:01, 12.71it/s, est. speed input: 13212.50 toks/s, output: 12.90 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:38<00:01, 12.63it/s, est. speed input: 13208.49 toks/s, output: 12.90 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:38<00:01, 12.66it/s, est. speed input: 13207.04 toks/s, output: 12.90 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:38<00:00, 12.66it/s, est. speed input: 13205.07 toks/s, output: 12.90 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:39<00:00, 12.59it/s, est. speed input: 13201.30 toks/s, output: 12.89 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:39<00:00, 13.70it/s, est. speed input: 13227.41 toks/s, output: 12.92 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:39<00:00, 13.70it/s, est. speed input: 13279.25 toks/s, output: 12.97 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:39<00:00, 12.97it/s, est. speed input: 13279.25 toks/s, output: 12.97 toks/s]
[rank0]:[W128 07:34:43.065741168 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 71.4s

测试结果:
  Requests/s:   12.66
  Tokens/s:     12973.02
  Total Reqs:   512
  Elapsed:      40.45s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     12960.36

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:34:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:34:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3641434) WARNING 01-28 07:35:15 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 13.07 requests/s, 13401.05 total tokens/s, 13.07 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 07:34:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:34:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:34:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:34:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:34:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:34:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:34:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:34:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:34:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:34:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:34:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:34:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:34:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:34:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:34:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:34:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:34:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:34:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:34:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:34:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:34:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:34:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:34:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:34:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:34:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:34:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:34:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:34:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3641434) [2026-01-28 07:34:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3641434) [2026-01-28 07:34:56] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3641434) [2026-01-28 07:34:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3641434) [2026-01-28 07:34:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3641434) [2026-01-28 07:34:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3641434) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3641434) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.62s/it]
(EngineCore_DP0 pid=3641434) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.62s/it]
(EngineCore_DP0 pid=3641434) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3641434) 2026-01-28 07:35:14,706 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3641434) 2026-01-28 07:35:14,842 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 61/1024 [00:00<00:01, 609.28it/s]
Adding requests:  12%|█▏        | 122/1024 [00:00<00:01, 559.33it/s]
Adding requests:  17%|█▋        | 179/1024 [00:00<00:01, 496.89it/s]
Adding requests:  22%|██▏       | 230/1024 [00:00<00:01, 494.30it/s]
Adding requests:  27%|██▋       | 280/1024 [00:00<00:01, 484.45it/s]
Adding requests:  32%|███▏      | 329/1024 [00:00<00:01, 485.15it/s]
Adding requests:  37%|███▋      | 378/1024 [00:00<00:01, 486.53it/s]
Adding requests:  42%|████▏     | 427/1024 [00:00<00:01, 479.89it/s]
Adding requests:  46%|████▋     | 476/1024 [00:00<00:01, 472.24it/s]
Adding requests:  51%|█████     | 524/1024 [00:01<00:01, 465.02it/s]
Adding requests:  56%|█████▌    | 571/1024 [00:01<00:01, 451.05it/s]
Adding requests:  61%|██████    | 621/1024 [00:01<00:00, 464.37it/s]
Adding requests:  65%|██████▌   | 668/1024 [00:01<00:00, 465.94it/s]
Adding requests:  70%|███████   | 718/1024 [00:01<00:00, 473.95it/s]
Adding requests:  75%|███████▍  | 766/1024 [00:01<00:00, 468.31it/s]
Adding requests:  79%|███████▉  | 814/1024 [00:01<00:00, 468.58it/s]
Adding requests:  84%|████████▍ | 863/1024 [00:01<00:00, 472.19it/s]
Adding requests:  89%|████████▉ | 913/1024 [00:01<00:00, 477.29it/s]
Adding requests:  94%|█████████▍| 961/1024 [00:02<00:00, 473.78it/s]
Adding requests:  99%|█████████▊| 1009/1024 [00:02<00:00, 465.02it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 476.29it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 26/1024 [00:00<00:19, 50.85it/s, est. speed input: 52067.52 toks/s, output: 50.85 toks/s]
Processed prompts:   3%|▎         | 34/1024 [00:01<00:36, 27.04it/s, est. speed input: 31021.50 toks/s, output: 30.29 toks/s]
Processed prompts:   4%|▍         | 42/1024 [00:01<00:48, 20.43it/s, est. speed input: 24808.48 toks/s, output: 24.23 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:02<00:55, 17.51it/s, est. speed input: 21873.00 toks/s, output: 21.36 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:02<01:00, 15.86it/s, est. speed input: 20099.41 toks/s, output: 19.63 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:03<01:04, 14.90it/s, est. speed input: 18942.91 toks/s, output: 18.50 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:04<01:06, 14.32it/s, est. speed input: 18142.24 toks/s, output: 17.72 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:04<01:07, 13.91it/s, est. speed input: 17528.57 toks/s, output: 17.12 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:05<01:08, 13.65it/s, est. speed input: 17062.99 toks/s, output: 16.66 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:06<01:08, 13.44it/s, est. speed input: 16675.60 toks/s, output: 16.28 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:06<01:08, 13.35it/s, est. speed input: 16379.34 toks/s, output: 16.00 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:07<01:08, 13.23it/s, est. speed input: 16114.56 toks/s, output: 15.74 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:07<01:08, 13.17it/s, est. speed input: 15898.71 toks/s, output: 15.53 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:08<01:07, 13.16it/s, est. speed input: 15723.72 toks/s, output: 15.36 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:09<01:07, 13.13it/s, est. speed input: 15565.20 toks/s, output: 15.20 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:09<01:06, 13.12it/s, est. speed input: 15428.07 toks/s, output: 15.07 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:10<01:06, 13.09it/s, est. speed input: 15304.53 toks/s, output: 14.95 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:10<01:05, 13.10it/s, est. speed input: 15200.52 toks/s, output: 14.84 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:11<01:05, 13.08it/s, est. speed input: 15101.27 toks/s, output: 14.75 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:12<01:04, 13.07it/s, est. speed input: 15013.33 toks/s, output: 14.66 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:12<01:04, 13.09it/s, est. speed input: 14938.47 toks/s, output: 14.59 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:13<01:03, 13.06it/s, est. speed input: 14863.05 toks/s, output: 14.51 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:13<01:02, 13.08it/s, est. speed input: 14800.95 toks/s, output: 14.45 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:14<01:02, 13.04it/s, est. speed input: 14735.76 toks/s, output: 14.39 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:15<01:01, 13.06it/s, est. speed input: 14682.63 toks/s, output: 14.34 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:15<01:01, 13.06it/s, est. speed input: 14632.71 toks/s, output: 14.29 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:16<01:00, 13.05it/s, est. speed input: 14584.51 toks/s, output: 14.24 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:17<00:59, 13.06it/s, est. speed input: 14541.84 toks/s, output: 14.20 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:17<00:59, 13.04it/s, est. speed input: 14498.44 toks/s, output: 14.16 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:18<00:58, 13.07it/s, est. speed input: 14463.73 toks/s, output: 14.12 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:18<00:58, 13.04it/s, est. speed input: 14425.72 toks/s, output: 14.09 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:19<00:57, 13.07it/s, est. speed input: 14394.74 toks/s, output: 14.06 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:20<00:56, 13.06it/s, est. speed input: 14362.68 toks/s, output: 14.03 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:20<00:56, 13.06it/s, est. speed input: 14333.26 toks/s, output: 14.00 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:21<00:55, 13.08it/s, est. speed input: 14307.57 toks/s, output: 13.97 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:21<00:55, 13.04it/s, est. speed input: 14278.48 toks/s, output: 13.94 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:22<00:54, 13.08it/s, est. speed input: 14256.78 toks/s, output: 13.92 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:23<00:53, 13.06it/s, est. speed input: 14232.44 toks/s, output: 13.90 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:23<00:53, 13.08it/s, est. speed input: 14212.44 toks/s, output: 13.88 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:24<00:51, 13.43it/s, est. speed input: 14222.58 toks/s, output: 13.89 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:24<00:50, 13.32it/s, est. speed input: 14202.43 toks/s, output: 13.87 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:25<00:50, 13.25it/s, est. speed input: 14183.26 toks/s, output: 13.85 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:26<00:50, 13.18it/s, est. speed input: 14162.98 toks/s, output: 13.83 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:26<00:49, 13.17it/s, est. speed input: 14146.89 toks/s, output: 13.82 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:27<00:49, 13.10it/s, est. speed input: 14126.61 toks/s, output: 13.80 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:28<00:48, 13.12it/s, est. speed input: 14112.44 toks/s, output: 13.78 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:28<00:48, 13.10it/s, est. speed input: 14096.41 toks/s, output: 13.77 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:29<00:47, 13.08it/s, est. speed input: 14081.11 toks/s, output: 13.75 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:29<00:46, 13.09it/s, est. speed input: 14067.55 toks/s, output: 13.74 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:30<00:46, 13.06it/s, est. speed input: 14051.78 toks/s, output: 13.72 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:31<00:45, 13.09it/s, est. speed input: 14040.46 toks/s, output: 13.71 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:31<00:45, 13.06it/s, est. speed input: 14026.28 toks/s, output: 13.70 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:32<00:44, 13.06it/s, est. speed input: 14013.94 toks/s, output: 13.69 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:32<00:42, 13.50it/s, est. speed input: 14030.66 toks/s, output: 13.70 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:33<00:42, 13.39it/s, est. speed input: 14020.25 toks/s, output: 13.69 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:34<00:42, 13.27it/s, est. speed input: 14007.28 toks/s, output: 13.68 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:34<00:41, 13.20it/s, est. speed input: 13995.48 toks/s, output: 13.67 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:35<00:41, 13.19it/s, est. speed input: 13986.72 toks/s, output: 13.66 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:35<00:40, 13.14it/s, est. speed input: 13975.55 toks/s, output: 13.65 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:36<00:40, 13.14it/s, est. speed input: 13966.92 toks/s, output: 13.64 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:37<00:39, 13.11it/s, est. speed input: 13956.97 toks/s, output: 13.63 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:37<00:38, 13.12it/s, est. speed input: 13948.84 toks/s, output: 13.62 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:38<00:38, 13.09it/s, est. speed input: 13938.71 toks/s, output: 13.61 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:38<00:37, 13.06it/s, est. speed input: 13928.71 toks/s, output: 13.60 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:39<00:37, 13.07it/s, est. speed input: 13920.59 toks/s, output: 13.59 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:40<00:36, 13.06it/s, est. speed input: 13911.94 toks/s, output: 13.59 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:40<00:35, 13.06it/s, est. speed input: 13904.10 toks/s, output: 13.58 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:41<00:35, 13.05it/s, est. speed input: 13895.40 toks/s, output: 13.57 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:42<00:34, 13.07it/s, est. speed input: 13888.54 toks/s, output: 13.56 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:42<00:34, 13.05it/s, est. speed input: 13880.67 toks/s, output: 13.56 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:43<00:33, 13.04it/s, est. speed input: 13872.48 toks/s, output: 13.55 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:43<00:32, 13.06it/s, est. speed input: 13866.18 toks/s, output: 13.54 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:44<00:32, 13.03it/s, est. speed input: 13858.27 toks/s, output: 13.53 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:45<00:31, 13.06it/s, est. speed input: 13852.37 toks/s, output: 13.53 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:45<00:31, 13.03it/s, est. speed input: 13844.48 toks/s, output: 13.52 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:46<00:30, 13.05it/s, est. speed input: 13838.77 toks/s, output: 13.51 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:46<00:29, 13.02it/s, est. speed input: 13831.42 toks/s, output: 13.51 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:47<00:29, 13.01it/s, est. speed input: 13824.28 toks/s, output: 13.50 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:48<00:28, 13.04it/s, est. speed input: 13819.34 toks/s, output: 13.50 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:48<00:28, 13.03it/s, est. speed input: 13812.84 toks/s, output: 13.49 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:49<00:27, 13.06it/s, est. speed input: 13808.50 toks/s, output: 13.48 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:50<00:26, 13.04it/s, est. speed input: 13802.20 toks/s, output: 13.48 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:50<00:26, 13.07it/s, est. speed input: 13798.07 toks/s, output: 13.47 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:51<00:25, 13.04it/s, est. speed input: 13791.98 toks/s, output: 13.47 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:51<00:25, 13.02it/s, est. speed input: 13785.89 toks/s, output: 13.46 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:52<00:24, 13.05it/s, est. speed input: 13781.76 toks/s, output: 13.46 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:53<00:23, 13.03it/s, est. speed input: 13776.03 toks/s, output: 13.45 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:53<00:23, 13.06it/s, est. speed input: 13772.48 toks/s, output: 13.45 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:54<00:22, 13.04it/s, est. speed input: 13766.94 toks/s, output: 13.44 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:54<00:21, 13.06it/s, est. speed input: 13763.35 toks/s, output: 13.44 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:55<00:21, 13.04it/s, est. speed input: 13758.08 toks/s, output: 13.44 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:56<00:20, 13.03it/s, est. speed input: 13753.53 toks/s, output: 13.43 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:56<00:20, 13.05it/s, est. speed input: 13749.72 toks/s, output: 13.43 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:57<00:19, 13.00it/s, est. speed input: 13743.85 toks/s, output: 13.42 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:57<00:18, 13.04it/s, est. speed input: 13740.57 toks/s, output: 13.42 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:58<00:18, 13.03it/s, est. speed input: 13736.30 toks/s, output: 13.41 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:59<00:17, 13.06it/s, est. speed input: 13733.37 toks/s, output: 13.41 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:59<00:16, 13.07it/s, est. speed input: 13729.93 toks/s, output: 13.41 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [01:00<00:16, 13.03it/s, est. speed input: 13725.18 toks/s, output: 13.40 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [01:01<00:15, 13.04it/s, est. speed input: 13721.58 toks/s, output: 13.40 toks/s]
Processed prompts:  81%|████████  | 826/1024 [01:01<00:15, 13.00it/s, est. speed input: 13716.67 toks/s, output: 13.40 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [01:02<00:14, 13.02it/s, est. speed input: 13713.41 toks/s, output: 13.39 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [01:02<00:13, 13.01it/s, est. speed input: 13709.30 toks/s, output: 13.39 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [01:03<00:13, 13.04it/s, est. speed input: 13706.37 toks/s, output: 13.39 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [01:04<00:12, 13.03it/s, est. speed input: 13702.91 toks/s, output: 13.38 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [01:04<00:12, 13.02it/s, est. speed input: 13699.18 toks/s, output: 13.38 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [01:05<00:11, 13.04it/s, est. speed input: 13696.39 toks/s, output: 13.38 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [01:05<00:10, 13.03it/s, est. speed input: 13692.70 toks/s, output: 13.37 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [01:06<00:10, 13.07it/s, est. speed input: 13690.75 toks/s, output: 13.37 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [01:07<00:09, 13.05it/s, est. speed input: 13687.47 toks/s, output: 13.37 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [01:07<00:09, 13.07it/s, est. speed input: 13684.97 toks/s, output: 13.36 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [01:08<00:08, 13.05it/s, est. speed input: 13681.60 toks/s, output: 13.36 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [01:09<00:07, 13.03it/s, est. speed input: 13678.38 toks/s, output: 13.36 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [01:09<00:07, 13.04it/s, est. speed input: 13675.71 toks/s, output: 13.36 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [01:10<00:06, 13.47it/s, est. speed input: 13685.67 toks/s, output: 13.36 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [01:10<00:05, 13.38it/s, est. speed input: 13683.86 toks/s, output: 13.36 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [01:11<00:05, 13.26it/s, est. speed input: 13680.49 toks/s, output: 13.36 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [01:12<00:04, 13.21it/s, est. speed input: 13678.34 toks/s, output: 13.36 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [01:12<00:04, 13.15it/s, est. speed input: 13675.23 toks/s, output: 13.35 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [01:13<00:03, 13.11it/s, est. speed input: 13672.32 toks/s, output: 13.35 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [01:13<00:02, 13.57it/s, est. speed input: 13683.05 toks/s, output: 13.36 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [01:14<00:02, 13.38it/s, est. speed input: 13679.68 toks/s, output: 13.36 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [01:15<00:01, 13.31it/s, est. speed input: 13677.94 toks/s, output: 13.36 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [01:15<00:01, 13.21it/s, est. speed input: 13674.88 toks/s, output: 13.35 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [01:16<00:00, 13.65it/s, est. speed input: 13685.43 toks/s, output: 13.36 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:16<00:00, 13.65it/s, est. speed input: 13766.06 toks/s, output: 13.44 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:16<00:00, 13.44it/s, est. speed input: 13766.06 toks/s, output: 13.44 toks/s]
[rank0]:[W128 07:36:34.462924175 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 111.4s

测试结果:
  Requests/s:   13.07
  Tokens/s:     13401.05
  Total Reqs:   1024
  Elapsed:      78.32s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     13387.98

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:36:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:36:46 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3643200) WARNING 01-28 07:37:11 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 13.29 requests/s, 13625.21 total tokens/s, 13.29 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 07:36:46] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:36:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:36:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:36:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:36:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:36:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:36:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:36:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:36:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:36:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:36:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:36:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:36:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:36:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:36:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:36:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:36:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:36:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:36:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:36:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:36:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:36:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:36:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:36:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:36:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:36:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:36:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:36:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3643200) [2026-01-28 07:36:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3643200) [2026-01-28 07:36:51] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3643200) [2026-01-28 07:36:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3643200) [2026-01-28 07:36:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3643200) [2026-01-28 07:36:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3643200) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3643200) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.68s/it]
(EngineCore_DP0 pid=3643200) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.68s/it]
(EngineCore_DP0 pid=3643200) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3643200) 2026-01-28 07:37:09,824 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3643200) 2026-01-28 07:37:09,983 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 60/2048 [00:00<00:03, 599.09it/s]
Adding requests:   6%|▌         | 120/2048 [00:00<00:03, 550.09it/s]
Adding requests:   9%|▊         | 176/2048 [00:00<00:04, 460.58it/s]
Adding requests:  11%|█         | 224/2048 [00:00<00:04, 450.41it/s]
Adding requests:  13%|█▎        | 271/2048 [00:00<00:03, 454.93it/s]
Adding requests:  16%|█▌        | 318/2048 [00:00<00:03, 444.72it/s]
Adding requests:  18%|█▊        | 363/2048 [00:00<00:03, 444.71it/s]
Adding requests:  20%|█▉        | 408/2048 [00:00<00:03, 441.98it/s]
Adding requests:  22%|██▏       | 453/2048 [00:00<00:03, 442.45it/s]
Adding requests:  24%|██▍       | 498/2048 [00:01<00:03, 435.94it/s]
Adding requests:  26%|██▋       | 542/2048 [00:01<00:03, 432.17it/s]
Adding requests:  29%|██▊       | 588/2048 [00:01<00:03, 437.57it/s]
Adding requests:  31%|███       | 633/2048 [00:01<00:03, 439.87it/s]
Adding requests:  33%|███▎      | 679/2048 [00:01<00:03, 442.38it/s]
Adding requests:  35%|███▌      | 726/2048 [00:01<00:02, 447.85it/s]
Adding requests:  38%|███▊      | 771/2048 [00:01<00:03, 424.48it/s]
Adding requests:  40%|███▉      | 814/2048 [00:01<00:02, 420.25it/s]
Adding requests:  42%|████▏     | 857/2048 [00:02<00:09, 129.86it/s]
Adding requests:  44%|████▍     | 898/2048 [00:02<00:07, 160.89it/s]
Adding requests:  46%|████▌     | 940/2048 [00:02<00:05, 196.44it/s]
Adding requests:  48%|████▊     | 985/2048 [00:03<00:04, 237.86it/s]
Adding requests:  50%|█████     | 1031/2048 [00:03<00:03, 279.55it/s]
Adding requests:  52%|█████▏    | 1073/2048 [00:03<00:03, 309.01it/s]
Adding requests:  55%|█████▍    | 1119/2048 [00:03<00:02, 343.08it/s]
Adding requests:  57%|█████▋    | 1164/2048 [00:03<00:02, 369.19it/s]
Adding requests:  59%|█████▉    | 1209/2048 [00:03<00:02, 389.64it/s]
Adding requests:  61%|██████    | 1253/2048 [00:03<00:01, 398.86it/s]
Adding requests:  63%|██████▎   | 1297/2048 [00:03<00:01, 404.88it/s]
Adding requests:  66%|██████▌   | 1343/2048 [00:03<00:01, 419.04it/s]
Adding requests:  68%|██████▊   | 1387/2048 [00:03<00:01, 420.73it/s]
Adding requests:  70%|██████▉   | 1431/2048 [00:04<00:01, 426.13it/s]
Adding requests:  72%|███████▏  | 1476/2048 [00:04<00:01, 430.87it/s]
Adding requests:  74%|███████▍  | 1521/2048 [00:04<00:01, 434.37it/s]
Adding requests:  76%|███████▋  | 1566/2048 [00:04<00:01, 437.51it/s]
Adding requests:  79%|███████▉  | 1613/2048 [00:04<00:00, 446.89it/s]
Adding requests:  81%|████████  | 1658/2048 [00:04<00:00, 441.22it/s]
Adding requests:  83%|████████▎ | 1704/2048 [00:04<00:00, 446.27it/s]
Adding requests:  85%|████████▌ | 1749/2048 [00:04<00:00, 440.45it/s]
Adding requests:  88%|████████▊ | 1796/2048 [00:04<00:00, 448.50it/s]
Adding requests:  90%|████████▉ | 1841/2048 [00:04<00:00, 448.93it/s]
Adding requests:  92%|█████████▏| 1887/2048 [00:05<00:00, 451.91it/s]
Adding requests:  94%|█████████▍| 1933/2048 [00:05<00:00, 444.79it/s]
Adding requests:  97%|█████████▋| 1978/2048 [00:05<00:00, 442.01it/s]
Adding requests:  99%|█████████▉| 2023/2048 [00:05<00:00, 420.43it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 376.11it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 66/2048 [00:00<00:24, 80.27it/s, est. speed input: 82205.67 toks/s, output: 80.28 toks/s]
Processed prompts:   4%|▍         | 82/2048 [00:02<00:56, 34.80it/s, est. speed input: 41287.34 toks/s, output: 40.32 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:03<01:20, 24.28it/s, est. speed input: 31017.34 toks/s, output: 30.29 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:04<01:37, 19.78it/s, est. speed input: 26324.27 toks/s, output: 25.71 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:05<01:50, 17.36it/s, est. speed input: 23599.75 toks/s, output: 23.05 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:06<01:59, 15.94it/s, est. speed input: 21832.35 toks/s, output: 21.32 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:08<02:05, 15.05it/s, est. speed input: 20597.42 toks/s, output: 20.11 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:09<02:09, 14.45it/s, est. speed input: 19670.31 toks/s, output: 19.21 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:10<02:11, 14.08it/s, est. speed input: 18968.56 toks/s, output: 18.52 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:11<02:12, 13.84it/s, est. speed input: 18418.61 toks/s, output: 17.99 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:12<02:13, 13.65it/s, est. speed input: 17960.58 toks/s, output: 17.54 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:14<02:13, 13.53it/s, est. speed input: 17583.73 toks/s, output: 17.17 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:15<02:13, 13.45it/s, est. speed input: 17268.43 toks/s, output: 16.86 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:16<02:12, 13.38it/s, est. speed input: 16994.56 toks/s, output: 16.60 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:17<02:11, 13.34it/s, est. speed input: 16761.86 toks/s, output: 16.37 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:18<02:11, 13.29it/s, est. speed input: 16551.36 toks/s, output: 16.16 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:20<02:09, 13.29it/s, est. speed input: 16374.55 toks/s, output: 15.99 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:21<02:06, 13.49it/s, est. speed input: 16264.39 toks/s, output: 15.88 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:22<02:06, 13.40it/s, est. speed input: 16116.57 toks/s, output: 15.74 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:23<02:05, 13.38it/s, est. speed input: 15991.25 toks/s, output: 15.62 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:24<02:04, 13.35it/s, est. speed input: 15876.45 toks/s, output: 15.50 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:26<02:03, 13.31it/s, est. speed input: 15767.49 toks/s, output: 15.40 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:27<02:02, 13.30it/s, est. speed input: 15670.92 toks/s, output: 15.30 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:28<02:01, 13.27it/s, est. speed input: 15579.91 toks/s, output: 15.21 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:29<01:58, 13.51it/s, est. speed input: 15536.15 toks/s, output: 15.17 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:30<01:57, 13.41it/s, est. speed input: 15456.18 toks/s, output: 15.09 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:32<01:57, 13.37it/s, est. speed input: 15386.11 toks/s, output: 15.03 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:33<01:56, 13.32it/s, est. speed input: 15317.81 toks/s, output: 14.96 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:34<01:55, 13.31it/s, est. speed input: 15258.58 toks/s, output: 14.90 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:35<01:54, 13.28it/s, est. speed input: 15200.34 toks/s, output: 14.84 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:36<01:53, 13.27it/s, est. speed input: 15146.85 toks/s, output: 14.79 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:38<01:52, 13.26it/s, est. speed input: 15095.90 toks/s, output: 14.74 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:39<01:51, 13.23it/s, est. speed input: 15046.51 toks/s, output: 14.69 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:40<01:49, 13.25it/s, est. speed input: 15003.29 toks/s, output: 14.65 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:41<01:48, 13.24it/s, est. speed input: 14960.87 toks/s, output: 14.61 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:42<01:47, 13.25it/s, est. speed input: 14922.70 toks/s, output: 14.57 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:44<01:46, 13.24it/s, est. speed input: 14884.49 toks/s, output: 14.54 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:45<01:44, 13.25it/s, est. speed input: 14849.97 toks/s, output: 14.50 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:46<01:43, 13.26it/s, est. speed input: 14817.58 toks/s, output: 14.47 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:47<01:42, 13.23it/s, est. speed input: 14783.96 toks/s, output: 14.44 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:49<01:41, 13.21it/s, est. speed input: 14751.03 toks/s, output: 14.41 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:50<01:40, 13.22it/s, est. speed input: 14723.03 toks/s, output: 14.38 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:51<01:39, 13.23it/s, est. speed input: 14695.35 toks/s, output: 14.35 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:52<01:37, 13.22it/s, est. speed input: 14668.79 toks/s, output: 14.32 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:53<01:36, 13.24it/s, est. speed input: 14644.76 toks/s, output: 14.30 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:55<01:35, 13.24it/s, est. speed input: 14620.50 toks/s, output: 14.28 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:56<01:34, 13.23it/s, est. speed input: 14596.78 toks/s, output: 14.25 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:57<01:32, 13.24it/s, est. speed input: 14575.56 toks/s, output: 14.23 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:58<01:31, 13.25it/s, est. speed input: 14555.30 toks/s, output: 14.21 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:59<01:30, 13.24it/s, est. speed input: 14534.54 toks/s, output: 14.19 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [01:01<01:29, 13.25it/s, est. speed input: 14515.73 toks/s, output: 14.18 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [01:02<01:27, 13.25it/s, est. speed input: 14497.76 toks/s, output: 14.16 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [01:03<01:26, 13.24it/s, est. speed input: 14479.29 toks/s, output: 14.14 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [01:04<01:25, 13.24it/s, est. speed input: 14461.96 toks/s, output: 14.12 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [01:05<01:22, 13.49it/s, est. speed input: 14461.61 toks/s, output: 14.12 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [01:07<01:22, 13.41it/s, est. speed input: 14445.29 toks/s, output: 14.11 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [01:08<01:21, 13.35it/s, est. speed input: 14428.97 toks/s, output: 14.09 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [01:09<01:18, 13.55it/s, est. speed input: 14428.41 toks/s, output: 14.09 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [01:10<01:18, 13.45it/s, est. speed input: 14413.10 toks/s, output: 14.08 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [01:11<01:17, 13.38it/s, est. speed input: 14398.56 toks/s, output: 14.06 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [01:13<01:16, 13.34it/s, est. speed input: 14384.84 toks/s, output: 14.05 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [01:14<01:15, 13.30it/s, est. speed input: 14370.44 toks/s, output: 14.03 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [01:15<01:14, 13.27it/s, est. speed input: 14356.97 toks/s, output: 14.02 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [01:16<01:13, 13.26it/s, est. speed input: 14344.40 toks/s, output: 14.01 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [01:17<01:12, 13.25it/s, est. speed input: 14331.78 toks/s, output: 14.00 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [01:19<01:11, 13.24it/s, est. speed input: 14319.63 toks/s, output: 13.98 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [01:20<01:10, 13.21it/s, est. speed input: 14306.77 toks/s, output: 13.97 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [01:21<01:08, 13.22it/s, est. speed input: 14295.70 toks/s, output: 13.96 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [01:22<01:06, 13.46it/s, est. speed input: 14297.11 toks/s, output: 13.96 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [01:23<01:05, 13.41it/s, est. speed input: 14287.15 toks/s, output: 13.95 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [01:25<01:04, 13.35it/s, est. speed input: 14276.27 toks/s, output: 13.94 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [01:26<01:03, 13.31it/s, est. speed input: 14266.16 toks/s, output: 13.93 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [01:27<01:02, 13.28it/s, est. speed input: 14255.65 toks/s, output: 13.92 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [01:28<01:01, 13.28it/s, est. speed input: 14246.59 toks/s, output: 13.91 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [01:29<01:00, 13.26it/s, est. speed input: 14237.04 toks/s, output: 13.90 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [01:31<00:57, 13.49it/s, est. speed input: 14238.88 toks/s, output: 13.91 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [01:32<00:57, 13.40it/s, est. speed input: 14229.46 toks/s, output: 13.90 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [01:33<00:55, 13.59it/s, est. speed input: 14231.44 toks/s, output: 13.90 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [01:34<00:54, 13.48it/s, est. speed input: 14222.76 toks/s, output: 13.89 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [01:35<00:53, 13.40it/s, est. speed input: 14214.05 toks/s, output: 13.88 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [01:37<00:52, 13.36it/s, est. speed input: 14206.19 toks/s, output: 13.87 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [01:38<00:51, 13.33it/s, est. speed input: 14198.40 toks/s, output: 13.87 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [01:39<00:50, 13.30it/s, est. speed input: 14190.61 toks/s, output: 13.86 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [01:40<00:49, 13.28it/s, est. speed input: 14182.81 toks/s, output: 13.85 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [01:41<00:48, 13.25it/s, est. speed input: 14174.77 toks/s, output: 13.84 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [01:43<00:47, 13.23it/s, est. speed input: 14166.92 toks/s, output: 13.83 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [01:44<00:45, 13.25it/s, est. speed input: 14160.54 toks/s, output: 13.83 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [01:45<00:44, 13.24it/s, est. speed input: 14153.49 toks/s, output: 13.82 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [01:46<00:43, 13.23it/s, est. speed input: 14146.12 toks/s, output: 13.81 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [01:47<00:42, 13.23it/s, est. speed input: 14139.37 toks/s, output: 13.81 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [01:49<00:40, 13.24it/s, est. speed input: 14133.22 toks/s, output: 13.80 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [01:50<00:39, 13.23it/s, est. speed input: 14126.38 toks/s, output: 13.80 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [01:51<00:38, 13.22it/s, est. speed input: 14119.86 toks/s, output: 13.79 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [01:52<00:37, 13.21it/s, est. speed input: 14113.29 toks/s, output: 13.78 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [01:53<00:36, 13.22it/s, est. speed input: 14107.51 toks/s, output: 13.78 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [01:55<00:34, 13.46it/s, est. speed input: 14110.21 toks/s, output: 13.78 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [01:56<00:33, 13.39it/s, est. speed input: 14104.51 toks/s, output: 13.77 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [01:57<00:32, 13.35it/s, est. speed input: 14099.04 toks/s, output: 13.77 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [01:58<00:31, 13.33it/s, est. speed input: 14093.83 toks/s, output: 13.76 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [01:59<00:29, 13.54it/s, est. speed input: 14096.77 toks/s, output: 13.77 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [02:01<00:28, 13.44it/s, est. speed input: 14091.08 toks/s, output: 13.76 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [02:02<00:27, 13.36it/s, est. speed input: 14085.22 toks/s, output: 13.76 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [02:03<00:26, 13.31it/s, est. speed input: 14079.44 toks/s, output: 13.75 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [02:04<00:25, 13.29it/s, est. speed input: 14074.53 toks/s, output: 13.74 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [02:05<00:23, 13.27it/s, est. speed input: 14069.54 toks/s, output: 13.74 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [02:07<00:22, 13.27it/s, est. speed input: 14065.01 toks/s, output: 13.74 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [02:08<00:21, 13.25it/s, est. speed input: 14059.67 toks/s, output: 13.73 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [02:09<00:20, 13.24it/s, est. speed input: 14054.90 toks/s, output: 13.73 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [02:10<00:19, 13.23it/s, est. speed input: 14049.96 toks/s, output: 13.72 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [02:11<00:18, 13.20it/s, est. speed input: 14044.50 toks/s, output: 13.72 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [02:13<00:16, 13.21it/s, est. speed input: 14039.84 toks/s, output: 13.71 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [02:14<00:15, 13.22it/s, est. speed input: 14035.71 toks/s, output: 13.71 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [02:15<00:14, 13.22it/s, est. speed input: 14031.20 toks/s, output: 13.70 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [02:16<00:12, 13.45it/s, est. speed input: 14034.02 toks/s, output: 13.71 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [02:17<00:11, 13.38it/s, est. speed input: 14029.70 toks/s, output: 13.70 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [02:19<00:10, 13.34it/s, est. speed input: 14025.66 toks/s, output: 13.70 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [02:20<00:09, 13.30it/s, est. speed input: 14021.21 toks/s, output: 13.69 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [02:21<00:08, 13.29it/s, est. speed input: 14017.42 toks/s, output: 13.69 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [02:22<00:06, 13.50it/s, est. speed input: 14020.20 toks/s, output: 13.69 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [02:23<00:05, 13.40it/s, est. speed input: 14015.68 toks/s, output: 13.69 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [02:25<00:04, 13.59it/s, est. speed input: 14018.69 toks/s, output: 13.69 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [02:26<00:03, 13.48it/s, est. speed input: 14014.78 toks/s, output: 13.69 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [02:27<00:02, 13.40it/s, est. speed input: 14010.90 toks/s, output: 13.68 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [02:28<00:01, 13.61it/s, est. speed input: 14014.34 toks/s, output: 13.69 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:28<00:00, 13.61it/s, est. speed input: 14110.79 toks/s, output: 13.78 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:28<00:00, 13.78it/s, est. speed input: 14110.79 toks/s, output: 13.78 toks/s]
[rank0]:[W128 07:39:46.227554749 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 191.8s

测试结果:
  Requests/s:   13.29
  Tokens/s:     13625.21
  Total Reqs:   2048
  Elapsed:      154.07s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     13611.92

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:40:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:40:04 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3646116) WARNING 01-28 07:40:32 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 13.32 requests/s, 13655.03 total tokens/s, 13.32 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 07:40:04] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:40:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:40:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:40:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:40:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:40:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:40:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:40:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:40:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:40:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:40:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:40:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:40:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:40:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:40:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:40:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:40:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:40:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:40:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:40:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:40:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:40:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:40:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:40:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:40:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:40:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:40:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:40:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3646116) [2026-01-28 07:40:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3646116) [2026-01-28 07:40:09] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3646116) [2026-01-28 07:40:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3646116) [2026-01-28 07:40:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3646116) [2026-01-28 07:40:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3646116) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3646116) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.69s/it]
(EngineCore_DP0 pid=3646116) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.69s/it]
(EngineCore_DP0 pid=3646116) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3646116) 2026-01-28 07:40:29,336 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3646116) 2026-01-28 07:40:29,814 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 67/4096 [00:00<00:06, 665.92it/s]
Adding requests:   3%|▎         | 134/4096 [00:00<00:06, 596.38it/s]
Adding requests:   5%|▍         | 195/4096 [00:00<00:07, 533.11it/s]
Adding requests:   6%|▌         | 250/4096 [00:00<00:07, 502.37it/s]
Adding requests:   7%|▋         | 301/4096 [00:00<00:07, 484.49it/s]
Adding requests:   9%|▊         | 350/4096 [00:00<00:07, 477.88it/s]
Adding requests:  10%|▉         | 398/4096 [00:00<00:07, 468.18it/s]
Adding requests:  11%|█         | 445/4096 [00:00<00:08, 453.75it/s]
Adding requests:  12%|█▏        | 491/4096 [00:01<00:07, 452.59it/s]
Adding requests:  13%|█▎        | 537/4096 [00:01<00:08, 441.42it/s]
Adding requests:  14%|█▍        | 585/4096 [00:01<00:07, 450.88it/s]
Adding requests:  15%|█▌        | 633/4096 [00:01<00:07, 457.50it/s]
Adding requests:  17%|█▋        | 683/4096 [00:01<00:07, 467.02it/s]
Adding requests:  18%|█▊        | 730/4096 [00:01<00:07, 458.25it/s]
Adding requests:  19%|█▉        | 776/4096 [00:01<00:07, 450.96it/s]
Adding requests:  20%|██        | 822/4096 [00:01<00:07, 444.38it/s]
Adding requests:  21%|██        | 868/4096 [00:01<00:07, 447.82it/s]
Adding requests:  22%|██▏       | 915/4096 [00:01<00:07, 454.24it/s]
Adding requests:  24%|██▎       | 963/4096 [00:02<00:06, 459.97it/s]
Adding requests:  25%|██▍       | 1010/4096 [00:02<00:06, 460.46it/s]
Adding requests:  26%|██▌       | 1057/4096 [00:02<00:06, 460.00it/s]
Adding requests:  27%|██▋       | 1104/4096 [00:02<00:07, 426.30it/s]
Adding requests:  28%|██▊       | 1150/4096 [00:02<00:06, 433.02it/s]
Adding requests:  29%|██▉       | 1198/4096 [00:02<00:06, 443.25it/s]
Adding requests:  30%|███       | 1245/4096 [00:02<00:06, 450.71it/s]
Adding requests:  32%|███▏      | 1291/4096 [00:02<00:06, 440.03it/s]
Adding requests:  33%|███▎      | 1336/4096 [00:02<00:06, 438.33it/s]
Adding requests:  34%|███▎      | 1380/4096 [00:03<00:06, 437.42it/s]
Adding requests:  35%|███▍      | 1427/4096 [00:03<00:05, 445.51it/s]
Adding requests:  36%|███▌      | 1474/4096 [00:03<00:05, 450.74it/s]
Adding requests:  37%|███▋      | 1522/4096 [00:03<00:05, 456.97it/s]
Adding requests:  38%|███▊      | 1571/4096 [00:03<00:05, 464.66it/s]
Adding requests:  40%|███▉      | 1618/4096 [00:03<00:05, 457.91it/s]
Adding requests:  41%|████      | 1666/4096 [00:03<00:05, 462.63it/s]
Adding requests:  42%|████▏     | 1713/4096 [00:03<00:05, 464.25it/s]
Adding requests:  43%|████▎     | 1761/4096 [00:03<00:04, 467.34it/s]
Adding requests:  44%|████▍     | 1808/4096 [00:03<00:04, 466.18it/s]
Adding requests:  45%|████▌     | 1857/4096 [00:04<00:04, 469.81it/s]
Adding requests:  46%|████▋     | 1904/4096 [00:04<00:04, 464.27it/s]
Adding requests:  48%|████▊     | 1951/4096 [00:04<00:04, 462.51it/s]
Adding requests:  49%|████▉     | 1998/4096 [00:04<00:04, 456.95it/s]
Adding requests:  50%|████▉     | 2044/4096 [00:04<00:04, 454.83it/s]
Adding requests:  51%|█████     | 2093/4096 [00:04<00:04, 463.90it/s]
Adding requests:  52%|█████▏    | 2140/4096 [00:04<00:04, 459.84it/s]
Adding requests:  53%|█████▎    | 2187/4096 [00:04<00:04, 460.00it/s]
Adding requests:  55%|█████▍    | 2235/4096 [00:04<00:04, 464.43it/s]
Adding requests:  56%|█████▌    | 2282/4096 [00:04<00:03, 461.99it/s]
Adding requests:  57%|█████▋    | 2329/4096 [00:05<00:04, 428.32it/s]
Adding requests:  58%|█████▊    | 2375/4096 [00:05<00:03, 435.31it/s]
Adding requests:  59%|█████▉    | 2420/4096 [00:05<00:03, 438.07it/s]
Adding requests:  60%|██████    | 2466/4096 [00:05<00:03, 444.02it/s]
Adding requests:  61%|██████▏   | 2512/4096 [00:05<00:03, 446.89it/s]
Adding requests:  63%|██████▎   | 2561/4096 [00:05<00:03, 458.28it/s]
Adding requests:  64%|██████▎   | 2608/4096 [00:05<00:03, 458.55it/s]
Adding requests:  65%|██████▍   | 2658/4096 [00:05<00:03, 468.51it/s]
Adding requests:  66%|██████▌   | 2705/4096 [00:05<00:03, 462.74it/s]
Adding requests:  67%|██████▋   | 2753/4096 [00:05<00:02, 466.10it/s]
Adding requests:  68%|██████▊   | 2800/4096 [00:06<00:02, 460.23it/s]
Adding requests:  70%|██████▉   | 2849/4096 [00:06<00:02, 466.22it/s]
Adding requests:  71%|███████   | 2896/4096 [00:06<00:02, 464.76it/s]
Adding requests:  72%|███████▏  | 2943/4096 [00:06<00:02, 453.69it/s]
Adding requests:  73%|███████▎  | 2989/4096 [00:06<00:02, 454.88it/s]
Adding requests:  74%|███████▍  | 3035/4096 [00:06<00:02, 448.83it/s]
Adding requests:  75%|███████▌  | 3081/4096 [00:06<00:02, 451.77it/s]
Adding requests:  76%|███████▋  | 3127/4096 [00:06<00:02, 452.38it/s]
Adding requests:  77%|███████▋  | 3173/4096 [00:06<00:02, 454.13it/s]
Adding requests:  79%|███████▊  | 3219/4096 [00:07<00:01, 455.47it/s]
Adding requests:  80%|███████▉  | 3267/4096 [00:07<00:01, 462.30it/s]
Adding requests:  81%|████████  | 3315/4096 [00:07<00:01, 466.27it/s]
Adding requests:  82%|████████▏ | 3362/4096 [00:07<00:01, 458.33it/s]
Adding requests:  83%|████████▎ | 3408/4096 [00:07<00:01, 458.54it/s]
Adding requests:  84%|████████▍ | 3456/4096 [00:07<00:01, 464.36it/s]
Adding requests:  86%|████████▌ | 3503/4096 [00:07<00:01, 455.44it/s]
Adding requests:  87%|████████▋ | 3552/4096 [00:07<00:01, 463.34it/s]
Adding requests:  88%|████████▊ | 3599/4096 [00:07<00:01, 456.86it/s]
Adding requests:  89%|████████▉ | 3647/4096 [00:07<00:00, 461.21it/s]
Adding requests:  90%|█████████ | 3694/4096 [00:08<00:00, 431.54it/s]
Adding requests:  91%|█████████▏| 3741/4096 [00:08<00:00, 440.37it/s]
Adding requests:  93%|█████████▎| 3790/4096 [00:08<00:00, 454.44it/s]
Adding requests:  94%|█████████▎| 3837/4096 [00:08<00:00, 457.12it/s]
Adding requests:  95%|█████████▍| 3887/4096 [00:08<00:00, 466.90it/s]
Adding requests:  96%|█████████▌| 3934/4096 [00:08<00:00, 465.64it/s]
Adding requests:  97%|█████████▋| 3984/4096 [00:08<00:00, 474.39it/s]
Adding requests:  98%|█████████▊| 4032/4096 [00:08<00:00, 470.49it/s]
Adding requests: 100%|█████████▉| 4080/4096 [00:08<00:00, 467.72it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 459.38it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 98/4096 [00:01<00:42, 94.34it/s, est. speed input: 96613.24 toks/s, output: 94.35 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:03<02:03, 32.02it/s, est. speed input: 38552.27 toks/s, output: 37.65 toks/s]
Processed prompts:   4%|▍         | 162/4096 [00:05<02:57, 22.17it/s, est. speed input: 28239.33 toks/s, output: 27.58 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:08<03:32, 18.37it/s, est. speed input: 23976.21 toks/s, output: 23.41 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:10<03:55, 16.40it/s, est. speed input: 21606.31 toks/s, output: 21.10 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:13<04:11, 15.28it/s, est. speed input: 20117.17 toks/s, output: 19.65 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:15<04:20, 14.60it/s, est. speed input: 19101.22 toks/s, output: 18.65 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:17<04:24, 14.28it/s, est. speed input: 18417.93 toks/s, output: 17.99 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:20<04:28, 13.96it/s, est. speed input: 17843.45 toks/s, output: 17.43 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:22<04:30, 13.72it/s, est. speed input: 17383.18 toks/s, output: 16.98 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:25<04:30, 13.58it/s, est. speed input: 17016.80 toks/s, output: 16.62 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:27<04:28, 13.59it/s, est. speed input: 16755.55 toks/s, output: 16.36 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:29<04:27, 13.49it/s, est. speed input: 16498.62 toks/s, output: 16.11 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:32<04:27, 13.40it/s, est. speed input: 16275.84 toks/s, output: 15.89 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:34<04:26, 13.34it/s, est. speed input: 16084.21 toks/s, output: 15.71 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:37<04:24, 13.32it/s, est. speed input: 15921.26 toks/s, output: 15.55 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:39<04:22, 13.29it/s, est. speed input: 15776.31 toks/s, output: 15.41 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:42<04:20, 13.27it/s, est. speed input: 15647.45 toks/s, output: 15.28 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:44<04:18, 13.25it/s, est. speed input: 15530.56 toks/s, output: 15.17 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:46<04:15, 13.25it/s, est. speed input: 15429.09 toks/s, output: 15.07 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:49<04:13, 13.24it/s, est. speed input: 15336.74 toks/s, output: 14.98 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:51<04:11, 13.24it/s, est. speed input: 15253.72 toks/s, output: 14.90 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:54<04:09, 13.23it/s, est. speed input: 15175.39 toks/s, output: 14.82 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:56<04:06, 13.23it/s, est. speed input: 15105.87 toks/s, output: 14.75 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:58<04:04, 13.23it/s, est. speed input: 15041.84 toks/s, output: 14.69 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [01:01<04:01, 13.23it/s, est. speed input: 14982.73 toks/s, output: 14.63 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [01:03<03:57, 13.34it/s, est. speed input: 14943.54 toks/s, output: 14.59 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [01:06<03:53, 13.42it/s, est. speed input: 14908.71 toks/s, output: 14.56 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [01:08<03:52, 13.37it/s, est. speed input: 14860.98 toks/s, output: 14.51 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [01:10<03:50, 13.32it/s, est. speed input: 14815.61 toks/s, output: 14.47 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [01:13<03:48, 13.29it/s, est. speed input: 14773.74 toks/s, output: 14.43 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [01:15<03:46, 13.27it/s, est. speed input: 14734.27 toks/s, output: 14.39 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [01:18<03:44, 13.26it/s, est. speed input: 14697.12 toks/s, output: 14.35 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [01:20<03:40, 13.37it/s, est. speed input: 14675.54 toks/s, output: 14.33 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [01:22<03:38, 13.33it/s, est. speed input: 14643.12 toks/s, output: 14.30 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [01:25<03:36, 13.29it/s, est. speed input: 14611.43 toks/s, output: 14.27 toks/s]
Processed prompts:  31%|███       | 1250/4096 [01:27<03:32, 13.39it/s, est. speed input: 14593.75 toks/s, output: 14.25 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [01:30<03:28, 13.47it/s, est. speed input: 14578.36 toks/s, output: 14.24 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [01:32<03:27, 13.39it/s, est. speed input: 14550.43 toks/s, output: 14.21 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [01:34<03:26, 13.33it/s, est. speed input: 14524.13 toks/s, output: 14.18 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [01:37<03:24, 13.30it/s, est. speed input: 14499.71 toks/s, output: 14.16 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [01:39<03:22, 13.28it/s, est. speed input: 14477.12 toks/s, output: 14.14 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [01:42<03:20, 13.27it/s, est. speed input: 14454.82 toks/s, output: 14.12 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [01:44<03:17, 13.25it/s, est. speed input: 14433.40 toks/s, output: 14.10 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [01:47<03:15, 13.23it/s, est. speed input: 14412.41 toks/s, output: 14.07 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [01:49<03:13, 13.23it/s, est. speed input: 14393.33 toks/s, output: 14.06 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [01:51<03:09, 13.35it/s, est. speed input: 14384.14 toks/s, output: 14.05 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [01:54<03:07, 13.30it/s, est. speed input: 14365.82 toks/s, output: 14.03 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [01:56<03:03, 13.40it/s, est. speed input: 14357.31 toks/s, output: 14.02 toks/s]
Processed prompts:  41%|████      | 1666/4096 [01:58<03:02, 13.34it/s, est. speed input: 14340.18 toks/s, output: 14.00 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [02:01<03:00, 13.30it/s, est. speed input: 14324.38 toks/s, output: 13.99 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [02:03<02:58, 13.27it/s, est. speed input: 14308.42 toks/s, output: 13.97 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [02:06<02:55, 13.26it/s, est. speed input: 14294.08 toks/s, output: 13.96 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [02:08<02:53, 13.24it/s, est. speed input: 14278.87 toks/s, output: 13.94 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [02:11<02:51, 13.23it/s, est. speed input: 14265.20 toks/s, output: 13.93 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [02:13<02:47, 13.36it/s, est. speed input: 14260.27 toks/s, output: 13.93 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [02:15<02:45, 13.31it/s, est. speed input: 14247.31 toks/s, output: 13.91 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [02:18<02:43, 13.28it/s, est. speed input: 14234.59 toks/s, output: 13.90 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [02:20<02:40, 13.39it/s, est. speed input: 14230.14 toks/s, output: 13.90 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [02:22<02:36, 13.46it/s, est. speed input: 14225.46 toks/s, output: 13.89 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [02:25<02:35, 13.39it/s, est. speed input: 14214.38 toks/s, output: 13.88 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [02:27<02:33, 13.32it/s, est. speed input: 14202.43 toks/s, output: 13.87 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [02:30<02:31, 13.28it/s, est. speed input: 14191.08 toks/s, output: 13.86 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [02:32<02:29, 13.26it/s, est. speed input: 14180.58 toks/s, output: 13.85 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [02:35<02:27, 13.25it/s, est. speed input: 14170.73 toks/s, output: 13.84 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [02:37<02:24, 13.24it/s, est. speed input: 14160.98 toks/s, output: 13.83 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [02:39<02:19, 13.48it/s, est. speed input: 14164.44 toks/s, output: 13.83 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [02:42<02:18, 13.39it/s, est. speed input: 14154.78 toks/s, output: 13.82 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [02:44<02:15, 13.47it/s, est. speed input: 14152.12 toks/s, output: 13.82 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [02:46<02:13, 13.39it/s, est. speed input: 14143.19 toks/s, output: 13.81 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [02:49<02:10, 13.46it/s, est. speed input: 14140.57 toks/s, output: 13.81 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [02:51<02:06, 13.64it/s, est. speed input: 14144.00 toks/s, output: 13.81 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [02:53<02:04, 13.62it/s, est. speed input: 14140.92 toks/s, output: 13.81 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [02:56<02:03, 13.51it/s, est. speed input: 14132.93 toks/s, output: 13.80 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [02:58<02:01, 13.41it/s, est. speed input: 14124.44 toks/s, output: 13.79 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [03:01<01:58, 13.48it/s, est. speed input: 14122.45 toks/s, output: 13.79 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [03:03<01:56, 13.40it/s, est. speed input: 14114.65 toks/s, output: 13.78 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [03:05<01:53, 13.47it/s, est. speed input: 14112.70 toks/s, output: 13.78 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [03:08<01:52, 13.39it/s, est. speed input: 14105.33 toks/s, output: 13.77 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [03:10<01:50, 13.35it/s, est. speed input: 14098.35 toks/s, output: 13.77 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [03:13<01:48, 13.31it/s, est. speed input: 14091.31 toks/s, output: 13.76 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [03:15<01:44, 13.40it/s, est. speed input: 14089.65 toks/s, output: 13.76 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [03:17<01:42, 13.35it/s, est. speed input: 14083.01 toks/s, output: 13.75 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [03:20<01:40, 13.31it/s, est. speed input: 14076.43 toks/s, output: 13.75 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [03:22<01:38, 13.29it/s, est. speed input: 14070.39 toks/s, output: 13.74 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [03:25<01:36, 13.27it/s, est. speed input: 14064.00 toks/s, output: 13.73 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [03:27<01:33, 13.37it/s, est. speed input: 14062.52 toks/s, output: 13.73 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [03:29<01:31, 13.32it/s, est. speed input: 14056.46 toks/s, output: 13.73 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [03:32<01:28, 13.30it/s, est. speed input: 14050.74 toks/s, output: 13.72 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [03:34<01:26, 13.27it/s, est. speed input: 14044.88 toks/s, output: 13.72 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [03:37<01:24, 13.25it/s, est. speed input: 14039.03 toks/s, output: 13.71 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [03:39<01:22, 13.24it/s, est. speed input: 14033.41 toks/s, output: 13.70 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [03:42<01:19, 13.24it/s, est. speed input: 14028.08 toks/s, output: 13.70 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [03:44<01:17, 13.24it/s, est. speed input: 14022.97 toks/s, output: 13.69 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [03:46<01:13, 13.48it/s, est. speed input: 14026.84 toks/s, output: 13.70 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [03:49<01:10, 13.51it/s, est. speed input: 14025.85 toks/s, output: 13.70 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [03:51<01:08, 13.43it/s, est. speed input: 14020.97 toks/s, output: 13.69 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [03:53<01:06, 13.37it/s, est. speed input: 14016.08 toks/s, output: 13.69 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [03:56<01:04, 13.45it/s, est. speed input: 14015.46 toks/s, output: 13.69 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [03:58<01:02, 13.38it/s, est. speed input: 14010.57 toks/s, output: 13.68 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [04:01<00:59, 13.33it/s, est. speed input: 14005.88 toks/s, output: 13.68 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [04:03<00:57, 13.30it/s, est. speed input: 14001.31 toks/s, output: 13.67 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [04:05<00:55, 13.29it/s, est. speed input: 13997.13 toks/s, output: 13.67 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [04:08<00:52, 13.27it/s, est. speed input: 13992.88 toks/s, output: 13.66 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [04:10<00:50, 13.37it/s, est. speed input: 13992.34 toks/s, output: 13.66 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [04:13<00:47, 13.33it/s, est. speed input: 13988.00 toks/s, output: 13.66 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [04:15<00:44, 13.54it/s, est. speed input: 13991.74 toks/s, output: 13.66 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [04:17<00:42, 13.45it/s, est. speed input: 13987.69 toks/s, output: 13.66 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [04:20<00:40, 13.38it/s, est. speed input: 13983.49 toks/s, output: 13.66 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [04:22<00:38, 13.33it/s, est. speed input: 13979.29 toks/s, output: 13.65 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [04:25<00:35, 13.31it/s, est. speed input: 13975.54 toks/s, output: 13.65 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [04:27<00:33, 13.28it/s, est. speed input: 13971.59 toks/s, output: 13.64 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [04:29<00:31, 13.27it/s, est. speed input: 13967.80 toks/s, output: 13.64 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [04:32<00:28, 13.36it/s, est. speed input: 13967.40 toks/s, output: 13.64 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [04:34<00:26, 13.31it/s, est. speed input: 13963.44 toks/s, output: 13.64 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [04:37<00:23, 13.29it/s, est. speed input: 13959.81 toks/s, output: 13.63 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [04:39<00:21, 13.28it/s, est. speed input: 13956.46 toks/s, output: 13.63 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [04:41<00:18, 13.38it/s, est. speed input: 13956.30 toks/s, output: 13.63 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [04:44<00:16, 13.32it/s, est. speed input: 13952.56 toks/s, output: 13.63 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [04:46<00:14, 13.30it/s, est. speed input: 13949.25 toks/s, output: 13.62 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [04:49<00:11, 13.27it/s, est. speed input: 13945.79 toks/s, output: 13.62 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [04:51<00:09, 13.26it/s, est. speed input: 13942.47 toks/s, output: 13.62 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [04:53<00:07, 13.24it/s, est. speed input: 13939.00 toks/s, output: 13.61 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [04:56<00:04, 13.60it/s, est. speed input: 13945.96 toks/s, output: 13.62 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [04:58<00:02, 13.62it/s, est. speed input: 13946.33 toks/s, output: 13.62 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:58<00:00, 13.62it/s, est. speed input: 14049.22 toks/s, output: 13.72 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:58<00:00, 13.72it/s, est. speed input: 14049.22 toks/s, output: 13.72 toks/s]
[rank0]:[W128 07:45:40.552579616 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 354.4s

测试结果:
  Requests/s:   13.32
  Tokens/s:     13655.03
  Total Reqs:   4096
  Elapsed:      307.46s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     13641.71


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,21.8257,11196.5887,5.8646
1024,1024,1,128,128,12.8501,13171.3831,9.9610
2048,1024,2,256,128,13.3492,13682.9084,19.1772
4096,1024,4,512,128,12.6566,12973.0184,40.4532
8192,1024,8,1024,128,13.0742,13401.0538,78.3222
16384,1024,16,2048,128,13.2929,13625.2101,154.0673
32768,1024,32,4096,128,13.3220,13655.0272,307.4619

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:45:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:45:47 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3650949) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3650949) WARNING 01-28 07:46:12 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 30.40 requests/s, 15597.42 total tokens/s, 30.40 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 07:45:46] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:45:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:45:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:45:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:45:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:45:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:45:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:45:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:45:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:45:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:45:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:45:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:45:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:45:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:45:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:45:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:45:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:45:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:45:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:45:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:45:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:45:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:45:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:45:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:45:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:45:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:45:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:45:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3650949) [2026-01-28 07:45:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3650949) [2026-01-28 07:45:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3650949) [2026-01-28 07:45:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3650949) [2026-01-28 07:45:51] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3650949) [2026-01-28 07:45:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3650949) [2026-01-28 07:45:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3650949) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3650949) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.14s/it]
(EngineCore_DP0 pid=3650949) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.14s/it]
(EngineCore_DP0 pid=3650949) 
(EngineCore_DP0 pid=3650949) [2026-01-28 07:46:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3650949) [2026-01-28 07:46:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3650949) [2026-01-28 07:46:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3650949) [2026-01-28 07:46:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3650949) [2026-01-28 07:46:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3650949) [2026-01-28 07:46:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3650949) [2026-01-28 07:46:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3650949) [2026-01-28 07:46:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3650949) 2026-01-28 07:46:11,319 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3650949) 2026-01-28 07:46:11,332 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1342.24it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:37,  3.36it/s, est. speed input: 1718.78 toks/s, output: 3.36 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:08, 14.23it/s, est. speed input: 6100.56 toks/s, output: 11.91 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 20.74it/s, est. speed input: 8534.66 toks/s, output: 16.67 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 24.89it/s, est. speed input: 10090.77 toks/s, output: 19.71 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:04, 27.54it/s, est. speed input: 11156.68 toks/s, output: 21.79 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 29.47it/s, est. speed input: 11963.82 toks/s, output: 23.37 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:03, 30.14it/s, est. speed input: 12481.89 toks/s, output: 24.38 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:03, 31.11it/s, est. speed input: 12963.03 toks/s, output: 25.32 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 32.04it/s, est. speed input: 13386.02 toks/s, output: 26.14 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 32.40it/s, est. speed input: 13701.13 toks/s, output: 26.76 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 32.75it/s, est. speed input: 13977.94 toks/s, output: 27.30 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 32.94it/s, est. speed input: 14208.11 toks/s, output: 27.75 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 33.22it/s, est. speed input: 14421.15 toks/s, output: 28.17 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 33.16it/s, est. speed input: 14582.90 toks/s, output: 28.48 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 33.40it/s, est. speed input: 14749.81 toks/s, output: 28.81 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:02, 33.01it/s, est. speed input: 14850.58 toks/s, output: 29.00 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 33.02it/s, est. speed input: 14963.36 toks/s, output: 29.22 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 33.42it/s, est. speed input: 15094.54 toks/s, output: 29.48 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 33.34it/s, est. speed input: 15186.92 toks/s, output: 29.66 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 33.42it/s, est. speed input: 15280.42 toks/s, output: 29.84 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 33.52it/s, est. speed input: 15367.88 toks/s, output: 30.02 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 33.61it/s, est. speed input: 15449.61 toks/s, output: 30.17 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 33.58it/s, est. speed input: 15519.09 toks/s, output: 30.31 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 33.41it/s, est. speed input: 15574.31 toks/s, output: 30.42 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:00, 32.94it/s, est. speed input: 15603.70 toks/s, output: 30.48 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 32.96it/s, est. speed input: 15651.31 toks/s, output: 30.57 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 33.27it/s, est. speed input: 15712.21 toks/s, output: 30.69 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 33.16it/s, est. speed input: 15751.21 toks/s, output: 30.76 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 33.23it/s, est. speed input: 15794.89 toks/s, output: 30.85 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 33.26it/s, est. speed input: 15835.27 toks/s, output: 30.93 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 33.23it/s, est. speed input: 15870.74 toks/s, output: 31.00 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:04<00:00, 33.28it/s, est. speed input: 15907.15 toks/s, output: 31.07 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 33.28it/s, est. speed input: 15934.49 toks/s, output: 31.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 31.12it/s, est. speed input: 15934.49 toks/s, output: 31.12 toks/s]
[rank0]:[W128 07:46:16.572299832 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.0s

测试结果:
  Requests/s:   30.40
  Tokens/s:     15597.42
  Total Reqs:   128
  Elapsed:      4.21s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     15567.02

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:46:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:46:23 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3651746) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3651746) WARNING 01-28 07:46:46 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.98 requests/s, 17408.67 total tokens/s, 16.98 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 07:46:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:46:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:46:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:46:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:46:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:46:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:46:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:46:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:46:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:46:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:46:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:46:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:46:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:46:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:46:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:46:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:46:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:46:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:46:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:46:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:46:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:46:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:46:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:46:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:46:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:46:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:46:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:46:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:27] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:27] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:27] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:27] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:27] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3651746) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3651746) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.25s/it]
(EngineCore_DP0 pid=3651746) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.25s/it]
(EngineCore_DP0 pid=3651746) 
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3651746) [2026-01-28 07:46:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3651746) 2026-01-28 07:46:45,878 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3651746) 2026-01-28 07:46:45,890 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  38%|███▊      | 49/128 [00:00<00:00, 483.93it/s]
Adding requests:  91%|█████████▏| 117/128 [00:00<00:00, 597.00it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 587.39it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:30,  4.12it/s, est. speed input: 4215.77 toks/s, output: 4.12 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:13,  9.53it/s, est. speed input: 8627.28 toks/s, output: 8.42 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:09, 12.52it/s, est. speed input: 10925.11 toks/s, output: 10.67 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:08, 14.14it/s, est. speed input: 12249.76 toks/s, output: 11.96 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:07, 15.30it/s, est. speed input: 13202.04 toks/s, output: 12.89 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 16.25it/s, est. speed input: 13959.51 toks/s, output: 13.63 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:06, 16.83it/s, est. speed input: 14513.46 toks/s, output: 14.17 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:06, 17.33it/s, est. speed input: 14977.93 toks/s, output: 14.63 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:06, 17.54it/s, est. speed input: 15315.69 toks/s, output: 14.96 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 17.70it/s, est. speed input: 15598.69 toks/s, output: 15.23 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:05, 17.87it/s, est. speed input: 15849.60 toks/s, output: 15.48 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 17.94it/s, est. speed input: 16050.91 toks/s, output: 15.67 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 17.68it/s, est. speed input: 16160.82 toks/s, output: 15.78 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 17.80it/s, est. speed input: 16314.15 toks/s, output: 15.93 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.86it/s, est. speed input: 16444.18 toks/s, output: 16.06 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 17.99it/s, est. speed input: 16575.48 toks/s, output: 16.19 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:05, 17.92it/s, est. speed input: 16663.92 toks/s, output: 16.27 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 17.94it/s, est. speed input: 16756.18 toks/s, output: 16.36 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 17.98it/s, est. speed input: 16842.16 toks/s, output: 16.45 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:04, 17.94it/s, est. speed input: 16910.57 toks/s, output: 16.51 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:04, 18.01it/s, est. speed input: 16986.32 toks/s, output: 16.59 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 17.99it/s, est. speed input: 17045.78 toks/s, output: 16.65 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 17.79it/s, est. speed input: 17076.23 toks/s, output: 16.68 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 17.93it/s, est. speed input: 17139.98 toks/s, output: 16.74 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 18.00it/s, est. speed input: 17195.51 toks/s, output: 16.79 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 17.92it/s, est. speed input: 17230.66 toks/s, output: 16.83 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 18.06it/s, est. speed input: 17286.38 toks/s, output: 16.88 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 18.08it/s, est. speed input: 17329.80 toks/s, output: 16.92 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:03, 18.06it/s, est. speed input: 17367.09 toks/s, output: 16.96 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:03, 18.07it/s, est. speed input: 17403.85 toks/s, output: 17.00 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 18.10it/s, est. speed input: 17441.17 toks/s, output: 17.03 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 17.62it/s, est. speed input: 17426.31 toks/s, output: 17.02 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 17.70it/s, est. speed input: 17452.44 toks/s, output: 17.04 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 17.81it/s, est. speed input: 17481.77 toks/s, output: 17.07 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 17.95it/s, est. speed input: 17515.43 toks/s, output: 17.10 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 17.88it/s, est. speed input: 17533.08 toks/s, output: 17.12 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 17.98it/s, est. speed input: 17561.75 toks/s, output: 17.15 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:02, 18.10it/s, est. speed input: 17593.19 toks/s, output: 17.18 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 18.13it/s, est. speed input: 17618.73 toks/s, output: 17.21 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 18.22it/s, est. speed input: 17648.45 toks/s, output: 17.23 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 17.91it/s, est. speed input: 17648.01 toks/s, output: 17.23 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 17.89it/s, est. speed input: 17662.61 toks/s, output: 17.25 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:04<00:02, 17.83it/s, est. speed input: 17673.39 toks/s, output: 17.26 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 17.90it/s, est. speed input: 17691.11 toks/s, output: 17.28 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 17.89it/s, est. speed input: 17704.25 toks/s, output: 17.29 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 17.99it/s, est. speed input: 17724.14 toks/s, output: 17.31 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:01, 18.09it/s, est. speed input: 17745.28 toks/s, output: 17.33 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 17.99it/s, est. speed input: 17754.77 toks/s, output: 17.34 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 17.96it/s, est. speed input: 17765.70 toks/s, output: 17.35 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 18.02it/s, est. speed input: 17781.63 toks/s, output: 17.36 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 17.70it/s, est. speed input: 17774.42 toks/s, output: 17.36 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:05<00:01, 17.79it/s, est. speed input: 17786.86 toks/s, output: 17.37 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 17.86it/s, est. speed input: 17798.91 toks/s, output: 17.38 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 17.92it/s, est. speed input: 17811.69 toks/s, output: 17.39 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 17.94it/s, est. speed input: 17822.18 toks/s, output: 17.40 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:00, 18.06it/s, est. speed input: 17838.75 toks/s, output: 17.42 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 18.12it/s, est. speed input: 17853.44 toks/s, output: 17.43 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 18.05it/s, est. speed input: 17861.03 toks/s, output: 17.44 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:06<00:00, 18.02it/s, est. speed input: 17869.87 toks/s, output: 17.45 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:06<00:00, 17.83it/s, est. speed input: 17868.87 toks/s, output: 17.45 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:06<00:00, 17.83it/s, est. speed input: 17875.14 toks/s, output: 17.46 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 17.84it/s, est. speed input: 17881.57 toks/s, output: 17.46 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 18.00it/s, est. speed input: 17896.06 toks/s, output: 17.48 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 18.02it/s, est. speed input: 17905.33 toks/s, output: 17.49 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 18.02it/s, est. speed input: 17912.22 toks/s, output: 17.49 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.49it/s, est. speed input: 17912.22 toks/s, output: 17.49 toks/s]
[rank0]:[W128 07:46:54.177145494 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 37.5s

测试结果:
  Requests/s:   16.98
  Tokens/s:     17408.67
  Total Reqs:   128
  Elapsed:      7.54s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     17391.69

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:47:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:47:01 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3652712) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3652712) WARNING 01-28 07:47:24 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.37 requests/s, 18827.57 total tokens/s, 18.37 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 07:47:01] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:47:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:47:01] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:47:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:01] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:01] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:47:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:47:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:47:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:47:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:47:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:47:04] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:47:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:47:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:47:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:47:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:47:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:47:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:47:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:47:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3652712) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3652712) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.58s/it]
(EngineCore_DP0 pid=3652712) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.58s/it]
(EngineCore_DP0 pid=3652712) 
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3652712) [2026-01-28 07:47:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3652712) 2026-01-28 07:47:23,906 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3652712) 2026-01-28 07:47:23,918 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  24%|██▍       | 61/256 [00:00<00:00, 605.90it/s]
Adding requests:  48%|████▊     | 122/256 [00:00<00:00, 606.00it/s]
Adding requests:  71%|███████▏  | 183/256 [00:00<00:00, 574.18it/s]
Adding requests:  94%|█████████▍| 241/256 [00:00<00:00, 575.35it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 578.43it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 8/256 [00:00<00:05, 44.10it/s, est. speed input: 45168.22 toks/s, output: 44.10 toks/s]
Processed prompts:   5%|▌         | 13/256 [00:00<00:07, 30.89it/s, est. speed input: 33482.48 toks/s, output: 32.70 toks/s]
Processed prompts:   7%|▋         | 17/256 [00:00<00:09, 25.16it/s, est. speed input: 28477.52 toks/s, output: 27.81 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:11, 20.70it/s, est. speed input: 24835.75 toks/s, output: 24.25 toks/s]
Processed prompts:   9%|▉         | 23/256 [00:00<00:10, 22.22it/s, est. speed input: 25209.63 toks/s, output: 24.62 toks/s]
Processed prompts:  10%|█         | 26/256 [00:01<00:12, 18.87it/s, est. speed input: 23089.00 toks/s, output: 22.55 toks/s]
Processed prompts:  11%|█▏        | 29/256 [00:01<00:10, 20.86it/s, est. speed input: 23573.21 toks/s, output: 23.02 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:01<00:12, 18.19it/s, est. speed input: 22206.71 toks/s, output: 21.69 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:01<00:12, 18.38it/s, est. speed input: 22031.90 toks/s, output: 21.52 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:01<00:11, 18.41it/s, est. speed input: 21835.29 toks/s, output: 21.32 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:01<00:11, 18.53it/s, est. speed input: 21688.40 toks/s, output: 21.18 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:01<00:11, 18.53it/s, est. speed input: 21534.58 toks/s, output: 21.03 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:02<00:11, 18.63it/s, est. speed input: 21419.65 toks/s, output: 20.92 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:02<00:11, 18.30it/s, est. speed input: 21230.98 toks/s, output: 20.73 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:02<00:11, 18.35it/s, est. speed input: 21119.76 toks/s, output: 20.62 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:02<00:11, 18.46it/s, est. speed input: 21031.10 toks/s, output: 20.54 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:02<00:11, 18.58it/s, est. speed input: 20956.87 toks/s, output: 20.47 toks/s]
Processed prompts:  20%|██        | 52/256 [00:02<00:11, 18.50it/s, est. speed input: 20862.46 toks/s, output: 20.37 toks/s]
Processed prompts:  21%|██        | 54/256 [00:02<00:10, 18.59it/s, est. speed input: 20798.26 toks/s, output: 20.31 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:02<00:10, 18.65it/s, est. speed input: 20738.80 toks/s, output: 20.25 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:02<00:10, 18.66it/s, est. speed input: 20679.18 toks/s, output: 20.19 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:02<00:10, 18.71it/s, est. speed input: 20628.60 toks/s, output: 20.15 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:03<00:10, 18.80it/s, est. speed input: 20589.14 toks/s, output: 20.11 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:03<00:10, 18.35it/s, est. speed input: 20489.04 toks/s, output: 20.01 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:03<00:10, 18.54it/s, est. speed input: 20455.90 toks/s, output: 19.98 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:03<00:10, 18.60it/s, est. speed input: 20416.04 toks/s, output: 19.94 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:03<00:09, 18.73it/s, est. speed input: 20388.29 toks/s, output: 19.91 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:03<00:09, 18.76it/s, est. speed input: 20355.99 toks/s, output: 19.88 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:03<00:09, 18.81it/s, est. speed input: 20328.29 toks/s, output: 19.85 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:03<00:09, 18.75it/s, est. speed input: 20292.69 toks/s, output: 19.82 toks/s]
Processed prompts:  30%|███       | 78/256 [00:03<00:09, 18.62it/s, est. speed input: 20250.34 toks/s, output: 19.78 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:04<00:09, 18.69it/s, est. speed input: 20225.92 toks/s, output: 19.75 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:04<00:09, 18.58it/s, est. speed input: 20187.28 toks/s, output: 19.71 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:04<00:09, 18.31it/s, est. speed input: 20133.60 toks/s, output: 19.66 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:04<00:09, 18.27it/s, est. speed input: 20095.51 toks/s, output: 19.62 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:04<00:09, 18.45it/s, est. speed input: 20077.16 toks/s, output: 19.61 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:04<00:08, 18.53it/s, est. speed input: 20056.29 toks/s, output: 19.59 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:04<00:08, 18.58it/s, est. speed input: 20035.42 toks/s, output: 19.57 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:04<00:08, 18.52it/s, est. speed input: 20008.37 toks/s, output: 19.54 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:04<00:08, 18.60it/s, est. speed input: 19991.63 toks/s, output: 19.52 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:05<00:08, 18.56it/s, est. speed input: 19968.53 toks/s, output: 19.50 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:05<00:08, 18.60it/s, est. speed input: 19951.32 toks/s, output: 19.48 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:05<00:08, 18.39it/s, est. speed input: 19917.41 toks/s, output: 19.45 toks/s]
Processed prompts:  41%|████      | 104/256 [00:05<00:08, 18.36it/s, est. speed input: 19893.06 toks/s, output: 19.43 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:05<00:08, 18.43it/s, est. speed input: 19875.94 toks/s, output: 19.41 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:05<00:07, 18.56it/s, est. speed input: 19865.50 toks/s, output: 19.40 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:05<00:07, 18.64it/s, est. speed input: 19854.71 toks/s, output: 19.39 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:05<00:07, 18.73it/s, est. speed input: 19846.10 toks/s, output: 19.38 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:05<00:07, 18.76it/s, est. speed input: 19835.85 toks/s, output: 19.37 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:05<00:07, 18.76it/s, est. speed input: 19824.58 toks/s, output: 19.36 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:06<00:07, 18.73it/s, est. speed input: 19812.28 toks/s, output: 19.35 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:06<00:07, 18.63it/s, est. speed input: 19795.17 toks/s, output: 19.33 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:06<00:07, 18.40it/s, est. speed input: 19768.82 toks/s, output: 19.31 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:06<00:07, 18.52it/s, est. speed input: 19760.49 toks/s, output: 19.30 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:06<00:06, 18.62it/s, est. speed input: 19752.92 toks/s, output: 19.29 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:06<00:06, 18.60it/s, est. speed input: 19740.74 toks/s, output: 19.28 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:06<00:06, 18.58it/s, est. speed input: 19728.76 toks/s, output: 19.27 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:06<00:06, 18.59it/s, est. speed input: 19718.29 toks/s, output: 19.26 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:06<00:06, 18.68it/s, est. speed input: 19712.59 toks/s, output: 19.25 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:07<00:06, 18.64it/s, est. speed input: 19701.60 toks/s, output: 19.24 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:07<00:06, 18.63it/s, est. speed input: 19691.77 toks/s, output: 19.23 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:07<00:06, 18.61it/s, est. speed input: 19681.86 toks/s, output: 19.22 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:07<00:06, 18.26it/s, est. speed input: 19654.37 toks/s, output: 19.19 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:07<00:06, 18.40it/s, est. speed input: 19647.51 toks/s, output: 19.19 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:07<00:05, 18.52it/s, est. speed input: 19642.14 toks/s, output: 19.18 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:07<00:05, 18.59it/s, est. speed input: 19636.00 toks/s, output: 19.18 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:07<00:05, 18.59it/s, est. speed input: 19627.92 toks/s, output: 19.17 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:07<00:05, 18.54it/s, est. speed input: 19617.64 toks/s, output: 19.16 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:08<00:05, 18.59it/s, est. speed input: 19611.61 toks/s, output: 19.15 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:08<00:05, 18.66it/s, est. speed input: 19607.15 toks/s, output: 19.15 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:08<00:05, 18.72it/s, est. speed input: 19603.52 toks/s, output: 19.14 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:08<00:05, 18.77it/s, est. speed input: 19600.03 toks/s, output: 19.14 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:08<00:05, 18.38it/s, est. speed input: 19577.78 toks/s, output: 19.12 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:08<00:04, 18.51it/s, est. speed input: 19573.92 toks/s, output: 19.12 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:08<00:04, 18.59it/s, est. speed input: 19569.74 toks/s, output: 19.11 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:08<00:04, 18.61it/s, est. speed input: 19564.18 toks/s, output: 19.11 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:08<00:04, 18.63it/s, est. speed input: 19558.82 toks/s, output: 19.10 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:09<00:04, 18.66it/s, est. speed input: 19554.21 toks/s, output: 19.10 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:09<00:04, 18.49it/s, est. speed input: 19541.81 toks/s, output: 19.08 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:09<00:04, 18.51it/s, est. speed input: 19535.59 toks/s, output: 19.08 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:09<00:04, 18.58it/s, est. speed input: 19531.80 toks/s, output: 19.07 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:09<00:04, 18.52it/s, est. speed input: 19523.50 toks/s, output: 19.07 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:09<00:04, 18.24it/s, est. speed input: 19505.94 toks/s, output: 19.05 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:09<00:03, 18.37it/s, est. speed input: 19501.54 toks/s, output: 19.04 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:09<00:03, 18.47it/s, est. speed input: 19498.09 toks/s, output: 19.04 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:09<00:03, 18.43it/s, est. speed input: 19490.05 toks/s, output: 19.03 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:09<00:03, 18.56it/s, est. speed input: 19488.43 toks/s, output: 19.03 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:10<00:03, 18.57it/s, est. speed input: 19483.59 toks/s, output: 19.03 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:10<00:03, 18.64it/s, est. speed input: 19481.10 toks/s, output: 19.02 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:10<00:03, 18.59it/s, est. speed input: 19475.17 toks/s, output: 19.02 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:10<00:03, 18.59it/s, est. speed input: 19470.63 toks/s, output: 19.01 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:10<00:03, 18.22it/s, est. speed input: 19452.81 toks/s, output: 19.00 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:10<00:02, 18.27it/s, est. speed input: 19446.49 toks/s, output: 18.99 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:10<00:02, 18.41it/s, est. speed input: 19443.94 toks/s, output: 18.99 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:10<00:02, 18.55it/s, est. speed input: 19443.00 toks/s, output: 18.99 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:10<00:02, 18.62it/s, est. speed input: 19441.02 toks/s, output: 18.99 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:11<00:02, 18.70it/s, est. speed input: 19439.95 toks/s, output: 18.98 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:11<00:02, 18.57it/s, est. speed input: 19432.77 toks/s, output: 18.98 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:11<00:02, 18.48it/s, est. speed input: 19425.82 toks/s, output: 18.97 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:11<00:02, 18.65it/s, est. speed input: 19426.67 toks/s, output: 18.97 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:11<00:02, 18.70it/s, est. speed input: 19425.05 toks/s, output: 18.97 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:11<00:01, 18.43it/s, est. speed input: 19413.88 toks/s, output: 18.96 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:11<00:01, 18.37it/s, est. speed input: 19406.86 toks/s, output: 18.95 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:11<00:01, 18.41it/s, est. speed input: 19402.57 toks/s, output: 18.95 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:11<00:01, 18.42it/s, est. speed input: 19397.80 toks/s, output: 18.94 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:12<00:01, 18.51it/s, est. speed input: 19395.88 toks/s, output: 18.94 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:12<00:01, 18.57it/s, est. speed input: 19393.77 toks/s, output: 18.94 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:12<00:01, 18.47it/s, est. speed input: 19387.34 toks/s, output: 18.93 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:12<00:01, 18.59it/s, est. speed input: 19386.77 toks/s, output: 18.93 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:12<00:01, 18.67it/s, est. speed input: 19386.19 toks/s, output: 18.93 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:12<00:00, 18.67it/s, est. speed input: 19383.87 toks/s, output: 18.93 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:12<00:00, 18.31it/s, est. speed input: 19370.81 toks/s, output: 18.92 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:12<00:00, 18.41it/s, est. speed input: 19368.47 toks/s, output: 18.91 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:12<00:00, 18.47it/s, est. speed input: 19366.08 toks/s, output: 18.91 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:13<00:00, 18.57it/s, est. speed input: 19365.19 toks/s, output: 18.91 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:13<00:00, 18.63it/s, est. speed input: 19364.02 toks/s, output: 18.91 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:13<00:00, 18.71it/s, est. speed input: 19363.80 toks/s, output: 18.91 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:13<00:00, 18.68it/s, est. speed input: 19361.36 toks/s, output: 18.91 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:13<00:00, 18.55it/s, est. speed input: 19355.92 toks/s, output: 18.90 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 18.55it/s, est. speed input: 19428.36 toks/s, output: 18.97 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 18.97it/s, est. speed input: 19428.36 toks/s, output: 18.97 toks/s]
[rank0]:[W128 07:47:38.857582673 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.7s

测试结果:
  Requests/s:   18.37
  Tokens/s:     18827.57
  Total Reqs:   256
  Elapsed:      13.94s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     18809.20

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:47:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:47:46 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3653712) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3653712) WARNING 01-28 07:48:09 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 17.15 requests/s, 17581.12 total tokens/s, 17.15 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 07:47:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:47:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:47:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:47:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:47:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:47:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:47:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:47:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:47:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:47:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:47:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:47:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:47:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:47:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:47:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:47:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:47:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:47:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:47:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3653712) [2026-01-28 07:47:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3653712) [2026-01-28 07:47:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3653712) [2026-01-28 07:47:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3653712) [2026-01-28 07:47:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3653712) [2026-01-28 07:47:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3653712) [2026-01-28 07:47:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3653712) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3653712) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.58s/it]
(EngineCore_DP0 pid=3653712) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.58s/it]
(EngineCore_DP0 pid=3653712) 
(EngineCore_DP0 pid=3653712) [2026-01-28 07:48:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3653712) [2026-01-28 07:48:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3653712) [2026-01-28 07:48:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3653712) [2026-01-28 07:48:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3653712) [2026-01-28 07:48:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3653712) [2026-01-28 07:48:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3653712) [2026-01-28 07:48:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3653712) [2026-01-28 07:48:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3653712) 2026-01-28 07:48:08,316 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3653712) 2026-01-28 07:48:08,327 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  12%|█▏        | 61/512 [00:00<00:00, 609.54it/s]
Adding requests:  24%|██▍       | 122/512 [00:00<00:00, 597.94it/s]
Adding requests:  36%|███▌      | 182/512 [00:00<00:00, 560.70it/s]
Adding requests:  47%|████▋     | 239/512 [00:00<00:00, 560.05it/s]
Adding requests:  58%|█████▊    | 296/512 [00:00<00:00, 553.91it/s]
Adding requests:  69%|██████▉   | 352/512 [00:00<00:00, 553.67it/s]
Adding requests:  80%|███████▉  | 408/512 [00:00<00:00, 545.04it/s]
Adding requests:  91%|█████████ | 464/512 [00:00<00:00, 547.78it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 552.81it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 14/512 [00:00<00:06, 78.09it/s, est. speed input: 79977.58 toks/s, output: 78.09 toks/s]
Processed prompts:   4%|▍         | 22/512 [00:00<00:16, 30.26it/s, est. speed input: 35085.52 toks/s, output: 34.26 toks/s]
Processed prompts:   5%|▌         | 27/512 [00:00<00:18, 26.81it/s, est. speed input: 31363.63 toks/s, output: 30.63 toks/s]
Processed prompts:   6%|▌         | 31/512 [00:01<00:20, 23.57it/s, est. speed input: 28494.49 toks/s, output: 27.83 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:01<00:23, 20.07it/s, est. speed input: 25837.46 toks/s, output: 25.23 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:01<00:24, 19.17it/s, est. speed input: 24631.79 toks/s, output: 24.05 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:01<00:25, 18.46it/s, est. speed input: 23683.14 toks/s, output: 23.13 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:02<00:25, 18.12it/s, est. speed input: 23016.95 toks/s, output: 22.48 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:02<00:25, 17.92it/s, est. speed input: 22497.66 toks/s, output: 21.97 toks/s]
Processed prompts:  11%|█         | 54/512 [00:02<00:25, 17.65it/s, est. speed input: 22025.17 toks/s, output: 21.51 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:02<00:25, 17.54it/s, est. speed input: 21659.78 toks/s, output: 21.15 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:02<00:25, 17.31it/s, est. speed input: 21303.20 toks/s, output: 20.80 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:03<00:25, 17.30it/s, est. speed input: 21041.63 toks/s, output: 20.55 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:03<00:25, 17.21it/s, est. speed input: 20794.51 toks/s, output: 20.31 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:03<00:25, 17.24it/s, est. speed input: 20602.87 toks/s, output: 20.12 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:03<00:25, 17.22it/s, est. speed input: 20423.74 toks/s, output: 19.94 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:04<00:24, 17.21it/s, est. speed input: 20265.02 toks/s, output: 19.79 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:04<00:24, 17.22it/s, est. speed input: 20126.27 toks/s, output: 19.65 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:04<00:24, 17.19it/s, est. speed input: 19995.30 toks/s, output: 19.53 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:04<00:24, 17.22it/s, est. speed input: 19886.10 toks/s, output: 19.42 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:05<00:24, 17.15it/s, est. speed input: 19769.74 toks/s, output: 19.31 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:05<00:23, 17.14it/s, est. speed input: 19671.08 toks/s, output: 19.21 toks/s]
Processed prompts:  21%|██        | 106/512 [00:05<00:23, 17.15it/s, est. speed input: 19583.46 toks/s, output: 19.12 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:05<00:23, 17.20it/s, est. speed input: 19510.14 toks/s, output: 19.05 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:06<00:23, 17.10it/s, est. speed input: 19421.69 toks/s, output: 18.97 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:06<00:22, 17.17it/s, est. speed input: 19359.62 toks/s, output: 18.91 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:06<00:22, 17.29it/s, est. speed input: 19311.63 toks/s, output: 18.86 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:06<00:22, 17.27it/s, est. speed input: 19253.86 toks/s, output: 18.80 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:06<00:22, 17.23it/s, est. speed input: 19195.91 toks/s, output: 18.75 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:07<00:22, 17.15it/s, est. speed input: 19136.12 toks/s, output: 18.69 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:07<00:21, 17.18it/s, est. speed input: 19089.74 toks/s, output: 18.64 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:07<00:21, 17.23it/s, est. speed input: 19050.47 toks/s, output: 18.60 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:07<00:21, 17.23it/s, est. speed input: 19008.81 toks/s, output: 18.56 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:08<00:21, 17.11it/s, est. speed input: 18956.49 toks/s, output: 18.51 toks/s]
Processed prompts:  30%|███       | 154/512 [00:08<00:20, 17.16it/s, est. speed input: 18921.39 toks/s, output: 18.48 toks/s]
Processed prompts:  31%|███       | 158/512 [00:08<00:20, 17.24it/s, est. speed input: 18893.24 toks/s, output: 18.45 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:08<00:20, 17.20it/s, est. speed input: 18856.04 toks/s, output: 18.41 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:09<00:20, 17.18it/s, est. speed input: 18821.98 toks/s, output: 18.38 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:09<00:20, 17.07it/s, est. speed input: 18780.96 toks/s, output: 18.34 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:09<00:19, 17.10it/s, est. speed input: 18751.57 toks/s, output: 18.31 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:09<00:19, 17.23it/s, est. speed input: 18732.91 toks/s, output: 18.29 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:09<00:19, 17.28it/s, est. speed input: 18711.49 toks/s, output: 18.27 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:10<00:19, 17.12it/s, est. speed input: 18675.70 toks/s, output: 18.24 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:10<00:18, 17.09it/s, est. speed input: 18647.77 toks/s, output: 18.21 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:10<00:18, 17.16it/s, est. speed input: 18628.08 toks/s, output: 18.19 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:10<00:18, 17.23it/s, est. speed input: 18611.03 toks/s, output: 18.17 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:11<00:18, 17.16it/s, est. speed input: 18585.73 toks/s, output: 18.15 toks/s]
Processed prompts:  40%|████      | 206/512 [00:11<00:17, 17.04it/s, est. speed input: 18555.75 toks/s, output: 18.12 toks/s]
Processed prompts:  41%|████      | 210/512 [00:11<00:17, 17.10it/s, est. speed input: 18538.07 toks/s, output: 18.10 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:11<00:17, 17.14it/s, est. speed input: 18520.73 toks/s, output: 18.09 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:12<00:17, 17.20it/s, est. speed input: 18506.20 toks/s, output: 18.07 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:12<00:16, 17.22it/s, est. speed input: 18490.74 toks/s, output: 18.06 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:12<00:16, 17.06it/s, est. speed input: 18464.00 toks/s, output: 18.03 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:12<00:16, 17.17it/s, est. speed input: 18452.96 toks/s, output: 18.02 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:12<00:16, 17.23it/s, est. speed input: 18441.14 toks/s, output: 18.01 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:13<00:15, 17.26it/s, est. speed input: 18429.05 toks/s, output: 18.00 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:13<00:15, 17.08it/s, est. speed input: 18404.59 toks/s, output: 17.97 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:13<00:15, 17.20it/s, est. speed input: 18396.27 toks/s, output: 17.97 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:13<00:15, 17.20it/s, est. speed input: 18383.13 toks/s, output: 17.95 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:14<00:14, 17.23it/s, est. speed input: 18372.47 toks/s, output: 17.94 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:14<00:14, 17.25it/s, est. speed input: 18361.49 toks/s, output: 17.93 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:14<00:14, 17.14it/s, est. speed input: 18344.11 toks/s, output: 17.91 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:14<00:14, 17.20it/s, est. speed input: 18335.34 toks/s, output: 17.91 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:15<00:14, 17.20it/s, est. speed input: 18324.27 toks/s, output: 17.89 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:15<00:13, 17.18it/s, est. speed input: 18312.46 toks/s, output: 17.88 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:15<00:13, 17.10it/s, est. speed input: 18297.44 toks/s, output: 17.87 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:15<00:13, 17.22it/s, est. speed input: 18291.89 toks/s, output: 17.86 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:16<00:13, 17.21it/s, est. speed input: 18281.74 toks/s, output: 17.85 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:16<00:12, 17.23it/s, est. speed input: 18273.51 toks/s, output: 17.85 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:16<00:12, 17.21it/s, est. speed input: 18263.45 toks/s, output: 17.84 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:16<00:12, 17.07it/s, est. speed input: 18247.94 toks/s, output: 17.82 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:16<00:12, 17.19it/s, est. speed input: 18243.14 toks/s, output: 17.82 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:17<00:11, 17.25it/s, est. speed input: 18237.48 toks/s, output: 17.81 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:17<00:11, 17.20it/s, est. speed input: 18227.08 toks/s, output: 17.80 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:17<00:11, 17.08it/s, est. speed input: 18213.60 toks/s, output: 17.79 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:17<00:11, 17.09it/s, est. speed input: 18204.73 toks/s, output: 17.78 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:18<00:11, 17.17it/s, est. speed input: 18198.91 toks/s, output: 17.77 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:18<00:10, 17.18it/s, est. speed input: 18191.82 toks/s, output: 17.77 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:18<00:10, 17.23it/s, est. speed input: 18186.41 toks/s, output: 17.76 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:18<00:10, 17.10it/s, est. speed input: 18173.96 toks/s, output: 17.75 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:19<00:10, 17.20it/s, est. speed input: 18170.15 toks/s, output: 17.74 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:19<00:09, 18.06it/s, est. speed input: 18198.19 toks/s, output: 17.77 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:19<00:09, 17.81it/s, est. speed input: 18192.11 toks/s, output: 17.77 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:19<00:09, 17.57it/s, est. speed input: 18183.17 toks/s, output: 17.76 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:19<00:09, 17.33it/s, est. speed input: 18171.50 toks/s, output: 17.75 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:20<00:08, 17.24it/s, est. speed input: 18162.76 toks/s, output: 17.74 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:20<00:08, 17.26it/s, est. speed input: 18157.99 toks/s, output: 17.73 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:20<00:08, 17.32it/s, est. speed input: 18154.85 toks/s, output: 17.73 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:20<00:08, 17.11it/s, est. speed input: 18142.14 toks/s, output: 17.72 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:21<00:08, 17.19it/s, est. speed input: 18138.41 toks/s, output: 17.71 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:21<00:07, 17.16it/s, est. speed input: 18131.44 toks/s, output: 17.71 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:21<00:07, 17.21it/s, est. speed input: 18127.38 toks/s, output: 17.70 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:21<00:07, 17.22it/s, est. speed input: 18122.07 toks/s, output: 17.70 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:22<00:07, 17.10it/s, est. speed input: 18112.50 toks/s, output: 17.69 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:22<00:06, 17.10it/s, est. speed input: 18106.20 toks/s, output: 17.68 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:22<00:06, 17.18it/s, est. speed input: 18103.10 toks/s, output: 17.68 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:22<00:06, 17.22it/s, est. speed input: 18099.06 toks/s, output: 17.67 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:22<00:06, 17.12it/s, est. speed input: 18090.93 toks/s, output: 17.67 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:23<00:05, 17.26it/s, est. speed input: 18090.04 toks/s, output: 17.67 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:23<00:05, 17.25it/s, est. speed input: 18085.53 toks/s, output: 17.66 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:23<00:05, 17.23it/s, est. speed input: 18080.83 toks/s, output: 17.66 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:23<00:05, 17.24it/s, est. speed input: 18077.03 toks/s, output: 17.65 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:24<00:05, 17.12it/s, est. speed input: 18068.88 toks/s, output: 17.65 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:24<00:04, 17.19it/s, est. speed input: 18065.91 toks/s, output: 17.64 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:24<00:04, 17.20it/s, est. speed input: 18062.06 toks/s, output: 17.64 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:24<00:04, 17.22it/s, est. speed input: 18058.53 toks/s, output: 17.64 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:25<00:04, 17.13it/s, est. speed input: 18051.48 toks/s, output: 17.63 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:25<00:03, 17.17it/s, est. speed input: 18048.00 toks/s, output: 17.62 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:25<00:03, 18.28it/s, est. speed input: 18077.27 toks/s, output: 17.65 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:25<00:03, 17.95it/s, est. speed input: 18073.14 toks/s, output: 17.65 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:25<00:03, 17.72it/s, est. speed input: 18069.03 toks/s, output: 17.65 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:26<00:02, 17.47it/s, est. speed input: 18062.24 toks/s, output: 17.64 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:26<00:02, 17.37it/s, est. speed input: 18057.72 toks/s, output: 17.63 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:26<00:02, 17.39it/s, est. speed input: 18055.96 toks/s, output: 17.63 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:26<00:02, 17.38it/s, est. speed input: 18053.60 toks/s, output: 17.63 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:27<00:01, 17.40it/s, est. speed input: 18051.93 toks/s, output: 17.63 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:27<00:01, 17.22it/s, est. speed input: 18044.79 toks/s, output: 17.62 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:27<00:01, 17.28it/s, est. speed input: 18043.19 toks/s, output: 17.62 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:27<00:01, 17.29it/s, est. speed input: 18040.57 toks/s, output: 17.62 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:28<00:01, 17.30it/s, est. speed input: 18038.07 toks/s, output: 17.62 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:28<00:00, 17.21it/s, est. speed input: 18032.81 toks/s, output: 17.61 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:28<00:00, 17.18it/s, est. speed input: 18028.74 toks/s, output: 17.61 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:28<00:00, 17.27it/s, est. speed input: 18027.59 toks/s, output: 17.61 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:28<00:00, 18.48it/s, est. speed input: 18056.35 toks/s, output: 17.63 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:28<00:00, 18.48it/s, est. speed input: 18127.04 toks/s, output: 17.70 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:28<00:00, 17.70it/s, est. speed input: 18127.04 toks/s, output: 17.70 toks/s]
[rank0]:[W128 07:48:39.096533840 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 60.3s

测试结果:
  Requests/s:   17.15
  Tokens/s:     17581.12
  Total Reqs:   512
  Elapsed:      29.85s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     17563.97

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:48:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:48:47 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3654912) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3654912) WARNING 01-28 07:49:11 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.85 requests/s, 17267.82 total tokens/s, 16.85 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 07:48:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:48:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:48:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:48:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:48:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:48:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:48:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:48:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:48:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:48:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:48:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:48:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:48:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:48:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:48:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:48:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:48:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:48:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:48:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:48:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:48:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:48:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:48:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:48:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:48:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:48:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:48:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:48:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3654912) [2026-01-28 07:48:52] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3654912) [2026-01-28 07:48:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3654912) [2026-01-28 07:48:52] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3654912) [2026-01-28 07:48:52] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3654912) [2026-01-28 07:48:52] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3654912) [2026-01-28 07:48:52] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3654912) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3654912) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.47s/it]
(EngineCore_DP0 pid=3654912) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.48s/it]
(EngineCore_DP0 pid=3654912) 
(EngineCore_DP0 pid=3654912) [2026-01-28 07:49:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3654912) [2026-01-28 07:49:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3654912) [2026-01-28 07:49:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3654912) [2026-01-28 07:49:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3654912) [2026-01-28 07:49:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3654912) [2026-01-28 07:49:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3654912) [2026-01-28 07:49:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3654912) [2026-01-28 07:49:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3654912) 2026-01-28 07:49:10,034 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3654912) 2026-01-28 07:49:10,098 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   6%|▋         | 66/1024 [00:00<00:01, 657.76it/s]
Adding requests:  13%|█▎        | 132/1024 [00:00<00:01, 623.00it/s]
Adding requests:  19%|█▉        | 195/1024 [00:00<00:01, 566.83it/s]
Adding requests:  25%|██▍       | 253/1024 [00:00<00:01, 564.01it/s]
Adding requests:  30%|███       | 310/1024 [00:00<00:01, 557.34it/s]
Adding requests:  36%|███▌      | 366/1024 [00:00<00:01, 551.65it/s]
Adding requests:  41%|████      | 422/1024 [00:00<00:01, 545.44it/s]
Adding requests:  47%|████▋     | 477/1024 [00:00<00:01, 541.31it/s]
Adding requests:  52%|█████▏    | 532/1024 [00:00<00:00, 522.32it/s]
Adding requests:  57%|█████▋    | 585/1024 [00:01<00:00, 521.47it/s]
Adding requests:  62%|██████▎   | 640/1024 [00:01<00:00, 527.33it/s]
Adding requests:  68%|██████▊   | 696/1024 [00:01<00:00, 534.57it/s]
Adding requests:  73%|███████▎  | 751/1024 [00:01<00:00, 538.68it/s]
Adding requests:  79%|███████▊  | 805/1024 [00:01<00:00, 537.09it/s]
Adding requests:  84%|████████▍ | 859/1024 [00:01<00:00, 536.53it/s]
Adding requests:  89%|████████▉ | 914/1024 [00:01<00:00, 537.80it/s]
Adding requests:  95%|█████████▍| 968/1024 [00:01<00:00, 533.47it/s]
Adding requests: 100%|█████████▉| 1023/1024 [00:01<00:00, 538.01it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 543.92it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 26/1024 [00:00<00:07, 131.92it/s, est. speed input: 135098.79 toks/s, output: 131.93 toks/s]
Processed prompts:   4%|▍         | 40/1024 [00:00<00:18, 52.65it/s, est. speed input: 61065.78 toks/s, output: 59.63 toks/s]   
Processed prompts:   5%|▍         | 48/1024 [00:01<00:28, 33.78it/s, est. speed input: 42868.04 toks/s, output: 41.86 toks/s]
Processed prompts:   5%|▌         | 53/1024 [00:01<00:40, 23.87it/s, est. speed input: 33521.99 toks/s, output: 32.74 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:02<00:51, 18.82it/s, est. speed input: 28389.39 toks/s, output: 27.72 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:02<00:52, 18.08it/s, est. speed input: 26292.33 toks/s, output: 25.68 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:03<00:53, 17.70it/s, est. speed input: 24905.12 toks/s, output: 24.32 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:03<00:54, 17.38it/s, est. speed input: 23845.35 toks/s, output: 23.29 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:03<00:54, 17.25it/s, est. speed input: 23083.76 toks/s, output: 22.54 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:04<00:54, 17.10it/s, est. speed input: 22451.77 toks/s, output: 21.93 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:04<00:53, 17.05it/s, est. speed input: 21963.95 toks/s, output: 21.45 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:05<00:53, 16.95it/s, est. speed input: 21537.44 toks/s, output: 21.03 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:05<00:53, 16.94it/s, est. speed input: 21198.18 toks/s, output: 20.70 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:06<00:52, 16.94it/s, est. speed input: 20911.40 toks/s, output: 20.42 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:06<00:52, 16.88it/s, est. speed input: 20649.79 toks/s, output: 20.17 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:07<00:51, 16.89it/s, est. speed input: 20434.27 toks/s, output: 19.96 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:07<00:51, 16.85it/s, est. speed input: 20232.52 toks/s, output: 19.76 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:08<00:51, 16.88it/s, est. speed input: 20068.42 toks/s, output: 19.60 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:08<00:50, 16.84it/s, est. speed input: 19909.78 toks/s, output: 19.44 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:09<00:50, 16.86it/s, est. speed input: 19776.05 toks/s, output: 19.31 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:09<00:49, 16.83it/s, est. speed input: 19646.71 toks/s, output: 19.19 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:10<00:49, 16.86it/s, est. speed input: 19539.27 toks/s, output: 19.08 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:10<00:48, 16.87it/s, est. speed input: 19440.57 toks/s, output: 18.98 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:11<00:48, 16.85it/s, est. speed input: 19344.34 toks/s, output: 18.89 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:11<00:47, 16.87it/s, est. speed input: 19262.06 toks/s, output: 18.81 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:12<00:47, 16.82it/s, est. speed input: 19176.65 toks/s, output: 18.73 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:12<00:46, 16.85it/s, est. speed input: 19106.58 toks/s, output: 18.66 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:13<00:46, 16.82it/s, est. speed input: 19035.45 toks/s, output: 18.59 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:13<00:45, 16.88it/s, est. speed input: 18979.17 toks/s, output: 18.53 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:13<00:45, 16.83it/s, est. speed input: 18915.02 toks/s, output: 18.47 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:14<00:45, 16.84it/s, est. speed input: 18861.23 toks/s, output: 18.42 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:14<00:44, 16.87it/s, est. speed input: 18813.45 toks/s, output: 18.37 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:15<00:44, 16.84it/s, est. speed input: 18762.02 toks/s, output: 18.32 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:15<00:43, 16.87it/s, est. speed input: 18720.33 toks/s, output: 18.28 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:16<00:43, 16.83it/s, est. speed input: 18673.47 toks/s, output: 18.24 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:16<00:42, 16.85it/s, est. speed input: 18635.32 toks/s, output: 18.20 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:17<00:42, 16.83it/s, est. speed input: 18594.89 toks/s, output: 18.16 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:17<00:41, 16.86it/s, est. speed input: 18561.92 toks/s, output: 18.13 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:18<00:41, 16.82it/s, est. speed input: 18524.05 toks/s, output: 18.09 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:18<00:39, 17.26it/s, est. speed input: 18531.56 toks/s, output: 18.10 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:19<00:39, 17.16it/s, est. speed input: 18501.99 toks/s, output: 18.07 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:19<00:39, 17.03it/s, est. speed input: 18468.25 toks/s, output: 18.04 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:20<00:38, 17.00it/s, est. speed input: 18442.25 toks/s, output: 18.01 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:20<00:38, 16.93it/s, est. speed input: 18412.20 toks/s, output: 17.98 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:21<00:38, 16.92it/s, est. speed input: 18387.71 toks/s, output: 17.96 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:21<00:37, 16.87it/s, est. speed input: 18360.48 toks/s, output: 17.93 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:22<00:37, 16.87it/s, est. speed input: 18336.77 toks/s, output: 17.91 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:22<00:36, 16.84it/s, est. speed input: 18312.44 toks/s, output: 17.88 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:22<00:36, 16.87it/s, est. speed input: 18292.70 toks/s, output: 17.86 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:23<00:35, 16.87it/s, est. speed input: 18271.91 toks/s, output: 17.84 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:23<00:35, 16.84it/s, est. speed input: 18250.11 toks/s, output: 17.82 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:24<00:34, 16.87it/s, est. speed input: 18232.59 toks/s, output: 17.81 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:24<00:34, 16.83it/s, est. speed input: 18211.72 toks/s, output: 17.78 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:25<00:33, 17.31it/s, est. speed input: 18224.96 toks/s, output: 17.80 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:25<00:33, 17.10it/s, est. speed input: 18202.24 toks/s, output: 17.78 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:26<00:32, 17.04it/s, est. speed input: 18186.18 toks/s, output: 17.76 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:26<00:32, 16.96it/s, est. speed input: 18168.04 toks/s, output: 17.74 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:27<00:31, 16.95it/s, est. speed input: 18153.59 toks/s, output: 17.73 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:27<00:31, 16.94it/s, est. speed input: 18139.43 toks/s, output: 17.71 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:28<00:31, 16.89it/s, est. speed input: 18123.21 toks/s, output: 17.70 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:28<00:30, 16.90it/s, est. speed input: 18109.87 toks/s, output: 17.69 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:29<00:30, 16.86it/s, est. speed input: 18094.49 toks/s, output: 17.67 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:29<00:29, 16.87it/s, est. speed input: 18081.52 toks/s, output: 17.66 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:30<00:29, 16.84it/s, est. speed input: 18067.32 toks/s, output: 17.64 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:30<00:29, 16.33it/s, est. speed input: 18025.24 toks/s, output: 17.60 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:31<00:29, 16.41it/s, est. speed input: 18009.28 toks/s, output: 17.59 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:31<00:28, 16.55it/s, est. speed input: 17998.53 toks/s, output: 17.58 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:31<00:27, 16.61it/s, est. speed input: 17986.05 toks/s, output: 17.56 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:32<00:27, 16.69it/s, est. speed input: 17975.91 toks/s, output: 17.55 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:32<00:26, 16.77it/s, est. speed input: 17967.11 toks/s, output: 17.55 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:33<00:26, 16.76it/s, est. speed input: 17955.00 toks/s, output: 17.53 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:33<00:25, 16.82it/s, est. speed input: 17947.13 toks/s, output: 17.53 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:34<00:25, 16.79it/s, est. speed input: 17935.37 toks/s, output: 17.51 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:34<00:24, 16.82it/s, est. speed input: 17926.94 toks/s, output: 17.51 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:35<00:24, 16.81it/s, est. speed input: 17916.70 toks/s, output: 17.50 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:35<00:23, 16.84it/s, est. speed input: 17908.99 toks/s, output: 17.49 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:36<00:23, 16.82it/s, est. speed input: 17899.21 toks/s, output: 17.48 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:36<00:22, 16.85it/s, est. speed input: 17891.87 toks/s, output: 17.47 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:37<00:22, 16.89it/s, est. speed input: 17885.35 toks/s, output: 17.47 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:37<00:21, 16.84it/s, est. speed input: 17875.93 toks/s, output: 17.46 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:38<00:21, 16.88it/s, est. speed input: 17869.70 toks/s, output: 17.45 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:38<00:20, 16.83it/s, est. speed input: 17860.22 toks/s, output: 17.44 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:39<00:20, 16.86it/s, est. speed input: 17853.93 toks/s, output: 17.44 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:39<00:19, 16.80it/s, est. speed input: 17844.62 toks/s, output: 17.43 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:40<00:19, 16.84it/s, est. speed input: 17838.64 toks/s, output: 17.42 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:40<00:18, 16.83it/s, est. speed input: 17831.33 toks/s, output: 17.41 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:41<00:18, 16.87it/s, est. speed input: 17826.00 toks/s, output: 17.41 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:41<00:17, 16.89it/s, est. speed input: 17820.46 toks/s, output: 17.40 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:41<00:17, 16.83it/s, est. speed input: 17812.14 toks/s, output: 17.39 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:42<00:16, 16.86it/s, est. speed input: 17806.72 toks/s, output: 17.39 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:42<00:16, 16.82it/s, est. speed input: 17799.44 toks/s, output: 17.38 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:43<00:16, 16.85it/s, est. speed input: 17794.13 toks/s, output: 17.38 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:43<00:15, 16.83it/s, est. speed input: 17787.65 toks/s, output: 17.37 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:44<00:15, 16.85it/s, est. speed input: 17782.23 toks/s, output: 17.37 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:44<00:14, 16.82it/s, est. speed input: 17775.79 toks/s, output: 17.36 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:45<00:14, 16.88it/s, est. speed input: 17772.09 toks/s, output: 17.36 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:45<00:13, 16.87it/s, est. speed input: 17766.56 toks/s, output: 17.35 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:46<00:13, 16.85it/s, est. speed input: 17760.97 toks/s, output: 17.34 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:46<00:12, 16.85it/s, est. speed input: 17755.85 toks/s, output: 17.34 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:47<00:12, 16.84it/s, est. speed input: 17750.27 toks/s, output: 17.33 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:47<00:11, 16.85it/s, est. speed input: 17745.74 toks/s, output: 17.33 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:48<00:11, 16.83it/s, est. speed input: 17740.05 toks/s, output: 17.32 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:48<00:10, 16.88it/s, est. speed input: 17736.75 toks/s, output: 17.32 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:49<00:10, 16.82it/s, est. speed input: 17730.55 toks/s, output: 17.31 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:49<00:09, 16.84it/s, est. speed input: 17726.45 toks/s, output: 17.31 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:50<00:09, 16.85it/s, est. speed input: 17721.97 toks/s, output: 17.31 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:50<00:08, 16.84it/s, est. speed input: 17717.35 toks/s, output: 17.30 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:50<00:08, 16.89it/s, est. speed input: 17714.53 toks/s, output: 17.30 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:51<00:07, 16.82it/s, est. speed input: 17708.40 toks/s, output: 17.29 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:51<00:07, 16.86it/s, est. speed input: 17705.38 toks/s, output: 17.29 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:52<00:07, 16.83it/s, est. speed input: 17700.38 toks/s, output: 17.29 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:52<00:06, 16.85it/s, est. speed input: 17696.93 toks/s, output: 17.28 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:53<00:06, 16.82it/s, est. speed input: 17692.09 toks/s, output: 17.28 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:53<00:05, 16.86it/s, est. speed input: 17689.05 toks/s, output: 17.27 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:54<00:04, 17.52it/s, est. speed input: 17704.92 toks/s, output: 17.29 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:54<00:04, 17.29it/s, est. speed input: 17700.14 toks/s, output: 17.29 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:55<00:04, 17.16it/s, est. speed input: 17696.44 toks/s, output: 17.28 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:55<00:03, 17.03it/s, est. speed input: 17691.74 toks/s, output: 17.28 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:56<00:03, 17.01it/s, est. speed input: 17689.04 toks/s, output: 17.27 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:56<00:02, 16.94it/s, est. speed input: 17684.69 toks/s, output: 17.27 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:57<00:02, 17.53it/s, est. speed input: 17698.25 toks/s, output: 17.28 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:57<00:01, 17.27it/s, est. speed input: 17693.29 toks/s, output: 17.28 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:58<00:01, 17.16it/s, est. speed input: 17690.08 toks/s, output: 17.28 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:58<00:00, 17.09it/s, est. speed input: 17687.36 toks/s, output: 17.27 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:58<00:00, 17.56it/s, est. speed input: 17698.37 toks/s, output: 17.28 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:58<00:00, 17.56it/s, est. speed input: 17802.65 toks/s, output: 17.39 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:58<00:00, 17.39it/s, est. speed input: 17802.65 toks/s, output: 17.39 toks/s]
[rank0]:[W128 07:50:12.131567086 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 93.0s

测试结果:
  Requests/s:   16.85
  Tokens/s:     17267.82
  Total Reqs:   1024
  Elapsed:      60.78s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     17250.97

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:50:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:50:23 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3656424) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3656424) WARNING 01-28 07:50:48 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 17.03 requests/s, 17451.29 total tokens/s, 17.03 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 07:50:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:50:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:50:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:50:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:50:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:50:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:50:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:50:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:50:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:50:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:50:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:50:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:50:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:50:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:50:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:50:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:50:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:50:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:50:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:50:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:50:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:50:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:50:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:50:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:50:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:50:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:50:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:50:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:27] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:27] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:27] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:27] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:27] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3656424) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3656424) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.64s/it]
(EngineCore_DP0 pid=3656424) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.64s/it]
(EngineCore_DP0 pid=3656424) 
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3656424) [2026-01-28 07:50:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3656424) 2026-01-28 07:50:46,589 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3656424) 2026-01-28 07:50:46,993 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 65/2048 [00:00<00:03, 639.02it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:03, 598.53it/s]
Adding requests:   9%|▉         | 190/2048 [00:00<00:03, 541.20it/s]
Adding requests:  12%|█▏        | 245/2048 [00:00<00:03, 524.21it/s]
Adding requests:  15%|█▍        | 298/2048 [00:00<00:03, 516.98it/s]
Adding requests:  17%|█▋        | 350/2048 [00:00<00:03, 516.48it/s]
Adding requests:  20%|█▉        | 402/2048 [00:00<00:03, 511.77it/s]
Adding requests:  22%|██▏       | 454/2048 [00:00<00:03, 508.85it/s]
Adding requests:  25%|██▍       | 506/2048 [00:00<00:03, 510.45it/s]
Adding requests:  27%|██▋       | 558/2048 [00:01<00:02, 505.55it/s]
Adding requests:  30%|██▉       | 609/2048 [00:01<00:02, 498.54it/s]
Adding requests:  32%|███▏      | 661/2048 [00:01<00:02, 504.28it/s]
Adding requests:  35%|███▍      | 714/2048 [00:01<00:02, 511.67it/s]
Adding requests:  37%|███▋      | 766/2048 [00:01<00:02, 492.73it/s]
Adding requests:  40%|███▉      | 816/2048 [00:01<00:02, 493.08it/s]
Adding requests:  42%|████▏     | 866/2048 [00:02<00:06, 189.47it/s]
Adding requests:  45%|████▍     | 915/2048 [00:02<00:04, 230.42it/s]
Adding requests:  47%|████▋     | 968/2048 [00:02<00:03, 279.03it/s]
Adding requests:  50%|████▉     | 1020/2048 [00:02<00:03, 324.64it/s]
Adding requests:  52%|█████▏    | 1071/2048 [00:02<00:02, 363.30it/s]
Adding requests:  55%|█████▍    | 1123/2048 [00:02<00:02, 399.42it/s]
Adding requests:  57%|█████▋    | 1176/2048 [00:02<00:02, 430.51it/s]
Adding requests:  60%|██████    | 1230/2048 [00:02<00:01, 459.04it/s]
Adding requests:  63%|██████▎   | 1282/2048 [00:03<00:01, 473.64it/s]
Adding requests:  65%|██████▌   | 1335/2048 [00:03<00:01, 488.59it/s]
Adding requests:  68%|██████▊   | 1387/2048 [00:03<00:01, 495.39it/s]
Adding requests:  70%|███████   | 1439/2048 [00:03<00:01, 502.37it/s]
Adding requests:  73%|███████▎  | 1494/2048 [00:03<00:01, 514.98it/s]
Adding requests:  76%|███████▌  | 1547/2048 [00:03<00:00, 518.46it/s]
Adding requests:  78%|███████▊  | 1604/2048 [00:03<00:00, 532.46it/s]
Adding requests:  81%|████████  | 1658/2048 [00:03<00:00, 521.91it/s]
Adding requests:  84%|████████▎ | 1711/2048 [00:03<00:00, 519.15it/s]
Adding requests:  86%|████████▌ | 1764/2048 [00:03<00:00, 515.92it/s]
Adding requests:  89%|████████▊ | 1816/2048 [00:04<00:00, 510.71it/s]
Adding requests:  91%|█████████ | 1868/2048 [00:04<00:00, 510.13it/s]
Adding requests:  94%|█████████▍| 1921/2048 [00:04<00:00, 514.93it/s]
Adding requests:  96%|█████████▋| 1973/2048 [00:04<00:00, 510.80it/s]
Adding requests:  99%|█████████▉| 2025/2048 [00:04<00:00, 493.22it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 450.58it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 66/2048 [00:00<00:10, 187.13it/s, est. speed input: 191633.97 toks/s, output: 187.13 toks/s]
Processed prompts:   4%|▍         | 85/2048 [00:01<00:35, 54.79it/s, est. speed input: 67169.21 toks/s, output: 65.59 toks/s]   
Processed prompts:   5%|▍         | 98/2048 [00:02<00:58, 33.10it/s, est. speed input: 44915.03 toks/s, output: 43.86 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:03<01:14, 26.13it/s, est. speed input: 36742.37 toks/s, output: 35.88 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:04<01:24, 22.68it/s, est. speed input: 32350.55 toks/s, output: 31.59 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:05<01:32, 20.64it/s, est. speed input: 29543.86 toks/s, output: 28.85 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:06<01:37, 19.41it/s, est. speed input: 27633.86 toks/s, output: 26.99 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:06<01:40, 18.67it/s, est. speed input: 26266.75 toks/s, output: 25.65 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:07<01:42, 18.14it/s, est. speed input: 25210.02 toks/s, output: 24.62 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:08<01:43, 17.76it/s, est. speed input: 24367.03 toks/s, output: 23.80 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:09<01:44, 17.50it/s, est. speed input: 23685.92 toks/s, output: 23.13 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:10<01:44, 17.33it/s, est. speed input: 23127.77 toks/s, output: 22.59 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:11<01:43, 17.24it/s, est. speed input: 22668.95 toks/s, output: 22.14 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:12<01:43, 17.14it/s, est. speed input: 22267.36 toks/s, output: 21.75 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:13<01:42, 17.08it/s, est. speed input: 21923.79 toks/s, output: 21.41 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:14<01:42, 17.06it/s, est. speed input: 21631.33 toks/s, output: 21.12 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:15<01:41, 17.01it/s, est. speed input: 21366.00 toks/s, output: 20.87 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:16<01:39, 17.24it/s, est. speed input: 21193.80 toks/s, output: 20.70 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:17<01:38, 17.16it/s, est. speed input: 20986.05 toks/s, output: 20.49 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:18<01:38, 17.10it/s, est. speed input: 20798.96 toks/s, output: 20.31 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:19<01:37, 17.06it/s, est. speed input: 20630.48 toks/s, output: 20.15 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:20<01:36, 17.07it/s, est. speed input: 20485.42 toks/s, output: 20.01 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:21<01:35, 17.06it/s, est. speed input: 20349.81 toks/s, output: 19.87 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:21<01:34, 17.05it/s, est. speed input: 20224.02 toks/s, output: 19.75 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:22<01:32, 17.26it/s, est. speed input: 20144.01 toks/s, output: 19.67 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:23<01:32, 17.16it/s, est. speed input: 20033.05 toks/s, output: 19.56 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:24<01:31, 17.08it/s, est. speed input: 19928.91 toks/s, output: 19.46 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:25<01:30, 17.04it/s, est. speed input: 19833.52 toks/s, output: 19.37 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:26<01:30, 17.04it/s, est. speed input: 19749.73 toks/s, output: 19.29 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:27<01:29, 17.00it/s, est. speed input: 19666.06 toks/s, output: 19.21 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:28<01:28, 16.98it/s, est. speed input: 19589.62 toks/s, output: 19.13 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:29<01:27, 16.99it/s, est. speed input: 19520.46 toks/s, output: 19.06 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:30<01:26, 16.97it/s, est. speed input: 19452.50 toks/s, output: 19.00 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:31<01:25, 16.96it/s, est. speed input: 19389.11 toks/s, output: 18.93 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:32<01:24, 16.95it/s, est. speed input: 19329.03 toks/s, output: 18.88 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:33<01:23, 16.94it/s, est. speed input: 19272.29 toks/s, output: 18.82 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:34<01:22, 16.97it/s, est. speed input: 19221.80 toks/s, output: 18.77 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:35<01:21, 16.98it/s, est. speed input: 19173.22 toks/s, output: 18.72 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:36<01:20, 17.00it/s, est. speed input: 19128.86 toks/s, output: 18.68 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:37<01:19, 17.02it/s, est. speed input: 19087.09 toks/s, output: 18.64 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:37<01:18, 17.03it/s, est. speed input: 19046.69 toks/s, output: 18.60 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:38<01:17, 17.01it/s, est. speed input: 19005.74 toks/s, output: 18.56 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:39<01:17, 16.97it/s, est. speed input: 18964.96 toks/s, output: 18.52 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:40<01:16, 16.97it/s, est. speed input: 18928.13 toks/s, output: 18.48 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:41<01:15, 16.99it/s, est. speed input: 18894.83 toks/s, output: 18.45 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:42<01:14, 16.97it/s, est. speed input: 18860.06 toks/s, output: 18.42 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:43<01:13, 16.97it/s, est. speed input: 18828.01 toks/s, output: 18.39 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:44<01:12, 16.94it/s, est. speed input: 18795.16 toks/s, output: 18.35 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:45<01:11, 16.96it/s, est. speed input: 18766.43 toks/s, output: 18.33 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:46<01:10, 16.96it/s, est. speed input: 18738.18 toks/s, output: 18.30 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:47<01:09, 16.95it/s, est. speed input: 18709.85 toks/s, output: 18.27 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:48<01:08, 16.95it/s, est. speed input: 18683.44 toks/s, output: 18.25 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:49<01:07, 16.95it/s, est. speed input: 18657.94 toks/s, output: 18.22 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:50<01:06, 16.94it/s, est. speed input: 18632.81 toks/s, output: 18.20 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:51<01:04, 17.24it/s, est. speed input: 18629.38 toks/s, output: 18.19 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:52<01:04, 17.14it/s, est. speed input: 18605.53 toks/s, output: 18.17 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:53<01:03, 17.09it/s, est. speed input: 18583.40 toks/s, output: 18.15 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:53<01:01, 17.40it/s, est. speed input: 18583.98 toks/s, output: 18.15 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:54<01:01, 17.27it/s, est. speed input: 18562.97 toks/s, output: 18.13 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:55<01:00, 17.16it/s, est. speed input: 18542.25 toks/s, output: 18.11 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:56<00:59, 17.08it/s, est. speed input: 18521.30 toks/s, output: 18.09 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:57<00:58, 17.05it/s, est. speed input: 18502.99 toks/s, output: 18.07 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:58<00:58, 17.04it/s, est. speed input: 18485.46 toks/s, output: 18.05 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:59<00:57, 17.02it/s, est. speed input: 18467.92 toks/s, output: 18.04 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [01:00<00:56, 17.00it/s, est. speed input: 18450.75 toks/s, output: 18.02 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [01:01<00:55, 16.99it/s, est. speed input: 18434.44 toks/s, output: 18.00 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [01:02<00:54, 16.95it/s, est. speed input: 18416.68 toks/s, output: 17.99 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [01:03<00:53, 16.96it/s, est. speed input: 18400.98 toks/s, output: 17.97 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [01:04<00:51, 17.25it/s, est. speed input: 18401.11 toks/s, output: 17.97 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [01:05<00:51, 17.15it/s, est. speed input: 18385.52 toks/s, output: 17.95 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [01:06<00:50, 17.12it/s, est. speed input: 18372.39 toks/s, output: 17.94 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [01:07<00:49, 17.05it/s, est. speed input: 18357.19 toks/s, output: 17.93 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [01:07<00:48, 17.01it/s, est. speed input: 18342.99 toks/s, output: 17.91 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [01:08<00:47, 17.01it/s, est. speed input: 18330.06 toks/s, output: 17.90 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [01:09<00:46, 16.98it/s, est. speed input: 18316.51 toks/s, output: 17.89 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [01:10<00:45, 17.25it/s, est. speed input: 18316.81 toks/s, output: 17.89 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [01:11<00:44, 17.15it/s, est. speed input: 18303.96 toks/s, output: 17.87 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [01:12<00:43, 17.37it/s, est. speed input: 18304.15 toks/s, output: 17.88 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [01:13<00:42, 17.24it/s, est. speed input: 18292.17 toks/s, output: 17.86 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [01:14<00:41, 17.13it/s, est. speed input: 18279.18 toks/s, output: 17.85 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [01:15<00:41, 17.07it/s, est. speed input: 18267.56 toks/s, output: 17.84 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [01:16<00:40, 17.05it/s, est. speed input: 18256.75 toks/s, output: 17.83 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [01:17<00:39, 17.01it/s, est. speed input: 18245.44 toks/s, output: 17.82 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [01:18<00:38, 16.98it/s, est. speed input: 18234.33 toks/s, output: 17.81 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [01:19<00:37, 16.97it/s, est. speed input: 18223.56 toks/s, output: 17.80 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [01:20<00:36, 16.96it/s, est. speed input: 18213.21 toks/s, output: 17.79 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [01:21<00:35, 17.00it/s, est. speed input: 18204.95 toks/s, output: 17.78 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [01:22<00:34, 16.97it/s, est. speed input: 18194.72 toks/s, output: 17.77 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [01:23<00:33, 16.95it/s, est. speed input: 18184.73 toks/s, output: 17.76 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [01:23<00:32, 16.97it/s, est. speed input: 18176.12 toks/s, output: 17.75 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [01:24<00:31, 16.95it/s, est. speed input: 18166.56 toks/s, output: 17.74 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [01:25<00:31, 16.95it/s, est. speed input: 18157.59 toks/s, output: 17.73 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [01:26<00:30, 16.95it/s, est. speed input: 18148.79 toks/s, output: 17.72 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [01:27<00:29, 16.94it/s, est. speed input: 18139.87 toks/s, output: 17.71 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [01:28<00:28, 16.94it/s, est. speed input: 18131.46 toks/s, output: 17.71 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [01:29<00:26, 17.22it/s, est. speed input: 18133.54 toks/s, output: 17.71 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [01:30<00:26, 17.10it/s, est. speed input: 18124.09 toks/s, output: 17.70 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [01:31<00:25, 17.07it/s, est. speed input: 18116.82 toks/s, output: 17.69 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [01:32<00:24, 17.05it/s, est. speed input: 18109.41 toks/s, output: 17.68 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [01:33<00:23, 17.30it/s, est. speed input: 18111.59 toks/s, output: 17.69 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [01:34<00:22, 17.18it/s, est. speed input: 18103.57 toks/s, output: 17.68 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [01:35<00:21, 17.11it/s, est. speed input: 18096.34 toks/s, output: 17.67 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [01:36<00:20, 17.05it/s, est. speed input: 18088.71 toks/s, output: 17.66 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [01:37<00:19, 17.06it/s, est. speed input: 18082.85 toks/s, output: 17.66 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [01:38<00:18, 17.01it/s, est. speed input: 18075.35 toks/s, output: 17.65 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [01:38<00:17, 16.99it/s, est. speed input: 18068.27 toks/s, output: 17.64 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [01:39<00:16, 17.00it/s, est. speed input: 18062.28 toks/s, output: 17.64 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [01:40<00:15, 16.97it/s, est. speed input: 18055.24 toks/s, output: 17.63 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [01:41<00:14, 16.96it/s, est. speed input: 18048.66 toks/s, output: 17.63 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [01:42<00:14, 16.94it/s, est. speed input: 18041.62 toks/s, output: 17.62 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [01:43<00:13, 16.96it/s, est. speed input: 18035.91 toks/s, output: 17.61 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [01:44<00:12, 16.95it/s, est. speed input: 18029.47 toks/s, output: 17.61 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [01:45<00:11, 16.97it/s, est. speed input: 18024.24 toks/s, output: 17.60 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [01:46<00:10, 17.25it/s, est. speed input: 18027.05 toks/s, output: 17.60 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [01:47<00:09, 17.17it/s, est. speed input: 18021.57 toks/s, output: 17.60 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [01:48<00:08, 17.10it/s, est. speed input: 18015.55 toks/s, output: 17.59 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [01:49<00:07, 17.05it/s, est. speed input: 18009.88 toks/s, output: 17.59 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [01:50<00:06, 17.01it/s, est. speed input: 18003.79 toks/s, output: 17.58 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [01:51<00:05, 17.25it/s, est. speed input: 18006.08 toks/s, output: 17.58 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [01:52<00:04, 17.15it/s, est. speed input: 18000.22 toks/s, output: 17.58 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [01:52<00:03, 17.37it/s, est. speed input: 18002.85 toks/s, output: 17.58 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [01:53<00:02, 17.23it/s, est. speed input: 17997.30 toks/s, output: 17.58 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [01:54<00:01, 17.17it/s, est. speed input: 17992.82 toks/s, output: 17.57 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [01:55<00:00, 17.38it/s, est. speed input: 17995.35 toks/s, output: 17.57 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [01:55<00:00, 17.38it/s, est. speed input: 18119.16 toks/s, output: 17.69 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [01:55<00:00, 17.69it/s, est. speed input: 18119.16 toks/s, output: 17.69 toks/s]
[rank0]:[W128 07:52:49.165850757 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 157.1s

测试结果:
  Requests/s:   17.03
  Tokens/s:     17451.29
  Total Reqs:   2048
  Elapsed:      120.29s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     17434.26

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:53:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:53:06 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3658855) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3658855) WARNING 01-28 07:53:32 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 17.11 requests/s, 17536.23 total tokens/s, 17.11 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 07:53:06] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:53:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:53:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:53:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:53:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:53:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:53:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:53:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:53:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:53:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:53:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:53:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:53:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:53:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:53:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:53:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:53:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:53:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:53:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:53:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:53:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:53:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:53:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:53:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:53:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:53:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:53:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:53:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:11] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3658855) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3658855) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.66s/it]
(EngineCore_DP0 pid=3658855) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.66s/it]
(EngineCore_DP0 pid=3658855) 
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3658855) [2026-01-28 07:53:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3658855) 2026-01-28 07:53:30,619 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3658855) 2026-01-28 07:53:30,753 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 65/4096 [00:00<00:06, 643.48it/s]
Adding requests:   3%|▎         | 130/4096 [00:00<00:06, 614.29it/s]
Adding requests:   5%|▍         | 192/4096 [00:00<00:07, 556.50it/s]
Adding requests:   6%|▌         | 249/4096 [00:00<00:07, 536.22it/s]
Adding requests:   7%|▋         | 303/4096 [00:00<00:07, 516.88it/s]
Adding requests:   9%|▊         | 355/4096 [00:00<00:07, 512.03it/s]
Adding requests:  10%|▉         | 407/4096 [00:00<00:07, 507.93it/s]
Adding requests:  11%|█         | 458/4096 [00:00<00:07, 505.88it/s]
Adding requests:  12%|█▏        | 509/4096 [00:00<00:07, 493.34it/s]
Adding requests:  14%|█▎        | 559/4096 [00:01<00:07, 491.28it/s]
Adding requests:  15%|█▍        | 609/4096 [00:01<00:07, 487.38it/s]
Adding requests:  16%|█▌        | 661/4096 [00:01<00:06, 496.90it/s]
Adding requests:  17%|█▋        | 713/4096 [00:01<00:06, 502.22it/s]
Adding requests:  19%|█▊        | 764/4096 [00:01<00:06, 497.30it/s]
Adding requests:  20%|█▉        | 814/4096 [00:01<00:06, 482.38it/s]
Adding requests:  21%|██        | 863/4096 [00:01<00:06, 484.48it/s]
Adding requests:  22%|██▏       | 914/4096 [00:01<00:06, 490.54it/s]
Adding requests:  24%|██▎       | 967/4096 [00:01<00:06, 499.36it/s]
Adding requests:  25%|██▍       | 1017/4096 [00:02<00:06, 497.99it/s]
Adding requests:  26%|██▌       | 1067/4096 [00:02<00:06, 497.09it/s]
Adding requests:  27%|██▋       | 1117/4096 [00:02<00:06, 487.87it/s]
Adding requests:  29%|██▊       | 1169/4096 [00:02<00:05, 494.65it/s]
Adding requests:  30%|██▉       | 1219/4096 [00:02<00:06, 473.75it/s]
Adding requests:  31%|███       | 1269/4096 [00:02<00:05, 480.53it/s]
Adding requests:  32%|███▏      | 1318/4096 [00:02<00:05, 478.70it/s]
Adding requests:  33%|███▎      | 1366/4096 [00:02<00:05, 477.74it/s]
Adding requests:  35%|███▍      | 1416/4096 [00:02<00:05, 481.70it/s]
Adding requests:  36%|███▌      | 1467/4096 [00:02<00:05, 486.57it/s]
Adding requests:  37%|███▋      | 1518/4096 [00:03<00:05, 491.66it/s]
Adding requests:  38%|███▊      | 1572/4096 [00:03<00:05, 504.48it/s]
Adding requests:  40%|███▉      | 1624/4096 [00:03<00:04, 508.99it/s]
Adding requests:  41%|████      | 1675/4096 [00:03<00:04, 507.76it/s]
Adding requests:  42%|████▏     | 1728/4096 [00:03<00:04, 513.74it/s]
Adding requests:  43%|████▎     | 1780/4096 [00:03<00:04, 512.40it/s]
Adding requests:  45%|████▍     | 1834/4096 [00:03<00:04, 518.96it/s]
Adding requests:  46%|████▌     | 1886/4096 [00:03<00:04, 517.92it/s]
Adding requests:  47%|████▋     | 1938/4096 [00:03<00:04, 514.49it/s]
Adding requests:  49%|████▊     | 1990/4096 [00:03<00:04, 513.12it/s]
Adding requests:  50%|████▉     | 2042/4096 [00:04<00:04, 507.39it/s]
Adding requests:  51%|█████     | 2096/4096 [00:04<00:03, 516.02it/s]
Adding requests:  52%|█████▏    | 2148/4096 [00:04<00:03, 511.27it/s]
Adding requests:  54%|█████▎    | 2200/4096 [00:04<00:03, 508.45it/s]
Adding requests:  55%|█████▌    | 2255/4096 [00:04<00:03, 519.95it/s]
Adding requests:  56%|█████▋    | 2308/4096 [00:04<00:03, 517.04it/s]
Adding requests:  58%|█████▊    | 2360/4096 [00:04<00:03, 517.42it/s]
Adding requests:  59%|█████▉    | 2413/4096 [00:04<00:03, 520.45it/s]
Adding requests:  60%|██████    | 2466/4096 [00:04<00:03, 488.97it/s]
Adding requests:  61%|██████▏   | 2516/4096 [00:04<00:03, 487.49it/s]
Adding requests:  63%|██████▎   | 2569/4096 [00:05<00:03, 498.77it/s]
Adding requests:  64%|██████▍   | 2621/4096 [00:05<00:02, 502.73it/s]
Adding requests:  65%|██████▌   | 2673/4096 [00:05<00:02, 505.78it/s]
Adding requests:  67%|██████▋   | 2726/4096 [00:05<00:02, 511.06it/s]
Adding requests:  68%|██████▊   | 2778/4096 [00:05<00:02, 507.24it/s]
Adding requests:  69%|██████▉   | 2830/4096 [00:05<00:02, 509.65it/s]
Adding requests:  70%|███████   | 2885/4096 [00:05<00:02, 520.58it/s]
Adding requests:  72%|███████▏  | 2938/4096 [00:05<00:02, 515.22it/s]
Adding requests:  73%|███████▎  | 2990/4096 [00:05<00:02, 506.78it/s]
Adding requests:  74%|███████▍  | 3041/4096 [00:06<00:02, 502.94it/s]
Adding requests:  75%|███████▌  | 3092/4096 [00:06<00:02, 496.09it/s]
Adding requests:  77%|███████▋  | 3143/4096 [00:06<00:01, 499.45it/s]
Adding requests:  78%|███████▊  | 3196/4096 [00:06<00:01, 506.35it/s]
Adding requests:  79%|███████▉  | 3250/4096 [00:06<00:01, 515.89it/s]
Adding requests:  81%|████████  | 3303/4096 [00:06<00:01, 517.74it/s]
Adding requests:  82%|████████▏ | 3356/4096 [00:06<00:01, 520.22it/s]
Adding requests:  83%|████████▎ | 3409/4096 [00:06<00:01, 523.07it/s]
Adding requests:  85%|████████▍ | 3462/4096 [00:06<00:01, 508.15it/s]
Adding requests:  86%|████████▌ | 3513/4096 [00:06<00:01, 508.50it/s]
Adding requests:  87%|████████▋ | 3564/4096 [00:07<00:01, 506.73it/s]
Adding requests:  88%|████████▊ | 3616/4096 [00:07<00:00, 508.93it/s]
Adding requests:  90%|████████▉ | 3671/4096 [00:07<00:00, 518.68it/s]
Adding requests:  91%|█████████ | 3723/4096 [00:07<00:00, 517.83it/s]
Adding requests:  92%|█████████▏| 3781/4096 [00:07<00:00, 534.40it/s]
Adding requests:  94%|█████████▎| 3835/4096 [00:07<00:00, 496.27it/s]
Adding requests:  95%|█████████▍| 3888/4096 [00:07<00:00, 504.45it/s]
Adding requests:  96%|█████████▌| 3939/4096 [00:07<00:00, 504.69it/s]
Adding requests:  97%|█████████▋| 3991/4096 [00:07<00:00, 507.28it/s]
Adding requests:  99%|█████████▊| 4044/4096 [00:07<00:00, 513.71it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 506.86it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:01<00:45, 86.82it/s, est. speed input: 88909.07 toks/s, output: 86.82 toks/s]
Processed prompts:   4%|▍         | 162/4096 [00:03<01:33, 42.13it/s, est. speed input: 49246.07 toks/s, output: 48.09 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:05<02:09, 30.19it/s, est. speed input: 37932.72 toks/s, output: 37.04 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:07<02:35, 24.90it/s, est. speed input: 32574.62 toks/s, output: 31.81 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:08<02:54, 22.05it/s, est. speed input: 29446.29 toks/s, output: 28.76 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:10<03:07, 20.34it/s, est. speed input: 27381.39 toks/s, output: 26.74 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:12<03:14, 19.44it/s, est. speed input: 26027.29 toks/s, output: 25.42 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:14<03:20, 18.68it/s, est. speed input: 24931.16 toks/s, output: 24.35 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:16<03:24, 18.18it/s, est. speed input: 24084.40 toks/s, output: 23.52 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:18<03:26, 17.83it/s, est. speed input: 23405.33 toks/s, output: 22.86 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:20<03:25, 17.77it/s, est. speed input: 22922.29 toks/s, output: 22.38 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:21<03:25, 17.55it/s, est. speed input: 22457.23 toks/s, output: 21.93 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:23<03:25, 17.40it/s, est. speed input: 22063.95 toks/s, output: 21.55 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:25<03:25, 17.29it/s, est. speed input: 21725.59 toks/s, output: 21.22 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:27<03:24, 17.23it/s, est. speed input: 21441.02 toks/s, output: 20.94 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:29<03:22, 17.20it/s, est. speed input: 21194.30 toks/s, output: 20.70 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:31<03:21, 17.17it/s, est. speed input: 20973.92 toks/s, output: 20.48 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:33<03:19, 17.15it/s, est. speed input: 20779.31 toks/s, output: 20.29 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:35<03:18, 17.12it/s, est. speed input: 20601.27 toks/s, output: 20.12 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:36<03:16, 17.10it/s, est. speed input: 20441.24 toks/s, output: 19.96 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:38<03:14, 17.09it/s, est. speed input: 20298.60 toks/s, output: 19.82 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:40<03:12, 17.09it/s, est. speed input: 20170.80 toks/s, output: 19.70 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:42<03:10, 17.09it/s, est. speed input: 20052.17 toks/s, output: 19.58 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:44<03:09, 17.07it/s, est. speed input: 19941.60 toks/s, output: 19.47 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:46<03:07, 17.07it/s, est. speed input: 19842.47 toks/s, output: 19.38 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:48<03:03, 17.22it/s, est. speed input: 19773.20 toks/s, output: 19.31 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:49<03:00, 17.36it/s, est. speed input: 19713.55 toks/s, output: 19.25 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:51<02:59, 17.28it/s, est. speed input: 19633.43 toks/s, output: 19.17 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:53<02:58, 17.22it/s, est. speed input: 19558.89 toks/s, output: 19.10 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:55<02:56, 17.20it/s, est. speed input: 19491.39 toks/s, output: 19.03 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:57<02:55, 17.17it/s, est. speed input: 19427.58 toks/s, output: 18.97 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:59<02:53, 17.15it/s, est. speed input: 19366.97 toks/s, output: 18.91 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [01:01<02:50, 17.29it/s, est. speed input: 19327.86 toks/s, output: 18.87 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [01:03<02:49, 17.21it/s, est. speed input: 19271.79 toks/s, output: 18.82 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [01:04<02:47, 17.17it/s, est. speed input: 19220.33 toks/s, output: 18.77 toks/s]
Processed prompts:  31%|███       | 1250/4096 [01:06<02:44, 17.29it/s, est. speed input: 19187.22 toks/s, output: 18.74 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [01:08<02:41, 17.38it/s, est. speed input: 19155.90 toks/s, output: 18.71 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [01:10<02:40, 17.32it/s, est. speed input: 19114.54 toks/s, output: 18.67 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [01:12<02:39, 17.25it/s, est. speed input: 19072.36 toks/s, output: 18.63 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [01:14<02:38, 17.16it/s, est. speed input: 19029.42 toks/s, output: 18.58 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [01:16<02:36, 17.14it/s, est. speed input: 18991.27 toks/s, output: 18.55 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [01:17<02:34, 17.13it/s, est. speed input: 18956.42 toks/s, output: 18.51 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [01:19<02:33, 17.11it/s, est. speed input: 18921.36 toks/s, output: 18.48 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [01:21<02:31, 17.08it/s, est. speed input: 18887.09 toks/s, output: 18.44 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [01:23<02:29, 17.06it/s, est. speed input: 18854.10 toks/s, output: 18.41 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [01:25<02:26, 17.23it/s, est. speed input: 18837.27 toks/s, output: 18.40 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [01:27<02:25, 17.19it/s, est. speed input: 18808.68 toks/s, output: 18.37 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [01:29<02:22, 17.30it/s, est. speed input: 18791.76 toks/s, output: 18.35 toks/s]
Processed prompts:  41%|████      | 1666/4096 [01:30<02:20, 17.24it/s, est. speed input: 18765.41 toks/s, output: 18.33 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [01:32<02:19, 17.17it/s, est. speed input: 18737.87 toks/s, output: 18.30 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [01:34<02:18, 17.12it/s, est. speed input: 18711.41 toks/s, output: 18.27 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [01:36<02:16, 17.09it/s, est. speed input: 18686.88 toks/s, output: 18.25 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [01:38<02:14, 17.08it/s, est. speed input: 18663.42 toks/s, output: 18.23 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [01:40<02:12, 17.10it/s, est. speed input: 18642.53 toks/s, output: 18.21 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [01:42<02:09, 17.25it/s, est. speed input: 18631.66 toks/s, output: 18.19 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [01:43<02:08, 17.17it/s, est. speed input: 18609.26 toks/s, output: 18.17 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [01:45<02:06, 17.15it/s, est. speed input: 18590.23 toks/s, output: 18.15 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [01:47<02:03, 17.30it/s, est. speed input: 18581.42 toks/s, output: 18.15 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [01:49<02:01, 17.39it/s, est. speed input: 18572.32 toks/s, output: 18.14 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [01:51<02:00, 17.29it/s, est. speed input: 18553.51 toks/s, output: 18.12 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [01:53<01:58, 17.22it/s, est. speed input: 18535.98 toks/s, output: 18.10 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [01:55<01:57, 17.16it/s, est. speed input: 18517.91 toks/s, output: 18.08 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [01:57<01:55, 17.14it/s, est. speed input: 18501.63 toks/s, output: 18.07 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [01:58<01:53, 17.12it/s, est. speed input: 18485.50 toks/s, output: 18.05 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [02:00<01:52, 17.09it/s, est. speed input: 18468.96 toks/s, output: 18.04 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [02:02<01:48, 17.37it/s, est. speed input: 18469.14 toks/s, output: 18.04 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [02:04<01:47, 17.27it/s, est. speed input: 18453.74 toks/s, output: 18.02 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [02:06<01:44, 17.39it/s, est. speed input: 18448.64 toks/s, output: 18.02 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [02:08<01:43, 17.28it/s, est. speed input: 18434.17 toks/s, output: 18.00 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [02:09<01:41, 17.38it/s, est. speed input: 18428.29 toks/s, output: 18.00 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [02:11<01:38, 17.60it/s, est. speed input: 18430.55 toks/s, output: 18.00 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [02:13<01:36, 17.59it/s, est. speed input: 18424.59 toks/s, output: 17.99 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [02:15<01:35, 17.41it/s, est. speed input: 18410.33 toks/s, output: 17.98 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [02:17<01:34, 17.30it/s, est. speed input: 18397.20 toks/s, output: 17.97 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [02:19<01:31, 17.39it/s, est. speed input: 18392.53 toks/s, output: 17.96 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [02:20<01:30, 17.30it/s, est. speed input: 18380.66 toks/s, output: 17.95 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [02:22<01:28, 17.39it/s, est. speed input: 18376.22 toks/s, output: 17.95 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [02:24<01:27, 17.25it/s, est. speed input: 18362.80 toks/s, output: 17.93 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [02:26<01:25, 17.18it/s, est. speed input: 18350.79 toks/s, output: 17.92 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [02:28<01:23, 17.16it/s, est. speed input: 18340.37 toks/s, output: 17.91 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [02:30<01:21, 17.27it/s, est. speed input: 18335.79 toks/s, output: 17.91 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [02:32<01:19, 17.21it/s, est. speed input: 18325.05 toks/s, output: 17.90 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [02:33<01:18, 17.19it/s, est. speed input: 18315.74 toks/s, output: 17.89 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [02:35<01:16, 17.16it/s, est. speed input: 18305.89 toks/s, output: 17.88 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [02:37<01:14, 17.15it/s, est. speed input: 18296.81 toks/s, output: 17.87 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [02:39<01:12, 17.27it/s, est. speed input: 18293.01 toks/s, output: 17.86 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [02:41<01:10, 17.21it/s, est. speed input: 18283.52 toks/s, output: 17.85 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [02:43<01:08, 17.17it/s, est. speed input: 18274.49 toks/s, output: 17.85 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [02:45<01:07, 17.14it/s, est. speed input: 18265.49 toks/s, output: 17.84 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [02:47<01:05, 17.11it/s, est. speed input: 18256.43 toks/s, output: 17.83 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [02:48<01:03, 17.11it/s, est. speed input: 18248.33 toks/s, output: 17.82 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [02:50<01:01, 17.11it/s, est. speed input: 18240.25 toks/s, output: 17.81 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [02:52<00:59, 17.09it/s, est. speed input: 18231.63 toks/s, output: 17.80 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [02:54<00:56, 17.40it/s, est. speed input: 18235.34 toks/s, output: 17.81 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [02:56<00:54, 17.46it/s, est. speed input: 18233.37 toks/s, output: 17.81 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [02:58<00:53, 17.34it/s, est. speed input: 18225.16 toks/s, output: 17.80 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [02:59<00:51, 17.25it/s, est. speed input: 18217.07 toks/s, output: 17.79 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [03:01<00:49, 17.37it/s, est. speed input: 18215.70 toks/s, output: 17.79 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [03:03<00:48, 17.25it/s, est. speed input: 18207.39 toks/s, output: 17.78 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [03:05<00:46, 17.18it/s, est. speed input: 18199.22 toks/s, output: 17.77 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [03:07<00:44, 17.14it/s, est. speed input: 18191.73 toks/s, output: 17.77 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [03:09<00:42, 17.11it/s, est. speed input: 18184.43 toks/s, output: 17.76 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [03:11<00:41, 17.08it/s, est. speed input: 18176.84 toks/s, output: 17.75 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [03:13<00:38, 17.21it/s, est. speed input: 18174.79 toks/s, output: 17.75 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [03:14<00:37, 17.16it/s, est. speed input: 18167.85 toks/s, output: 17.74 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [03:16<00:34, 17.44it/s, est. speed input: 18171.35 toks/s, output: 17.75 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [03:18<00:33, 17.32it/s, est. speed input: 18164.57 toks/s, output: 17.74 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [03:20<00:31, 17.22it/s, est. speed input: 18157.53 toks/s, output: 17.73 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [03:22<00:29, 17.18it/s, est. speed input: 18151.38 toks/s, output: 17.73 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [03:24<00:27, 17.13it/s, est. speed input: 18144.68 toks/s, output: 17.72 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [03:26<00:26, 17.10it/s, est. speed input: 18138.17 toks/s, output: 17.71 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [03:27<00:24, 17.08it/s, est. speed input: 18131.93 toks/s, output: 17.71 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [03:29<00:22, 17.24it/s, est. speed input: 18131.29 toks/s, output: 17.71 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [03:31<00:20, 17.18it/s, est. speed input: 18125.13 toks/s, output: 17.70 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [03:33<00:18, 17.12it/s, est. speed input: 18118.66 toks/s, output: 17.69 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [03:35<00:16, 17.10it/s, est. speed input: 18113.04 toks/s, output: 17.69 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [03:37<00:14, 17.27it/s, est. speed input: 18113.04 toks/s, output: 17.69 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [03:39<00:12, 17.19it/s, est. speed input: 18106.98 toks/s, output: 17.68 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [03:40<00:11, 17.13it/s, est. speed input: 18100.94 toks/s, output: 17.68 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [03:42<00:09, 17.11it/s, est. speed input: 18095.64 toks/s, output: 17.67 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [03:44<00:07, 17.10it/s, est. speed input: 18090.49 toks/s, output: 17.67 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [03:46<00:05, 17.09it/s, est. speed input: 18085.34 toks/s, output: 17.66 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [03:49<00:04, 14.66it/s, est. speed input: 17999.02 toks/s, output: 17.58 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [03:51<00:01, 15.41it/s, est. speed input: 17998.56 toks/s, output: 17.58 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [03:51<00:00, 15.41it/s, est. speed input: 18131.33 toks/s, output: 17.71 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [03:51<00:00, 17.71it/s, est. speed input: 18131.33 toks/s, output: 17.71 toks/s]
[rank0]:[W128 07:57:32.770003706 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 283.5s

测试结果:
  Requests/s:   17.11
  Tokens/s:     17536.23
  Total Reqs:   4096
  Elapsed:      239.41s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     17519.12


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,30.4043,15597.4194,4.2099
1024,1024,1,128,128,16.9841,17408.6741,7.5365
2048,1024,2,256,128,18.3684,18827.5670,13.9370
4096,1024,4,512,128,17.1523,17581.1227,29.8502
8192,1024,8,1024,128,16.8467,17267.8198,60.7836
16384,1024,16,2048,128,17.0256,17451.2894,120.2891
32768,1024,32,4096,128,17.1085,17536.2299,239.4129

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:57:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:57:38 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3662922) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3662922) WARNING 01-28 07:58:06 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 20.99 requests/s, 10766.71 total tokens/s, 20.99 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 07:57:38] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:57:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:57:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:57:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:57:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:57:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:57:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:57:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:57:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:57:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:57:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:57:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:57:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:57:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:57:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:57:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:57:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:57:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:57:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:57:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:57:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:57:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:57:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:57:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:57:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:57:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:57:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:57:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3662922) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3662922) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.10s/it]
(EngineCore_DP0 pid=3662922) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.10s/it]
(EngineCore_DP0 pid=3662922) 
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3662922) [2026-01-28 07:57:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3662922) 2026-01-28 07:58:05,398 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3662922) 2026-01-28 07:58:05,422 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   1%|          | 1/128 [00:00<01:49,  1.16it/s]
Adding requests:   2%|▏         | 2/128 [00:01<01:03,  2.00it/s]
Adding requests:   2%|▏         | 3/128 [00:01<00:43,  2.85it/s]
Adding requests:   3%|▎         | 4/128 [00:01<00:31,  3.90it/s]
Adding requests:   5%|▍         | 6/128 [00:01<00:20,  5.96it/s]
Adding requests:   6%|▋         | 8/128 [00:01<00:14,  8.10it/s]
Adding requests:   8%|▊         | 10/128 [00:01<00:11, 10.31it/s]
Adding requests:  10%|█         | 13/128 [00:01<00:07, 14.45it/s]
Adding requests:  13%|█▎        | 17/128 [00:02<00:05, 19.38it/s]
Adding requests:  16%|█▋        | 21/128 [00:02<00:04, 23.79it/s]
Adding requests:  20%|██        | 26/128 [00:02<00:03, 30.02it/s]
Adding requests:  27%|██▋       | 34/128 [00:02<00:02, 43.11it/s]
Adding requests:  38%|███▊      | 48/128 [00:02<00:01, 68.47it/s]
Adding requests:  52%|█████▏    | 66/128 [00:02<00:00, 98.38it/s]
Adding requests:  84%|████████▎ | 107/128 [00:02<00:00, 185.81it/s]
Adding requests: 100%|██████████| 128/128 [00:02<00:00, 47.35it/s] 

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:00, 368.61it/s, est. speed input: 188784.84 toks/s, output: 368.64 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 46.05it/s, est. speed input: 27138.60 toks/s, output: 53.00 toks/s]   
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 39.26it/s, est. speed input: 23361.15 toks/s, output: 45.63 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 36.58it/s, est. speed input: 21991.66 toks/s, output: 42.95 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 34.81it/s, est. speed input: 21197.83 toks/s, output: 41.40 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 33.70it/s, est. speed input: 20728.46 toks/s, output: 40.49 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 32.83it/s, est. speed input: 20389.10 toks/s, output: 39.82 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 31.92it/s, est. speed input: 20070.60 toks/s, output: 39.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 31.92it/s, est. speed input: 19940.58 toks/s, output: 38.95 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.94it/s, est. speed input: 19940.58 toks/s, output: 38.95 toks/s]
[rank0]:[W128 07:58:12.643126840 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.7s

测试结果:
  Requests/s:   20.99
  Tokens/s:     10766.71
  Total Reqs:   128
  Elapsed:      6.10s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     10745.72

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:58:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:58:29 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3663889) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3663889) WARNING 01-28 07:58:55 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.22 requests/s, 15605.06 total tokens/s, 15.22 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 07:58:29] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:58:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:58:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:58:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:58:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:58:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:58:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:58:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:58:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:58:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:58:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:58:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:58:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:58:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:58:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:58:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:58:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:58:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:58:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:58:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:58:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:58:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:58:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:58:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:58:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:58:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:58:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:58:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:33] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:33] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:33] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:33] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:33] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3663889) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3663889) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.55s/it]
(EngineCore_DP0 pid=3663889) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.55s/it]
(EngineCore_DP0 pid=3663889) 
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3663889) [2026-01-28 07:58:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3663889) 2026-01-28 07:58:55,012 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3663889) 2026-01-28 07:58:55,023 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  51%|█████     | 65/128 [00:00<00:00, 639.82it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 654.94it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:18,  7.01it/s, est. speed input: 7178.52 toks/s, output: 7.01 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:10, 11.85it/s, est. speed input: 11348.36 toks/s, output: 11.08 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:09, 13.51it/s, est. speed input: 12827.81 toks/s, output: 12.53 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:08, 14.35it/s, est. speed input: 13607.79 toks/s, output: 13.29 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:07, 14.95it/s, est. speed input: 14153.64 toks/s, output: 13.82 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 15.23it/s, est. speed input: 14478.88 toks/s, output: 14.14 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:07, 15.48it/s, est. speed input: 14743.47 toks/s, output: 14.40 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 15.68it/s, est. speed input: 14954.52 toks/s, output: 14.60 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:07, 15.42it/s, est. speed input: 14987.96 toks/s, output: 14.64 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:07, 15.46it/s, est. speed input: 15080.06 toks/s, output: 14.73 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 15.53it/s, est. speed input: 15168.56 toks/s, output: 14.81 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 15.54it/s, est. speed input: 15233.13 toks/s, output: 14.88 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 15.65it/s, est. speed input: 15311.46 toks/s, output: 14.95 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 15.74it/s, est. speed input: 15382.44 toks/s, output: 15.02 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:06, 15.84it/s, est. speed input: 15452.70 toks/s, output: 15.09 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:06, 15.92it/s, est. speed input: 15516.08 toks/s, output: 15.15 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:05, 15.94it/s, est. speed input: 15565.37 toks/s, output: 15.20 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 15.58it/s, est. speed input: 15541.45 toks/s, output: 15.18 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 15.59it/s, est. speed input: 15564.28 toks/s, output: 15.20 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 15.65it/s, est. speed input: 15595.33 toks/s, output: 15.23 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 15.74it/s, est. speed input: 15629.42 toks/s, output: 15.26 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 15.71it/s, est. speed input: 15647.55 toks/s, output: 15.28 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:05, 15.81it/s, est. speed input: 15680.59 toks/s, output: 15.31 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:05, 15.90it/s, est. speed input: 15713.30 toks/s, output: 15.34 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:05, 15.79it/s, est. speed input: 15721.50 toks/s, output: 15.35 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 15.53it/s, est. speed input: 15704.72 toks/s, output: 15.34 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 15.54it/s, est. speed input: 15713.91 toks/s, output: 15.35 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 15.70it/s, est. speed input: 15739.84 toks/s, output: 15.37 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 15.78it/s, est. speed input: 15761.09 toks/s, output: 15.39 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 15.85it/s, est. speed input: 15781.49 toks/s, output: 15.41 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 15.93it/s, est. speed input: 15804.48 toks/s, output: 15.43 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:04<00:04, 15.95it/s, est. speed input: 15821.90 toks/s, output: 15.45 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:03, 15.94it/s, est. speed input: 15836.41 toks/s, output: 15.47 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 15.59it/s, est. speed input: 15815.97 toks/s, output: 15.45 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 15.59it/s, est. speed input: 15820.57 toks/s, output: 15.45 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 15.58it/s, est. speed input: 15823.91 toks/s, output: 15.45 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 15.69it/s, est. speed input: 15837.38 toks/s, output: 15.47 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 15.67it/s, est. speed input: 15841.90 toks/s, output: 15.47 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 15.79it/s, est. speed input: 15856.79 toks/s, output: 15.49 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:05<00:03, 15.87it/s, est. speed input: 15871.25 toks/s, output: 15.50 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:05<00:02, 15.91it/s, est. speed input: 15883.47 toks/s, output: 15.51 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 15.61it/s, est. speed input: 15869.27 toks/s, output: 15.50 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 15.58it/s, est. speed input: 15869.98 toks/s, output: 15.50 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 15.63it/s, est. speed input: 15875.81 toks/s, output: 15.50 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 15.65it/s, est. speed input: 15879.63 toks/s, output: 15.51 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 15.75it/s, est. speed input: 15890.66 toks/s, output: 15.52 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 15.84it/s, est. speed input: 15901.68 toks/s, output: 15.53 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:06<00:02, 15.81it/s, est. speed input: 15906.65 toks/s, output: 15.53 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:06<00:01, 15.73it/s, est. speed input: 15906.83 toks/s, output: 15.53 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 15.81it/s, est. speed input: 15916.06 toks/s, output: 15.54 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 15.51it/s, est. speed input: 15901.16 toks/s, output: 15.53 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 15.61it/s, est. speed input: 15907.52 toks/s, output: 15.53 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 15.68it/s, est. speed input: 15913.41 toks/s, output: 15.54 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 15.81it/s, est. speed input: 15923.91 toks/s, output: 15.55 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:07<00:01, 15.85it/s, est. speed input: 15931.19 toks/s, output: 15.56 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:07<00:01, 15.92it/s, est. speed input: 15940.92 toks/s, output: 15.57 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:07<00:00, 15.93it/s, est. speed input: 15947.51 toks/s, output: 15.57 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 15.95it/s, est. speed input: 15954.79 toks/s, output: 15.58 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 15.65it/s, est. speed input: 15944.06 toks/s, output: 15.57 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 15.76it/s, est. speed input: 15951.93 toks/s, output: 15.58 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 15.79it/s, est. speed input: 15956.24 toks/s, output: 15.58 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 15.75it/s, est. speed input: 15957.68 toks/s, output: 15.58 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:08<00:00, 15.72it/s, est. speed input: 15958.57 toks/s, output: 15.58 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:08<00:00, 15.73it/s, est. speed input: 15961.37 toks/s, output: 15.59 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 15.73it/s, est. speed input: 15963.76 toks/s, output: 15.59 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 15.59it/s, est. speed input: 15963.76 toks/s, output: 15.59 toks/s]
[rank0]:[W128 07:59:04.222205499 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.8s

测试结果:
  Requests/s:   15.22
  Tokens/s:     15605.06
  Total Reqs:   128
  Elapsed:      8.41s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     15589.83

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 07:59:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 07:59:10 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3664655) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3664655) WARNING 01-28 07:59:36 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.89 requests/s, 16292.02 total tokens/s, 15.89 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 07:59:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:59:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:59:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:59:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:59:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:59:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:59:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:59:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:59:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:59:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:59:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:59:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:59:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:59:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 07:59:14] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 07:59:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 07:59:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 07:59:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:59:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:59:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:59:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:59:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 07:59:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 07:59:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 07:59:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 07:59:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 07:59:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 07:59:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:15] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3664655) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3664655) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.60s/it]
(EngineCore_DP0 pid=3664655) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.60s/it]
(EngineCore_DP0 pid=3664655) 
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3664655) [2026-01-28 07:59:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3664655) 2026-01-28 07:59:36,208 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3664655) 2026-01-28 07:59:36,220 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 36/256 [00:00<00:00, 357.75it/s]
Adding requests:  37%|███▋      | 95/256 [00:00<00:00, 491.63it/s]
Adding requests:  59%|█████▉    | 151/256 [00:00<00:00, 520.82it/s]
Adding requests:  82%|████████▏ | 209/256 [00:00<00:00, 543.04it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 528.44it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 8/256 [00:00<00:06, 38.59it/s, est. speed input: 39522.94 toks/s, output: 38.59 toks/s]
Processed prompts:   5%|▍         | 12/256 [00:00<00:09, 24.41it/s, est. speed input: 26983.81 toks/s, output: 26.35 toks/s]
Processed prompts:   6%|▌         | 15/256 [00:00<00:09, 24.45it/s, est. speed input: 26589.51 toks/s, output: 25.97 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:12, 18.43it/s, est. speed input: 22168.60 toks/s, output: 21.65 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:13, 17.85it/s, est. speed input: 21431.07 toks/s, output: 20.93 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:01<00:13, 17.36it/s, est. speed input: 20846.08 toks/s, output: 20.36 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:01<00:13, 16.97it/s, est. speed input: 20379.67 toks/s, output: 19.90 toks/s]
Processed prompts:  10%|█         | 26/256 [00:01<00:13, 16.80it/s, est. speed input: 20043.46 toks/s, output: 19.57 toks/s]
Processed prompts:  11%|█         | 28/256 [00:01<00:13, 16.55it/s, est. speed input: 19723.07 toks/s, output: 19.26 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:01<00:13, 16.43it/s, est. speed input: 19472.76 toks/s, output: 19.02 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:01<00:13, 16.29it/s, est. speed input: 19240.96 toks/s, output: 18.79 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:01<00:13, 16.02it/s, est. speed input: 18993.96 toks/s, output: 18.55 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:01<00:13, 16.04it/s, est. speed input: 18833.27 toks/s, output: 18.39 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:02<00:13, 16.11it/s, est. speed input: 18705.68 toks/s, output: 18.27 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:02<00:13, 16.06it/s, est. speed input: 18571.50 toks/s, output: 18.14 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:02<00:13, 16.06it/s, est. speed input: 18457.96 toks/s, output: 18.03 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:02<00:13, 16.09it/s, est. speed input: 18360.51 toks/s, output: 17.93 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:02<00:12, 16.20it/s, est. speed input: 18289.32 toks/s, output: 17.86 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:02<00:12, 16.20it/s, est. speed input: 18212.96 toks/s, output: 17.79 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:02<00:12, 15.97it/s, est. speed input: 18103.62 toks/s, output: 17.68 toks/s]
Processed prompts:  20%|██        | 52/256 [00:02<00:12, 16.01it/s, est. speed input: 18036.13 toks/s, output: 17.61 toks/s]
Processed prompts:  21%|██        | 54/256 [00:03<00:12, 16.08it/s, est. speed input: 17979.36 toks/s, output: 17.56 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:03<00:12, 16.12it/s, est. speed input: 17927.10 toks/s, output: 17.51 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:03<00:12, 16.12it/s, est. speed input: 17873.38 toks/s, output: 17.45 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:03<00:12, 16.14it/s, est. speed input: 17827.16 toks/s, output: 17.41 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:03<00:11, 16.21it/s, est. speed input: 17790.92 toks/s, output: 17.37 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:03<00:11, 16.23it/s, est. speed input: 17753.17 toks/s, output: 17.34 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:03<00:11, 16.26it/s, est. speed input: 17719.92 toks/s, output: 17.30 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:03<00:11, 15.89it/s, est. speed input: 17644.23 toks/s, output: 17.23 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:04<00:11, 15.97it/s, est. speed input: 17610.62 toks/s, output: 17.20 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:04<00:11, 16.03it/s, est. speed input: 17579.55 toks/s, output: 17.17 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:04<00:11, 16.06it/s, est. speed input: 17549.25 toks/s, output: 17.14 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:04<00:11, 16.15it/s, est. speed input: 17527.46 toks/s, output: 17.12 toks/s]
Processed prompts:  30%|███       | 78/256 [00:04<00:11, 16.16it/s, est. speed input: 17501.51 toks/s, output: 17.09 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:04<00:10, 16.09it/s, est. speed input: 17469.73 toks/s, output: 17.06 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:04<00:10, 16.10it/s, est. speed input: 17444.95 toks/s, output: 17.04 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:04<00:10, 15.82it/s, est. speed input: 17395.28 toks/s, output: 16.99 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:05<00:10, 15.83it/s, est. speed input: 17366.32 toks/s, output: 16.96 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:05<00:10, 15.91it/s, est. speed input: 17344.99 toks/s, output: 16.94 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:05<00:10, 15.97it/s, est. speed input: 17325.72 toks/s, output: 16.92 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:05<00:10, 16.08it/s, est. speed input: 17311.77 toks/s, output: 16.91 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:05<00:10, 16.10it/s, est. speed input: 17294.81 toks/s, output: 16.89 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:05<00:09, 16.05it/s, est. speed input: 17273.41 toks/s, output: 16.87 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:05<00:09, 16.05it/s, est. speed input: 17255.49 toks/s, output: 16.85 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:05<00:09, 16.19it/s, est. speed input: 17248.89 toks/s, output: 16.84 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:06<00:09, 15.88it/s, est. speed input: 17212.52 toks/s, output: 16.81 toks/s]
Processed prompts:  41%|████      | 104/256 [00:06<00:09, 15.94it/s, est. speed input: 17197.06 toks/s, output: 16.79 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:06<00:09, 16.03it/s, est. speed input: 17186.10 toks/s, output: 16.78 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:06<00:09, 16.05it/s, est. speed input: 17172.67 toks/s, output: 16.77 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:06<00:09, 16.12it/s, est. speed input: 17163.22 toks/s, output: 16.76 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:06<00:08, 16.06it/s, est. speed input: 17147.14 toks/s, output: 16.75 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:06<00:08, 16.14it/s, est. speed input: 17139.79 toks/s, output: 16.74 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:06<00:08, 16.25it/s, est. speed input: 17135.32 toks/s, output: 16.73 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:07<00:08, 16.04it/s, est. speed input: 17113.97 toks/s, output: 16.71 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:07<00:08, 16.04it/s, est. speed input: 17101.92 toks/s, output: 16.70 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:07<00:08, 16.09it/s, est. speed input: 17093.22 toks/s, output: 16.69 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:07<00:08, 16.13it/s, est. speed input: 17085.36 toks/s, output: 16.68 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:07<00:08, 16.16it/s, est. speed input: 17077.80 toks/s, output: 16.68 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:07<00:07, 16.20it/s, est. speed input: 17071.37 toks/s, output: 16.67 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:07<00:07, 16.15it/s, est. speed input: 17060.92 toks/s, output: 16.66 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:07<00:07, 16.14it/s, est. speed input: 17052.09 toks/s, output: 16.65 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:08<00:07, 16.06it/s, est. speed input: 17039.79 toks/s, output: 16.64 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:08<00:07, 15.88it/s, est. speed input: 17020.90 toks/s, output: 16.62 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:08<00:07, 15.95it/s, est. speed input: 17013.33 toks/s, output: 16.61 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:08<00:07, 15.99it/s, est. speed input: 17005.33 toks/s, output: 16.61 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:08<00:07, 16.06it/s, est. speed input: 16999.49 toks/s, output: 16.60 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:08<00:06, 16.09it/s, est. speed input: 16993.27 toks/s, output: 16.59 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:08<00:06, 16.09it/s, est. speed input: 16985.89 toks/s, output: 16.59 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:08<00:06, 16.17it/s, est. speed input: 16982.59 toks/s, output: 16.58 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:09<00:06, 16.06it/s, est. speed input: 16971.46 toks/s, output: 16.57 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:09<00:06, 15.77it/s, est. speed input: 16950.06 toks/s, output: 16.55 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:09<00:06, 15.83it/s, est. speed input: 16942.14 toks/s, output: 16.55 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:09<00:06, 15.92it/s, est. speed input: 16936.85 toks/s, output: 16.54 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:09<00:06, 16.04it/s, est. speed input: 16933.78 toks/s, output: 16.54 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:09<00:05, 16.06it/s, est. speed input: 16928.04 toks/s, output: 16.53 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:09<00:05, 16.08it/s, est. speed input: 16923.01 toks/s, output: 16.53 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:09<00:05, 16.07it/s, est. speed input: 16916.86 toks/s, output: 16.52 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:10<00:05, 16.08it/s, est. speed input: 16911.32 toks/s, output: 16.51 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:10<00:05, 16.10it/s, est. speed input: 16906.84 toks/s, output: 16.51 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:10<00:05, 15.85it/s, est. speed input: 16891.11 toks/s, output: 16.50 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:10<00:05, 15.89it/s, est. speed input: 16884.93 toks/s, output: 16.49 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:10<00:05, 15.99it/s, est. speed input: 16881.64 toks/s, output: 16.49 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:10<00:05, 15.98it/s, est. speed input: 16875.26 toks/s, output: 16.48 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:10<00:04, 16.01it/s, est. speed input: 16870.79 toks/s, output: 16.48 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:10<00:04, 16.05it/s, est. speed input: 16866.87 toks/s, output: 16.47 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:11<00:04, 16.05it/s, est. speed input: 16862.14 toks/s, output: 16.47 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:11<00:04, 15.95it/s, est. speed input: 16853.40 toks/s, output: 16.46 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:11<00:04, 15.76it/s, est. speed input: 16840.21 toks/s, output: 16.45 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:11<00:04, 15.83it/s, est. speed input: 16835.23 toks/s, output: 16.44 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:11<00:04, 15.89it/s, est. speed input: 16830.58 toks/s, output: 16.44 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:11<00:04, 15.85it/s, est. speed input: 16822.92 toks/s, output: 16.43 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:11<00:03, 15.96it/s, est. speed input: 16820.75 toks/s, output: 16.43 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:11<00:03, 15.94it/s, est. speed input: 16814.90 toks/s, output: 16.42 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:12<00:03, 16.06it/s, est. speed input: 16814.24 toks/s, output: 16.42 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:12<00:03, 16.04it/s, est. speed input: 16809.91 toks/s, output: 16.42 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:12<00:03, 15.79it/s, est. speed input: 16797.05 toks/s, output: 16.40 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:12<00:03, 15.91it/s, est. speed input: 16794.71 toks/s, output: 16.40 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:12<00:03, 16.06it/s, est. speed input: 16794.93 toks/s, output: 16.40 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:12<00:02, 16.10it/s, est. speed input: 16793.07 toks/s, output: 16.40 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:12<00:02, 16.13it/s, est. speed input: 16790.89 toks/s, output: 16.40 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:12<00:02, 16.05it/s, est. speed input: 16785.51 toks/s, output: 16.39 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:13<00:02, 16.06it/s, est. speed input: 16782.63 toks/s, output: 16.39 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:13<00:02, 16.05it/s, est. speed input: 16779.04 toks/s, output: 16.39 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:13<00:02, 16.05it/s, est. speed input: 16775.92 toks/s, output: 16.38 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:13<00:02, 15.75it/s, est. speed input: 16762.99 toks/s, output: 16.37 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:13<00:02, 15.90it/s, est. speed input: 16762.01 toks/s, output: 16.37 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:13<00:02, 15.98it/s, est. speed input: 16760.00 toks/s, output: 16.37 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:13<00:01, 16.07it/s, est. speed input: 16759.34 toks/s, output: 16.37 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:13<00:01, 16.14it/s, est. speed input: 16758.89 toks/s, output: 16.37 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:14<00:01, 16.10it/s, est. speed input: 16755.64 toks/s, output: 16.36 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:14<00:01, 16.07it/s, est. speed input: 16752.39 toks/s, output: 16.36 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:14<00:01, 16.10it/s, est. speed input: 16750.75 toks/s, output: 16.36 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:14<00:01, 15.94it/s, est. speed input: 16743.72 toks/s, output: 16.35 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:14<00:01, 15.97it/s, est. speed input: 16740.85 toks/s, output: 16.35 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:14<00:00, 16.08it/s, est. speed input: 16740.80 toks/s, output: 16.35 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:14<00:00, 16.09it/s, est. speed input: 16738.72 toks/s, output: 16.35 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:14<00:00, 16.06it/s, est. speed input: 16735.80 toks/s, output: 16.34 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:15<00:00, 16.11it/s, est. speed input: 16734.89 toks/s, output: 16.34 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:15<00:00, 16.06it/s, est. speed input: 16731.52 toks/s, output: 16.34 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:15<00:00, 16.11it/s, est. speed input: 16730.66 toks/s, output: 16.34 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:15<00:00, 16.19it/s, est. speed input: 16730.84 toks/s, output: 16.34 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:15<00:00, 15.84it/s, est. speed input: 16719.89 toks/s, output: 16.33 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:15<00:00, 15.84it/s, est. speed input: 16782.19 toks/s, output: 16.39 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:15<00:00, 16.39it/s, est. speed input: 16782.19 toks/s, output: 16.39 toks/s]
[rank0]:[W128 07:59:53.246449524 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.0s

测试结果:
  Requests/s:   15.89
  Tokens/s:     16292.02
  Total Reqs:   256
  Elapsed:      16.11s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     16276.12

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:00:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:00:00 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3665536) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3665536) WARNING 01-28 08:00:26 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.76 requests/s, 15128.11 total tokens/s, 14.76 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 08:00:00] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:00:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:00:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:00:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:00:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:00:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:00:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:00:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:00:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:00:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:00:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:00:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:00:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:00:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:00:03] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:00:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:00:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:00:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:00:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:00:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:00:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:00:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:00:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:00:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:00:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:00:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:00:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:00:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3665536) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3665536) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.72s/it]
(EngineCore_DP0 pid=3665536) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.72s/it]
(EngineCore_DP0 pid=3665536) 
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3665536) [2026-01-28 08:00:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3665536) 2026-01-28 08:00:26,193 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3665536) 2026-01-28 08:00:26,205 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 41/512 [00:00<00:01, 408.67it/s]
Adding requests:  20%|█▉        | 102/512 [00:00<00:00, 522.26it/s]
Adding requests:  31%|███       | 159/512 [00:00<00:00, 543.02it/s]
Adding requests:  42%|████▏     | 215/512 [00:00<00:00, 549.48it/s]
Adding requests:  53%|█████▎    | 273/512 [00:00<00:00, 558.09it/s]
Adding requests:  64%|██████▍   | 329/512 [00:00<00:00, 550.42it/s]
Adding requests:  75%|███████▌  | 385/512 [00:00<00:00, 553.25it/s]
Adding requests:  86%|████████▌ | 441/512 [00:00<00:00, 549.06it/s]
Adding requests:  97%|█████████▋| 496/512 [00:00<00:00, 547.54it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 542.69it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 14/512 [00:00<00:12, 40.46it/s, est. speed input: 41438.59 toks/s, output: 40.46 toks/s]
Processed prompts:   4%|▎         | 19/512 [00:00<00:17, 28.91it/s, est. speed input: 31599.10 toks/s, output: 30.86 toks/s]
Processed prompts:   4%|▍         | 22/512 [00:00<00:23, 21.06it/s, est. speed input: 25263.92 toks/s, output: 24.67 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:01<00:25, 18.79it/s, est. speed input: 22988.84 toks/s, output: 22.45 toks/s]
Processed prompts:   6%|▌         | 30/512 [00:01<00:27, 17.50it/s, est. speed input: 21582.11 toks/s, output: 21.08 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:01<00:28, 16.59it/s, est. speed input: 20554.23 toks/s, output: 20.07 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:01<00:29, 15.94it/s, est. speed input: 19774.61 toks/s, output: 19.31 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:02<00:30, 15.58it/s, est. speed input: 19215.06 toks/s, output: 18.76 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:02<00:30, 15.37it/s, est. speed input: 18793.41 toks/s, output: 18.35 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:02<00:30, 15.18it/s, est. speed input: 18432.78 toks/s, output: 18.00 toks/s]
Processed prompts:  11%|█         | 54/512 [00:03<00:30, 14.97it/s, est. speed input: 18109.02 toks/s, output: 17.68 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:03<00:30, 14.94it/s, est. speed input: 17875.48 toks/s, output: 17.46 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:03<00:30, 14.96it/s, est. speed input: 17688.03 toks/s, output: 17.27 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:03<00:29, 14.91it/s, est. speed input: 17510.47 toks/s, output: 17.10 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:04<00:29, 14.83it/s, est. speed input: 17344.91 toks/s, output: 16.94 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:04<00:29, 14.87it/s, est. speed input: 17222.07 toks/s, output: 16.82 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:04<00:29, 14.90it/s, est. speed input: 17113.80 toks/s, output: 16.71 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:04<00:28, 14.83it/s, est. speed input: 16998.38 toks/s, output: 16.60 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:05<00:28, 14.79it/s, est. speed input: 16896.75 toks/s, output: 16.50 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:05<00:28, 14.80it/s, est. speed input: 16811.89 toks/s, output: 16.42 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:05<00:28, 14.83it/s, est. speed input: 16738.93 toks/s, output: 16.35 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:06<00:28, 14.77it/s, est. speed input: 16659.63 toks/s, output: 16.27 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:06<00:27, 14.75it/s, est. speed input: 16590.97 toks/s, output: 16.20 toks/s]
Processed prompts:  21%|██        | 106/512 [00:06<00:27, 14.79it/s, est. speed input: 16534.72 toks/s, output: 16.15 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:06<00:27, 14.81it/s, est. speed input: 16482.85 toks/s, output: 16.10 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:07<00:27, 14.72it/s, est. speed input: 16419.93 toks/s, output: 16.04 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:07<00:26, 14.69it/s, est. speed input: 16366.43 toks/s, output: 15.98 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:07<00:26, 14.74it/s, est. speed input: 16325.44 toks/s, output: 15.94 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:07<00:26, 14.82it/s, est. speed input: 16293.53 toks/s, output: 15.91 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:08<00:25, 14.72it/s, est. speed input: 16244.52 toks/s, output: 15.86 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:08<00:25, 14.78it/s, est. speed input: 16213.80 toks/s, output: 15.83 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:08<00:25, 14.85it/s, est. speed input: 16188.48 toks/s, output: 15.81 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:08<00:24, 14.92it/s, est. speed input: 16166.36 toks/s, output: 15.79 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:09<00:24, 14.76it/s, est. speed input: 16124.05 toks/s, output: 15.75 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:09<00:24, 14.82it/s, est. speed input: 16101.29 toks/s, output: 15.72 toks/s]
Processed prompts:  30%|███       | 154/512 [00:09<00:24, 14.80it/s, est. speed input: 16073.97 toks/s, output: 15.70 toks/s]
Processed prompts:  31%|███       | 158/512 [00:10<00:23, 14.84it/s, est. speed input: 16052.97 toks/s, output: 15.68 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:10<00:23, 14.83it/s, est. speed input: 16029.45 toks/s, output: 15.65 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:10<00:23, 14.83it/s, est. speed input: 16008.37 toks/s, output: 15.63 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:10<00:23, 14.87it/s, est. speed input: 15991.17 toks/s, output: 15.62 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:11<00:22, 14.87it/s, est. speed input: 15973.05 toks/s, output: 15.60 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:11<00:22, 14.73it/s, est. speed input: 15943.77 toks/s, output: 15.57 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:11<00:22, 14.77it/s, est. speed input: 15927.11 toks/s, output: 15.55 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:11<00:21, 14.84it/s, est. speed input: 15914.35 toks/s, output: 15.54 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:12<00:21, 14.87it/s, est. speed input: 15901.03 toks/s, output: 15.53 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:12<00:21, 14.77it/s, est. speed input: 15879.13 toks/s, output: 15.51 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:12<00:21, 14.74it/s, est. speed input: 15860.86 toks/s, output: 15.49 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:13<00:20, 14.81it/s, est. speed input: 15850.03 toks/s, output: 15.48 toks/s]
Processed prompts:  40%|████      | 206/512 [00:13<00:20, 14.85it/s, est. speed input: 15838.82 toks/s, output: 15.47 toks/s]
Processed prompts:  41%|████      | 210/512 [00:13<00:20, 14.76it/s, est. speed input: 15820.34 toks/s, output: 15.45 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:13<00:20, 14.80it/s, est. speed input: 15809.42 toks/s, output: 15.44 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:14<00:19, 14.81it/s, est. speed input: 15797.10 toks/s, output: 15.43 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:14<00:19, 14.84it/s, est. speed input: 15787.31 toks/s, output: 15.42 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:14<00:19, 14.77it/s, est. speed input: 15772.03 toks/s, output: 15.40 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:14<00:19, 14.80it/s, est. speed input: 15762.02 toks/s, output: 15.39 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:15<00:18, 14.80it/s, est. speed input: 15751.41 toks/s, output: 15.38 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:15<00:18, 14.85it/s, est. speed input: 15744.18 toks/s, output: 15.38 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:15<00:18, 14.73it/s, est. speed input: 15727.36 toks/s, output: 15.36 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:16<00:17, 14.79it/s, est. speed input: 15720.21 toks/s, output: 15.35 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:16<00:17, 14.81it/s, est. speed input: 15711.61 toks/s, output: 15.34 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:16<00:17, 14.87it/s, est. speed input: 15706.21 toks/s, output: 15.34 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:16<00:17, 14.80it/s, est. speed input: 15694.42 toks/s, output: 15.33 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:17<00:16, 14.83it/s, est. speed input: 15687.66 toks/s, output: 15.32 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:17<00:16, 14.84it/s, est. speed input: 15680.21 toks/s, output: 15.31 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:17<00:16, 14.85it/s, est. speed input: 15673.61 toks/s, output: 15.31 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:17<00:16, 14.78it/s, est. speed input: 15662.79 toks/s, output: 15.30 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:18<00:15, 14.82it/s, est. speed input: 15657.01 toks/s, output: 15.29 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:18<00:15, 14.85it/s, est. speed input: 15651.61 toks/s, output: 15.28 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:18<00:15, 14.77it/s, est. speed input: 15641.04 toks/s, output: 15.27 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:18<00:14, 14.80it/s, est. speed input: 15635.34 toks/s, output: 15.27 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:19<00:14, 14.79it/s, est. speed input: 15628.07 toks/s, output: 15.26 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:19<00:14, 14.81it/s, est. speed input: 15622.16 toks/s, output: 15.26 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:19<00:14, 14.76it/s, est. speed input: 15613.61 toks/s, output: 15.25 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:20<00:13, 14.74it/s, est. speed input: 15605.91 toks/s, output: 15.24 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:20<00:13, 14.79it/s, est. speed input: 15601.41 toks/s, output: 15.24 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:20<00:13, 14.80it/s, est. speed input: 15595.77 toks/s, output: 15.23 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:20<00:13, 14.73it/s, est. speed input: 15586.87 toks/s, output: 15.22 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:21<00:12, 14.75it/s, est. speed input: 15581.37 toks/s, output: 15.22 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:21<00:12, 14.75it/s, est. speed input: 15575.46 toks/s, output: 15.21 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:21<00:12, 14.74it/s, est. speed input: 15569.08 toks/s, output: 15.20 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:21<00:12, 14.76it/s, est. speed input: 15563.99 toks/s, output: 15.20 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:22<00:11, 14.78it/s, est. speed input: 15559.26 toks/s, output: 15.19 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:22<00:10, 15.58it/s, est. speed input: 15586.21 toks/s, output: 15.22 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:22<00:10, 15.36it/s, est. speed input: 15581.90 toks/s, output: 15.22 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:23<00:10, 15.13it/s, est. speed input: 15574.80 toks/s, output: 15.21 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:23<00:10, 15.05it/s, est. speed input: 15570.61 toks/s, output: 15.21 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:23<00:10, 15.00it/s, est. speed input: 15566.86 toks/s, output: 15.20 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:23<00:10, 14.98it/s, est. speed input: 15563.85 toks/s, output: 15.20 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:24<00:09, 14.88it/s, est. speed input: 15557.59 toks/s, output: 15.19 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:24<00:09, 14.95it/s, est. speed input: 15556.69 toks/s, output: 15.19 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:24<00:09, 14.92it/s, est. speed input: 15552.77 toks/s, output: 15.19 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:24<00:09, 14.83it/s, est. speed input: 15546.51 toks/s, output: 15.18 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:25<00:08, 14.77it/s, est. speed input: 15540.29 toks/s, output: 15.18 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:25<00:08, 14.76it/s, est. speed input: 15535.60 toks/s, output: 15.17 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:25<00:08, 14.76it/s, est. speed input: 15531.23 toks/s, output: 15.17 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:25<00:07, 14.77it/s, est. speed input: 15527.01 toks/s, output: 15.16 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:26<00:07, 14.71it/s, est. speed input: 15520.72 toks/s, output: 15.16 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:26<00:07, 14.77it/s, est. speed input: 15518.24 toks/s, output: 15.15 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:26<00:07, 14.79it/s, est. speed input: 15515.06 toks/s, output: 15.15 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:27<00:06, 14.79it/s, est. speed input: 15511.31 toks/s, output: 15.15 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:27<00:06, 14.72it/s, est. speed input: 15505.31 toks/s, output: 15.14 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:27<00:06, 14.78it/s, est. speed input: 15503.01 toks/s, output: 15.14 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:27<00:06, 14.82it/s, est. speed input: 15500.73 toks/s, output: 15.14 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:28<00:05, 14.82it/s, est. speed input: 15497.68 toks/s, output: 15.13 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:28<00:05, 14.69it/s, est. speed input: 15490.44 toks/s, output: 15.13 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:28<00:05, 14.71it/s, est. speed input: 15486.89 toks/s, output: 15.12 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:28<00:05, 14.76it/s, est. speed input: 15484.36 toks/s, output: 15.12 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:29<00:04, 14.76it/s, est. speed input: 15481.00 toks/s, output: 15.12 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:29<00:04, 14.72it/s, est. speed input: 15476.35 toks/s, output: 15.11 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:29<00:03, 15.64it/s, est. speed input: 15500.53 toks/s, output: 15.14 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:29<00:03, 15.43it/s, est. speed input: 15498.92 toks/s, output: 15.14 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:30<00:03, 15.24it/s, est. speed input: 15495.84 toks/s, output: 15.13 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:30<00:03, 15.02it/s, est. speed input: 15490.48 toks/s, output: 15.13 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:30<00:03, 14.96it/s, est. speed input: 15487.78 toks/s, output: 15.12 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:31<00:02, 14.92it/s, est. speed input: 15485.12 toks/s, output: 15.12 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:31<00:02, 14.94it/s, est. speed input: 15483.76 toks/s, output: 15.12 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:31<00:02, 14.84it/s, est. speed input: 15479.33 toks/s, output: 15.12 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:31<00:02, 14.83it/s, est. speed input: 15476.70 toks/s, output: 15.11 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:32<00:01, 14.82it/s, est. speed input: 15473.97 toks/s, output: 15.11 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:32<00:01, 14.39it/s, est. speed input: 15458.59 toks/s, output: 15.10 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:32<00:01, 14.50it/s, est. speed input: 15455.95 toks/s, output: 15.09 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:32<00:00, 14.68it/s, est. speed input: 15456.08 toks/s, output: 15.09 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:33<00:00, 14.74it/s, est. speed input: 15454.20 toks/s, output: 15.09 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:33<00:00, 14.70it/s, est. speed input: 15450.26 toks/s, output: 15.09 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:33<00:00, 15.80it/s, est. speed input: 15475.84 toks/s, output: 15.11 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:33<00:00, 15.80it/s, est. speed input: 15536.48 toks/s, output: 15.17 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:33<00:00, 15.17it/s, est. speed input: 15536.48 toks/s, output: 15.17 toks/s]
[rank0]:[W128 08:01:01.840939148 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 68.5s

测试结果:
  Requests/s:   14.76
  Tokens/s:     15128.11
  Total Reqs:   512
  Elapsed:      34.69s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     15113.35

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:01:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:01:10 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3666692) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3666692) WARNING 01-28 08:01:37 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.57 requests/s, 14933.18 total tokens/s, 14.57 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 08:01:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:01:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:01:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:01:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:01:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:01:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:01:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:01:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:01:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:01:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:01:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:01:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:01:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:01:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:01:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:01:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:01:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:01:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:01:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:01:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:01:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:01:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:01:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:01:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:01:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:01:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:01:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:01:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3666692) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3666692) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.64s/it]
(EngineCore_DP0 pid=3666692) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.64s/it]
(EngineCore_DP0 pid=3666692) 
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3666692) [2026-01-28 08:01:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3666692) 2026-01-28 08:01:36,333 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3666692) 2026-01-28 08:01:36,396 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:01, 626.49it/s]
Adding requests:  12%|█▏        | 126/1024 [00:00<00:01, 599.23it/s]
Adding requests:  18%|█▊        | 187/1024 [00:00<00:01, 562.33it/s]
Adding requests:  24%|██▍       | 244/1024 [00:00<00:01, 552.98it/s]
Adding requests:  29%|██▉       | 300/1024 [00:00<00:01, 547.86it/s]
Adding requests:  35%|███▍      | 356/1024 [00:00<00:01, 548.58it/s]
Adding requests:  40%|████      | 411/1024 [00:00<00:01, 543.79it/s]
Adding requests:  46%|████▌     | 466/1024 [00:00<00:01, 536.60it/s]
Adding requests:  51%|█████     | 520/1024 [00:00<00:00, 526.22it/s]
Adding requests:  56%|█████▌    | 573/1024 [00:01<00:00, 514.22it/s]
Adding requests:  61%|██████▏   | 629/1024 [00:01<00:00, 525.10it/s]
Adding requests:  67%|██████▋   | 684/1024 [00:01<00:00, 532.11it/s]
Adding requests:  72%|███████▏  | 738/1024 [00:01<00:00, 531.69it/s]
Adding requests:  77%|███████▋  | 792/1024 [00:01<00:00, 522.97it/s]
Adding requests:  83%|████████▎ | 845/1024 [00:01<00:00, 511.24it/s]
Adding requests:  88%|████████▊ | 900/1024 [00:01<00:00, 521.01it/s]
Adding requests:  93%|█████████▎| 954/1024 [00:01<00:00, 525.83it/s]
Adding requests:  98%|█████████▊| 1008/1024 [00:01<00:00, 527.68it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 535.31it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 26/1024 [00:00<00:18, 55.08it/s, est. speed input: 56402.04 toks/s, output: 55.08 toks/s]
Processed prompts:   3%|▎         | 34/1024 [00:01<00:32, 30.00it/s, est. speed input: 34305.99 toks/s, output: 33.50 toks/s]
Processed prompts:   4%|▍         | 42/1024 [00:01<00:43, 22.61it/s, est. speed input: 27422.01 toks/s, output: 26.78 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:02<00:50, 19.36it/s, est. speed input: 24163.47 toks/s, output: 23.60 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:02<00:54, 17.64it/s, est. speed input: 22281.69 toks/s, output: 21.76 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:03<00:57, 16.61it/s, est. speed input: 21040.33 toks/s, output: 20.55 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:03<00:59, 15.96it/s, est. speed input: 20156.58 toks/s, output: 19.68 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:04<01:00, 15.48it/s, est. speed input: 19470.39 toks/s, output: 19.01 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:04<01:01, 15.20it/s, est. speed input: 18962.11 toks/s, output: 18.52 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:05<01:01, 15.00it/s, est. speed input: 18548.99 toks/s, output: 18.11 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:05<01:01, 14.89it/s, est. speed input: 18220.63 toks/s, output: 17.79 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:06<01:01, 14.75it/s, est. speed input: 17929.77 toks/s, output: 17.51 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:07<01:01, 14.73it/s, est. speed input: 17704.37 toks/s, output: 17.29 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:07<01:01, 14.65it/s, est. speed input: 17495.14 toks/s, output: 17.09 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:08<01:00, 14.65it/s, est. speed input: 17328.60 toks/s, output: 16.92 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:08<01:00, 14.61it/s, est. speed input: 17171.69 toks/s, output: 16.77 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:09<00:59, 14.61it/s, est. speed input: 17041.05 toks/s, output: 16.64 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:09<00:59, 14.57it/s, est. speed input: 16916.23 toks/s, output: 16.52 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:10<00:58, 14.61it/s, est. speed input: 16818.21 toks/s, output: 16.42 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:10<00:58, 14.56it/s, est. speed input: 16716.19 toks/s, output: 16.32 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:11<00:57, 14.57it/s, est. speed input: 16631.54 toks/s, output: 16.24 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:12<00:57, 14.55it/s, est. speed input: 16549.71 toks/s, output: 16.16 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:12<00:56, 14.56it/s, est. speed input: 16478.38 toks/s, output: 16.09 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:13<00:56, 14.52it/s, est. speed input: 16407.30 toks/s, output: 16.02 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:13<00:55, 14.55it/s, est. speed input: 16348.85 toks/s, output: 15.97 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:14<00:54, 14.52it/s, est. speed input: 16288.88 toks/s, output: 15.91 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:14<00:54, 14.55it/s, est. speed input: 16240.08 toks/s, output: 15.86 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:15<00:53, 14.51it/s, est. speed input: 16186.35 toks/s, output: 15.81 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:15<00:53, 14.52it/s, est. speed input: 16141.74 toks/s, output: 15.76 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:16<00:52, 14.49it/s, est. speed input: 16095.66 toks/s, output: 15.72 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:16<00:52, 14.51it/s, est. speed input: 16056.42 toks/s, output: 15.68 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:17<00:51, 14.53it/s, est. speed input: 16021.44 toks/s, output: 15.65 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:18<00:51, 14.53it/s, est. speed input: 15986.11 toks/s, output: 15.61 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:18<00:50, 14.56it/s, est. speed input: 15957.15 toks/s, output: 15.58 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:19<00:49, 14.54it/s, est. speed input: 15924.88 toks/s, output: 15.55 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:19<00:49, 14.57it/s, est. speed input: 15899.27 toks/s, output: 15.53 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:20<00:48, 14.57it/s, est. speed input: 15872.04 toks/s, output: 15.50 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:20<00:48, 14.56it/s, est. speed input: 15846.15 toks/s, output: 15.47 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:21<00:47, 14.53it/s, est. speed input: 15819.50 toks/s, output: 15.45 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:21<00:46, 14.91it/s, est. speed input: 15829.35 toks/s, output: 15.46 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:22<00:45, 14.79it/s, est. speed input: 15805.60 toks/s, output: 15.44 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:22<00:45, 14.74it/s, est. speed input: 15785.71 toks/s, output: 15.42 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:23<00:45, 14.66it/s, est. speed input: 15762.96 toks/s, output: 15.39 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:24<00:44, 14.64it/s, est. speed input: 15744.34 toks/s, output: 15.38 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:24<00:44, 14.59it/s, est. speed input: 15724.06 toks/s, output: 15.36 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:25<00:43, 14.59it/s, est. speed input: 15707.14 toks/s, output: 15.34 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:25<00:43, 14.56it/s, est. speed input: 15688.36 toks/s, output: 15.32 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:26<00:42, 14.62it/s, est. speed input: 15676.38 toks/s, output: 15.31 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:26<00:42, 14.59it/s, est. speed input: 15660.06 toks/s, output: 15.29 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:27<00:41, 14.58it/s, est. speed input: 15644.95 toks/s, output: 15.28 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:27<00:41, 14.55it/s, est. speed input: 15628.85 toks/s, output: 15.26 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:28<00:40, 14.56it/s, est. speed input: 15615.44 toks/s, output: 15.25 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:29<00:40, 14.53it/s, est. speed input: 15600.27 toks/s, output: 15.23 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:29<00:38, 15.04it/s, est. speed input: 15619.37 toks/s, output: 15.25 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:30<00:38, 14.88it/s, est. speed input: 15605.90 toks/s, output: 15.24 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:30<00:37, 14.79it/s, est. speed input: 15593.97 toks/s, output: 15.23 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:31<00:37, 14.71it/s, est. speed input: 15581.10 toks/s, output: 15.22 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:31<00:36, 14.67it/s, est. speed input: 15569.79 toks/s, output: 15.20 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:32<00:36, 14.59it/s, est. speed input: 15555.60 toks/s, output: 15.19 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:32<00:36, 14.58it/s, est. speed input: 15544.62 toks/s, output: 15.18 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:33<00:35, 14.56it/s, est. speed input: 15533.26 toks/s, output: 15.17 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:33<00:35, 14.56it/s, est. speed input: 15523.07 toks/s, output: 15.16 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:34<00:34, 14.57it/s, est. speed input: 15513.75 toks/s, output: 15.15 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:35<00:33, 14.55it/s, est. speed input: 15503.22 toks/s, output: 15.14 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:35<00:33, 14.54it/s, est. speed input: 15493.41 toks/s, output: 15.13 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:36<00:32, 14.49it/s, est. speed input: 15481.70 toks/s, output: 15.12 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:36<00:32, 14.53it/s, est. speed input: 15473.82 toks/s, output: 15.11 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:37<00:31, 14.52it/s, est. speed input: 15464.90 toks/s, output: 15.10 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:37<00:31, 14.57it/s, est. speed input: 15458.45 toks/s, output: 15.10 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:38<00:30, 14.54it/s, est. speed input: 15449.54 toks/s, output: 15.09 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:38<00:30, 14.59it/s, est. speed input: 15443.73 toks/s, output: 15.08 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:39<00:29, 14.54it/s, est. speed input: 15434.42 toks/s, output: 15.07 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:39<00:28, 14.57it/s, est. speed input: 15428.23 toks/s, output: 15.07 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:40<00:28, 14.52it/s, est. speed input: 15419.04 toks/s, output: 15.06 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:41<00:27, 14.55it/s, est. speed input: 15413.11 toks/s, output: 15.05 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:41<00:27, 14.52it/s, est. speed input: 15405.01 toks/s, output: 15.04 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:42<00:26, 14.55it/s, est. speed input: 15399.33 toks/s, output: 15.04 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:42<00:26, 14.58it/s, est. speed input: 15394.12 toks/s, output: 15.03 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:43<00:25, 14.58it/s, est. speed input: 15388.13 toks/s, output: 15.03 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:43<00:25, 14.55it/s, est. speed input: 15381.25 toks/s, output: 15.02 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:44<00:24, 14.56it/s, est. speed input: 15375.66 toks/s, output: 15.02 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:44<00:24, 14.50it/s, est. speed input: 15367.33 toks/s, output: 15.01 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:45<00:23, 14.53it/s, est. speed input: 15362.44 toks/s, output: 15.00 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:46<00:23, 14.52it/s, est. speed input: 15356.09 toks/s, output: 15.00 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:46<00:22, 14.56it/s, est. speed input: 15352.14 toks/s, output: 14.99 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:47<00:21, 14.55it/s, est. speed input: 15346.53 toks/s, output: 14.99 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:47<00:21, 14.56it/s, est. speed input: 15341.70 toks/s, output: 14.98 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:48<00:20, 14.52it/s, est. speed input: 15335.30 toks/s, output: 14.98 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:48<00:20, 14.50it/s, est. speed input: 15329.32 toks/s, output: 14.97 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:49<00:19, 14.54it/s, est. speed input: 15325.34 toks/s, output: 14.97 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:49<00:19, 14.52it/s, est. speed input: 15319.81 toks/s, output: 14.96 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:50<00:18, 14.55it/s, est. speed input: 15315.83 toks/s, output: 14.96 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:50<00:18, 14.51it/s, est. speed input: 15309.90 toks/s, output: 14.95 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:51<00:17, 14.54it/s, est. speed input: 15306.17 toks/s, output: 14.95 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:52<00:16, 14.51it/s, est. speed input: 15300.86 toks/s, output: 14.94 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:52<00:16, 14.55it/s, est. speed input: 15297.45 toks/s, output: 14.94 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:53<00:15, 14.53it/s, est. speed input: 15292.51 toks/s, output: 14.93 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:53<00:15, 14.57it/s, est. speed input: 15289.82 toks/s, output: 14.93 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:54<00:14, 14.53it/s, est. speed input: 15284.79 toks/s, output: 14.93 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:54<00:14, 14.55it/s, est. speed input: 15281.26 toks/s, output: 14.92 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:55<00:13, 14.54it/s, est. speed input: 15277.09 toks/s, output: 14.92 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:55<00:13, 14.56it/s, est. speed input: 15274.13 toks/s, output: 14.92 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:56<00:12, 14.57it/s, est. speed input: 15270.86 toks/s, output: 14.91 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:57<00:11, 14.59it/s, est. speed input: 15268.05 toks/s, output: 14.91 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:57<00:11, 14.56it/s, est. speed input: 15263.90 toks/s, output: 14.91 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:58<00:10, 14.57it/s, est. speed input: 15260.94 toks/s, output: 14.90 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:58<00:10, 14.53it/s, est. speed input: 15256.41 toks/s, output: 14.90 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:59<00:09, 14.53it/s, est. speed input: 15252.99 toks/s, output: 14.90 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:59<00:09, 14.49it/s, est. speed input: 15248.35 toks/s, output: 14.89 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [01:00<00:08, 14.55it/s, est. speed input: 15246.35 toks/s, output: 14.89 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [01:00<00:08, 14.51it/s, est. speed input: 15241.96 toks/s, output: 14.88 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [01:01<00:07, 14.57it/s, est. speed input: 15240.48 toks/s, output: 14.88 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [01:01<00:07, 14.54it/s, est. speed input: 15236.70 toks/s, output: 14.88 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [01:02<00:06, 14.58it/s, est. speed input: 15234.76 toks/s, output: 14.88 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [01:03<00:05, 15.04it/s, est. speed input: 15245.60 toks/s, output: 14.89 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [01:03<00:05, 14.90it/s, est. speed input: 15242.82 toks/s, output: 14.89 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [01:04<00:04, 14.73it/s, est. speed input: 15238.01 toks/s, output: 14.88 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [01:04<00:04, 14.67it/s, est. speed input: 15235.01 toks/s, output: 14.88 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [01:05<00:03, 14.62it/s, est. speed input: 15231.78 toks/s, output: 14.87 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [01:05<00:03, 14.57it/s, est. speed input: 15228.17 toks/s, output: 14.87 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [01:06<00:02, 15.14it/s, est. speed input: 15241.42 toks/s, output: 14.88 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [01:06<00:01, 15.01it/s, est. speed input: 15240.13 toks/s, output: 14.88 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [01:07<00:01, 14.85it/s, est. speed input: 15236.83 toks/s, output: 14.88 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [01:07<00:00, 14.74it/s, est. speed input: 15233.60 toks/s, output: 14.88 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [01:08<00:00, 15.27it/s, est. speed input: 15246.41 toks/s, output: 14.89 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:08<00:00, 15.27it/s, est. speed input: 15336.22 toks/s, output: 14.98 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:08<00:00, 14.98it/s, est. speed input: 15336.22 toks/s, output: 14.98 toks/s]
[rank0]:[W128 08:02:47.901224078 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 106.1s

测试结果:
  Requests/s:   14.57
  Tokens/s:     14933.18
  Total Reqs:   1024
  Elapsed:      70.29s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     14918.61

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:02:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:02:59 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3668383) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3668383) WARNING 01-28 08:03:27 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.69 requests/s, 15057.82 total tokens/s, 14.69 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 08:02:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:02:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:02:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:02:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:02:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:02:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:02:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:02:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:02:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:02:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:02:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:02:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:02:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:02:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:03:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:03:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:03:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:03:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:03:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:03:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:03:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:03:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:03:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:03:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:03:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:03:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:03:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:03:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3668383) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3668383) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.59s/it]
(EngineCore_DP0 pid=3668383) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.59s/it]
(EngineCore_DP0 pid=3668383) 
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3668383) [2026-01-28 08:03:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3668383) 2026-01-28 08:03:25,651 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3668383) 2026-01-28 08:03:25,762 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 61/2048 [00:00<00:03, 607.24it/s]
Adding requests:   6%|▌         | 122/2048 [00:00<00:03, 579.90it/s]
Adding requests:   9%|▉         | 181/2048 [00:00<00:03, 547.10it/s]
Adding requests:  12%|█▏        | 237/2048 [00:00<00:03, 549.54it/s]
Adding requests:  14%|█▍        | 293/2048 [00:00<00:03, 536.28it/s]
Adding requests:  17%|█▋        | 347/2048 [00:00<00:03, 528.22it/s]
Adding requests:  20%|█▉        | 402/2048 [00:00<00:03, 534.13it/s]
Adding requests:  22%|██▏       | 458/2048 [00:00<00:02, 539.86it/s]
Adding requests:  25%|██▌       | 513/2048 [00:00<00:02, 530.30it/s]
Adding requests:  28%|██▊       | 567/2048 [00:01<00:02, 533.14it/s]
Adding requests:  30%|███       | 623/2048 [00:01<00:02, 538.33it/s]
Adding requests:  33%|███▎      | 679/2048 [00:01<00:02, 542.56it/s]
Adding requests:  36%|███▌      | 734/2048 [00:01<00:02, 544.41it/s]
Adding requests:  39%|███▊      | 789/2048 [00:01<00:02, 514.80it/s]
Adding requests:  41%|████      | 841/2048 [00:01<00:02, 509.00it/s]
Adding requests:  44%|████▎     | 893/2048 [00:02<00:05, 213.17it/s]
Adding requests:  46%|████▌     | 946/2048 [00:02<00:04, 258.74it/s]
Adding requests:  49%|████▉     | 999/2048 [00:02<00:03, 304.44it/s]
Adding requests:  51%|█████▏    | 1051/2048 [00:02<00:02, 346.58it/s]
Adding requests:  54%|█████▍    | 1104/2048 [00:02<00:02, 385.87it/s]
Adding requests:  56%|█████▋    | 1156/2048 [00:02<00:02, 416.55it/s]
Adding requests:  59%|█████▉    | 1212/2048 [00:02<00:01, 452.78it/s]
Adding requests:  62%|██████▏   | 1264/2048 [00:02<00:01, 464.22it/s]
Adding requests:  64%|██████▍   | 1318/2048 [00:02<00:01, 484.26it/s]
Adding requests:  67%|██████▋   | 1373/2048 [00:03<00:01, 500.85it/s]
Adding requests:  70%|██████▉   | 1430/2048 [00:03<00:01, 517.25it/s]
Adding requests:  73%|███████▎  | 1485/2048 [00:03<00:01, 526.35it/s]
Adding requests:  75%|███████▌  | 1539/2048 [00:03<00:00, 526.37it/s]
Adding requests:  78%|███████▊  | 1596/2048 [00:03<00:00, 537.83it/s]
Adding requests:  81%|████████  | 1651/2048 [00:03<00:00, 531.82it/s]
Adding requests:  83%|████████▎ | 1705/2048 [00:03<00:00, 526.62it/s]
Adding requests:  86%|████████▌ | 1759/2048 [00:03<00:00, 530.30it/s]
Adding requests:  89%|████████▊ | 1814/2048 [00:03<00:00, 534.30it/s]
Adding requests:  91%|█████████ | 1868/2048 [00:04<00:00, 527.02it/s]
Adding requests:  94%|█████████▍| 1922/2048 [00:04<00:00, 529.21it/s]
Adding requests:  96%|█████████▋| 1976/2048 [00:04<00:00, 528.45it/s]
Adding requests:  99%|█████████▉| 2029/2048 [00:04<00:00, 514.23it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 470.41it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 50/2048 [00:00<00:08, 224.14it/s, est. speed input: 229545.07 toks/s, output: 224.15 toks/s]
Processed prompts:   4%|▎         | 73/2048 [00:01<00:42, 46.56it/s, est. speed input: 56949.72 toks/s, output: 55.61 toks/s]   
Processed prompts:   4%|▍         | 84/2048 [00:02<01:14, 26.25it/s, est. speed input: 35743.17 toks/s, output: 34.91 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:03<01:36, 20.29it/s, est. speed input: 28664.06 toks/s, output: 27.99 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:04<01:46, 18.12it/s, est. speed input: 25427.66 toks/s, output: 24.83 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:05<01:53, 16.88it/s, est. speed input: 23418.91 toks/s, output: 22.87 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:06<01:57, 16.12it/s, est. speed input: 22061.36 toks/s, output: 21.54 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:07<02:00, 15.66it/s, est. speed input: 21091.19 toks/s, output: 20.60 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:08<02:01, 15.35it/s, est. speed input: 20354.68 toks/s, output: 19.88 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:10<02:02, 15.15it/s, est. speed input: 19782.58 toks/s, output: 19.32 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:11<02:02, 15.00it/s, est. speed input: 19314.07 toks/s, output: 18.86 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:12<02:02, 14.91it/s, est. speed input: 18935.54 toks/s, output: 18.49 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:13<02:01, 14.84it/s, est. speed input: 18616.27 toks/s, output: 18.18 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:14<02:00, 14.79it/s, est. speed input: 18345.42 toks/s, output: 17.92 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:15<02:00, 14.75it/s, est. speed input: 18109.36 toks/s, output: 17.68 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:16<01:59, 14.73it/s, est. speed input: 17907.03 toks/s, output: 17.49 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:17<01:58, 14.72it/s, est. speed input: 17730.28 toks/s, output: 17.31 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:18<01:57, 14.70it/s, est. speed input: 17572.16 toks/s, output: 17.16 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:19<01:55, 14.86it/s, est. speed input: 17468.46 toks/s, output: 17.06 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:20<01:54, 14.81it/s, est. speed input: 17342.52 toks/s, output: 16.94 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:21<01:53, 14.77it/s, est. speed input: 17227.31 toks/s, output: 16.82 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:23<01:52, 14.72it/s, est. speed input: 17120.63 toks/s, output: 16.72 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:24<01:51, 14.70it/s, est. speed input: 17024.02 toks/s, output: 16.62 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:25<01:51, 14.68it/s, est. speed input: 16935.42 toks/s, output: 16.54 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:26<01:50, 14.67it/s, est. speed input: 16855.34 toks/s, output: 16.46 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:27<01:47, 14.92it/s, est. speed input: 16820.17 toks/s, output: 16.43 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:28<01:46, 14.83it/s, est. speed input: 16749.66 toks/s, output: 16.36 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:29<01:45, 14.79it/s, est. speed input: 16686.47 toks/s, output: 16.30 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:30<01:45, 14.74it/s, est. speed input: 16626.09 toks/s, output: 16.24 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:31<01:44, 14.71it/s, est. speed input: 16569.94 toks/s, output: 16.18 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:32<01:43, 14.69it/s, est. speed input: 16517.35 toks/s, output: 16.13 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:33<01:42, 14.67it/s, est. speed input: 16467.66 toks/s, output: 16.08 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:35<01:41, 14.66it/s, est. speed input: 16421.10 toks/s, output: 16.04 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:36<01:40, 14.66it/s, est. speed input: 16378.26 toks/s, output: 15.99 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:37<01:39, 14.64it/s, est. speed input: 16336.11 toks/s, output: 15.95 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:38<01:38, 14.63it/s, est. speed input: 16296.83 toks/s, output: 15.91 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:39<01:37, 14.63it/s, est. speed input: 16260.06 toks/s, output: 15.88 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:40<01:36, 14.63it/s, est. speed input: 16225.72 toks/s, output: 15.85 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:41<01:34, 14.63it/s, est. speed input: 16193.43 toks/s, output: 15.81 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:42<01:33, 14.63it/s, est. speed input: 16161.83 toks/s, output: 15.78 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:43<01:32, 14.63it/s, est. speed input: 16132.71 toks/s, output: 15.75 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:44<01:31, 14.65it/s, est. speed input: 16106.45 toks/s, output: 15.73 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:45<01:30, 14.64it/s, est. speed input: 16079.51 toks/s, output: 15.70 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:47<01:29, 14.62it/s, est. speed input: 16052.12 toks/s, output: 15.68 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:48<01:28, 14.62it/s, est. speed input: 16027.93 toks/s, output: 15.65 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:49<01:27, 14.63it/s, est. speed input: 16005.47 toks/s, output: 15.63 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:50<01:26, 14.63it/s, est. speed input: 15983.04 toks/s, output: 15.61 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:51<01:25, 14.63it/s, est. speed input: 15961.66 toks/s, output: 15.59 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:52<01:24, 14.64it/s, est. speed input: 15941.79 toks/s, output: 15.57 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:53<01:23, 14.62it/s, est. speed input: 15921.26 toks/s, output: 15.55 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:54<01:21, 14.62it/s, est. speed input: 15902.13 toks/s, output: 15.53 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:55<01:20, 14.63it/s, est. speed input: 15884.46 toks/s, output: 15.51 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:56<01:19, 14.62it/s, est. speed input: 15866.32 toks/s, output: 15.49 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:58<01:18, 14.63it/s, est. speed input: 15849.94 toks/s, output: 15.48 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:59<01:17, 14.62it/s, est. speed input: 15833.40 toks/s, output: 15.46 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [01:00<01:15, 14.88it/s, est. speed input: 15834.70 toks/s, output: 15.46 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [01:01<01:14, 14.81it/s, est. speed input: 15819.41 toks/s, output: 15.45 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [01:02<01:13, 14.74it/s, est. speed input: 15804.24 toks/s, output: 15.43 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [01:03<01:11, 14.99it/s, est. speed input: 15806.86 toks/s, output: 15.44 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [01:04<01:10, 14.89it/s, est. speed input: 15793.75 toks/s, output: 15.42 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [01:05<01:10, 14.80it/s, est. speed input: 15779.44 toks/s, output: 15.41 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [01:06<01:09, 14.74it/s, est. speed input: 15765.68 toks/s, output: 15.40 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [01:07<01:08, 14.70it/s, est. speed input: 15752.55 toks/s, output: 15.38 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [01:08<01:07, 14.67it/s, est. speed input: 15739.80 toks/s, output: 15.37 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [01:09<01:06, 14.66it/s, est. speed input: 15727.85 toks/s, output: 15.36 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [01:11<01:05, 14.66it/s, est. speed input: 15716.94 toks/s, output: 15.35 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [01:12<01:04, 14.64it/s, est. speed input: 15705.37 toks/s, output: 15.34 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [01:13<01:03, 14.64it/s, est. speed input: 15694.79 toks/s, output: 15.33 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [01:14<01:02, 14.64it/s, est. speed input: 15684.16 toks/s, output: 15.32 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [01:15<01:00, 14.88it/s, est. speed input: 15686.49 toks/s, output: 15.32 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [01:16<00:59, 14.79it/s, est. speed input: 15675.84 toks/s, output: 15.31 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [01:17<00:58, 14.73it/s, est. speed input: 15665.49 toks/s, output: 15.30 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [01:18<00:57, 14.69it/s, est. speed input: 15655.60 toks/s, output: 15.29 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [01:19<00:56, 14.67it/s, est. speed input: 15646.02 toks/s, output: 15.28 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [01:20<00:55, 14.65it/s, est. speed input: 15636.69 toks/s, output: 15.27 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [01:21<00:54, 14.64it/s, est. speed input: 15627.80 toks/s, output: 15.26 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [01:22<00:52, 14.88it/s, est. speed input: 15630.56 toks/s, output: 15.26 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [01:24<00:51, 14.81it/s, est. speed input: 15622.19 toks/s, output: 15.26 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [01:25<00:49, 15.02it/s, est. speed input: 15625.56 toks/s, output: 15.26 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [01:26<00:49, 14.89it/s, est. speed input: 15616.91 toks/s, output: 15.25 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [01:27<00:48, 14.80it/s, est. speed input: 15608.69 toks/s, output: 15.24 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [01:28<00:47, 14.74it/s, est. speed input: 15600.53 toks/s, output: 15.23 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [01:29<00:46, 14.71it/s, est. speed input: 15592.93 toks/s, output: 15.23 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [01:30<00:45, 14.68it/s, est. speed input: 15585.26 toks/s, output: 15.22 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [01:31<00:44, 14.65it/s, est. speed input: 15577.52 toks/s, output: 15.21 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [01:32<00:43, 14.63it/s, est. speed input: 15569.85 toks/s, output: 15.20 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [01:33<00:42, 14.61it/s, est. speed input: 15562.20 toks/s, output: 15.20 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [01:34<00:41, 14.62it/s, est. speed input: 15555.50 toks/s, output: 15.19 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [01:36<00:40, 14.62it/s, est. speed input: 15548.91 toks/s, output: 15.18 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [01:37<00:39, 14.60it/s, est. speed input: 15541.69 toks/s, output: 15.18 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [01:38<00:38, 14.59it/s, est. speed input: 15534.60 toks/s, output: 15.17 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [01:39<00:37, 14.60it/s, est. speed input: 15528.32 toks/s, output: 15.16 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [01:40<00:36, 14.60it/s, est. speed input: 15522.07 toks/s, output: 15.16 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [01:41<00:34, 14.60it/s, est. speed input: 15516.06 toks/s, output: 15.15 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [01:42<00:33, 14.60it/s, est. speed input: 15509.96 toks/s, output: 15.15 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [01:43<00:32, 14.61it/s, est. speed input: 15504.36 toks/s, output: 15.14 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [01:44<00:31, 14.89it/s, est. speed input: 15508.76 toks/s, output: 15.15 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [01:45<00:30, 14.81it/s, est. speed input: 15503.24 toks/s, output: 15.14 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [01:46<00:29, 14.76it/s, est. speed input: 15498.11 toks/s, output: 15.13 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [01:47<00:28, 14.73it/s, est. speed input: 15493.07 toks/s, output: 15.13 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [01:49<00:26, 14.96it/s, est. speed input: 15497.08 toks/s, output: 15.13 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [01:50<00:25, 14.85it/s, est. speed input: 15491.48 toks/s, output: 15.13 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [01:51<00:24, 14.76it/s, est. speed input: 15485.66 toks/s, output: 15.12 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [01:52<00:23, 14.71it/s, est. speed input: 15480.52 toks/s, output: 15.12 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [01:53<00:22, 14.68it/s, est. speed input: 15475.45 toks/s, output: 15.11 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [01:54<00:21, 14.67it/s, est. speed input: 15470.92 toks/s, output: 15.11 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [01:55<00:20, 14.66it/s, est. speed input: 15466.42 toks/s, output: 15.10 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [01:56<00:19, 14.63it/s, est. speed input: 15461.25 toks/s, output: 15.10 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [01:57<00:18, 14.62it/s, est. speed input: 15456.25 toks/s, output: 15.09 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [01:58<00:17, 14.62it/s, est. speed input: 15451.94 toks/s, output: 15.09 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [01:59<00:16, 14.61it/s, est. speed input: 15447.26 toks/s, output: 15.09 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [02:01<00:15, 14.61it/s, est. speed input: 15442.68 toks/s, output: 15.08 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [02:02<00:14, 14.60it/s, est. speed input: 15438.21 toks/s, output: 15.08 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [02:03<00:13, 14.61it/s, est. speed input: 15434.01 toks/s, output: 15.07 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [02:04<00:11, 14.86it/s, est. speed input: 15437.64 toks/s, output: 15.08 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [02:05<00:10, 14.78it/s, est. speed input: 15433.43 toks/s, output: 15.07 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [02:06<00:09, 14.73it/s, est. speed input: 15429.30 toks/s, output: 15.07 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [02:07<00:08, 14.70it/s, est. speed input: 15425.39 toks/s, output: 15.06 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [02:08<00:07, 14.68it/s, est. speed input: 15421.64 toks/s, output: 15.06 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [02:09<00:06, 14.93it/s, est. speed input: 15425.66 toks/s, output: 15.06 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [02:10<00:05, 14.84it/s, est. speed input: 15421.98 toks/s, output: 15.06 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [02:11<00:04, 15.05it/s, est. speed input: 15425.93 toks/s, output: 15.06 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [02:12<00:03, 14.91it/s, est. speed input: 15421.98 toks/s, output: 15.06 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [02:14<00:02, 14.82it/s, est. speed input: 15418.34 toks/s, output: 15.06 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [02:15<00:00, 15.03it/s, est. speed input: 15422.16 toks/s, output: 15.06 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:15<00:00, 15.03it/s, est. speed input: 15528.28 toks/s, output: 15.16 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:15<00:00, 15.16it/s, est. speed input: 15528.28 toks/s, output: 15.16 toks/s]
[rank0]:[W128 08:05:47.352386565 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 179.4s

测试结果:
  Requests/s:   14.69
  Tokens/s:     15057.82
  Total Reqs:   2048
  Elapsed:      139.41s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     15043.13

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:06:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:06:04 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3671118) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3671118) WARNING 01-28 08:06:34 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.91 requests/s, 15282.95 total tokens/s, 14.91 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 08:06:04] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:06:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:06:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:06:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:06:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:06:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:06:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:06:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:06:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:06:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:06:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:06:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:06:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:06:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:06:07] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:06:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:06:07] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:06:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:06:07] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:06:07] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:06:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:06:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:06:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:06:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:06:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:06:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:06:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:06:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3671118) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3671118) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.20s/it]
(EngineCore_DP0 pid=3671118) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.20s/it]
(EngineCore_DP0 pid=3671118) 
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3671118) [2026-01-28 08:06:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3671118) 2026-01-28 08:06:31,579 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3671118) 2026-01-28 08:06:31,960 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 66/4096 [00:00<00:06, 658.69it/s]
Adding requests:   3%|▎         | 132/4096 [00:00<00:06, 625.67it/s]
Adding requests:   5%|▍         | 195/4096 [00:00<00:07, 557.23it/s]
Adding requests:   6%|▌         | 252/4096 [00:00<00:07, 544.96it/s]
Adding requests:   7%|▋         | 307/4096 [00:00<00:07, 529.95it/s]
Adding requests:   9%|▉         | 361/4096 [00:00<00:07, 525.05it/s]
Adding requests:  10%|█         | 414/4096 [00:00<00:07, 516.09it/s]
Adding requests:  11%|█▏        | 468/4096 [00:00<00:06, 520.04it/s]
Adding requests:  13%|█▎        | 521/4096 [00:00<00:07, 508.80it/s]
Adding requests:  14%|█▍        | 576/4096 [00:01<00:06, 519.78it/s]
Adding requests:  15%|█▌        | 629/4096 [00:01<00:06, 518.18it/s]
Adding requests:  17%|█▋        | 685/4096 [00:01<00:06, 529.93it/s]
Adding requests:  18%|█▊        | 739/4096 [00:01<00:06, 528.73it/s]
Adding requests:  19%|█▉        | 792/4096 [00:01<00:06, 519.20it/s]
Adding requests:  21%|██        | 844/4096 [00:01<00:06, 505.61it/s]
Adding requests:  22%|██▏       | 899/4096 [00:01<00:06, 516.43it/s]
Adding requests:  23%|██▎       | 951/4096 [00:01<00:06, 516.65it/s]
Adding requests:  25%|██▍       | 1004/4096 [00:01<00:05, 518.54it/s]
Adding requests:  26%|██▌       | 1058/4096 [00:02<00:05, 523.44it/s]
Adding requests:  27%|██▋       | 1111/4096 [00:02<00:05, 515.47it/s]
Adding requests:  28%|██▊       | 1164/4096 [00:02<00:05, 518.30it/s]
Adding requests:  30%|██▉       | 1216/4096 [00:02<00:05, 483.63it/s]
Adding requests:  31%|███       | 1267/4096 [00:02<00:05, 488.99it/s]
Adding requests:  32%|███▏      | 1317/4096 [00:02<00:05, 490.61it/s]
Adding requests:  33%|███▎      | 1371/4096 [00:02<00:05, 503.34it/s]
Adding requests:  35%|███▍      | 1424/4096 [00:02<00:05, 509.96it/s]
Adding requests:  36%|███▌      | 1479/4096 [00:02<00:05, 520.58it/s]
Adding requests:  38%|███▊      | 1536/4096 [00:02<00:04, 533.20it/s]
Adding requests:  39%|███▉      | 1590/4096 [00:03<00:04, 534.66it/s]
Adding requests:  40%|████      | 1644/4096 [00:03<00:04, 536.02it/s]
Adding requests:  41%|████▏     | 1698/4096 [00:03<00:04, 534.58it/s]
Adding requests:  43%|████▎     | 1752/4096 [00:03<00:04, 535.71it/s]
Adding requests:  44%|████▍     | 1807/4096 [00:03<00:04, 537.78it/s]
Adding requests:  45%|████▌     | 1861/4096 [00:03<00:04, 535.07it/s]
Adding requests:  47%|████▋     | 1915/4096 [00:03<00:04, 530.88it/s]
Adding requests:  48%|████▊     | 1969/4096 [00:03<00:04, 526.62it/s]
Adding requests:  49%|████▉     | 2024/4096 [00:03<00:03, 532.89it/s]
Adding requests:  51%|█████     | 2078/4096 [00:03<00:03, 534.17it/s]
Adding requests:  52%|█████▏    | 2132/4096 [00:04<00:03, 531.22it/s]
Adding requests:  53%|█████▎    | 2186/4096 [00:04<00:03, 521.55it/s]
Adding requests:  55%|█████▍    | 2239/4096 [00:04<00:03, 520.82it/s]
Adding requests:  56%|█████▌    | 2295/4096 [00:04<00:03, 530.89it/s]
Adding requests:  57%|█████▋    | 2349/4096 [00:04<00:03, 523.03it/s]
Adding requests:  59%|█████▊    | 2402/4096 [00:04<00:03, 518.82it/s]
Adding requests:  60%|█████▉    | 2454/4096 [00:04<00:03, 482.47it/s]
Adding requests:  61%|██████▏   | 2509/4096 [00:04<00:03, 499.39it/s]
Adding requests:  63%|██████▎   | 2562/4096 [00:04<00:03, 505.38it/s]
Adding requests:  64%|██████▍   | 2615/4096 [00:05<00:02, 510.58it/s]
Adding requests:  65%|██████▌   | 2672/4096 [00:05<00:02, 526.74it/s]
Adding requests:  67%|██████▋   | 2725/4096 [00:05<00:02, 522.84it/s]
Adding requests:  68%|██████▊   | 2778/4096 [00:05<00:02, 524.79it/s]
Adding requests:  69%|██████▉   | 2831/4096 [00:05<00:02, 523.92it/s]
Adding requests:  70%|███████   | 2886/4096 [00:05<00:02, 531.59it/s]
Adding requests:  72%|███████▏  | 2940/4096 [00:05<00:02, 524.58it/s]
Adding requests:  73%|███████▎  | 2994/4096 [00:05<00:02, 528.97it/s]
Adding requests:  74%|███████▍  | 3047/4096 [00:05<00:01, 528.53it/s]
Adding requests:  76%|███████▌  | 3100/4096 [00:05<00:01, 519.80it/s]
Adding requests:  77%|███████▋  | 3153/4096 [00:06<00:01, 515.93it/s]
Adding requests:  78%|███████▊  | 3206/4096 [00:06<00:01, 518.63it/s]
Adding requests:  80%|███████▉  | 3260/4096 [00:06<00:01, 524.23it/s]
Adding requests:  81%|████████  | 3314/4096 [00:06<00:01, 525.93it/s]
Adding requests:  82%|████████▏ | 3367/4096 [00:06<00:01, 517.00it/s]
Adding requests:  83%|████████▎ | 3420/4096 [00:06<00:01, 518.31it/s]
Adding requests:  85%|████████▍ | 3472/4096 [00:06<00:01, 505.69it/s]
Adding requests:  86%|████████▌ | 3524/4096 [00:06<00:01, 508.40it/s]
Adding requests:  87%|████████▋ | 3575/4096 [00:06<00:01, 506.32it/s]
Adding requests:  89%|████████▊ | 3627/4096 [00:06<00:00, 507.54it/s]
Adding requests:  90%|████████▉ | 3683/4096 [00:07<00:00, 520.10it/s]
Adding requests:  91%|█████████ | 3736/4096 [00:07<00:00, 520.09it/s]
Adding requests:  93%|█████████▎| 3789/4096 [00:07<00:00, 512.53it/s]
Adding requests:  94%|█████████▍| 3841/4096 [00:07<00:00, 503.84it/s]
Adding requests:  95%|█████████▌| 3894/4096 [00:07<00:00, 510.77it/s]
Adding requests:  96%|█████████▋| 3946/4096 [00:07<00:00, 510.61it/s]
Adding requests:  98%|█████████▊| 3999/4096 [00:07<00:00, 514.95it/s]
Adding requests:  99%|█████████▉| 4051/4096 [00:07<00:00, 515.89it/s]
Adding requests: 100%|██████████| 4096/4096 [00:07<00:00, 520.35it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 98/4096 [00:01<00:41, 96.16it/s, est. speed input: 98475.79 toks/s, output: 96.16 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:03<01:53, 35.05it/s, est. speed input: 41917.30 toks/s, output: 40.93 toks/s]
Processed prompts:   4%|▍         | 162/4096 [00:05<02:40, 24.58it/s, est. speed input: 31104.36 toks/s, output: 30.38 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:07<03:10, 20.44it/s, est. speed input: 26524.95 toks/s, output: 25.90 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:09<03:31, 18.32it/s, est. speed input: 23991.74 toks/s, output: 23.43 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:11<03:44, 17.11it/s, est. speed input: 22390.80 toks/s, output: 21.87 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:13<03:52, 16.35it/s, est. speed input: 21279.58 toks/s, output: 20.78 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:16<03:56, 15.98it/s, est. speed input: 20527.61 toks/s, output: 20.05 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:18<03:59, 15.62it/s, est. speed input: 19895.13 toks/s, output: 19.43 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:20<04:01, 15.37it/s, est. speed input: 19396.95 toks/s, output: 18.94 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:22<04:01, 15.21it/s, est. speed input: 18993.78 toks/s, output: 18.55 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:24<03:59, 15.21it/s, est. speed input: 18701.39 toks/s, output: 18.26 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:26<03:59, 15.09it/s, est. speed input: 18416.65 toks/s, output: 17.98 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:28<03:58, 14.99it/s, est. speed input: 18172.07 toks/s, output: 17.75 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:31<03:57, 14.93it/s, est. speed input: 17962.06 toks/s, output: 17.54 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:33<03:56, 14.90it/s, est. speed input: 17782.03 toks/s, output: 17.37 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:35<03:54, 14.88it/s, est. speed input: 17623.31 toks/s, output: 17.21 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:37<03:52, 14.85it/s, est. speed input: 17480.97 toks/s, output: 17.07 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:39<03:50, 14.85it/s, est. speed input: 17357.26 toks/s, output: 16.95 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:41<03:48, 14.83it/s, est. speed input: 17243.35 toks/s, output: 16.84 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:44<03:46, 14.83it/s, est. speed input: 17142.42 toks/s, output: 16.74 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:46<03:44, 14.82it/s, est. speed input: 17049.80 toks/s, output: 16.65 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:48<03:42, 14.81it/s, est. speed input: 16964.24 toks/s, output: 16.57 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:50<03:40, 14.80it/s, est. speed input: 16886.13 toks/s, output: 16.49 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:52<03:38, 14.81it/s, est. speed input: 16816.79 toks/s, output: 16.42 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:54<03:35, 14.81it/s, est. speed input: 16752.22 toks/s, output: 16.36 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:56<03:31, 14.94it/s, est. speed input: 16709.82 toks/s, output: 16.32 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:59<03:28, 15.03it/s, est. speed input: 16670.92 toks/s, output: 16.28 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [01:01<03:27, 14.96it/s, est. speed input: 16617.91 toks/s, output: 16.23 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [01:03<03:25, 14.90it/s, est. speed input: 16566.65 toks/s, output: 16.18 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [01:05<03:24, 14.87it/s, est. speed input: 16520.05 toks/s, output: 16.13 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [01:07<03:22, 14.86it/s, est. speed input: 16477.12 toks/s, output: 16.09 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [01:09<03:20, 14.84it/s, est. speed input: 16435.84 toks/s, output: 16.05 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [01:12<03:16, 14.95it/s, est. speed input: 16410.61 toks/s, output: 16.03 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [01:14<03:15, 14.90it/s, est. speed input: 16374.24 toks/s, output: 15.99 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [01:16<03:13, 14.87it/s, est. speed input: 16339.50 toks/s, output: 15.96 toks/s]
Processed prompts:  31%|███       | 1250/4096 [01:18<03:10, 14.97it/s, est. speed input: 16319.00 toks/s, output: 15.94 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [01:20<03:07, 15.04it/s, est. speed input: 16299.75 toks/s, output: 15.92 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [01:22<03:05, 14.97it/s, est. speed input: 16269.90 toks/s, output: 15.89 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [01:24<03:04, 14.92it/s, est. speed input: 16241.40 toks/s, output: 15.86 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [01:27<03:02, 14.89it/s, est. speed input: 16214.90 toks/s, output: 15.83 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [01:29<03:00, 14.88it/s, est. speed input: 16190.76 toks/s, output: 15.81 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [01:31<02:58, 14.86it/s, est. speed input: 16166.46 toks/s, output: 15.79 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [01:33<02:56, 14.84it/s, est. speed input: 16143.37 toks/s, output: 15.76 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [01:35<02:54, 14.84it/s, est. speed input: 16121.63 toks/s, output: 15.74 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [01:37<02:52, 14.82it/s, est. speed input: 16099.62 toks/s, output: 15.72 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [01:39<02:49, 14.94it/s, est. speed input: 16088.79 toks/s, output: 15.71 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [01:42<02:47, 14.90it/s, est. speed input: 16069.56 toks/s, output: 15.69 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [01:44<02:44, 15.01it/s, est. speed input: 16060.62 toks/s, output: 15.68 toks/s]
Processed prompts:  41%|████      | 1666/4096 [01:46<02:42, 14.94it/s, est. speed input: 16042.10 toks/s, output: 15.67 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [01:48<02:40, 14.91it/s, est. speed input: 16024.88 toks/s, output: 15.65 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [01:50<02:39, 14.88it/s, est. speed input: 16008.21 toks/s, output: 15.63 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [01:52<02:37, 14.85it/s, est. speed input: 15991.58 toks/s, output: 15.62 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [01:54<02:35, 14.83it/s, est. speed input: 15975.74 toks/s, output: 15.60 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [01:57<02:33, 14.83it/s, est. speed input: 15960.82 toks/s, output: 15.59 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [01:59<02:29, 14.95it/s, est. speed input: 15954.39 toks/s, output: 15.58 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [02:01<02:27, 14.91it/s, est. speed input: 15940.57 toks/s, output: 15.57 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [02:03<02:26, 14.88it/s, est. speed input: 15926.92 toks/s, output: 15.55 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [02:05<02:22, 14.98it/s, est. speed input: 15921.35 toks/s, output: 15.55 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [02:07<02:20, 15.06it/s, est. speed input: 15916.26 toks/s, output: 15.54 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [02:09<02:18, 14.98it/s, est. speed input: 15903.44 toks/s, output: 15.53 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [02:12<02:17, 14.92it/s, est. speed input: 15891.01 toks/s, output: 15.52 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [02:14<02:15, 14.88it/s, est. speed input: 15879.17 toks/s, output: 15.51 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [02:16<02:13, 14.86it/s, est. speed input: 15867.86 toks/s, output: 15.50 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [02:18<02:11, 14.84it/s, est. speed input: 15856.64 toks/s, output: 15.48 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [02:20<02:09, 14.83it/s, est. speed input: 15845.96 toks/s, output: 15.47 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [02:22<02:05, 15.07it/s, est. speed input: 15848.42 toks/s, output: 15.48 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [02:24<02:03, 14.99it/s, est. speed input: 15838.40 toks/s, output: 15.47 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [02:27<02:00, 15.06it/s, est. speed input: 15834.94 toks/s, output: 15.46 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [02:29<01:59, 14.97it/s, est. speed input: 15824.70 toks/s, output: 15.45 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [02:31<01:56, 15.04it/s, est. speed input: 15820.91 toks/s, output: 15.45 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [02:33<01:53, 15.24it/s, est. speed input: 15824.78 toks/s, output: 15.45 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [02:35<01:51, 15.24it/s, est. speed input: 15821.60 toks/s, output: 15.45 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [02:37<01:49, 15.11it/s, est. speed input: 15812.85 toks/s, output: 15.44 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [02:39<01:48, 15.02it/s, est. speed input: 15804.04 toks/s, output: 15.43 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [02:41<01:45, 15.08it/s, est. speed input: 15801.37 toks/s, output: 15.43 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [02:44<01:44, 14.99it/s, est. speed input: 15792.47 toks/s, output: 15.42 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [02:46<01:41, 15.06it/s, est. speed input: 15790.17 toks/s, output: 15.42 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [02:48<01:40, 14.98it/s, est. speed input: 15781.98 toks/s, output: 15.41 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [02:50<01:38, 14.94it/s, est. speed input: 15774.37 toks/s, output: 15.40 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [02:52<01:36, 14.90it/s, est. speed input: 15766.84 toks/s, output: 15.40 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [02:54<01:33, 15.00it/s, est. speed input: 15764.99 toks/s, output: 15.40 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [02:56<01:31, 14.94it/s, est. speed input: 15757.56 toks/s, output: 15.39 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [02:59<01:30, 14.90it/s, est. speed input: 15750.38 toks/s, output: 15.38 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [03:01<01:28, 14.88it/s, est. speed input: 15743.48 toks/s, output: 15.37 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [03:03<01:26, 14.85it/s, est. speed input: 15736.46 toks/s, output: 15.37 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [03:05<01:23, 14.97it/s, est. speed input: 15735.00 toks/s, output: 15.37 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [03:07<01:21, 14.91it/s, est. speed input: 15728.17 toks/s, output: 15.36 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [03:09<01:19, 14.88it/s, est. speed input: 15721.61 toks/s, output: 15.35 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [03:11<01:17, 14.85it/s, est. speed input: 15714.96 toks/s, output: 15.35 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [03:14<01:15, 14.84it/s, est. speed input: 15708.81 toks/s, output: 15.34 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [03:16<01:13, 14.83it/s, est. speed input: 15702.93 toks/s, output: 15.33 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [03:18<01:11, 14.81it/s, est. speed input: 15696.68 toks/s, output: 15.33 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [03:20<01:09, 14.80it/s, est. speed input: 15690.52 toks/s, output: 15.32 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [03:22<01:05, 15.05it/s, est. speed input: 15694.12 toks/s, output: 15.33 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [03:24<01:03, 15.11it/s, est. speed input: 15693.45 toks/s, output: 15.33 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [03:26<01:01, 15.02it/s, est. speed input: 15687.95 toks/s, output: 15.32 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [03:29<00:59, 14.95it/s, est. speed input: 15682.33 toks/s, output: 15.31 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [03:31<00:57, 15.03it/s, est. speed input: 15681.25 toks/s, output: 15.31 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [03:33<00:55, 14.95it/s, est. speed input: 15675.74 toks/s, output: 15.31 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [03:35<00:53, 14.90it/s, est. speed input: 15670.14 toks/s, output: 15.30 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [03:37<00:51, 14.87it/s, est. speed input: 15665.02 toks/s, output: 15.30 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [03:39<00:49, 14.85it/s, est. speed input: 15659.94 toks/s, output: 15.29 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [03:42<00:47, 14.82it/s, est. speed input: 15654.72 toks/s, output: 15.29 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [03:44<00:44, 14.94it/s, est. speed input: 15654.11 toks/s, output: 15.29 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [03:46<00:42, 14.91it/s, est. speed input: 15649.57 toks/s, output: 15.28 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [03:48<00:40, 15.15it/s, est. speed input: 15653.72 toks/s, output: 15.29 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [03:50<00:38, 15.05it/s, est. speed input: 15649.29 toks/s, output: 15.28 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [03:52<00:36, 14.97it/s, est. speed input: 15644.73 toks/s, output: 15.28 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [03:54<00:34, 14.93it/s, est. speed input: 15640.50 toks/s, output: 15.27 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [03:56<00:32, 14.89it/s, est. speed input: 15636.08 toks/s, output: 15.27 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [03:59<00:29, 14.87it/s, est. speed input: 15631.90 toks/s, output: 15.27 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [04:01<00:27, 14.85it/s, est. speed input: 15627.67 toks/s, output: 15.26 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [04:03<00:25, 14.96it/s, est. speed input: 15627.29 toks/s, output: 15.26 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [04:05<00:23, 14.91it/s, est. speed input: 15623.20 toks/s, output: 15.26 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [04:07<00:21, 14.88it/s, est. speed input: 15619.19 toks/s, output: 15.25 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [04:09<00:19, 14.86it/s, est. speed input: 15615.37 toks/s, output: 15.25 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [04:11<00:16, 14.97it/s, est. speed input: 15615.09 toks/s, output: 15.25 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [04:14<00:14, 14.91it/s, est. speed input: 15611.07 toks/s, output: 15.25 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [04:16<00:12, 14.88it/s, est. speed input: 15607.28 toks/s, output: 15.24 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [04:18<00:10, 14.86it/s, est. speed input: 15603.49 toks/s, output: 15.24 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [04:20<00:08, 14.84it/s, est. speed input: 15599.71 toks/s, output: 15.23 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [04:22<00:06, 14.83it/s, est. speed input: 15596.07 toks/s, output: 15.23 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [04:24<00:04, 15.21it/s, est. speed input: 15603.37 toks/s, output: 15.24 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [04:26<00:01, 15.22it/s, est. speed input: 15603.45 toks/s, output: 15.24 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:26<00:00, 15.22it/s, est. speed input: 15718.56 toks/s, output: 15.35 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:26<00:00, 15.35it/s, est. speed input: 15718.56 toks/s, output: 15.35 toks/s]
[rank0]:[W128 08:11:09.878372058 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 322.6s

测试结果:
  Requests/s:   14.91
  Tokens/s:     15282.95
  Total Reqs:   4096
  Elapsed:      274.71s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     15268.04


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,20.9877,10766.7101,6.0988
1024,1024,1,128,128,15.2244,15605.0582,8.4075
2048,1024,2,256,128,15.8947,16292.0179,16.1060
4096,1024,4,512,128,14.7591,15128.1072,34.6904
8192,1024,8,1024,128,14.5690,14933.1828,70.2864
16384,1024,16,2048,128,14.6906,15057.8218,139.4093
32768,1024,32,4096,128,14.9102,15282.9478,274.7114

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:11:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:11:16 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3675579) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3675579) WARNING 01-28 08:11:44 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 27.43 requests/s, 14071.15 total tokens/s, 27.43 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:11:15] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:11:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:11:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:11:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:11:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:11:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:11:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:11:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:11:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:11:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:11:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:11:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:11:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:11:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:11:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:11:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:11:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:11:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3675579) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3675579) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.95s/it]
(EngineCore_DP0 pid=3675579) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.95s/it]
(EngineCore_DP0 pid=3675579) 
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3675579) [2026-01-28 08:11:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3675579) 2026-01-28 08:11:44,236 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3675579) 2026-01-28 08:11:44,250 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  70%|██████▉   | 89/128 [00:00<00:00, 889.40it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 981.29it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 17.46it/s, est. speed input: 8942.78 toks/s, output: 17.46 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 23.50it/s, est. speed input: 11551.61 toks/s, output: 22.56 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:04, 25.77it/s, est. speed input: 12546.73 toks/s, output: 24.50 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:04, 26.86it/s, est. speed input: 13053.32 toks/s, output: 25.49 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:04, 27.35it/s, est. speed input: 13328.87 toks/s, output: 26.03 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:04, 27.16it/s, est. speed input: 13398.78 toks/s, output: 26.17 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:03, 27.50it/s, est. speed input: 13545.88 toks/s, output: 26.46 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:03, 28.00it/s, est. speed input: 13708.98 toks/s, output: 26.77 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:03, 28.30it/s, est. speed input: 13830.08 toks/s, output: 27.01 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:03, 28.37it/s, est. speed input: 13907.26 toks/s, output: 27.16 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:03, 28.58it/s, est. speed input: 13992.65 toks/s, output: 27.33 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:03, 28.63it/s, est. speed input: 14051.91 toks/s, output: 27.44 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:03, 28.66it/s, est. speed input: 14102.27 toks/s, output: 27.54 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:03, 28.59it/s, est. speed input: 14133.83 toks/s, output: 27.60 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 28.11it/s, est. speed input: 14114.12 toks/s, output: 27.57 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 27.93it/s, est. speed input: 14112.54 toks/s, output: 27.56 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 28.06it/s, est. speed input: 14136.52 toks/s, output: 27.61 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 28.13it/s, est. speed input: 14156.38 toks/s, output: 27.65 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:02<00:02, 28.32it/s, est. speed input: 14186.16 toks/s, output: 27.71 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:02<00:02, 28.53it/s, est. speed input: 14219.33 toks/s, output: 27.77 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:02<00:02, 28.69it/s, est. speed input: 14249.92 toks/s, output: 27.83 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:02, 28.72it/s, est. speed input: 14271.63 toks/s, output: 27.87 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:02<00:02, 28.70it/s, est. speed input: 14288.72 toks/s, output: 27.91 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:01, 28.81it/s, est. speed input: 14312.66 toks/s, output: 27.95 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:01, 28.33it/s, est. speed input: 14298.42 toks/s, output: 27.93 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 28.21it/s, est. speed input: 14298.69 toks/s, output: 27.93 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 28.52it/s, est. speed input: 14323.06 toks/s, output: 27.97 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 28.71it/s, est. speed input: 14344.11 toks/s, output: 28.02 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:03<00:01, 28.73it/s, est. speed input: 14357.39 toks/s, output: 28.04 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:03<00:01, 28.74it/s, est. speed input: 14369.33 toks/s, output: 28.06 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:03<00:01, 28.76it/s, est. speed input: 14381.54 toks/s, output: 28.09 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:03<00:01, 28.96it/s, est. speed input: 14402.42 toks/s, output: 28.13 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:03<00:01, 28.97it/s, est. speed input: 14415.80 toks/s, output: 28.16 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 28.89it/s, est. speed input: 14423.85 toks/s, output: 28.17 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:03<00:00, 28.56it/s, est. speed input: 14418.68 toks/s, output: 28.16 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:03<00:00, 28.38it/s, est. speed input: 14416.00 toks/s, output: 28.16 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:03<00:00, 28.58it/s, est. speed input: 14427.91 toks/s, output: 28.18 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:04<00:00, 28.41it/s, est. speed input: 14425.76 toks/s, output: 28.18 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:04<00:00, 28.14it/s, est. speed input: 14417.07 toks/s, output: 28.16 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:04<00:00, 28.27it/s, est. speed input: 14422.27 toks/s, output: 28.17 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:04<00:00, 28.48it/s, est. speed input: 14432.51 toks/s, output: 28.19 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:04<00:00, 28.60it/s, est. speed input: 14440.66 toks/s, output: 28.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 28.76it/s, est. speed input: 14451.59 toks/s, output: 28.23 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 28.76it/s, est. speed input: 14451.59 toks/s, output: 28.23 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 28.22it/s, est. speed input: 14451.59 toks/s, output: 28.23 toks/s]
[rank0]:[W128 08:11:49.842270508 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.0s

测试结果:
  Requests/s:   27.43
  Tokens/s:     14071.15
  Total Reqs:   128
  Elapsed:      4.67s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     14043.73

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:11:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:11:56 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3676338) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3676338) WARNING 01-28 08:12:24 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.89 requests/s, 15267.27 total tokens/s, 14.89 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:11:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:11:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:11:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:11:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:11:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:11:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:11:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:11:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:11:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:11:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:12:00] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:12:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:12:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:12:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:12:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:12:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:12:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:12:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:12:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3676338) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3676338) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.46s/it]
(EngineCore_DP0 pid=3676338) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.46s/it]
(EngineCore_DP0 pid=3676338) 
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3676338) [2026-01-28 08:12:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3676338) 2026-01-28 08:12:24,336 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3676338) 2026-01-28 08:12:24,348 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  52%|█████▏    | 67/128 [00:00<00:00, 663.64it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 663.22it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:19,  6.46it/s, est. speed input: 6620.06 toks/s, output: 6.46 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:11, 11.15it/s, est. speed input: 10649.72 toks/s, output: 10.40 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:09, 12.92it/s, est. speed input: 12187.65 toks/s, output: 11.90 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:08, 13.97it/s, est. speed input: 13093.16 toks/s, output: 12.79 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:08, 14.29it/s, est. speed input: 13511.41 toks/s, output: 13.19 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:08, 14.62it/s, est. speed input: 13853.30 toks/s, output: 13.53 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:07, 14.93it/s, est. speed input: 14143.41 toks/s, output: 13.81 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 15.14it/s, est. speed input: 14362.78 toks/s, output: 14.03 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:07, 15.35it/s, est. speed input: 14558.52 toks/s, output: 14.22 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:07, 15.45it/s, est. speed input: 14701.44 toks/s, output: 14.36 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 15.41it/s, est. speed input: 14790.13 toks/s, output: 14.44 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 15.42it/s, est. speed input: 14873.87 toks/s, output: 14.53 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 15.27it/s, est. speed input: 14905.81 toks/s, output: 14.56 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 15.43it/s, est. speed input: 14994.84 toks/s, output: 14.64 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:06, 15.50it/s, est. speed input: 15062.78 toks/s, output: 14.71 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:06, 15.51it/s, est. speed input: 15113.21 toks/s, output: 14.76 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:06, 15.49it/s, est. speed input: 15153.76 toks/s, output: 14.80 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 15.55it/s, est. speed input: 15204.05 toks/s, output: 14.85 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 15.63it/s, est. speed input: 15255.02 toks/s, output: 14.90 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 15.54it/s, est. speed input: 15277.74 toks/s, output: 14.92 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 15.24it/s, est. speed input: 15260.72 toks/s, output: 14.90 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 15.32it/s, est. speed input: 15288.33 toks/s, output: 14.93 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:03<00:05, 15.35it/s, est. speed input: 15309.82 toks/s, output: 14.95 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:05, 15.36it/s, est. speed input: 15328.07 toks/s, output: 14.97 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:05, 15.46it/s, est. speed input: 15356.95 toks/s, output: 15.00 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 15.57it/s, est. speed input: 15389.18 toks/s, output: 15.03 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 15.58it/s, est. speed input: 15410.77 toks/s, output: 15.05 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 15.59it/s, est. speed input: 15431.35 toks/s, output: 15.07 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 15.23it/s, est. speed input: 15407.60 toks/s, output: 15.05 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 15.34it/s, est. speed input: 15426.40 toks/s, output: 15.06 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:04<00:04, 15.43it/s, est. speed input: 15444.81 toks/s, output: 15.08 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:04<00:04, 15.38it/s, est. speed input: 15450.38 toks/s, output: 15.09 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:04, 15.36it/s, est. speed input: 15457.27 toks/s, output: 15.09 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 15.39it/s, est. speed input: 15468.32 toks/s, output: 15.11 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 15.38it/s, est. speed input: 15475.28 toks/s, output: 15.11 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 15.33it/s, est. speed input: 15478.88 toks/s, output: 15.12 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 15.11it/s, est. speed input: 15464.73 toks/s, output: 15.10 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 15.25it/s, est. speed input: 15476.80 toks/s, output: 15.11 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:05<00:03, 15.32it/s, est. speed input: 15486.75 toks/s, output: 15.12 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:05<00:03, 15.34it/s, est. speed input: 15493.69 toks/s, output: 15.13 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:05<00:03, 15.46it/s, est. speed input: 15508.45 toks/s, output: 15.14 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 15.50it/s, est. speed input: 15519.37 toks/s, output: 15.16 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 15.45it/s, est. speed input: 15523.58 toks/s, output: 15.16 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 15.50it/s, est. speed input: 15534.22 toks/s, output: 15.17 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 15.28it/s, est. speed input: 15524.97 toks/s, output: 15.16 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:06<00:02, 15.30it/s, est. speed input: 15529.49 toks/s, output: 15.17 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:06<00:02, 15.29it/s, est. speed input: 15531.33 toks/s, output: 15.17 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:06<00:02, 15.30it/s, est. speed input: 15535.03 toks/s, output: 15.17 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:06<00:02, 15.42it/s, est. speed input: 15545.83 toks/s, output: 15.18 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 15.47it/s, est. speed input: 15554.18 toks/s, output: 15.19 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 15.44it/s, est. speed input: 15557.56 toks/s, output: 15.19 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 15.50it/s, est. speed input: 15566.34 toks/s, output: 15.20 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 15.35it/s, est. speed input: 15562.34 toks/s, output: 15.20 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:07<00:01, 15.37it/s, est. speed input: 15566.32 toks/s, output: 15.20 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:07<00:01, 15.40it/s, est. speed input: 15571.44 toks/s, output: 15.21 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:07<00:01, 15.43it/s, est. speed input: 15577.09 toks/s, output: 15.21 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:07<00:00, 15.53it/s, est. speed input: 15586.66 toks/s, output: 15.22 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 15.50it/s, est. speed input: 15590.59 toks/s, output: 15.23 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 15.55it/s, est. speed input: 15598.07 toks/s, output: 15.23 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 15.45it/s, est. speed input: 15597.53 toks/s, output: 15.23 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 15.29it/s, est. speed input: 15592.54 toks/s, output: 15.23 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:08<00:00, 15.32it/s, est. speed input: 15595.12 toks/s, output: 15.23 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:08<00:00, 15.30it/s, est. speed input: 15595.68 toks/s, output: 15.23 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:08<00:00, 15.35it/s, est. speed input: 15599.52 toks/s, output: 15.23 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 15.35it/s, est. speed input: 15604.35 toks/s, output: 15.24 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 15.24it/s, est. speed input: 15604.35 toks/s, output: 15.24 toks/s]
[rank0]:[W128 08:12:33.697248981 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.7s

测试结果:
  Requests/s:   14.89
  Tokens/s:     15267.27
  Total Reqs:   128
  Elapsed:      8.59s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     15252.38

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:12:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:12:40 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3677138) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3677138) WARNING 01-28 08:13:07 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.54 requests/s, 15926.62 total tokens/s, 15.54 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 08:12:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:12:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:12:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:12:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:12:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:12:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:12:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:12:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:12:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:12:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:12:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:12:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:12:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:12:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:12:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:12:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:12:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:12:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:12:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3677138) [2026-01-28 08:12:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3677138) [2026-01-28 08:12:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3677138) [2026-01-28 08:12:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3677138) [2026-01-28 08:12:44] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3677138) [2026-01-28 08:12:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3677138) [2026-01-28 08:12:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3677138) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3677138) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.32s/it]
(EngineCore_DP0 pid=3677138) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.32s/it]
(EngineCore_DP0 pid=3677138) 
(EngineCore_DP0 pid=3677138) [2026-01-28 08:13:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3677138) [2026-01-28 08:13:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3677138) [2026-01-28 08:13:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3677138) [2026-01-28 08:13:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3677138) [2026-01-28 08:13:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3677138) [2026-01-28 08:13:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3677138) [2026-01-28 08:13:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3677138) [2026-01-28 08:13:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3677138) 2026-01-28 08:13:07,247 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3677138) 2026-01-28 08:13:07,258 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  23%|██▎       | 60/256 [00:00<00:00, 591.10it/s]
Adding requests:  47%|████▋     | 120/256 [00:00<00:00, 592.83it/s]
Adding requests:  70%|███████   | 180/256 [00:00<00:00, 581.71it/s]
Adding requests:  93%|█████████▎| 239/256 [00:00<00:00, 571.43it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 577.43it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 6/256 [00:00<00:04, 51.71it/s, est. speed input: 52965.00 toks/s, output: 51.72 toks/s]
Processed prompts:   5%|▍         | 12/256 [00:00<00:11, 21.74it/s, est. speed input: 24386.92 toks/s, output: 23.81 toks/s]
Processed prompts:   6%|▋         | 16/256 [00:00<00:12, 19.17it/s, est. speed input: 21655.98 toks/s, output: 21.15 toks/s]
Processed prompts:   7%|▋         | 19/256 [00:00<00:11, 20.27it/s, est. speed input: 22046.09 toks/s, output: 21.53 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:01<00:13, 16.92it/s, est. speed input: 19869.10 toks/s, output: 19.40 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:01<00:13, 16.68it/s, est. speed input: 19508.36 toks/s, output: 19.05 toks/s]
Processed prompts:  10%|█         | 26/256 [00:01<00:14, 16.27it/s, est. speed input: 19117.54 toks/s, output: 18.67 toks/s]
Processed prompts:  11%|█         | 28/256 [00:01<00:14, 16.08it/s, est. speed input: 18844.40 toks/s, output: 18.40 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:01<00:14, 15.97it/s, est. speed input: 18627.14 toks/s, output: 18.19 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:01<00:14, 15.95it/s, est. speed input: 18461.35 toks/s, output: 18.03 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:01<00:13, 15.91it/s, est. speed input: 18309.61 toks/s, output: 17.88 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:02<00:13, 15.88it/s, est. speed input: 18178.08 toks/s, output: 17.75 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:02<00:13, 15.76it/s, est. speed input: 18038.70 toks/s, output: 17.62 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:02<00:13, 15.79it/s, est. speed input: 17937.94 toks/s, output: 17.52 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:02<00:13, 15.56it/s, est. speed input: 17798.02 toks/s, output: 17.38 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:02<00:13, 15.64it/s, est. speed input: 17719.10 toks/s, output: 17.30 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:02<00:13, 15.62it/s, est. speed input: 17633.79 toks/s, output: 17.22 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:02<00:13, 15.69it/s, est. speed input: 17570.95 toks/s, output: 17.16 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:02<00:13, 15.79it/s, est. speed input: 17521.55 toks/s, output: 17.11 toks/s]
Processed prompts:  20%|██        | 52/256 [00:03<00:12, 15.77it/s, est. speed input: 17462.97 toks/s, output: 17.05 toks/s]
Processed prompts:  21%|██        | 54/256 [00:03<00:12, 15.86it/s, est. speed input: 17423.47 toks/s, output: 17.02 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:03<00:12, 15.75it/s, est. speed input: 17362.35 toks/s, output: 16.96 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:03<00:12, 15.80it/s, est. speed input: 17323.82 toks/s, output: 16.92 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:03<00:12, 15.53it/s, est. speed input: 17247.34 toks/s, output: 16.84 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:03<00:12, 15.60it/s, est. speed input: 17209.45 toks/s, output: 16.81 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:03<00:12, 15.64it/s, est. speed input: 17172.97 toks/s, output: 16.77 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:03<00:12, 15.70it/s, est. speed input: 17142.02 toks/s, output: 16.74 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:04<00:12, 15.66it/s, est. speed input: 17104.44 toks/s, output: 16.70 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:04<00:11, 15.61it/s, est. speed input: 17066.15 toks/s, output: 16.67 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:04<00:11, 15.60it/s, est. speed input: 17033.23 toks/s, output: 16.63 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:04<00:11, 15.64it/s, est. speed input: 17006.49 toks/s, output: 16.61 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:04<00:11, 15.52it/s, est. speed input: 16966.93 toks/s, output: 16.57 toks/s]
Processed prompts:  30%|███       | 78/256 [00:04<00:11, 15.59it/s, est. speed input: 16944.83 toks/s, output: 16.55 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:04<00:11, 15.64it/s, est. speed input: 16923.73 toks/s, output: 16.53 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:04<00:11, 15.63it/s, est. speed input: 16898.85 toks/s, output: 16.50 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:05<00:11, 15.60it/s, est. speed input: 16873.68 toks/s, output: 16.48 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:05<00:10, 15.73it/s, est. speed input: 16863.25 toks/s, output: 16.47 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:05<00:10, 15.75it/s, est. speed input: 16847.10 toks/s, output: 16.45 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:05<00:10, 15.73it/s, est. speed input: 16828.90 toks/s, output: 16.43 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:05<00:10, 15.50it/s, est. speed input: 16793.69 toks/s, output: 16.40 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:05<00:10, 15.52it/s, est. speed input: 16774.61 toks/s, output: 16.38 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:05<00:10, 15.54it/s, est. speed input: 16756.37 toks/s, output: 16.36 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:05<00:10, 15.54it/s, est. speed input: 16738.73 toks/s, output: 16.35 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:06<00:09, 15.60it/s, est. speed input: 16725.80 toks/s, output: 16.33 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:06<00:09, 15.62it/s, est. speed input: 16711.73 toks/s, output: 16.32 toks/s]
Processed prompts:  41%|████      | 104/256 [00:06<00:09, 15.61it/s, est. speed input: 16696.82 toks/s, output: 16.31 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:06<00:09, 15.64it/s, est. speed input: 16684.44 toks/s, output: 16.29 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:06<00:09, 15.43it/s, est. speed input: 16657.11 toks/s, output: 16.27 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:06<00:09, 15.60it/s, est. speed input: 16652.14 toks/s, output: 16.26 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:06<00:09, 15.72it/s, est. speed input: 16647.51 toks/s, output: 16.26 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:07<00:09, 15.74it/s, est. speed input: 16638.85 toks/s, output: 16.25 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:07<00:08, 15.75it/s, est. speed input: 16630.40 toks/s, output: 16.24 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:07<00:08, 15.71it/s, est. speed input: 16618.97 toks/s, output: 16.23 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:07<00:08, 15.70it/s, est. speed input: 16609.17 toks/s, output: 16.22 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:07<00:08, 15.63it/s, est. speed input: 16595.84 toks/s, output: 16.21 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:07<00:08, 15.48it/s, est. speed input: 16577.22 toks/s, output: 16.19 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:07<00:08, 15.46it/s, est. speed input: 16564.17 toks/s, output: 16.18 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:07<00:08, 15.54it/s, est. speed input: 16556.87 toks/s, output: 16.17 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:08<00:08, 15.49it/s, est. speed input: 16543.28 toks/s, output: 16.16 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:08<00:07, 15.62it/s, est. speed input: 16539.95 toks/s, output: 16.15 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:08<00:07, 15.69it/s, est. speed input: 16535.47 toks/s, output: 16.15 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:08<00:07, 15.73it/s, est. speed input: 16530.39 toks/s, output: 16.14 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:08<00:07, 15.73it/s, est. speed input: 16524.08 toks/s, output: 16.14 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:08<00:07, 15.67it/s, est. speed input: 16515.03 toks/s, output: 16.13 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:08<00:07, 15.46it/s, est. speed input: 16497.48 toks/s, output: 16.11 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:08<00:07, 15.61it/s, est. speed input: 16495.45 toks/s, output: 16.11 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:09<00:07, 15.70it/s, est. speed input: 16492.71 toks/s, output: 16.11 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:09<00:06, 15.71it/s, est. speed input: 16487.50 toks/s, output: 16.10 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:09<00:06, 15.73it/s, est. speed input: 16482.89 toks/s, output: 16.10 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:09<00:06, 15.78it/s, est. speed input: 16480.33 toks/s, output: 16.09 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:09<00:06, 15.75it/s, est. speed input: 16474.39 toks/s, output: 16.09 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:09<00:06, 15.70it/s, est. speed input: 16467.61 toks/s, output: 16.08 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:09<00:06, 15.48it/s, est. speed input: 16452.62 toks/s, output: 16.07 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:09<00:06, 15.62it/s, est. speed input: 16451.00 toks/s, output: 16.07 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:10<00:06, 15.62it/s, est. speed input: 16445.48 toks/s, output: 16.06 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:10<00:05, 15.62it/s, est. speed input: 16439.51 toks/s, output: 16.05 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:10<00:05, 15.60it/s, est. speed input: 16433.40 toks/s, output: 16.05 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:10<00:05, 15.61it/s, est. speed input: 16428.20 toks/s, output: 16.04 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:10<00:05, 15.65it/s, est. speed input: 16424.42 toks/s, output: 16.04 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:10<00:05, 15.64it/s, est. speed input: 16419.16 toks/s, output: 16.03 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:10<00:05, 15.48it/s, est. speed input: 16407.82 toks/s, output: 16.02 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:10<00:05, 15.52it/s, est. speed input: 16403.10 toks/s, output: 16.02 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:11<00:04, 15.63it/s, est. speed input: 16401.56 toks/s, output: 16.02 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:11<00:04, 15.65it/s, est. speed input: 16398.01 toks/s, output: 16.01 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:11<00:04, 15.60it/s, est. speed input: 16391.54 toks/s, output: 16.01 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:11<00:04, 15.62it/s, est. speed input: 16387.91 toks/s, output: 16.00 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:11<00:04, 15.70it/s, est. speed input: 16386.69 toks/s, output: 16.00 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:11<00:04, 15.73it/s, est. speed input: 16384.53 toks/s, output: 16.00 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:11<00:04, 15.56it/s, est. speed input: 16375.09 toks/s, output: 15.99 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:12<00:04, 15.57it/s, est. speed input: 16370.56 toks/s, output: 15.99 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:12<00:03, 15.55it/s, est. speed input: 16365.58 toks/s, output: 15.98 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:12<00:03, 15.68it/s, est. speed input: 16365.53 toks/s, output: 15.98 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:12<00:03, 15.77it/s, est. speed input: 16365.51 toks/s, output: 15.98 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:12<00:03, 15.81it/s, est. speed input: 16364.71 toks/s, output: 15.98 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:12<00:03, 15.83it/s, est. speed input: 16363.56 toks/s, output: 15.98 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:12<00:03, 15.87it/s, est. speed input: 16363.64 toks/s, output: 15.98 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:12<00:03, 15.82it/s, est. speed input: 16360.64 toks/s, output: 15.98 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:13<00:03, 15.64it/s, est. speed input: 16353.25 toks/s, output: 15.97 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:13<00:02, 15.69it/s, est. speed input: 16351.45 toks/s, output: 15.97 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:13<00:02, 15.62it/s, est. speed input: 16346.47 toks/s, output: 15.96 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:13<00:02, 15.63it/s, est. speed input: 16343.35 toks/s, output: 15.96 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:13<00:02, 15.61it/s, est. speed input: 16339.44 toks/s, output: 15.96 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:13<00:02, 15.68it/s, est. speed input: 16338.41 toks/s, output: 15.96 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:13<00:02, 15.74it/s, est. speed input: 16337.65 toks/s, output: 15.95 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:13<00:02, 15.75it/s, est. speed input: 16335.96 toks/s, output: 15.95 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:14<00:02, 15.50it/s, est. speed input: 16326.06 toks/s, output: 15.94 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:14<00:01, 15.63it/s, est. speed input: 16326.07 toks/s, output: 15.94 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:14<00:01, 15.62it/s, est. speed input: 16322.89 toks/s, output: 15.94 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:14<00:01, 15.67it/s, est. speed input: 16321.44 toks/s, output: 15.94 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:14<00:01, 15.65it/s, est. speed input: 16318.57 toks/s, output: 15.94 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:14<00:01, 15.71it/s, est. speed input: 16317.72 toks/s, output: 15.94 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:14<00:01, 15.71it/s, est. speed input: 16315.79 toks/s, output: 15.93 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:14<00:01, 15.68it/s, est. speed input: 16312.91 toks/s, output: 15.93 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:15<00:01, 15.50it/s, est. speed input: 16305.35 toks/s, output: 15.92 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:15<00:00, 15.58it/s, est. speed input: 16304.11 toks/s, output: 15.92 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:15<00:00, 15.62it/s, est. speed input: 16302.46 toks/s, output: 15.92 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:15<00:00, 15.69it/s, est. speed input: 16301.88 toks/s, output: 15.92 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:15<00:00, 15.66it/s, est. speed input: 16298.99 toks/s, output: 15.92 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:15<00:00, 15.64it/s, est. speed input: 16296.38 toks/s, output: 15.91 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:15<00:00, 15.71it/s, est. speed input: 16296.14 toks/s, output: 15.91 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:15<00:00, 15.59it/s, est. speed input: 16291.13 toks/s, output: 15.91 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 15.59it/s, est. speed input: 16352.22 toks/s, output: 15.97 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 15.97it/s, est. speed input: 16352.22 toks/s, output: 15.97 toks/s]
[rank0]:[W128 08:13:24.633990586 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.0s

测试结果:
  Requests/s:   15.54
  Tokens/s:     15926.62
  Total Reqs:   256
  Elapsed:      16.48s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     15911.08

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:13:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:13:31 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3678036) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3678036) WARNING 01-28 08:13:59 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.61 requests/s, 14971.39 total tokens/s, 14.61 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 08:13:31] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:13:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:13:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:13:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:13:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:13:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:13:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:13:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:13:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:13:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:13:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:13:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:13:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:13:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:13:35] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:13:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:13:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:13:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:13:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:13:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:13:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:13:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:13:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:13:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:13:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:13:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:13:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:13:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:36] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:36] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:36] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3678036) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3678036) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.36s/it]
(EngineCore_DP0 pid=3678036) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.36s/it]
(EngineCore_DP0 pid=3678036) 
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3678036) [2026-01-28 08:13:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3678036) 2026-01-28 08:13:59,113 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3678036) 2026-01-28 08:13:59,125 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  12%|█▏        | 60/512 [00:00<00:00, 598.41it/s]
Adding requests:  23%|██▎       | 120/512 [00:00<00:00, 580.47it/s]
Adding requests:  35%|███▍      | 179/512 [00:00<00:00, 554.19it/s]
Adding requests:  46%|████▌     | 235/512 [00:00<00:00, 555.80it/s]
Adding requests:  57%|█████▋    | 291/512 [00:00<00:00, 547.13it/s]
Adding requests:  68%|██████▊   | 346/512 [00:00<00:00, 543.25it/s]
Adding requests:  79%|███████▊  | 402/512 [00:00<00:00, 546.53it/s]
Adding requests:  89%|████████▉ | 457/512 [00:00<00:00, 540.34it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 536.27it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 546.70it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 14/512 [00:00<00:11, 41.76it/s, est. speed input: 42762.19 toks/s, output: 41.76 toks/s]
Processed prompts:   4%|▎         | 19/512 [00:00<00:16, 29.24it/s, est. speed input: 32062.66 toks/s, output: 31.31 toks/s]
Processed prompts:   4%|▍         | 22/512 [00:00<00:23, 21.20it/s, est. speed input: 25525.47 toks/s, output: 24.93 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:01<00:25, 18.78it/s, est. speed input: 23104.81 toks/s, output: 22.56 toks/s]
Processed prompts:   6%|▌         | 30/512 [00:01<00:27, 17.35it/s, est. speed input: 21573.81 toks/s, output: 21.07 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:01<00:29, 16.40it/s, est. speed input: 20490.58 toks/s, output: 20.01 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:01<00:30, 15.74it/s, est. speed input: 19680.18 toks/s, output: 19.22 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:02<00:30, 15.37it/s, est. speed input: 19100.95 toks/s, output: 18.65 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:02<00:30, 15.20it/s, est. speed input: 18678.34 toks/s, output: 18.24 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:02<00:30, 14.97it/s, est. speed input: 18296.56 toks/s, output: 17.87 toks/s]
Processed prompts:  11%|█         | 54/512 [00:03<00:30, 14.85it/s, est. speed input: 17995.01 toks/s, output: 17.57 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:03<00:30, 14.86it/s, est. speed input: 17772.96 toks/s, output: 17.36 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:03<00:30, 14.83it/s, est. speed input: 17573.91 toks/s, output: 17.16 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:03<00:30, 14.71it/s, est. speed input: 17375.31 toks/s, output: 16.97 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:04<00:29, 14.77it/s, est. speed input: 17239.54 toks/s, output: 16.84 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:04<00:29, 14.78it/s, est. speed input: 17113.38 toks/s, output: 16.71 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:04<00:29, 14.79it/s, est. speed input: 17001.09 toks/s, output: 16.60 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:04<00:29, 14.64it/s, est. speed input: 16868.66 toks/s, output: 16.47 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:05<00:29, 14.64it/s, est. speed input: 16771.97 toks/s, output: 16.38 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:05<00:28, 14.67it/s, est. speed input: 16688.35 toks/s, output: 16.30 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:05<00:28, 14.62it/s, est. speed input: 16600.77 toks/s, output: 16.21 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:06<00:28, 14.60it/s, est. speed input: 16523.94 toks/s, output: 16.14 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:06<00:27, 14.65it/s, est. speed input: 16463.67 toks/s, output: 16.08 toks/s]
Processed prompts:  21%|██        | 106/512 [00:06<00:27, 14.66it/s, est. speed input: 16405.73 toks/s, output: 16.02 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:06<00:27, 14.65it/s, est. speed input: 16349.02 toks/s, output: 15.97 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:07<00:27, 14.55it/s, est. speed input: 16283.25 toks/s, output: 15.90 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:07<00:27, 14.57it/s, est. speed input: 16234.97 toks/s, output: 15.85 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:07<00:26, 14.59it/s, est. speed input: 16190.94 toks/s, output: 15.81 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:07<00:26, 14.60it/s, est. speed input: 16149.00 toks/s, output: 15.77 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:08<00:26, 14.53it/s, est. speed input: 16100.98 toks/s, output: 15.72 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:08<00:25, 14.57it/s, est. speed input: 16066.62 toks/s, output: 15.69 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:08<00:25, 14.62it/s, est. speed input: 16036.35 toks/s, output: 15.66 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:09<00:25, 14.66it/s, est. speed input: 16008.07 toks/s, output: 15.63 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:09<00:25, 14.58it/s, est. speed input: 15970.99 toks/s, output: 15.60 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:09<00:24, 14.60it/s, est. speed input: 15943.34 toks/s, output: 15.57 toks/s]
Processed prompts:  30%|███       | 154/512 [00:09<00:24, 14.62it/s, est. speed input: 15917.45 toks/s, output: 15.54 toks/s]
Processed prompts:  31%|███       | 158/512 [00:10<00:24, 14.59it/s, est. speed input: 15889.72 toks/s, output: 15.52 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:10<00:23, 14.59it/s, est. speed input: 15864.75 toks/s, output: 15.49 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:10<00:23, 14.63it/s, est. speed input: 15844.87 toks/s, output: 15.47 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:10<00:23, 14.67it/s, est. speed input: 15826.89 toks/s, output: 15.46 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:11<00:23, 14.59it/s, est. speed input: 15800.22 toks/s, output: 15.43 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:11<00:22, 14.60it/s, est. speed input: 15780.92 toks/s, output: 15.41 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:11<00:22, 14.63it/s, est. speed input: 15763.76 toks/s, output: 15.39 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:12<00:22, 14.66it/s, est. speed input: 15749.01 toks/s, output: 15.38 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:12<00:22, 14.59it/s, est. speed input: 15726.85 toks/s, output: 15.36 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:12<00:21, 14.57it/s, est. speed input: 15708.03 toks/s, output: 15.34 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:12<00:21, 14.59it/s, est. speed input: 15692.97 toks/s, output: 15.33 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:13<00:21, 14.63it/s, est. speed input: 15680.65 toks/s, output: 15.31 toks/s]
Processed prompts:  40%|████      | 206/512 [00:13<00:21, 14.56it/s, est. speed input: 15660.79 toks/s, output: 15.29 toks/s]
Processed prompts:  41%|████      | 210/512 [00:13<00:20, 14.58it/s, est. speed input: 15647.73 toks/s, output: 15.28 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:14<00:20, 14.62it/s, est. speed input: 15636.42 toks/s, output: 15.27 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:14<00:20, 14.60it/s, est. speed input: 15622.37 toks/s, output: 15.26 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:14<00:19, 14.57it/s, est. speed input: 15607.73 toks/s, output: 15.24 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:14<00:19, 14.66it/s, est. speed input: 15600.49 toks/s, output: 15.23 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:15<00:19, 14.66it/s, est. speed input: 15589.98 toks/s, output: 15.22 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:15<00:18, 14.68it/s, est. speed input: 15580.89 toks/s, output: 15.22 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:15<00:18, 14.60it/s, est. speed input: 15566.73 toks/s, output: 15.20 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:15<00:18, 14.66it/s, est. speed input: 15559.75 toks/s, output: 15.20 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:16<00:18, 14.63it/s, est. speed input: 15548.36 toks/s, output: 15.18 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:16<00:17, 14.62it/s, est. speed input: 15538.58 toks/s, output: 15.17 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:16<00:17, 14.57it/s, est. speed input: 15526.39 toks/s, output: 15.16 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:17<00:17, 14.65it/s, est. speed input: 15520.94 toks/s, output: 15.16 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:17<00:17, 14.70it/s, est. speed input: 15515.64 toks/s, output: 15.15 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:17<00:16, 14.56it/s, est. speed input: 15501.06 toks/s, output: 15.14 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:17<00:16, 14.67it/s, est. speed input: 15497.59 toks/s, output: 15.13 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:18<00:16, 14.68it/s, est. speed input: 15491.03 toks/s, output: 15.13 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:18<00:15, 14.67it/s, est. speed input: 15483.87 toks/s, output: 15.12 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:18<00:15, 14.63it/s, est. speed input: 15475.17 toks/s, output: 15.11 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:18<00:15, 14.68it/s, est. speed input: 15470.38 toks/s, output: 15.11 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:19<00:15, 14.72it/s, est. speed input: 15466.35 toks/s, output: 15.10 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:19<00:14, 14.77it/s, est. speed input: 15463.01 toks/s, output: 15.10 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:19<00:14, 14.67it/s, est. speed input: 15453.67 toks/s, output: 15.09 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:20<00:14, 14.67it/s, est. speed input: 15447.71 toks/s, output: 15.09 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:20<00:14, 14.67it/s, est. speed input: 15441.88 toks/s, output: 15.08 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:20<00:13, 14.70it/s, est. speed input: 15438.06 toks/s, output: 15.08 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:20<00:13, 14.59it/s, est. speed input: 15427.57 toks/s, output: 15.07 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:21<00:13, 14.60it/s, est. speed input: 15422.07 toks/s, output: 15.06 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:21<00:12, 14.66it/s, est. speed input: 15418.61 toks/s, output: 15.06 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:21<00:12, 14.68it/s, est. speed input: 15414.20 toks/s, output: 15.05 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:21<00:12, 14.57it/s, est. speed input: 15404.73 toks/s, output: 15.04 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:22<00:12, 14.62it/s, est. speed input: 15400.86 toks/s, output: 15.04 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:22<00:11, 14.61it/s, est. speed input: 15395.04 toks/s, output: 15.03 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:22<00:11, 15.38it/s, est. speed input: 15420.73 toks/s, output: 15.06 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:22<00:11, 15.05it/s, est. speed input: 15411.73 toks/s, output: 15.05 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:23<00:10, 14.89it/s, est. speed input: 15405.56 toks/s, output: 15.04 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:23<00:10, 14.81it/s, est. speed input: 15400.49 toks/s, output: 15.04 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:23<00:10, 14.77it/s, est. speed input: 15396.33 toks/s, output: 15.04 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:24<00:10, 14.62it/s, est. speed input: 15387.47 toks/s, output: 15.03 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:24<00:09, 14.65it/s, est. speed input: 15383.94 toks/s, output: 15.02 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:24<00:09, 14.63it/s, est. speed input: 15378.88 toks/s, output: 15.02 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:24<00:09, 14.73it/s, est. speed input: 15378.26 toks/s, output: 15.02 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:25<00:09, 14.58it/s, est. speed input: 15369.59 toks/s, output: 15.01 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:25<00:08, 14.65it/s, est. speed input: 15367.22 toks/s, output: 15.01 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:25<00:08, 14.70it/s, est. speed input: 15365.22 toks/s, output: 15.01 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:26<00:08, 14.63it/s, est. speed input: 15359.32 toks/s, output: 15.00 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:26<00:08, 14.64it/s, est. speed input: 15355.97 toks/s, output: 15.00 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:26<00:07, 14.70it/s, est. speed input: 15354.10 toks/s, output: 14.99 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:26<00:07, 14.66it/s, est. speed input: 15349.65 toks/s, output: 14.99 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:27<00:07, 14.55it/s, est. speed input: 15342.44 toks/s, output: 14.98 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:27<00:07, 14.54it/s, est. speed input: 15337.69 toks/s, output: 14.98 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:27<00:06, 14.55it/s, est. speed input: 15333.48 toks/s, output: 14.97 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:27<00:06, 14.55it/s, est. speed input: 15329.27 toks/s, output: 14.97 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:28<00:06, 14.46it/s, est. speed input: 15322.10 toks/s, output: 14.96 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:28<00:05, 14.51it/s, est. speed input: 15318.78 toks/s, output: 14.96 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:28<00:05, 14.53it/s, est. speed input: 15315.06 toks/s, output: 14.96 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:29<00:05, 14.63it/s, est. speed input: 15314.29 toks/s, output: 14.96 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:29<00:05, 14.56it/s, est. speed input: 15308.89 toks/s, output: 14.95 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:29<00:04, 14.58it/s, est. speed input: 15305.69 toks/s, output: 14.95 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:29<00:04, 14.56it/s, est. speed input: 15301.62 toks/s, output: 14.94 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:30<00:04, 15.47it/s, est. speed input: 15325.54 toks/s, output: 14.97 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:30<00:03, 15.14it/s, est. speed input: 15320.41 toks/s, output: 14.96 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:30<00:03, 14.94it/s, est. speed input: 15316.20 toks/s, output: 14.96 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:30<00:03, 14.84it/s, est. speed input: 15313.04 toks/s, output: 14.95 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:31<00:03, 14.77it/s, est. speed input: 15309.99 toks/s, output: 14.95 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:31<00:02, 14.64it/s, est. speed input: 15304.52 toks/s, output: 14.95 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:31<00:02, 14.66it/s, est. speed input: 15302.34 toks/s, output: 14.94 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:31<00:02, 14.73it/s, est. speed input: 15301.90 toks/s, output: 14.94 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:32<00:02, 14.69it/s, est. speed input: 15298.79 toks/s, output: 14.94 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:32<00:01, 14.59it/s, est. speed input: 15293.88 toks/s, output: 14.94 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:32<00:01, 14.59it/s, est. speed input: 15290.74 toks/s, output: 14.93 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:33<00:01, 14.61it/s, est. speed input: 15288.53 toks/s, output: 14.93 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:33<00:00, 14.61it/s, est. speed input: 15285.93 toks/s, output: 14.93 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:33<00:00, 14.59it/s, est. speed input: 15282.54 toks/s, output: 14.92 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:33<00:00, 14.66it/s, est. speed input: 15281.76 toks/s, output: 14.92 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:34<00:00, 15.75it/s, est. speed input: 15307.85 toks/s, output: 14.95 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:34<00:00, 15.75it/s, est. speed input: 15367.83 toks/s, output: 15.01 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:34<00:00, 15.01it/s, est. speed input: 15367.83 toks/s, output: 15.01 toks/s]
[rank0]:[W128 08:14:35.178635376 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 70.5s

测试结果:
  Requests/s:   14.61
  Tokens/s:     14971.39
  Total Reqs:   512
  Elapsed:      35.05s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     14956.78

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:14:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:14:43 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3679216) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3679216) WARNING 01-28 08:15:12 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.38 requests/s, 14736.91 total tokens/s, 14.38 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 08:14:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:14:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:14:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:14:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:14:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:14:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:14:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:14:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:14:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:14:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:14:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:14:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:14:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:14:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:14:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:14:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:14:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:14:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:14:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:14:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:14:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:14:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:14:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:14:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:14:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:14:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:14:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:14:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3679216) [2026-01-28 08:14:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3679216) [2026-01-28 08:14:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3679216) [2026-01-28 08:14:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3679216) [2026-01-28 08:14:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3679216) [2026-01-28 08:14:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3679216) [2026-01-28 08:14:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3679216) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3679216) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.35s/it]
(EngineCore_DP0 pid=3679216) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.35s/it]
(EngineCore_DP0 pid=3679216) 
(EngineCore_DP0 pid=3679216) [2026-01-28 08:15:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3679216) [2026-01-28 08:15:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3679216) [2026-01-28 08:15:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3679216) [2026-01-28 08:15:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3679216) [2026-01-28 08:15:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3679216) [2026-01-28 08:15:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3679216) [2026-01-28 08:15:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3679216) [2026-01-28 08:15:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3679216) 2026-01-28 08:15:11,375 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3679216) 2026-01-28 08:15:11,445 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 60/1024 [00:00<00:01, 591.16it/s]
Adding requests:  12%|█▏        | 120/1024 [00:00<00:01, 583.38it/s]
Adding requests:  17%|█▋        | 179/1024 [00:00<00:01, 540.44it/s]
Adding requests:  23%|██▎       | 234/1024 [00:00<00:01, 542.43it/s]
Adding requests:  28%|██▊       | 289/1024 [00:00<00:01, 534.39it/s]
Adding requests:  33%|███▎      | 343/1024 [00:00<00:01, 530.11it/s]
Adding requests:  39%|███▉      | 397/1024 [00:00<00:01, 528.89it/s]
Adding requests:  44%|████▍     | 450/1024 [00:00<00:01, 521.58it/s]
Adding requests:  49%|████▉     | 503/1024 [00:00<00:00, 522.31it/s]
Adding requests:  54%|█████▍    | 556/1024 [00:01<00:00, 505.21it/s]
Adding requests:  59%|█████▉    | 608/1024 [00:01<00:00, 508.35it/s]
Adding requests:  65%|██████▍   | 662/1024 [00:01<00:00, 515.76it/s]
Adding requests:  70%|███████   | 718/1024 [00:01<00:00, 526.82it/s]
Adding requests:  75%|███████▌  | 771/1024 [00:01<00:00, 524.97it/s]
Adding requests:  80%|████████  | 824/1024 [00:01<00:00, 513.81it/s]
Adding requests:  86%|████████▌ | 878/1024 [00:01<00:00, 520.08it/s]
Adding requests:  91%|█████████▏| 935/1024 [00:01<00:00, 530.39it/s]
Adding requests:  97%|█████████▋| 989/1024 [00:01<00:00, 529.20it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 527.80it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 26/1024 [00:00<00:17, 55.64it/s, est. speed input: 56977.54 toks/s, output: 55.64 toks/s]
Processed prompts:   3%|▎         | 34/1024 [00:01<00:33, 29.50it/s, est. speed input: 33855.90 toks/s, output: 33.06 toks/s]
Processed prompts:   4%|▍         | 42/1024 [00:01<00:43, 22.44it/s, est. speed input: 27211.27 toks/s, output: 26.57 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:02<00:50, 19.13it/s, est. speed input: 23920.40 toks/s, output: 23.36 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:02<00:55, 17.45it/s, est. speed input: 22061.36 toks/s, output: 21.54 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:03<00:58, 16.40it/s, est. speed input: 20807.46 toks/s, output: 20.32 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:03<01:00, 15.77it/s, est. speed input: 19937.63 toks/s, output: 19.47 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:04<01:01, 15.30it/s, est. speed input: 19259.93 toks/s, output: 18.81 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:04<01:02, 15.03it/s, est. speed input: 18755.91 toks/s, output: 18.32 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:05<01:02, 14.83it/s, est. speed input: 18345.20 toks/s, output: 17.92 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:06<01:02, 14.71it/s, est. speed input: 18016.37 toks/s, output: 17.59 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:06<01:02, 14.60it/s, est. speed input: 17736.50 toks/s, output: 17.32 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:07<01:02, 14.54it/s, est. speed input: 17504.48 toks/s, output: 17.09 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:07<01:01, 14.48it/s, est. speed input: 17299.50 toks/s, output: 16.89 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:08<01:01, 14.49it/s, est. speed input: 17137.23 toks/s, output: 16.74 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:08<01:00, 14.43it/s, est. speed input: 16978.75 toks/s, output: 16.58 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:09<01:00, 14.43it/s, est. speed input: 16846.89 toks/s, output: 16.45 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:09<00:59, 14.42it/s, est. speed input: 16729.43 toks/s, output: 16.34 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:10<00:59, 14.40it/s, est. speed input: 16622.01 toks/s, output: 16.23 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:11<00:58, 14.41it/s, est. speed input: 16529.29 toks/s, output: 16.14 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:11<00:58, 14.37it/s, est. speed input: 16436.83 toks/s, output: 16.05 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:12<00:57, 14.39it/s, est. speed input: 16361.88 toks/s, output: 15.98 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:12<00:57, 14.38it/s, est. speed input: 16288.62 toks/s, output: 15.91 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:13<00:56, 14.40it/s, est. speed input: 16225.80 toks/s, output: 15.85 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:13<00:56, 14.37it/s, est. speed input: 16162.10 toks/s, output: 15.78 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:14<00:55, 14.39it/s, est. speed input: 16108.54 toks/s, output: 15.73 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:14<00:55, 14.35it/s, est. speed input: 16051.72 toks/s, output: 15.68 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:15<00:54, 14.38it/s, est. speed input: 16006.82 toks/s, output: 15.63 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:16<00:53, 14.34it/s, est. speed input: 15957.26 toks/s, output: 15.58 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:16<00:53, 14.39it/s, est. speed input: 15920.74 toks/s, output: 15.55 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:17<00:52, 14.37it/s, est. speed input: 15879.52 toks/s, output: 15.51 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:17<00:52, 14.38it/s, est. speed input: 15844.40 toks/s, output: 15.47 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:18<00:51, 14.37it/s, est. speed input: 15808.99 toks/s, output: 15.44 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:18<00:51, 14.39it/s, est. speed input: 15779.00 toks/s, output: 15.41 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:19<00:50, 14.39it/s, est. speed input: 15748.83 toks/s, output: 15.38 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:19<00:49, 14.37it/s, est. speed input: 15718.93 toks/s, output: 15.35 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:20<00:49, 14.34it/s, est. speed input: 15688.38 toks/s, output: 15.32 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:21<00:48, 14.34it/s, est. speed input: 15661.42 toks/s, output: 15.29 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:21<00:48, 14.37it/s, est. speed input: 15639.65 toks/s, output: 15.27 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:22<00:46, 14.71it/s, est. speed input: 15646.53 toks/s, output: 15.28 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:22<00:46, 14.61it/s, est. speed input: 15624.10 toks/s, output: 15.26 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:23<00:46, 14.52it/s, est. speed input: 15600.66 toks/s, output: 15.24 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:23<00:45, 14.48it/s, est. speed input: 15580.54 toks/s, output: 15.22 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:24<00:45, 14.46it/s, est. speed input: 15561.89 toks/s, output: 15.20 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:24<00:44, 14.48it/s, est. speed input: 15546.38 toks/s, output: 15.18 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:25<00:44, 14.42it/s, est. speed input: 15526.31 toks/s, output: 15.16 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:26<00:43, 14.42it/s, est. speed input: 15509.89 toks/s, output: 15.15 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:26<00:43, 14.40it/s, est. speed input: 15492.83 toks/s, output: 15.13 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:27<00:42, 14.40it/s, est. speed input: 15477.36 toks/s, output: 15.11 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:27<00:42, 14.37it/s, est. speed input: 15460.61 toks/s, output: 15.10 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:28<00:41, 14.36it/s, est. speed input: 15445.05 toks/s, output: 15.08 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:28<00:41, 14.36it/s, est. speed input: 15430.81 toks/s, output: 15.07 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:29<00:40, 14.40it/s, est. speed input: 15419.53 toks/s, output: 15.06 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:29<00:38, 14.76it/s, est. speed input: 15430.74 toks/s, output: 15.07 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:30<00:38, 14.68it/s, est. speed input: 15420.20 toks/s, output: 15.06 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:30<00:38, 14.56it/s, est. speed input: 15405.67 toks/s, output: 15.04 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:31<00:37, 14.52it/s, est. speed input: 15394.36 toks/s, output: 15.03 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:32<00:37, 14.48it/s, est. speed input: 15382.97 toks/s, output: 15.02 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:32<00:36, 14.45it/s, est. speed input: 15371.64 toks/s, output: 15.01 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:33<00:36, 14.41it/s, est. speed input: 15359.59 toks/s, output: 15.00 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:33<00:35, 14.41it/s, est. speed input: 15349.68 toks/s, output: 14.99 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:34<00:35, 14.40it/s, est. speed input: 15339.77 toks/s, output: 14.98 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:34<00:34, 14.39it/s, est. speed input: 15329.50 toks/s, output: 14.97 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:35<00:34, 14.39it/s, est. speed input: 15320.17 toks/s, output: 14.96 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:35<00:33, 14.35it/s, est. speed input: 15309.01 toks/s, output: 14.95 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:36<00:33, 14.36it/s, est. speed input: 15300.36 toks/s, output: 14.94 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:37<00:32, 14.32it/s, est. speed input: 15289.24 toks/s, output: 14.93 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:37<00:32, 14.35it/s, est. speed input: 15281.24 toks/s, output: 14.92 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:38<00:31, 14.34it/s, est. speed input: 15272.18 toks/s, output: 14.91 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:38<00:31, 14.36it/s, est. speed input: 15264.60 toks/s, output: 14.91 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:39<00:30, 14.33it/s, est. speed input: 15255.52 toks/s, output: 14.90 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:39<00:29, 14.36it/s, est. speed input: 15248.84 toks/s, output: 14.89 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:40<00:29, 14.35it/s, est. speed input: 15240.63 toks/s, output: 14.88 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:41<00:28, 14.36it/s, est. speed input: 15233.86 toks/s, output: 14.88 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:41<00:28, 14.35it/s, est. speed input: 15226.27 toks/s, output: 14.87 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:42<00:27, 14.38it/s, est. speed input: 15220.50 toks/s, output: 14.86 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:42<00:27, 14.35it/s, est. speed input: 15212.83 toks/s, output: 14.86 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:43<00:26, 14.38it/s, est. speed input: 15207.67 toks/s, output: 14.85 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:43<00:26, 14.36it/s, est. speed input: 15200.54 toks/s, output: 14.84 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:44<00:25, 14.32it/s, est. speed input: 15192.51 toks/s, output: 14.84 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:44<00:24, 14.35it/s, est. speed input: 15187.10 toks/s, output: 14.83 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:45<00:24, 14.32it/s, est. speed input: 15180.03 toks/s, output: 14.82 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:46<00:23, 14.34it/s, est. speed input: 15174.67 toks/s, output: 14.82 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:46<00:23, 14.31it/s, est. speed input: 15167.42 toks/s, output: 14.81 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:47<00:22, 14.34it/s, est. speed input: 15162.80 toks/s, output: 14.81 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:47<00:22, 14.32it/s, est. speed input: 15156.04 toks/s, output: 14.80 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:48<00:21, 14.35it/s, est. speed input: 15151.50 toks/s, output: 14.80 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:48<00:21, 14.31it/s, est. speed input: 15144.88 toks/s, output: 14.79 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:49<00:20, 14.35it/s, est. speed input: 15140.83 toks/s, output: 14.79 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:49<00:19, 14.34it/s, est. speed input: 15135.60 toks/s, output: 14.78 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:50<00:19, 14.36it/s, est. speed input: 15131.20 toks/s, output: 14.78 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:51<00:18, 14.35it/s, est. speed input: 15126.27 toks/s, output: 14.77 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:51<00:18, 14.41it/s, est. speed input: 15123.84 toks/s, output: 14.77 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:52<00:17, 14.37it/s, est. speed input: 15118.26 toks/s, output: 14.76 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:52<00:17, 14.35it/s, est. speed input: 15113.34 toks/s, output: 14.76 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:53<00:16, 14.35it/s, est. speed input: 15109.08 toks/s, output: 14.75 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:53<00:15, 14.39it/s, est. speed input: 15106.03 toks/s, output: 14.75 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:54<00:15, 14.36it/s, est. speed input: 15101.09 toks/s, output: 14.75 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:54<00:14, 14.31it/s, est. speed input: 15095.49 toks/s, output: 14.74 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:55<00:14, 14.34it/s, est. speed input: 15092.06 toks/s, output: 14.74 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:56<00:13, 14.32it/s, est. speed input: 15087.29 toks/s, output: 14.73 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:56<00:13, 14.36it/s, est. speed input: 15084.58 toks/s, output: 14.73 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:57<00:12, 14.34it/s, est. speed input: 15080.18 toks/s, output: 14.73 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:57<00:12, 14.34it/s, est. speed input: 15076.32 toks/s, output: 14.72 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:58<00:11, 14.35it/s, est. speed input: 15072.85 toks/s, output: 14.72 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:58<00:10, 14.40it/s, est. speed input: 15070.88 toks/s, output: 14.72 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:59<00:10, 14.34it/s, est. speed input: 15065.77 toks/s, output: 14.71 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:59<00:09, 14.35it/s, est. speed input: 15062.51 toks/s, output: 14.71 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [01:00<00:09, 14.34it/s, est. speed input: 15058.75 toks/s, output: 14.71 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [01:01<00:08, 14.33it/s, est. speed input: 15055.12 toks/s, output: 14.70 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [01:01<00:08, 14.33it/s, est. speed input: 15051.63 toks/s, output: 14.70 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [01:02<00:07, 14.35it/s, est. speed input: 15048.91 toks/s, output: 14.70 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [01:02<00:07, 14.32it/s, est. speed input: 15044.78 toks/s, output: 14.69 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [01:03<00:06, 14.35it/s, est. speed input: 15042.31 toks/s, output: 14.69 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [01:03<00:05, 14.81it/s, est. speed input: 15052.84 toks/s, output: 14.70 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [01:04<00:05, 14.67it/s, est. speed input: 15049.87 toks/s, output: 14.70 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [01:04<00:04, 14.53it/s, est. speed input: 15045.65 toks/s, output: 14.69 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [01:05<00:04, 14.47it/s, est. speed input: 15042.53 toks/s, output: 14.69 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [01:06<00:03, 14.40it/s, est. speed input: 15038.50 toks/s, output: 14.69 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [01:06<00:03, 14.35it/s, est. speed input: 15034.68 toks/s, output: 14.68 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [01:07<00:02, 14.85it/s, est. speed input: 15045.82 toks/s, output: 14.69 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [01:07<00:02, 14.68it/s, est. speed input: 15042.49 toks/s, output: 14.69 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [01:08<00:01, 14.58it/s, est. speed input: 15039.62 toks/s, output: 14.69 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [01:08<00:00, 14.48it/s, est. speed input: 15036.14 toks/s, output: 14.68 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [01:09<00:00, 14.93it/s, est. speed input: 15046.50 toks/s, output: 14.69 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:09<00:00, 14.93it/s, est. speed input: 15135.13 toks/s, output: 14.78 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:09<00:00, 14.78it/s, est. speed input: 15135.13 toks/s, output: 14.78 toks/s]
[rank0]:[W128 08:16:23.896743387 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 108.8s

测试结果:
  Requests/s:   14.38
  Tokens/s:     14736.91
  Total Reqs:   1024
  Elapsed:      71.22s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     14722.53

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:16:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:16:35 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3680953) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3680953) WARNING 01-28 08:17:05 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.48 requests/s, 14843.97 total tokens/s, 14.48 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 08:16:35] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:16:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:16:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:16:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:16:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:16:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:16:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:16:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:16:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:16:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:16:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:16:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:16:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:16:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:16:38] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:16:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:16:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:16:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:16:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:16:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:16:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:16:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:16:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:16:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:16:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:16:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:16:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:16:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:39] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:39] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:39] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:39] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:39] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3680953) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3680953) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.72s/it]
(EngineCore_DP0 pid=3680953) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.72s/it]
(EngineCore_DP0 pid=3680953) 
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3680953) [2026-01-28 08:16:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3680953) 2026-01-28 08:17:03,960 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3680953) 2026-01-28 08:17:04,138 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 61/2048 [00:00<00:03, 603.65it/s]
Adding requests:   6%|▌         | 122/2048 [00:00<00:03, 587.41it/s]
Adding requests:   9%|▉         | 181/2048 [00:00<00:03, 529.15it/s]
Adding requests:  12%|█▏        | 238/2048 [00:00<00:03, 541.83it/s]
Adding requests:  14%|█▍        | 293/2048 [00:00<00:03, 543.85it/s]
Adding requests:  17%|█▋        | 348/2048 [00:00<00:03, 539.95it/s]
Adding requests:  20%|█▉        | 403/2048 [00:00<00:03, 530.30it/s]
Adding requests:  22%|██▏       | 457/2048 [00:00<00:03, 526.22it/s]
Adding requests:  25%|██▍       | 510/2048 [00:00<00:02, 516.79it/s]
Adding requests:  27%|██▋       | 563/2048 [00:01<00:02, 519.47it/s]
Adding requests:  30%|███       | 619/2048 [00:01<00:02, 528.22it/s]
Adding requests:  33%|███▎      | 674/2048 [00:01<00:02, 534.44it/s]
Adding requests:  36%|███▌      | 729/2048 [00:01<00:02, 536.71it/s]
Adding requests:  38%|███▊      | 783/2048 [00:01<00:02, 512.92it/s]
Adding requests:  41%|████      | 835/2048 [00:01<00:02, 511.88it/s]
Adding requests:  43%|████▎     | 887/2048 [00:02<00:05, 221.93it/s]
Adding requests:  46%|████▌     | 941/2048 [00:02<00:04, 269.69it/s]
Adding requests:  49%|████▊     | 996/2048 [00:02<00:03, 318.79it/s]
Adding requests:  51%|█████▏    | 1050/2048 [00:02<00:02, 362.12it/s]
Adding requests:  54%|█████▍    | 1102/2048 [00:02<00:02, 396.44it/s]
Adding requests:  56%|█████▋    | 1155/2048 [00:02<00:02, 428.53it/s]
Adding requests:  59%|█████▉    | 1212/2048 [00:02<00:01, 463.83it/s]
Adding requests:  62%|██████▏   | 1265/2048 [00:02<00:01, 480.68it/s]
Adding requests:  64%|██████▍   | 1318/2048 [00:02<00:01, 493.62it/s]
Adding requests:  67%|██████▋   | 1372/2048 [00:03<00:01, 506.68it/s]
Adding requests:  70%|██████▉   | 1427/2048 [00:03<00:01, 517.49it/s]
Adding requests:  72%|███████▏  | 1482/2048 [00:03<00:01, 526.64it/s]
Adding requests:  75%|███████▌  | 1536/2048 [00:03<00:00, 530.24it/s]
Adding requests:  78%|███████▊  | 1591/2048 [00:03<00:00, 533.62it/s]
Adding requests:  80%|████████  | 1647/2048 [00:03<00:00, 539.25it/s]
Adding requests:  83%|████████▎ | 1702/2048 [00:03<00:00, 534.23it/s]
Adding requests:  86%|████████▌ | 1758/2048 [00:03<00:00, 539.08it/s]
Adding requests:  89%|████████▊ | 1814/2048 [00:03<00:00, 543.55it/s]
Adding requests:  91%|█████████▏| 1869/2048 [00:03<00:00, 534.58it/s]
Adding requests:  94%|█████████▍| 1924/2048 [00:04<00:00, 537.31it/s]
Adding requests:  97%|█████████▋| 1978/2048 [00:04<00:00, 533.20it/s]
Adding requests:  99%|█████████▉| 2032/2048 [00:04<00:00, 510.48it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 475.01it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 50/2048 [00:00<00:12, 161.70it/s, est. speed input: 165620.01 toks/s, output: 161.71 toks/s]
Processed prompts:   3%|▎         | 67/2048 [00:01<00:50, 39.36it/s, est. speed input: 48519.39 toks/s, output: 47.38 toks/s]   
Processed prompts:   4%|▍         | 82/2048 [00:02<01:17, 25.28it/s, est. speed input: 33288.86 toks/s, output: 32.51 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:03<01:34, 20.54it/s, est. speed input: 27650.22 toks/s, output: 27.00 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:04<01:46, 18.19it/s, est. speed input: 24637.09 toks/s, output: 24.06 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:05<01:53, 16.87it/s, est. speed input: 22776.95 toks/s, output: 22.24 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:06<01:58, 16.07it/s, est. speed input: 21512.61 toks/s, output: 21.01 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:08<02:01, 15.57it/s, est. speed input: 20604.42 toks/s, output: 20.12 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:09<02:02, 15.23it/s, est. speed input: 19908.73 toks/s, output: 19.44 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:10<02:03, 14.97it/s, est. speed input: 19353.08 toks/s, output: 18.90 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:11<02:04, 14.82it/s, est. speed input: 18912.41 toks/s, output: 18.47 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:12<02:03, 14.71it/s, est. speed input: 18548.29 toks/s, output: 18.11 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:13<02:03, 14.64it/s, est. speed input: 18246.96 toks/s, output: 17.82 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:14<02:02, 14.58it/s, est. speed input: 17986.15 toks/s, output: 17.56 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:15<02:01, 14.57it/s, est. speed input: 17769.48 toks/s, output: 17.35 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:16<02:00, 14.54it/s, est. speed input: 17576.41 toks/s, output: 17.16 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:18<02:00, 14.51it/s, est. speed input: 17405.45 toks/s, output: 17.00 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:19<01:59, 14.50it/s, est. speed input: 17256.35 toks/s, output: 16.85 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:20<01:56, 14.67it/s, est. speed input: 17160.76 toks/s, output: 16.76 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:21<01:56, 14.59it/s, est. speed input: 17035.18 toks/s, output: 16.64 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:22<01:55, 14.54it/s, est. speed input: 16923.19 toks/s, output: 16.53 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:23<01:54, 14.51it/s, est. speed input: 16821.53 toks/s, output: 16.43 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:24<01:53, 14.49it/s, est. speed input: 16730.70 toks/s, output: 16.34 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:25<01:52, 14.48it/s, est. speed input: 16648.25 toks/s, output: 16.26 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:26<01:51, 14.47it/s, est. speed input: 16570.89 toks/s, output: 16.18 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:27<01:48, 14.67it/s, est. speed input: 16531.75 toks/s, output: 16.14 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:28<01:48, 14.59it/s, est. speed input: 16463.38 toks/s, output: 16.08 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:30<01:47, 14.55it/s, est. speed input: 16402.47 toks/s, output: 16.02 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:31<01:46, 14.53it/s, est. speed input: 16346.22 toks/s, output: 15.96 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:32<01:45, 14.49it/s, est. speed input: 16290.85 toks/s, output: 15.91 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:33<01:44, 14.48it/s, est. speed input: 16241.49 toks/s, output: 15.86 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:34<01:43, 14.47it/s, est. speed input: 16194.92 toks/s, output: 15.82 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:35<01:42, 14.46it/s, est. speed input: 16150.72 toks/s, output: 15.77 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:36<01:41, 14.45it/s, est. speed input: 16109.51 toks/s, output: 15.73 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:37<01:40, 14.45it/s, est. speed input: 16071.20 toks/s, output: 15.69 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:38<01:39, 14.43it/s, est. speed input: 16033.01 toks/s, output: 15.66 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:40<01:38, 14.44it/s, est. speed input: 15999.42 toks/s, output: 15.62 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:41<01:37, 14.44it/s, est. speed input: 15966.60 toks/s, output: 15.59 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:42<01:36, 14.44it/s, est. speed input: 15935.88 toks/s, output: 15.56 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:43<01:35, 14.44it/s, est. speed input: 15906.09 toks/s, output: 15.53 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:44<01:33, 14.45it/s, est. speed input: 15879.73 toks/s, output: 15.51 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:45<01:32, 14.45it/s, est. speed input: 15853.53 toks/s, output: 15.48 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:46<01:31, 14.45it/s, est. speed input: 15828.41 toks/s, output: 15.46 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:47<01:30, 14.43it/s, est. speed input: 15803.12 toks/s, output: 15.43 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:48<01:29, 14.43it/s, est. speed input: 15779.83 toks/s, output: 15.41 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:50<01:28, 14.43it/s, est. speed input: 15757.58 toks/s, output: 15.39 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:51<01:27, 14.43it/s, est. speed input: 15736.03 toks/s, output: 15.37 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:52<01:26, 14.44it/s, est. speed input: 15716.20 toks/s, output: 15.35 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:53<01:25, 14.44it/s, est. speed input: 15697.14 toks/s, output: 15.33 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:54<01:24, 14.43it/s, est. speed input: 15678.03 toks/s, output: 15.31 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:55<01:22, 14.44it/s, est. speed input: 15660.57 toks/s, output: 15.29 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:56<01:21, 14.42it/s, est. speed input: 15642.48 toks/s, output: 15.28 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:57<01:20, 14.42it/s, est. speed input: 15625.21 toks/s, output: 15.26 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:58<01:19, 14.42it/s, est. speed input: 15609.04 toks/s, output: 15.24 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [01:00<01:18, 14.42it/s, est. speed input: 15593.31 toks/s, output: 15.23 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [01:01<01:16, 14.65it/s, est. speed input: 15593.44 toks/s, output: 15.23 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [01:02<01:15, 14.57it/s, est. speed input: 15578.08 toks/s, output: 15.21 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [01:03<01:14, 14.51it/s, est. speed input: 15563.00 toks/s, output: 15.20 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [01:04<01:12, 14.73it/s, est. speed input: 15563.77 toks/s, output: 15.20 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [01:05<01:12, 14.62it/s, est. speed input: 15549.50 toks/s, output: 15.19 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [01:06<01:11, 14.56it/s, est. speed input: 15536.40 toks/s, output: 15.17 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [01:07<01:10, 14.50it/s, est. speed input: 15523.02 toks/s, output: 15.16 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [01:08<01:09, 14.47it/s, est. speed input: 15510.22 toks/s, output: 15.15 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [01:09<01:08, 14.46it/s, est. speed input: 15498.54 toks/s, output: 15.14 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [01:11<01:07, 14.44it/s, est. speed input: 15486.94 toks/s, output: 15.12 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [01:12<01:06, 14.42it/s, est. speed input: 15475.08 toks/s, output: 15.11 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [01:13<01:05, 14.42it/s, est. speed input: 15464.22 toks/s, output: 15.10 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [01:14<01:04, 14.42it/s, est. speed input: 15453.85 toks/s, output: 15.09 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [01:15<01:03, 14.42it/s, est. speed input: 15443.48 toks/s, output: 15.08 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [01:16<01:01, 14.65it/s, est. speed input: 15445.54 toks/s, output: 15.08 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [01:17<01:00, 14.57it/s, est. speed input: 15435.11 toks/s, output: 15.07 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [01:18<00:59, 14.52it/s, est. speed input: 15425.56 toks/s, output: 15.06 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [01:19<00:58, 14.50it/s, est. speed input: 15417.10 toks/s, output: 15.06 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [01:20<00:57, 14.49it/s, est. speed input: 15408.60 toks/s, output: 15.05 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [01:22<00:56, 14.47it/s, est. speed input: 15400.06 toks/s, output: 15.04 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [01:23<00:55, 14.45it/s, est. speed input: 15391.32 toks/s, output: 15.03 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [01:24<00:53, 14.67it/s, est. speed input: 15393.46 toks/s, output: 15.03 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [01:25<00:52, 14.57it/s, est. speed input: 15384.21 toks/s, output: 15.02 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [01:26<00:50, 14.78it/s, est. speed input: 15387.67 toks/s, output: 15.03 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [01:27<00:50, 14.65it/s, est. speed input: 15378.75 toks/s, output: 15.02 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [01:28<00:49, 14.57it/s, est. speed input: 15370.67 toks/s, output: 15.01 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [01:29<00:48, 14.50it/s, est. speed input: 15362.36 toks/s, output: 15.00 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [01:30<00:47, 14.49it/s, est. speed input: 15355.36 toks/s, output: 15.00 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [01:31<00:46, 14.48it/s, est. speed input: 15349.04 toks/s, output: 14.99 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [01:33<00:45, 14.48it/s, est. speed input: 15342.69 toks/s, output: 14.98 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [01:34<00:44, 14.46it/s, est. speed input: 15335.99 toks/s, output: 14.98 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [01:35<00:43, 14.44it/s, est. speed input: 15328.90 toks/s, output: 14.97 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [01:36<00:42, 14.43it/s, est. speed input: 15322.09 toks/s, output: 14.96 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [01:37<00:40, 14.44it/s, est. speed input: 15316.25 toks/s, output: 14.96 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [01:38<00:39, 14.43it/s, est. speed input: 15310.10 toks/s, output: 14.95 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [01:39<00:38, 14.41it/s, est. speed input: 15303.51 toks/s, output: 14.94 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [01:40<00:37, 14.41it/s, est. speed input: 15297.52 toks/s, output: 14.94 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [01:41<00:36, 14.40it/s, est. speed input: 15291.23 toks/s, output: 14.93 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [01:43<00:35, 14.41it/s, est. speed input: 15285.48 toks/s, output: 14.93 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [01:44<00:34, 14.41it/s, est. speed input: 15279.91 toks/s, output: 14.92 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [01:45<00:33, 14.40it/s, est. speed input: 15274.07 toks/s, output: 14.92 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [01:46<00:31, 14.64it/s, est. speed input: 15277.34 toks/s, output: 14.92 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [01:47<00:30, 14.57it/s, est. speed input: 15271.70 toks/s, output: 14.91 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [01:48<00:29, 14.52it/s, est. speed input: 15266.40 toks/s, output: 14.91 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [01:49<00:28, 14.50it/s, est. speed input: 15261.70 toks/s, output: 14.90 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [01:50<00:27, 14.69it/s, est. speed input: 15264.29 toks/s, output: 14.91 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [01:51<00:26, 14.61it/s, est. speed input: 15259.32 toks/s, output: 14.90 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [01:52<00:25, 14.54it/s, est. speed input: 15254.16 toks/s, output: 14.90 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [01:54<00:24, 14.47it/s, est. speed input: 15248.34 toks/s, output: 14.89 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [01:55<00:23, 14.45it/s, est. speed input: 15243.43 toks/s, output: 14.89 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [01:56<00:22, 14.44it/s, est. speed input: 15238.82 toks/s, output: 14.88 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [01:57<00:20, 14.43it/s, est. speed input: 15234.38 toks/s, output: 14.88 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [01:58<00:19, 14.43it/s, est. speed input: 15229.86 toks/s, output: 14.87 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [01:59<00:18, 14.42it/s, est. speed input: 15225.42 toks/s, output: 14.87 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [02:00<00:17, 14.41it/s, est. speed input: 15220.87 toks/s, output: 14.86 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [02:01<00:16, 14.42it/s, est. speed input: 15216.86 toks/s, output: 14.86 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [02:02<00:15, 14.41it/s, est. speed input: 15212.50 toks/s, output: 14.86 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [02:04<00:14, 14.41it/s, est. speed input: 15208.50 toks/s, output: 14.85 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [02:05<00:13, 14.42it/s, est. speed input: 15204.59 toks/s, output: 14.85 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [02:06<00:11, 14.65it/s, est. speed input: 15207.91 toks/s, output: 14.85 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [02:07<00:10, 14.59it/s, est. speed input: 15204.25 toks/s, output: 14.85 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [02:08<00:09, 14.54it/s, est. speed input: 15200.60 toks/s, output: 14.84 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [02:09<00:08, 14.50it/s, est. speed input: 15196.86 toks/s, output: 14.84 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [02:10<00:07, 14.48it/s, est. speed input: 15193.08 toks/s, output: 14.84 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [02:11<00:06, 14.71it/s, est. speed input: 15196.84 toks/s, output: 14.84 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [02:12<00:05, 14.61it/s, est. speed input: 15192.80 toks/s, output: 14.84 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [02:13<00:04, 14.80it/s, est. speed input: 15196.23 toks/s, output: 14.84 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [02:14<00:03, 14.68it/s, est. speed input: 15192.52 toks/s, output: 14.84 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [02:16<00:02, 14.59it/s, est. speed input: 15188.66 toks/s, output: 14.83 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [02:17<00:00, 14.76it/s, est. speed input: 15191.52 toks/s, output: 14.84 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:17<00:00, 14.76it/s, est. speed input: 15296.04 toks/s, output: 14.94 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:17<00:00, 14.94it/s, est. speed input: 15296.04 toks/s, output: 14.94 toks/s]
[rank0]:[W128 08:19:27.337303570 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 183.5s

测试结果:
  Requests/s:   14.48
  Tokens/s:     14843.97
  Total Reqs:   2048
  Elapsed:      141.42s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     14829.49

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:19:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:19:44 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3683744) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3683744) WARNING 01-28 08:20:17 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.56 requests/s, 14922.22 total tokens/s, 14.56 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 08:19:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:19:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:19:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:19:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:19:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:19:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:19:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:19:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:19:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:19:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:19:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:19:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:19:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:19:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:19:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:19:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:19:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:19:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:19:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:19:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:19:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:19:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:19:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:19:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:19:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:19:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:19:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:19:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3683744) [2026-01-28 08:19:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3683744) [2026-01-28 08:19:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3683744) [2026-01-28 08:19:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3683744) [2026-01-28 08:19:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3683744) [2026-01-28 08:19:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3683744) [2026-01-28 08:19:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3683744) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3683744) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.46s/it]
(EngineCore_DP0 pid=3683744) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.46s/it]
(EngineCore_DP0 pid=3683744) 
(EngineCore_DP0 pid=3683744) [2026-01-28 08:20:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3683744) [2026-01-28 08:20:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3683744) [2026-01-28 08:20:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3683744) [2026-01-28 08:20:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3683744) [2026-01-28 08:20:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3683744) [2026-01-28 08:20:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3683744) [2026-01-28 08:20:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3683744) [2026-01-28 08:20:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3683744) 2026-01-28 08:20:13,702 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3683744) 2026-01-28 08:20:14,191 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 62/4096 [00:00<00:06, 613.30it/s]
Adding requests:   3%|▎         | 124/4096 [00:00<00:06, 607.52it/s]
Adding requests:   5%|▍         | 185/4096 [00:00<00:07, 555.34it/s]
Adding requests:   6%|▌         | 241/4096 [00:00<00:07, 548.84it/s]
Adding requests:   7%|▋         | 297/4096 [00:00<00:07, 529.55it/s]
Adding requests:   9%|▊         | 351/4096 [00:00<00:07, 522.88it/s]
Adding requests:  10%|▉         | 405/4096 [00:00<00:06, 527.36it/s]
Adding requests:  11%|█         | 459/4096 [00:00<00:06, 528.80it/s]
Adding requests:  12%|█▎        | 512/4096 [00:00<00:06, 523.54it/s]
Adding requests:  14%|█▍        | 565/4096 [00:01<00:06, 516.50it/s]
Adding requests:  15%|█▌        | 619/4096 [00:01<00:06, 521.67it/s]
Adding requests:  17%|█▋        | 676/4096 [00:01<00:06, 535.92it/s]
Adding requests:  18%|█▊        | 731/4096 [00:01<00:06, 538.36it/s]
Adding requests:  19%|█▉        | 785/4096 [00:01<00:06, 535.60it/s]
Adding requests:  20%|██        | 839/4096 [00:01<00:06, 521.23it/s]
Adding requests:  22%|██▏       | 895/4096 [00:01<00:06, 529.76it/s]
Adding requests:  23%|██▎       | 949/4096 [00:01<00:05, 531.53it/s]
Adding requests:  24%|██▍       | 1003/4096 [00:01<00:05, 532.34it/s]
Adding requests:  26%|██▌       | 1057/4096 [00:01<00:05, 528.82it/s]
Adding requests:  27%|██▋       | 1110/4096 [00:02<00:05, 526.36it/s]
Adding requests:  28%|██▊       | 1165/4096 [00:02<00:05, 533.34it/s]
Adding requests:  30%|██▉       | 1219/4096 [00:02<00:05, 501.39it/s]
Adding requests:  31%|███       | 1271/4096 [00:02<00:05, 504.74it/s]
Adding requests:  32%|███▏      | 1322/4096 [00:02<00:05, 503.24it/s]
Adding requests:  34%|███▎      | 1376/4096 [00:02<00:05, 509.94it/s]
Adding requests:  35%|███▍      | 1428/4096 [00:02<00:05, 511.47it/s]
Adding requests:  36%|███▌      | 1482/4096 [00:02<00:05, 517.75it/s]
Adding requests:  37%|███▋      | 1535/4096 [00:02<00:04, 519.69it/s]
Adding requests:  39%|███▉      | 1588/4096 [00:03<00:04, 519.69it/s]
Adding requests:  40%|████      | 1641/4096 [00:03<00:04, 512.39it/s]
Adding requests:  41%|████▏     | 1693/4096 [00:03<00:04, 514.39it/s]
Adding requests:  43%|████▎     | 1747/4096 [00:03<00:04, 518.91it/s]
Adding requests:  44%|████▍     | 1802/4096 [00:03<00:04, 526.14it/s]
Adding requests:  45%|████▌     | 1856/4096 [00:03<00:04, 530.17it/s]
Adding requests:  47%|████▋     | 1910/4096 [00:03<00:04, 528.38it/s]
Adding requests:  48%|████▊     | 1964/4096 [00:03<00:04, 530.37it/s]
Adding requests:  49%|████▉     | 2020/4096 [00:03<00:03, 537.51it/s]
Adding requests:  51%|█████     | 2075/4096 [00:03<00:03, 540.63it/s]
Adding requests:  52%|█████▏    | 2130/4096 [00:04<00:03, 526.89it/s]
Adding requests:  53%|█████▎    | 2183/4096 [00:04<00:03, 518.10it/s]
Adding requests:  55%|█████▍    | 2236/4096 [00:04<00:03, 520.16it/s]
Adding requests:  56%|█████▌    | 2290/4096 [00:04<00:03, 524.58it/s]
Adding requests:  57%|█████▋    | 2343/4096 [00:04<00:03, 519.78it/s]
Adding requests:  59%|█████▊    | 2397/4096 [00:04<00:03, 525.64it/s]
Adding requests:  60%|█████▉    | 2450/4096 [00:04<00:03, 508.11it/s]
Adding requests:  61%|██████    | 2502/4096 [00:04<00:03, 507.59it/s]
Adding requests:  62%|██████▏   | 2559/4096 [00:04<00:02, 522.47it/s]
Adding requests:  64%|██████▍   | 2612/4096 [00:04<00:02, 521.63it/s]
Adding requests:  65%|██████▌   | 2667/4096 [00:05<00:02, 528.23it/s]
Adding requests:  66%|██████▋   | 2720/4096 [00:05<00:02, 524.57it/s]
Adding requests:  68%|██████▊   | 2773/4096 [00:05<00:02, 524.54it/s]
Adding requests:  69%|██████▉   | 2827/4096 [00:05<00:02, 528.22it/s]
Adding requests:  70%|███████   | 2881/4096 [00:05<00:02, 530.69it/s]
Adding requests:  72%|███████▏  | 2935/4096 [00:05<00:02, 525.98it/s]
Adding requests:  73%|███████▎  | 2991/4096 [00:05<00:02, 532.62it/s]
Adding requests:  74%|███████▍  | 3045/4096 [00:05<00:01, 532.32it/s]
Adding requests:  76%|███████▌  | 3099/4096 [00:05<00:01, 526.49it/s]
Adding requests:  77%|███████▋  | 3152/4096 [00:06<00:01, 519.96it/s]
Adding requests:  78%|███████▊  | 3206/4096 [00:06<00:01, 524.70it/s]
Adding requests:  80%|███████▉  | 3262/4096 [00:06<00:01, 533.18it/s]
Adding requests:  81%|████████  | 3318/4096 [00:06<00:01, 539.04it/s]
Adding requests:  82%|████████▏ | 3372/4096 [00:06<00:01, 535.16it/s]
Adding requests:  84%|████████▎ | 3427/4096 [00:06<00:01, 536.98it/s]
Adding requests:  85%|████████▍ | 3481/4096 [00:06<00:01, 524.19it/s]
Adding requests:  86%|████████▋ | 3534/4096 [00:06<00:01, 524.07it/s]
Adding requests:  88%|████████▊ | 3589/4096 [00:06<00:00, 531.39it/s]
Adding requests:  89%|████████▉ | 3643/4096 [00:06<00:00, 523.29it/s]
Adding requests:  90%|█████████ | 3697/4096 [00:07<00:00, 528.01it/s]
Adding requests:  92%|█████████▏| 3751/4096 [00:07<00:00, 530.63it/s]
Adding requests:  93%|█████████▎| 3805/4096 [00:07<00:00, 504.08it/s]
Adding requests:  94%|█████████▍| 3859/4096 [00:07<00:00, 512.94it/s]
Adding requests:  96%|█████████▌| 3914/4096 [00:07<00:00, 523.25it/s]
Adding requests:  97%|█████████▋| 3969/4096 [00:07<00:00, 529.69it/s]
Adding requests:  98%|█████████▊| 4023/4096 [00:07<00:00, 522.75it/s]
Adding requests: 100%|█████████▉| 4076/4096 [00:07<00:00, 516.05it/s]
Adding requests: 100%|██████████| 4096/4096 [00:07<00:00, 525.38it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 98/4096 [00:01<00:50, 79.82it/s, est. speed input: 81742.97 toks/s, output: 79.82 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:03<02:00, 32.88it/s, est. speed input: 38832.94 toks/s, output: 37.92 toks/s]
Processed prompts:   4%|▍         | 162/4096 [00:05<02:46, 23.58it/s, est. speed input: 29464.30 toks/s, output: 28.77 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:07<03:17, 19.77it/s, est. speed input: 25358.97 toks/s, output: 24.76 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:10<03:37, 17.80it/s, est. speed input: 23056.35 toks/s, output: 22.52 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:12<03:50, 16.68it/s, est. speed input: 21596.27 toks/s, output: 21.09 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:14<03:58, 15.97it/s, est. speed input: 20574.09 toks/s, output: 20.09 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:16<04:01, 15.61it/s, est. speed input: 19873.55 toks/s, output: 19.41 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:18<04:05, 15.27it/s, est. speed input: 19289.62 toks/s, output: 18.84 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:21<04:06, 15.03it/s, est. speed input: 18821.62 toks/s, output: 18.38 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:23<04:07, 14.87it/s, est. speed input: 18445.66 toks/s, output: 18.01 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:25<04:21, 13.94it/s, est. speed input: 17837.19 toks/s, output: 17.42 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:28<04:16, 14.11it/s, est. speed input: 17602.22 toks/s, output: 17.19 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:30<04:11, 14.22it/s, est. speed input: 17401.54 toks/s, output: 16.99 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:32<04:08, 14.30it/s, est. speed input: 17227.84 toks/s, output: 16.82 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:34<04:04, 14.37it/s, est. speed input: 17078.74 toks/s, output: 16.68 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:36<04:01, 14.42it/s, est. speed input: 16947.57 toks/s, output: 16.55 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:39<03:59, 14.45it/s, est. speed input: 16829.83 toks/s, output: 16.44 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:41<03:56, 14.47it/s, est. speed input: 16725.61 toks/s, output: 16.33 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:43<03:54, 14.49it/s, est. speed input: 16631.27 toks/s, output: 16.24 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:45<03:51, 14.50it/s, est. speed input: 16547.48 toks/s, output: 16.16 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:47<03:49, 14.51it/s, est. speed input: 16469.83 toks/s, output: 16.08 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:50<03:47, 14.49it/s, est. speed input: 16395.80 toks/s, output: 16.01 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:52<03:44, 14.50it/s, est. speed input: 16331.90 toks/s, output: 15.95 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:54<03:42, 14.50it/s, est. speed input: 16271.42 toks/s, output: 15.89 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:56<03:40, 14.49it/s, est. speed input: 16214.90 toks/s, output: 15.83 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:58<03:38, 14.47it/s, est. speed input: 16161.08 toks/s, output: 15.78 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [01:01<03:34, 14.60it/s, est. speed input: 16129.74 toks/s, output: 15.75 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [01:03<03:32, 14.57it/s, est. speed input: 16085.23 toks/s, output: 15.71 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [01:05<03:30, 14.56it/s, est. speed input: 16044.02 toks/s, output: 15.67 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [01:07<03:28, 14.54it/s, est. speed input: 16005.29 toks/s, output: 15.63 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [01:09<03:26, 14.54it/s, est. speed input: 15969.57 toks/s, output: 15.60 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [01:12<03:24, 14.54it/s, est. speed input: 15936.51 toks/s, output: 15.56 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [01:14<03:20, 14.65it/s, est. speed input: 15917.71 toks/s, output: 15.54 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [01:16<03:19, 14.59it/s, est. speed input: 15885.48 toks/s, output: 15.51 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [01:18<03:17, 14.55it/s, est. speed input: 15854.83 toks/s, output: 15.48 toks/s]
Processed prompts:  31%|███       | 1250/4096 [01:20<03:14, 14.66it/s, est. speed input: 15839.60 toks/s, output: 15.47 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [01:22<03:10, 14.74it/s, est. speed input: 15825.50 toks/s, output: 15.45 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [01:25<03:09, 14.67it/s, est. speed input: 15800.59 toks/s, output: 15.43 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [01:27<03:08, 14.62it/s, est. speed input: 15776.35 toks/s, output: 15.41 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [01:29<03:07, 14.50it/s, est. speed input: 15746.40 toks/s, output: 15.38 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [01:31<03:05, 14.50it/s, est. speed input: 15725.03 toks/s, output: 15.36 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [01:34<03:03, 14.49it/s, est. speed input: 15703.37 toks/s, output: 15.34 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [01:36<03:00, 14.49it/s, est. speed input: 15683.56 toks/s, output: 15.32 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [01:38<02:58, 14.50it/s, est. speed input: 15665.42 toks/s, output: 15.30 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [01:40<02:56, 14.50it/s, est. speed input: 15647.39 toks/s, output: 15.28 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [01:42<02:52, 14.62it/s, est. speed input: 15639.77 toks/s, output: 15.27 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [01:45<02:51, 14.58it/s, est. speed input: 15622.75 toks/s, output: 15.26 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [01:47<02:47, 14.68it/s, est. speed input: 15615.56 toks/s, output: 15.25 toks/s]
Processed prompts:  41%|████      | 1666/4096 [01:49<02:46, 14.62it/s, est. speed input: 15599.99 toks/s, output: 15.23 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [01:51<02:44, 14.58it/s, est. speed input: 15585.02 toks/s, output: 15.22 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [01:53<02:42, 14.57it/s, est. speed input: 15571.54 toks/s, output: 15.21 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [01:55<02:40, 14.55it/s, est. speed input: 15557.76 toks/s, output: 15.19 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [01:58<02:38, 14.54it/s, est. speed input: 15544.57 toks/s, output: 15.18 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [02:00<02:36, 14.52it/s, est. speed input: 15531.53 toks/s, output: 15.17 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [02:02<02:34, 14.45it/s, est. speed input: 15515.19 toks/s, output: 15.15 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [02:04<02:32, 14.46it/s, est. speed input: 15503.21 toks/s, output: 15.14 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [02:07<02:30, 14.48it/s, est. speed input: 15492.36 toks/s, output: 15.13 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [02:09<02:26, 14.61it/s, est. speed input: 15488.42 toks/s, output: 15.13 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [02:11<02:23, 14.69it/s, est. speed input: 15484.71 toks/s, output: 15.12 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [02:13<02:21, 14.65it/s, est. speed input: 15474.90 toks/s, output: 15.11 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [02:15<02:20, 14.60it/s, est. speed input: 15464.64 toks/s, output: 15.10 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [02:17<02:18, 14.57it/s, est. speed input: 15454.60 toks/s, output: 15.09 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [02:20<02:16, 14.55it/s, est. speed input: 15445.15 toks/s, output: 15.08 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [02:22<02:14, 14.54it/s, est. speed input: 15436.09 toks/s, output: 15.07 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [02:24<02:12, 14.52it/s, est. speed input: 15427.01 toks/s, output: 15.07 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [02:26<02:07, 14.77it/s, est. speed input: 15431.32 toks/s, output: 15.07 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [02:28<02:06, 14.68it/s, est. speed input: 15422.56 toks/s, output: 15.06 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [02:31<02:03, 14.75it/s, est. speed input: 15420.55 toks/s, output: 15.06 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [02:33<02:01, 14.67it/s, est. speed input: 15412.08 toks/s, output: 15.05 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [02:35<02:00, 14.65it/s, est. speed input: 15405.33 toks/s, output: 15.04 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [02:37<01:56, 14.87it/s, est. speed input: 15410.48 toks/s, output: 15.05 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [02:39<01:53, 14.90it/s, est. speed input: 15409.39 toks/s, output: 15.05 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [02:41<01:52, 14.77it/s, est. speed input: 15401.54 toks/s, output: 15.04 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [02:44<01:51, 14.67it/s, est. speed input: 15393.00 toks/s, output: 15.03 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [02:46<01:48, 14.74it/s, est. speed input: 15391.43 toks/s, output: 15.03 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [02:48<01:46, 14.66it/s, est. speed input: 15384.19 toks/s, output: 15.02 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [02:50<01:44, 14.74it/s, est. speed input: 15382.75 toks/s, output: 15.02 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [02:52<01:42, 14.67it/s, est. speed input: 15376.13 toks/s, output: 15.02 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [02:54<01:40, 14.63it/s, est. speed input: 15369.84 toks/s, output: 15.01 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [02:57<01:38, 14.58it/s, est. speed input: 15363.00 toks/s, output: 15.00 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [02:59<01:35, 14.68it/s, est. speed input: 15362.04 toks/s, output: 15.00 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [03:01<01:33, 14.62it/s, est. speed input: 15355.67 toks/s, output: 15.00 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [03:03<01:31, 14.59it/s, est. speed input: 15349.67 toks/s, output: 14.99 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [03:05<01:29, 14.56it/s, est. speed input: 15343.75 toks/s, output: 14.98 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [03:08<01:28, 14.37it/s, est. speed input: 15330.80 toks/s, output: 14.97 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [03:10<01:25, 14.53it/s, est. speed input: 15330.18 toks/s, output: 14.97 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [03:12<01:23, 14.53it/s, est. speed input: 15325.10 toks/s, output: 14.97 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [03:14<01:21, 14.53it/s, est. speed input: 15319.81 toks/s, output: 14.96 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [03:16<01:19, 14.52it/s, est. speed input: 15314.39 toks/s, output: 14.96 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [03:19<01:17, 14.51it/s, est. speed input: 15309.04 toks/s, output: 14.95 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [03:21<01:14, 14.50it/s, est. speed input: 15303.71 toks/s, output: 14.95 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [03:23<01:12, 14.51it/s, est. speed input: 15299.24 toks/s, output: 14.94 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [03:25<01:10, 14.51it/s, est. speed input: 15294.44 toks/s, output: 14.94 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [03:27<01:06, 14.78it/s, est. speed input: 15299.78 toks/s, output: 14.94 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [03:30<01:04, 14.82it/s, est. speed input: 15299.30 toks/s, output: 14.94 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [03:32<01:02, 14.73it/s, est. speed input: 15294.91 toks/s, output: 14.94 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [03:34<01:00, 14.66it/s, est. speed input: 15290.38 toks/s, output: 14.93 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [03:36<00:58, 14.73it/s, est. speed input: 15290.11 toks/s, output: 14.93 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [03:38<00:57, 14.55it/s, est. speed input: 15281.59 toks/s, output: 14.92 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [03:41<00:54, 14.53it/s, est. speed input: 15277.33 toks/s, output: 14.92 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [03:43<00:52, 14.53it/s, est. speed input: 15273.13 toks/s, output: 14.92 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [03:45<00:50, 14.51it/s, est. speed input: 15268.83 toks/s, output: 14.91 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [03:47<00:48, 14.51it/s, est. speed input: 15264.72 toks/s, output: 14.91 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [03:49<00:45, 14.64it/s, est. speed input: 15265.09 toks/s, output: 14.91 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [03:52<00:43, 14.59it/s, est. speed input: 15260.88 toks/s, output: 14.90 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [03:54<00:40, 14.83it/s, est. speed input: 15265.69 toks/s, output: 14.91 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [03:56<00:38, 14.72it/s, est. speed input: 15261.55 toks/s, output: 14.90 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [03:58<00:36, 14.66it/s, est. speed input: 15257.84 toks/s, output: 14.90 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [04:00<00:34, 14.60it/s, est. speed input: 15253.66 toks/s, output: 14.90 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [04:02<00:32, 14.57it/s, est. speed input: 15250.04 toks/s, output: 14.89 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [04:05<00:30, 14.56it/s, est. speed input: 15246.76 toks/s, output: 14.89 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [04:07<00:28, 14.54it/s, est. speed input: 15243.27 toks/s, output: 14.89 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [04:09<00:26, 14.66it/s, est. speed input: 15243.59 toks/s, output: 14.89 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [04:11<00:24, 14.43it/s, est. speed input: 15234.56 toks/s, output: 14.88 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [04:14<00:22, 14.44it/s, est. speed input: 15230.80 toks/s, output: 14.87 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [04:16<00:19, 14.45it/s, est. speed input: 15227.38 toks/s, output: 14.87 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [04:18<00:17, 14.58it/s, est. speed input: 15227.62 toks/s, output: 14.87 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [04:20<00:15, 14.56it/s, est. speed input: 15224.44 toks/s, output: 14.87 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [04:22<00:13, 14.53it/s, est. speed input: 15221.08 toks/s, output: 14.86 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [04:24<00:10, 14.53it/s, est. speed input: 15218.04 toks/s, output: 14.86 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [04:27<00:08, 14.51it/s, est. speed input: 15214.88 toks/s, output: 14.86 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [04:29<00:06, 14.52it/s, est. speed input: 15212.03 toks/s, output: 14.86 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [04:31<00:04, 14.90it/s, est. speed input: 15219.77 toks/s, output: 14.86 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [04:33<00:02, 14.91it/s, est. speed input: 15220.38 toks/s, output: 14.86 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:33<00:00, 14.91it/s, est. speed input: 15332.66 toks/s, output: 14.97 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:33<00:00, 14.97it/s, est. speed input: 15332.66 toks/s, output: 14.97 toks/s]
[rank0]:[W128 08:24:59.057833596 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 331.8s

测试结果:
  Requests/s:   14.56
  Tokens/s:     14922.22
  Total Reqs:   4096
  Elapsed:      281.35s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     14907.66


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,27.4292,14071.1543,4.6666
1024,1024,1,128,128,14.8949,15267.2722,8.5935
2048,1024,2,256,128,15.5382,15926.6226,16.4756
4096,1024,4,512,128,14.6062,14971.3904,35.0535
8192,1024,8,1024,128,14.3775,14736.9062,71.2225
16384,1024,16,2048,128,14.4819,14843.9675,141.4177
32768,1024,32,4096,128,14.5583,14922.2230,281.3522

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:25:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:25:05 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3688337) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3688337) WARNING 01-28 08:25:35 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 25.79 requests/s, 13231.89 total tokens/s, 25.79 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:25:05] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:25:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:25:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:25:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:25:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:25:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:25:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:25:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:25:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:25:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:25:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:25:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:25:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:25:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:25:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:25:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:25:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:25:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:09] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3688337) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3688337) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:18<00:00, 18.35s/it]
(EngineCore_DP0 pid=3688337) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:18<00:00, 18.35s/it]
(EngineCore_DP0 pid=3688337) 
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3688337) [2026-01-28 08:25:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3688337) 2026-01-28 08:25:35,113 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3688337) 2026-01-28 08:25:35,128 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  99%|█████████▉| 127/128 [00:00<00:00, 1265.75it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1264.14it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:28,  4.44it/s, est. speed input: 2275.20 toks/s, output: 4.44 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:08, 13.83it/s, est. speed input: 6112.41 toks/s, output: 11.94 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:06, 18.83it/s, est. speed input: 8060.18 toks/s, output: 15.74 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:05, 22.05it/s, est. speed input: 9304.53 toks/s, output: 18.17 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 23.85it/s, est. speed input: 10091.89 toks/s, output: 19.71 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:04, 24.91it/s, est. speed input: 10636.36 toks/s, output: 20.77 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:04, 25.69it/s, est. speed input: 11058.49 toks/s, output: 21.60 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:04, 26.40it/s, est. speed input: 11414.14 toks/s, output: 22.29 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:03, 26.90it/s, est. speed input: 11702.17 toks/s, output: 22.86 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:03, 26.64it/s, est. speed input: 11859.66 toks/s, output: 23.16 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:03, 27.01it/s, est. speed input: 12057.55 toks/s, output: 23.55 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:03, 27.08it/s, est. speed input: 12202.59 toks/s, output: 23.83 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:03, 27.25it/s, est. speed input: 12340.76 toks/s, output: 24.10 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:03, 27.45it/s, est. speed input: 12469.04 toks/s, output: 24.35 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:03, 27.30it/s, est. speed input: 12554.09 toks/s, output: 24.52 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:03, 27.29it/s, est. speed input: 12636.52 toks/s, output: 24.68 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 27.20it/s, est. speed input: 12703.33 toks/s, output: 24.81 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:02, 27.28it/s, est. speed input: 12774.24 toks/s, output: 24.95 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:02<00:02, 26.92it/s, est. speed input: 12805.59 toks/s, output: 25.01 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:02<00:02, 26.96it/s, est. speed input: 12856.09 toks/s, output: 25.11 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:02, 27.26it/s, est. speed input: 12921.18 toks/s, output: 25.24 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:02<00:02, 27.30it/s, est. speed input: 12969.26 toks/s, output: 25.33 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:02<00:02, 27.39it/s, est. speed input: 13016.88 toks/s, output: 25.42 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:02<00:02, 27.47it/s, est. speed input: 13062.12 toks/s, output: 25.51 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:02, 27.46it/s, est. speed input: 13099.76 toks/s, output: 25.59 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 27.52it/s, est. speed input: 13138.75 toks/s, output: 25.66 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:03<00:01, 27.65it/s, est. speed input: 13179.99 toks/s, output: 25.74 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:03<00:01, 27.55it/s, est. speed input: 13208.06 toks/s, output: 25.80 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:03<00:01, 27.06it/s, est. speed input: 13211.17 toks/s, output: 25.80 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:03<00:01, 27.21it/s, est. speed input: 13239.99 toks/s, output: 25.86 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:03<00:01, 27.30it/s, est. speed input: 13266.53 toks/s, output: 25.91 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:03<00:01, 27.35it/s, est. speed input: 13290.20 toks/s, output: 25.96 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:01, 27.36it/s, est. speed input: 13311.86 toks/s, output: 26.00 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:03<00:01, 27.47it/s, est. speed input: 13336.76 toks/s, output: 26.05 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:03<00:00, 27.35it/s, est. speed input: 13351.44 toks/s, output: 26.08 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:04<00:00, 27.50it/s, est. speed input: 13375.85 toks/s, output: 26.12 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:04<00:00, 27.47it/s, est. speed input: 13392.86 toks/s, output: 26.16 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:04<00:00, 27.24it/s, est. speed input: 13400.24 toks/s, output: 26.17 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:04<00:00, 27.29it/s, est. speed input: 13416.04 toks/s, output: 26.20 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:04<00:00, 27.30it/s, est. speed input: 13429.96 toks/s, output: 26.23 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:04<00:00, 27.46it/s, est. speed input: 13449.19 toks/s, output: 26.27 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:04<00:00, 27.36it/s, est. speed input: 13459.65 toks/s, output: 26.29 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:04<00:00, 27.47it/s, est. speed input: 13476.12 toks/s, output: 26.32 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 27.47it/s, est. speed input: 13483.70 toks/s, output: 26.34 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 26.33it/s, est. speed input: 13483.70 toks/s, output: 26.34 toks/s]
[rank0]:[W128 08:25:41.036450551 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.9s

测试结果:
  Requests/s:   25.79
  Tokens/s:     13231.89
  Total Reqs:   128
  Elapsed:      4.96s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     13206.10

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:25:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:25:47 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3689102) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3689102) WARNING 01-28 08:26:16 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.40 requests/s, 14756.65 total tokens/s, 14.40 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:25:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:25:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:25:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:25:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:25:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:25:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:25:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:25:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:25:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:25:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:25:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:25:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:25:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:25:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:25:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:25:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:25:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:25:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3689102) [2026-01-28 08:25:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3689102) [2026-01-28 08:25:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3689102) [2026-01-28 08:25:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3689102) [2026-01-28 08:25:51] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3689102) [2026-01-28 08:25:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3689102) [2026-01-28 08:25:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3689102) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3689102) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.32s/it]
(EngineCore_DP0 pid=3689102) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.32s/it]
(EngineCore_DP0 pid=3689102) 
(EngineCore_DP0 pid=3689102) [2026-01-28 08:26:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3689102) [2026-01-28 08:26:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3689102) [2026-01-28 08:26:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3689102) [2026-01-28 08:26:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3689102) [2026-01-28 08:26:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3689102) [2026-01-28 08:26:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3689102) [2026-01-28 08:26:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3689102) [2026-01-28 08:26:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3689102) 2026-01-28 08:26:15,690 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3689102) 2026-01-28 08:26:15,701 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  55%|█████▍    | 70/128 [00:00<00:00, 697.19it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 705.69it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:22,  5.56it/s, est. speed input: 5691.47 toks/s, output: 5.56 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:12, 10.30it/s, est. speed input: 9720.10 toks/s, output: 9.49 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:11, 10.73it/s, est. speed input: 10340.01 toks/s, output: 10.10 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:10, 12.04it/s, est. speed input: 11314.30 toks/s, output: 11.05 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:09, 13.02it/s, est. speed input: 12023.04 toks/s, output: 11.74 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:08, 13.71it/s, est. speed input: 12542.71 toks/s, output: 12.25 toks/s]
Processed prompts:  10%|█         | 13/128 [00:01<00:08, 13.91it/s, est. speed input: 12827.43 toks/s, output: 12.53 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 14.26it/s, est. speed input: 13121.12 toks/s, output: 12.81 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:07, 14.48it/s, est. speed input: 13350.57 toks/s, output: 13.04 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:07, 14.71it/s, est. speed input: 13555.33 toks/s, output: 13.24 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:07, 14.66it/s, est. speed input: 13673.29 toks/s, output: 13.35 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:07, 14.83it/s, est. speed input: 13820.87 toks/s, output: 13.50 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 14.90it/s, est. speed input: 13938.29 toks/s, output: 13.61 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 14.96it/s, est. speed input: 14040.31 toks/s, output: 13.71 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:02<00:06, 15.04it/s, est. speed input: 14136.52 toks/s, output: 13.81 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:06, 15.10it/s, est. speed input: 14223.98 toks/s, output: 13.89 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:06, 15.07it/s, est. speed input: 14287.12 toks/s, output: 13.95 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:06, 15.12it/s, est. speed input: 14355.89 toks/s, output: 14.02 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:06, 14.78it/s, est. speed input: 14358.28 toks/s, output: 14.02 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 14.92it/s, est. speed input: 14417.57 toks/s, output: 14.08 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 14.94it/s, est. speed input: 14460.28 toks/s, output: 14.12 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:03<00:05, 15.09it/s, est. speed input: 14518.20 toks/s, output: 14.18 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:03<00:05, 15.12it/s, est. speed input: 14561.88 toks/s, output: 14.22 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:05, 15.09it/s, est. speed input: 14594.29 toks/s, output: 14.25 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:05, 15.14it/s, est. speed input: 14634.48 toks/s, output: 14.29 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:05, 15.25it/s, est. speed input: 14679.43 toks/s, output: 14.34 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:05, 14.89it/s, est. speed input: 14670.36 toks/s, output: 14.33 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 14.96it/s, est. speed input: 14698.50 toks/s, output: 14.35 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 14.98it/s, est. speed input: 14721.45 toks/s, output: 14.38 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:04<00:04, 15.02it/s, est. speed input: 14746.57 toks/s, output: 14.40 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:04<00:04, 15.00it/s, est. speed input: 14763.70 toks/s, output: 14.42 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:04<00:04, 15.05it/s, est. speed input: 14787.67 toks/s, output: 14.44 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:04, 15.11it/s, est. speed input: 14812.01 toks/s, output: 14.46 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:04, 15.09it/s, est. speed input: 14828.69 toks/s, output: 14.48 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 14.85it/s, est. speed input: 14824.36 toks/s, output: 14.48 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 14.89it/s, est. speed input: 14837.84 toks/s, output: 14.49 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:05<00:03, 15.01it/s, est. speed input: 14859.42 toks/s, output: 14.51 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:05<00:03, 15.05it/s, est. speed input: 14876.03 toks/s, output: 14.53 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:05<00:03, 14.96it/s, est. speed input: 14882.33 toks/s, output: 14.53 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:05<00:03, 14.99it/s, est. speed input: 14895.32 toks/s, output: 14.55 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:05<00:03, 14.99it/s, est. speed input: 14906.49 toks/s, output: 14.56 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 15.07it/s, est. speed input: 14922.77 toks/s, output: 14.57 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 14.71it/s, est. speed input: 14906.36 toks/s, output: 14.56 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 14.92it/s, est. speed input: 14926.33 toks/s, output: 14.58 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:06<00:02, 14.91it/s, est. speed input: 14933.06 toks/s, output: 14.58 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:06<00:02, 14.92it/s, est. speed input: 14941.29 toks/s, output: 14.59 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:06<00:02, 14.92it/s, est. speed input: 14948.35 toks/s, output: 14.60 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:06<00:02, 14.97it/s, est. speed input: 14958.58 toks/s, output: 14.61 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:06<00:02, 15.06it/s, est. speed input: 14971.68 toks/s, output: 14.62 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 14.86it/s, est. speed input: 14967.54 toks/s, output: 14.62 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 14.77it/s, est. speed input: 14966.45 toks/s, output: 14.62 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:07<00:01, 14.92it/s, est. speed input: 14978.81 toks/s, output: 14.63 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:07<00:01, 15.02it/s, est. speed input: 14990.96 toks/s, output: 14.64 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:07<00:01, 14.94it/s, est. speed input: 14993.15 toks/s, output: 14.64 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:07<00:01, 14.96it/s, est. speed input: 14999.93 toks/s, output: 14.65 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:07<00:01, 14.99it/s, est. speed input: 15007.55 toks/s, output: 14.66 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:07<00:01, 14.99it/s, est. speed input: 15013.44 toks/s, output: 14.66 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 14.82it/s, est. speed input: 15009.21 toks/s, output: 14.66 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 14.85it/s, est. speed input: 15013.73 toks/s, output: 14.66 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:08<00:00, 14.95it/s, est. speed input: 15022.44 toks/s, output: 14.67 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:08<00:00, 14.94it/s, est. speed input: 15026.62 toks/s, output: 14.67 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:08<00:00, 15.00it/s, est. speed input: 15034.06 toks/s, output: 14.68 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:08<00:00, 14.97it/s, est. speed input: 15037.52 toks/s, output: 14.69 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:08<00:00, 15.04it/s, est. speed input: 15045.67 toks/s, output: 14.69 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 15.04it/s, est. speed input: 15050.82 toks/s, output: 14.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.70it/s, est. speed input: 15050.82 toks/s, output: 14.70 toks/s]
[rank0]:[W128 08:26:25.350109952 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.3s

测试结果:
  Requests/s:   14.40
  Tokens/s:     14756.65
  Total Reqs:   128
  Elapsed:      8.89s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     14742.25

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:26:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:26:31 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3689997) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3689997) WARNING 01-28 08:27:00 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.14 requests/s, 15522.60 total tokens/s, 15.14 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 08:26:31] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:26:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:26:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:26:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:26:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:26:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:26:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:26:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:26:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:26:35] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:26:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:26:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:26:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:26:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:26:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:26:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:26:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:26:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:36] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:36] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:36] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3689997) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3689997) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.36s/it]
(EngineCore_DP0 pid=3689997) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.36s/it]
(EngineCore_DP0 pid=3689997) 
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3689997) [2026-01-28 08:26:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3689997) 2026-01-28 08:27:00,076 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3689997) 2026-01-28 08:27:00,088 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  24%|██▍       | 61/256 [00:00<00:00, 606.41it/s]
Adding requests:  48%|████▊     | 122/256 [00:00<00:00, 591.40it/s]
Adding requests:  71%|███████   | 182/256 [00:00<00:00, 578.21it/s]
Adding requests:  94%|█████████▍| 240/256 [00:00<00:00, 574.27it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 576.82it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 6/256 [00:00<00:05, 47.15it/s, est. speed input: 48289.35 toks/s, output: 47.15 toks/s]
Processed prompts:   4%|▍         | 11/256 [00:00<00:09, 26.46it/s, est. speed input: 29189.56 toks/s, output: 28.51 toks/s]
Processed prompts:   6%|▌         | 15/256 [00:00<00:11, 20.67it/s, est. speed input: 23719.26 toks/s, output: 23.16 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:14, 16.67it/s, est. speed input: 20199.73 toks/s, output: 19.73 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:01<00:14, 16.39it/s, est. speed input: 19659.57 toks/s, output: 19.20 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:01<00:14, 16.18it/s, est. speed input: 19244.87 toks/s, output: 18.79 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:01<00:14, 16.00it/s, est. speed input: 18911.84 toks/s, output: 18.47 toks/s]
Processed prompts:  10%|█         | 26/256 [00:01<00:14, 15.76it/s, est. speed input: 18593.87 toks/s, output: 18.16 toks/s]
Processed prompts:  11%|█         | 28/256 [00:01<00:14, 15.70it/s, est. speed input: 18374.84 toks/s, output: 17.94 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:01<00:14, 15.65it/s, est. speed input: 18184.16 toks/s, output: 17.76 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:01<00:14, 15.62it/s, est. speed input: 18025.11 toks/s, output: 17.60 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:01<00:14, 15.29it/s, est. speed input: 17803.85 toks/s, output: 17.39 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:02<00:14, 15.36it/s, est. speed input: 17686.14 toks/s, output: 17.27 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:02<00:14, 15.27it/s, est. speed input: 17550.67 toks/s, output: 17.14 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:02<00:14, 15.33it/s, est. speed input: 17456.25 toks/s, output: 17.05 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:02<00:14, 15.28it/s, est. speed input: 17353.48 toks/s, output: 16.95 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:02<00:13, 15.36it/s, est. speed input: 17283.10 toks/s, output: 16.88 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:02<00:13, 15.30it/s, est. speed input: 17199.08 toks/s, output: 16.80 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:02<00:13, 15.34it/s, est. speed input: 17134.98 toks/s, output: 16.73 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:03<00:13, 15.07it/s, est. speed input: 17029.19 toks/s, output: 16.63 toks/s]
Processed prompts:  20%|██        | 52/256 [00:03<00:13, 15.17it/s, est. speed input: 16977.09 toks/s, output: 16.58 toks/s]
Processed prompts:  21%|██        | 54/256 [00:03<00:13, 15.32it/s, est. speed input: 16940.83 toks/s, output: 16.54 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:03<00:13, 15.34it/s, est. speed input: 16895.34 toks/s, output: 16.50 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:03<00:12, 15.33it/s, est. speed input: 16850.45 toks/s, output: 16.46 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:03<00:12, 15.32it/s, est. speed input: 16808.23 toks/s, output: 16.41 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:03<00:12, 15.29it/s, est. speed input: 16765.75 toks/s, output: 16.37 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:03<00:12, 15.25it/s, est. speed input: 16724.13 toks/s, output: 16.33 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:04<00:12, 15.07it/s, est. speed input: 16666.92 toks/s, output: 16.28 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:04<00:12, 15.14it/s, est. speed input: 16635.41 toks/s, output: 16.25 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:04<00:12, 15.08it/s, est. speed input: 16594.27 toks/s, output: 16.21 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:04<00:12, 15.19it/s, est. speed input: 16571.59 toks/s, output: 16.18 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:04<00:11, 15.28it/s, est. speed input: 16551.67 toks/s, output: 16.16 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:04<00:11, 15.23it/s, est. speed input: 16522.00 toks/s, output: 16.13 toks/s]
Processed prompts:  30%|███       | 78/256 [00:04<00:11, 15.33it/s, est. speed input: 16506.02 toks/s, output: 16.12 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:04<00:11, 15.38it/s, est. speed input: 16489.34 toks/s, output: 16.10 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:05<00:11, 15.06it/s, est. speed input: 16441.54 toks/s, output: 16.06 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:05<00:11, 15.11it/s, est. speed input: 16420.04 toks/s, output: 16.04 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:05<00:11, 15.17it/s, est. speed input: 16402.18 toks/s, output: 16.02 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:05<00:11, 15.11it/s, est. speed input: 16376.37 toks/s, output: 15.99 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:05<00:10, 15.25it/s, est. speed input: 16366.96 toks/s, output: 15.98 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:05<00:10, 15.35it/s, est. speed input: 16357.76 toks/s, output: 15.97 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:05<00:10, 15.37it/s, est. speed input: 16345.50 toks/s, output: 15.96 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:06<00:10, 15.39it/s, est. speed input: 16333.99 toks/s, output: 15.95 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:06<00:10, 15.17it/s, est. speed input: 16305.21 toks/s, output: 15.92 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:06<00:10, 15.25it/s, est. speed input: 16294.63 toks/s, output: 15.91 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:06<00:10, 15.20it/s, est. speed input: 16277.03 toks/s, output: 15.90 toks/s]
Processed prompts:  41%|████      | 104/256 [00:06<00:09, 15.25it/s, est. speed input: 16266.49 toks/s, output: 15.89 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:06<00:09, 15.29it/s, est. speed input: 16256.12 toks/s, output: 15.88 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:06<00:09, 15.31it/s, est. speed input: 16246.08 toks/s, output: 15.87 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:06<00:09, 15.31it/s, est. speed input: 16235.34 toks/s, output: 15.85 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:07<00:09, 15.26it/s, est. speed input: 16221.95 toks/s, output: 15.84 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:07<00:09, 15.06it/s, est. speed input: 16197.90 toks/s, output: 15.82 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:07<00:09, 15.22it/s, est. speed input: 16194.18 toks/s, output: 15.81 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:07<00:09, 15.29it/s, est. speed input: 16187.99 toks/s, output: 15.81 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:07<00:08, 15.22it/s, est. speed input: 16174.59 toks/s, output: 15.80 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:07<00:08, 15.31it/s, est. speed input: 16169.91 toks/s, output: 15.79 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:07<00:08, 15.37it/s, est. speed input: 16165.29 toks/s, output: 15.79 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:07<00:08, 15.33it/s, est. speed input: 16155.94 toks/s, output: 15.78 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:08<00:08, 15.33it/s, est. speed input: 16148.35 toks/s, output: 15.77 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:08<00:08, 15.15it/s, est. speed input: 16131.41 toks/s, output: 15.75 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:08<00:08, 15.17it/s, est. speed input: 16122.46 toks/s, output: 15.74 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:08<00:08, 15.07it/s, est. speed input: 16108.08 toks/s, output: 15.73 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:08<00:07, 15.20it/s, est. speed input: 16104.49 toks/s, output: 15.73 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:08<00:07, 15.25it/s, est. speed input: 16099.03 toks/s, output: 15.72 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:08<00:07, 15.21it/s, est. speed input: 16089.88 toks/s, output: 15.71 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:09<00:07, 15.21it/s, est. speed input: 16082.57 toks/s, output: 15.71 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:09<00:07, 15.27it/s, est. speed input: 16077.99 toks/s, output: 15.70 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:09<00:07, 15.08it/s, est. speed input: 16062.61 toks/s, output: 15.69 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:09<00:07, 15.17it/s, est. speed input: 16058.20 toks/s, output: 15.68 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:09<00:06, 15.27it/s, est. speed input: 16055.90 toks/s, output: 15.68 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:09<00:06, 15.29it/s, est. speed input: 16051.06 toks/s, output: 15.67 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:09<00:06, 15.33it/s, est. speed input: 16047.83 toks/s, output: 15.67 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:09<00:06, 15.33it/s, est. speed input: 16043.15 toks/s, output: 15.67 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:10<00:06, 15.31it/s, est. speed input: 16037.94 toks/s, output: 15.66 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:10<00:06, 15.35it/s, est. speed input: 16034.85 toks/s, output: 15.66 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:10<00:06, 15.15it/s, est. speed input: 16022.12 toks/s, output: 15.65 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:10<00:06, 15.11it/s, est. speed input: 16014.09 toks/s, output: 15.64 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:10<00:05, 15.21it/s, est. speed input: 16011.59 toks/s, output: 15.64 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:10<00:05, 15.32it/s, est. speed input: 16010.87 toks/s, output: 15.64 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:10<00:05, 15.34it/s, est. speed input: 16007.99 toks/s, output: 15.63 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:11<00:05, 15.28it/s, est. speed input: 16001.84 toks/s, output: 15.63 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:11<00:05, 15.35it/s, est. speed input: 16000.53 toks/s, output: 15.63 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:11<00:05, 15.41it/s, est. speed input: 15999.62 toks/s, output: 15.62 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:11<00:05, 15.13it/s, est. speed input: 15985.94 toks/s, output: 15.61 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:11<00:04, 15.23it/s, est. speed input: 15984.32 toks/s, output: 15.61 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:11<00:04, 15.30it/s, est. speed input: 15982.78 toks/s, output: 15.61 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:11<00:04, 15.30it/s, est. speed input: 15979.19 toks/s, output: 15.60 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:11<00:04, 15.21it/s, est. speed input: 15972.44 toks/s, output: 15.60 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:12<00:04, 15.28it/s, est. speed input: 15970.53 toks/s, output: 15.60 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:12<00:04, 15.35it/s, est. speed input: 15969.80 toks/s, output: 15.60 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:12<00:04, 15.28it/s, est. speed input: 15964.62 toks/s, output: 15.59 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:12<00:04, 15.10it/s, est. speed input: 15954.37 toks/s, output: 15.58 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:12<00:03, 15.16it/s, est. speed input: 15951.60 toks/s, output: 15.58 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:12<00:03, 15.26it/s, est. speed input: 15950.69 toks/s, output: 15.58 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:12<00:03, 15.32it/s, est. speed input: 15949.42 toks/s, output: 15.58 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:12<00:03, 15.28it/s, est. speed input: 15945.40 toks/s, output: 15.57 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:13<00:03, 15.25it/s, est. speed input: 15941.44 toks/s, output: 15.57 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:13<00:03, 15.26it/s, est. speed input: 15938.61 toks/s, output: 15.57 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:13<00:03, 15.30it/s, est. speed input: 15936.97 toks/s, output: 15.56 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:13<00:03, 15.07it/s, est. speed input: 15926.32 toks/s, output: 15.55 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:13<00:02, 15.16it/s, est. speed input: 15924.58 toks/s, output: 15.55 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:13<00:02, 15.20it/s, est. speed input: 15922.24 toks/s, output: 15.55 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:13<00:02, 15.20it/s, est. speed input: 15918.74 toks/s, output: 15.55 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:14<00:02, 15.23it/s, est. speed input: 15916.58 toks/s, output: 15.54 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:14<00:02, 15.32it/s, est. speed input: 15916.30 toks/s, output: 15.54 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:14<00:02, 15.29it/s, est. speed input: 15913.23 toks/s, output: 15.54 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:14<00:02, 15.33it/s, est. speed input: 15912.25 toks/s, output: 15.54 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:14<00:01, 15.08it/s, est. speed input: 15902.50 toks/s, output: 15.53 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:14<00:01, 15.04it/s, est. speed input: 15897.16 toks/s, output: 15.52 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:14<00:01, 15.12it/s, est. speed input: 15895.09 toks/s, output: 15.52 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:14<00:01, 15.23it/s, est. speed input: 15894.87 toks/s, output: 15.52 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:15<00:01, 15.15it/s, est. speed input: 15889.84 toks/s, output: 15.52 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:15<00:01, 15.19it/s, est. speed input: 15887.81 toks/s, output: 15.52 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:15<00:01, 15.16it/s, est. speed input: 15884.10 toks/s, output: 15.51 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:15<00:01, 15.15it/s, est. speed input: 15880.58 toks/s, output: 15.51 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:15<00:00, 15.02it/s, est. speed input: 15873.74 toks/s, output: 15.50 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:15<00:00, 15.08it/s, est. speed input: 15871.18 toks/s, output: 15.50 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:15<00:00, 15.17it/s, est. speed input: 15870.35 toks/s, output: 15.50 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:16<00:00, 15.16it/s, est. speed input: 15867.22 toks/s, output: 15.50 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:16<00:00, 15.17it/s, est. speed input: 15864.76 toks/s, output: 15.49 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:16<00:00, 15.27it/s, est. speed input: 15864.97 toks/s, output: 15.49 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:16<00:00, 15.35it/s, est. speed input: 15865.18 toks/s, output: 15.49 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 15.35it/s, est. speed input: 15926.47 toks/s, output: 15.55 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 15.55it/s, est. speed input: 15926.47 toks/s, output: 15.55 toks/s]
[rank0]:[W128 08:27:17.948652062 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.5s

测试结果:
  Requests/s:   15.14
  Tokens/s:     15522.60
  Total Reqs:   256
  Elapsed:      16.90s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     15507.45

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:27:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:27:25 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3690936) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3690936) WARNING 01-28 08:27:54 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.16 requests/s, 14510.94 total tokens/s, 14.16 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 08:27:24] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:27:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:27:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:27:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:27:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:27:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:27:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:27:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:27:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:27:28] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:27:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:27:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:27:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:27:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:27:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:27:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:27:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:27:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:29] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3690936) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3690936) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.38s/it]
(EngineCore_DP0 pid=3690936) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.38s/it]
(EngineCore_DP0 pid=3690936) 
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3690936) [2026-01-28 08:27:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3690936) 2026-01-28 08:27:53,360 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3690936) 2026-01-28 08:27:53,371 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  12%|█▏        | 61/512 [00:00<00:00, 599.77it/s]
Adding requests:  24%|██▎       | 121/512 [00:00<00:00, 572.23it/s]
Adding requests:  35%|███▍      | 179/512 [00:00<00:00, 555.65it/s]
Adding requests:  46%|████▋     | 238/512 [00:00<00:00, 564.97it/s]
Adding requests:  58%|█████▊    | 295/512 [00:00<00:00, 552.74it/s]
Adding requests:  69%|██████▉   | 352/512 [00:00<00:00, 558.10it/s]
Adding requests:  80%|███████▉  | 408/512 [00:00<00:00, 556.72it/s]
Adding requests:  91%|█████████ | 464/512 [00:00<00:00, 553.62it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 555.43it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 14/512 [00:00<00:13, 37.56it/s, est. speed input: 38465.82 toks/s, output: 37.56 toks/s]
Processed prompts:   4%|▎         | 18/512 [00:00<00:19, 25.50it/s, est. speed input: 28226.31 toks/s, output: 27.56 toks/s]
Processed prompts:   4%|▍         | 22/512 [00:00<00:23, 20.56it/s, est. speed input: 23999.28 toks/s, output: 23.44 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:01<00:26, 18.12it/s, est. speed input: 21789.28 toks/s, output: 21.28 toks/s]
Processed prompts:   6%|▌         | 30/512 [00:01<00:28, 16.72it/s, est. speed input: 20413.11 toks/s, output: 19.93 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:01<00:30, 15.84it/s, est. speed input: 19460.16 toks/s, output: 19.00 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:02<00:31, 15.19it/s, est. speed input: 18729.29 toks/s, output: 18.29 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:02<00:31, 14.92it/s, est. speed input: 18249.34 toks/s, output: 17.82 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:02<00:31, 14.71it/s, est. speed input: 17858.11 toks/s, output: 17.44 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:02<00:31, 14.48it/s, est. speed input: 17507.88 toks/s, output: 17.10 toks/s]
Processed prompts:  11%|█         | 54/512 [00:03<00:31, 14.34it/s, est. speed input: 17229.17 toks/s, output: 16.83 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:03<00:31, 14.32it/s, est. speed input: 17019.21 toks/s, output: 16.62 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:03<00:31, 14.24it/s, est. speed input: 16820.75 toks/s, output: 16.43 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:04<00:31, 14.19it/s, est. speed input: 16651.86 toks/s, output: 16.26 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:04<00:31, 14.23it/s, est. speed input: 16523.33 toks/s, output: 16.14 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:04<00:30, 14.27it/s, est. speed input: 16414.13 toks/s, output: 16.03 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:04<00:30, 14.23it/s, est. speed input: 16301.95 toks/s, output: 15.92 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:05<00:30, 14.12it/s, est. speed input: 16186.52 toks/s, output: 15.81 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:05<00:30, 14.18it/s, est. speed input: 16108.64 toks/s, output: 15.73 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:05<00:29, 14.20it/s, est. speed input: 16033.63 toks/s, output: 15.66 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:06<00:29, 14.23it/s, est. speed input: 15968.95 toks/s, output: 15.59 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:06<00:29, 14.13it/s, est. speed input: 15890.26 toks/s, output: 15.52 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:06<00:28, 14.15it/s, est. speed input: 15831.76 toks/s, output: 15.46 toks/s]
Processed prompts:  21%|██        | 106/512 [00:06<00:28, 14.17it/s, est. speed input: 15780.77 toks/s, output: 15.41 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:07<00:28, 14.12it/s, est. speed input: 15722.44 toks/s, output: 15.35 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:07<00:28, 14.21it/s, est. speed input: 15687.53 toks/s, output: 15.32 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:07<00:27, 14.28it/s, est. speed input: 15655.69 toks/s, output: 15.29 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:07<00:27, 14.31it/s, est. speed input: 15622.66 toks/s, output: 15.26 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:08<00:27, 14.19it/s, est. speed input: 15575.38 toks/s, output: 15.21 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:08<00:26, 14.19it/s, est. speed input: 15540.52 toks/s, output: 15.18 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:08<00:26, 14.22it/s, est. speed input: 15511.84 toks/s, output: 15.15 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:09<00:26, 14.18it/s, est. speed input: 15478.09 toks/s, output: 15.12 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:09<00:26, 14.12it/s, est. speed input: 15442.39 toks/s, output: 15.08 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:09<00:25, 14.17it/s, est. speed input: 15419.52 toks/s, output: 15.06 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:09<00:25, 14.18it/s, est. speed input: 15394.91 toks/s, output: 15.03 toks/s]
Processed prompts:  30%|███       | 154/512 [00:10<00:25, 14.15it/s, est. speed input: 15368.13 toks/s, output: 15.01 toks/s]
Processed prompts:  31%|███       | 158/512 [00:10<00:25, 14.09it/s, est. speed input: 15338.31 toks/s, output: 14.98 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:10<00:24, 14.13it/s, est. speed input: 15318.17 toks/s, output: 14.96 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:11<00:24, 14.17it/s, est. speed input: 15300.60 toks/s, output: 14.94 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:11<00:24, 14.12it/s, est. speed input: 15276.38 toks/s, output: 14.92 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:11<00:23, 14.13it/s, est. speed input: 15257.78 toks/s, output: 14.90 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:11<00:23, 14.20it/s, est. speed input: 15244.96 toks/s, output: 14.89 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:12<00:23, 14.29it/s, est. speed input: 15235.77 toks/s, output: 14.88 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:12<00:22, 14.24it/s, est. speed input: 15218.60 toks/s, output: 14.86 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:12<00:22, 14.24it/s, est. speed input: 15204.32 toks/s, output: 14.85 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:13<00:22, 14.30it/s, est. speed input: 15195.50 toks/s, output: 14.84 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:13<00:21, 14.31it/s, est. speed input: 15184.96 toks/s, output: 14.83 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:13<00:21, 14.20it/s, est. speed input: 15166.16 toks/s, output: 14.81 toks/s]
Processed prompts:  40%|████      | 206/512 [00:13<00:21, 14.16it/s, est. speed input: 15150.41 toks/s, output: 14.80 toks/s]
Processed prompts:  41%|████      | 210/512 [00:14<00:21, 14.20it/s, est. speed input: 15140.34 toks/s, output: 14.79 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:14<00:20, 14.20it/s, est. speed input: 15128.46 toks/s, output: 14.77 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:14<00:20, 14.15it/s, est. speed input: 15113.82 toks/s, output: 14.76 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:15<00:20, 14.15it/s, est. speed input: 15102.18 toks/s, output: 14.75 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:15<00:20, 14.19it/s, est. speed input: 15093.62 toks/s, output: 14.74 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:15<00:20, 14.08it/s, est. speed input: 15076.56 toks/s, output: 14.72 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:15<00:19, 14.13it/s, est. speed input: 15067.63 toks/s, output: 14.71 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:16<00:19, 14.16it/s, est. speed input: 15059.27 toks/s, output: 14.71 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:16<00:19, 14.12it/s, est. speed input: 15047.40 toks/s, output: 14.69 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:16<00:18, 14.04it/s, est. speed input: 15032.42 toks/s, output: 14.68 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:17<00:18, 14.08it/s, est. speed input: 15023.66 toks/s, output: 14.67 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:17<00:18, 14.18it/s, est. speed input: 15019.57 toks/s, output: 14.67 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:17<00:17, 14.22it/s, est. speed input: 15014.09 toks/s, output: 14.66 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:17<00:17, 14.12it/s, est. speed input: 15001.29 toks/s, output: 14.65 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:18<00:17, 14.20it/s, est. speed input: 14997.42 toks/s, output: 14.65 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:18<00:17, 14.22it/s, est. speed input: 14991.58 toks/s, output: 14.64 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:18<00:16, 14.15it/s, est. speed input: 14981.22 toks/s, output: 14.63 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:19<00:16, 14.10it/s, est. speed input: 14971.54 toks/s, output: 14.62 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:19<00:16, 14.10it/s, est. speed input: 14963.76 toks/s, output: 14.61 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:19<00:15, 14.14it/s, est. speed input: 14957.86 toks/s, output: 14.61 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:19<00:15, 14.05it/s, est. speed input: 14946.41 toks/s, output: 14.60 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:20<00:15, 14.10it/s, est. speed input: 14941.27 toks/s, output: 14.59 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:20<00:15, 14.13it/s, est. speed input: 14935.66 toks/s, output: 14.59 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:20<00:14, 14.16it/s, est. speed input: 14930.76 toks/s, output: 14.58 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:21<00:14, 14.08it/s, est. speed input: 14920.94 toks/s, output: 14.57 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:21<00:14, 14.06it/s, est. speed input: 14913.25 toks/s, output: 14.56 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:21<00:14, 14.13it/s, est. speed input: 14909.97 toks/s, output: 14.56 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:21<00:13, 14.17it/s, est. speed input: 14906.00 toks/s, output: 14.56 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:22<00:13, 14.09it/s, est. speed input: 14897.10 toks/s, output: 14.55 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:22<00:13, 14.17it/s, est. speed input: 14894.67 toks/s, output: 14.55 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:22<00:12, 14.22it/s, est. speed input: 14892.35 toks/s, output: 14.54 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:22<00:12, 14.18it/s, est. speed input: 14886.47 toks/s, output: 14.54 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:23<00:12, 14.20it/s, est. speed input: 14882.85 toks/s, output: 14.53 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:23<00:11, 14.89it/s, est. speed input: 14906.45 toks/s, output: 14.56 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:23<00:11, 14.76it/s, est. speed input: 14905.17 toks/s, output: 14.56 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:24<00:11, 14.50it/s, est. speed input: 14897.49 toks/s, output: 14.55 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:24<00:10, 14.41it/s, est. speed input: 14893.63 toks/s, output: 14.54 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:24<00:10, 14.34it/s, est. speed input: 14889.10 toks/s, output: 14.54 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:24<00:10, 14.26it/s, est. speed input: 14883.85 toks/s, output: 14.53 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:25<00:10, 14.19it/s, est. speed input: 14877.74 toks/s, output: 14.53 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:25<00:10, 14.19it/s, est. speed input: 14874.12 toks/s, output: 14.53 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:25<00:09, 14.18it/s, est. speed input: 14869.75 toks/s, output: 14.52 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:26<00:09, 14.17it/s, est. speed input: 14865.87 toks/s, output: 14.52 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:26<00:09, 14.07it/s, est. speed input: 14858.34 toks/s, output: 14.51 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:26<00:08, 14.14it/s, est. speed input: 14856.14 toks/s, output: 14.51 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:26<00:08, 14.16it/s, est. speed input: 14852.96 toks/s, output: 14.50 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:27<00:08, 14.17it/s, est. speed input: 14849.56 toks/s, output: 14.50 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:27<00:08, 14.06it/s, est. speed input: 14842.11 toks/s, output: 14.49 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:27<00:07, 14.09it/s, est. speed input: 14838.47 toks/s, output: 14.49 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:28<00:07, 14.16it/s, est. speed input: 14836.87 toks/s, output: 14.49 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:28<00:07, 14.08it/s, est. speed input: 14830.70 toks/s, output: 14.48 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:28<00:06, 14.13it/s, est. speed input: 14828.29 toks/s, output: 14.48 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:28<00:06, 14.17it/s, est. speed input: 14826.33 toks/s, output: 14.48 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:29<00:06, 14.15it/s, est. speed input: 14822.60 toks/s, output: 14.48 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:29<00:06, 14.08it/s, est. speed input: 14816.86 toks/s, output: 14.47 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:29<00:05, 14.08it/s, est. speed input: 14813.08 toks/s, output: 14.47 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:30<00:05, 14.16it/s, est. speed input: 14811.95 toks/s, output: 14.46 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:30<00:05, 14.16it/s, est. speed input: 14809.18 toks/s, output: 14.46 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:30<00:04, 14.07it/s, est. speed input: 14803.47 toks/s, output: 14.46 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:30<00:04, 14.17it/s, est. speed input: 14803.05 toks/s, output: 14.46 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:31<00:04, 15.03it/s, est. speed input: 14825.98 toks/s, output: 14.48 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:31<00:03, 14.79it/s, est. speed input: 14823.92 toks/s, output: 14.48 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:31<00:03, 14.58it/s, est. speed input: 14820.69 toks/s, output: 14.47 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:31<00:03, 14.52it/s, est. speed input: 14819.88 toks/s, output: 14.47 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:32<00:03, 14.41it/s, est. speed input: 14817.08 toks/s, output: 14.47 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:32<00:02, 14.31it/s, est. speed input: 14813.42 toks/s, output: 14.47 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:32<00:02, 14.28it/s, est. speed input: 14811.32 toks/s, output: 14.46 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:33<00:02, 14.23it/s, est. speed input: 14808.07 toks/s, output: 14.46 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:33<00:02, 14.23it/s, est. speed input: 14806.14 toks/s, output: 14.46 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:33<00:01, 14.13it/s, est. speed input: 14801.41 toks/s, output: 14.45 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:33<00:01, 14.16it/s, est. speed input: 14799.28 toks/s, output: 14.45 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:34<00:01, 14.17it/s, est. speed input: 14797.18 toks/s, output: 14.45 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:34<00:00, 14.22it/s, est. speed input: 14796.33 toks/s, output: 14.45 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:34<00:00, 14.22it/s, est. speed input: 14794.26 toks/s, output: 14.45 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:35<00:00, 14.26it/s, est. speed input: 14793.48 toks/s, output: 14.45 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:35<00:00, 15.29it/s, est. speed input: 14818.39 toks/s, output: 14.47 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:35<00:00, 15.29it/s, est. speed input: 14876.43 toks/s, output: 14.53 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:35<00:00, 14.53it/s, est. speed input: 14876.43 toks/s, output: 14.53 toks/s]
[rank0]:[W128 08:28:30.506865332 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 72.6s

测试结果:
  Requests/s:   14.16
  Tokens/s:     14510.94
  Total Reqs:   512
  Elapsed:      36.17s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     14496.79

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:28:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:28:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3692153) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3692153) WARNING 01-28 08:29:09 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.19 requests/s, 14547.23 total tokens/s, 14.19 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 08:28:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:28:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:28:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:28:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:28:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:28:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:28:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:28:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:28:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:28:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:28:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:28:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:28:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:28:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:28:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:28:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:28:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:28:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3692153) [2026-01-28 08:28:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3692153) [2026-01-28 08:28:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3692153) [2026-01-28 08:28:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3692153) [2026-01-28 08:28:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3692153) [2026-01-28 08:28:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3692153) [2026-01-28 08:28:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3692153) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3692153) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.39s/it]
(EngineCore_DP0 pid=3692153) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.39s/it]
(EngineCore_DP0 pid=3692153) 
(EngineCore_DP0 pid=3692153) [2026-01-28 08:29:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3692153) [2026-01-28 08:29:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3692153) [2026-01-28 08:29:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3692153) [2026-01-28 08:29:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3692153) [2026-01-28 08:29:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3692153) [2026-01-28 08:29:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3692153) [2026-01-28 08:29:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3692153) [2026-01-28 08:29:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3692153) 2026-01-28 08:29:07,870 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3692153) 2026-01-28 08:29:07,915 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 61/1024 [00:00<00:01, 599.96it/s]
Adding requests:  12%|█▏        | 121/1024 [00:00<00:01, 577.31it/s]
Adding requests:  17%|█▋        | 179/1024 [00:00<00:01, 544.34it/s]
Adding requests:  23%|██▎       | 236/1024 [00:00<00:01, 551.40it/s]
Adding requests:  29%|██▊       | 292/1024 [00:00<00:01, 535.37it/s]
Adding requests:  34%|███▍      | 346/1024 [00:00<00:01, 532.18it/s]
Adding requests:  39%|███▉      | 401/1024 [00:00<00:01, 537.14it/s]
Adding requests:  44%|████▍     | 455/1024 [00:00<00:01, 527.40it/s]
Adding requests:  50%|████▉     | 508/1024 [00:00<00:00, 524.19it/s]
Adding requests:  55%|█████▍    | 561/1024 [00:01<00:00, 508.52it/s]
Adding requests:  60%|█████▉    | 614/1024 [00:01<00:00, 513.64it/s]
Adding requests:  65%|██████▌   | 669/1024 [00:01<00:00, 523.64it/s]
Adding requests:  71%|███████   | 724/1024 [00:01<00:00, 529.42it/s]
Adding requests:  76%|███████▌  | 778/1024 [00:01<00:00, 518.83it/s]
Adding requests:  81%|████████  | 830/1024 [00:01<00:00, 506.57it/s]
Adding requests:  86%|████████▋ | 884/1024 [00:01<00:00, 513.85it/s]
Adding requests:  92%|█████████▏| 938/1024 [00:01<00:00, 519.37it/s]
Adding requests:  97%|█████████▋| 992/1024 [00:01<00:00, 524.24it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 527.01it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 26/1024 [00:00<00:18, 54.05it/s, est. speed input: 55348.61 toks/s, output: 54.05 toks/s]
Processed prompts:   3%|▎         | 34/1024 [00:01<00:34, 29.08it/s, est. speed input: 33309.42 toks/s, output: 32.53 toks/s]
Processed prompts:   4%|▍         | 42/1024 [00:01<00:44, 22.13it/s, est. speed input: 26796.11 toks/s, output: 26.17 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:02<00:51, 18.92it/s, est. speed input: 23601.15 toks/s, output: 23.05 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:02<00:56, 17.17it/s, est. speed input: 21719.91 toks/s, output: 21.21 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:03<00:59, 16.15it/s, est. speed input: 20492.44 toks/s, output: 20.01 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:03<01:01, 15.51it/s, est. speed input: 19625.36 toks/s, output: 19.17 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:04<01:02, 15.12it/s, est. speed input: 18992.06 toks/s, output: 18.55 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:04<01:02, 14.83it/s, est. speed input: 18487.56 toks/s, output: 18.05 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:05<01:03, 14.67it/s, est. speed input: 18099.36 toks/s, output: 17.68 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:06<01:03, 14.50it/s, est. speed input: 17761.76 toks/s, output: 17.35 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:06<01:03, 14.43it/s, est. speed input: 17498.10 toks/s, output: 17.09 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:07<01:02, 14.33it/s, est. speed input: 17257.01 toks/s, output: 16.85 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:07<01:02, 14.30it/s, est. speed input: 17064.69 toks/s, output: 16.66 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:08<01:02, 14.23it/s, est. speed input: 16883.00 toks/s, output: 16.49 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:08<01:01, 14.24it/s, est. speed input: 16741.57 toks/s, output: 16.35 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:09<01:01, 14.26it/s, est. speed input: 16617.72 toks/s, output: 16.23 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:10<01:00, 14.28it/s, est. speed input: 16508.97 toks/s, output: 16.12 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:10<01:00, 14.22it/s, est. speed input: 16398.32 toks/s, output: 16.01 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:11<00:59, 14.22it/s, est. speed input: 16305.32 toks/s, output: 15.92 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:11<00:58, 14.24it/s, est. speed input: 16225.81 toks/s, output: 15.85 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:12<00:58, 14.20it/s, est. speed input: 16143.88 toks/s, output: 15.77 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:12<00:57, 14.22it/s, est. speed input: 16077.25 toks/s, output: 15.70 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:13<00:57, 14.19it/s, est. speed input: 16008.30 toks/s, output: 15.63 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:13<00:56, 14.22it/s, est. speed input: 15954.28 toks/s, output: 15.58 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:14<00:56, 14.19it/s, est. speed input: 15896.18 toks/s, output: 15.52 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:15<00:55, 14.21it/s, est. speed input: 15847.57 toks/s, output: 15.48 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:15<00:55, 14.19it/s, est. speed input: 15798.28 toks/s, output: 15.43 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:16<00:54, 14.21it/s, est. speed input: 15756.80 toks/s, output: 15.39 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:16<00:53, 14.22it/s, est. speed input: 15717.51 toks/s, output: 15.35 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:17<00:53, 14.24it/s, est. speed input: 15682.04 toks/s, output: 15.31 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:17<00:52, 14.19it/s, est. speed input: 15642.64 toks/s, output: 15.28 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:18<00:52, 14.22it/s, est. speed input: 15611.34 toks/s, output: 15.25 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:19<00:51, 14.18it/s, est. speed input: 15576.89 toks/s, output: 15.21 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:19<00:51, 14.19it/s, est. speed input: 15547.10 toks/s, output: 15.18 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:20<00:50, 14.21it/s, est. speed input: 15520.82 toks/s, output: 15.16 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:20<00:50, 14.18it/s, est. speed input: 15491.37 toks/s, output: 15.13 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:21<00:49, 14.20it/s, est. speed input: 15468.08 toks/s, output: 15.11 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:21<00:48, 14.20it/s, est. speed input: 15443.53 toks/s, output: 15.08 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:22<00:47, 14.57it/s, est. speed input: 15453.69 toks/s, output: 15.09 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:22<00:47, 14.42it/s, est. speed input: 15428.45 toks/s, output: 15.07 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:23<00:46, 14.38it/s, est. speed input: 15409.07 toks/s, output: 15.05 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:24<00:46, 14.32it/s, est. speed input: 15388.94 toks/s, output: 15.03 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:24<00:45, 14.30it/s, est. speed input: 15370.83 toks/s, output: 15.01 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:25<00:45, 14.25it/s, est. speed input: 15350.56 toks/s, output: 14.99 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:25<00:44, 14.24it/s, est. speed input: 15333.13 toks/s, output: 14.97 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:26<00:44, 14.21it/s, est. speed input: 15314.84 toks/s, output: 14.96 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:26<00:43, 14.23it/s, est. speed input: 15300.02 toks/s, output: 14.94 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:27<00:43, 14.21it/s, est. speed input: 15283.67 toks/s, output: 14.93 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:28<00:42, 14.22it/s, est. speed input: 15269.92 toks/s, output: 14.91 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:28<00:42, 14.21it/s, est. speed input: 15255.03 toks/s, output: 14.90 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:29<00:41, 14.20it/s, est. speed input: 15240.88 toks/s, output: 14.88 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:29<00:40, 14.22it/s, est. speed input: 15229.04 toks/s, output: 14.87 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:30<00:39, 14.64it/s, est. speed input: 15243.85 toks/s, output: 14.89 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:30<00:41, 13.52it/s, est. speed input: 15165.18 toks/s, output: 14.81 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:31<00:40, 13.69it/s, est. speed input: 15152.27 toks/s, output: 14.80 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:32<00:39, 13.87it/s, est. speed input: 15143.41 toks/s, output: 14.79 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:32<00:38, 13.95it/s, est. speed input: 15131.92 toks/s, output: 14.78 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:33<00:38, 14.05it/s, est. speed input: 15123.17 toks/s, output: 14.77 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:33<00:37, 14.08it/s, est. speed input: 15112.60 toks/s, output: 14.76 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:34<00:36, 14.12it/s, est. speed input: 15103.64 toks/s, output: 14.75 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:34<00:36, 14.14it/s, est. speed input: 15094.34 toks/s, output: 14.74 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:35<00:35, 14.18it/s, est. speed input: 15086.66 toks/s, output: 14.73 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:35<00:34, 14.17it/s, est. speed input: 15077.44 toks/s, output: 14.72 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:36<00:34, 14.22it/s, est. speed input: 15071.10 toks/s, output: 14.72 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:37<00:33, 14.19it/s, est. speed input: 15061.95 toks/s, output: 14.71 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:37<00:33, 14.14it/s, est. speed input: 15051.27 toks/s, output: 14.70 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:38<00:32, 14.15it/s, est. speed input: 15043.57 toks/s, output: 14.69 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:38<00:32, 14.16it/s, est. speed input: 15035.67 toks/s, output: 14.68 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:39<00:31, 14.21it/s, est. speed input: 15030.84 toks/s, output: 14.68 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:39<00:30, 14.20it/s, est. speed input: 15023.40 toks/s, output: 14.67 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:40<00:30, 14.25it/s, est. speed input: 15019.24 toks/s, output: 14.67 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:41<00:29, 14.22it/s, est. speed input: 15012.00 toks/s, output: 14.66 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:41<00:29, 14.24it/s, est. speed input: 15006.88 toks/s, output: 14.66 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:42<00:28, 14.17it/s, est. speed input: 14998.00 toks/s, output: 14.65 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:42<00:28, 14.18it/s, est. speed input: 14992.03 toks/s, output: 14.64 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:43<00:27, 14.17it/s, est. speed input: 14985.24 toks/s, output: 14.63 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:43<00:26, 14.21it/s, est. speed input: 14980.89 toks/s, output: 14.63 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:44<00:26, 14.18it/s, est. speed input: 14974.04 toks/s, output: 14.62 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:45<00:25, 14.18it/s, est. speed input: 14968.24 toks/s, output: 14.62 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:45<00:25, 14.16it/s, est. speed input: 14962.14 toks/s, output: 14.61 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:46<00:24, 14.13it/s, est. speed input: 14954.95 toks/s, output: 14.60 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:46<00:24, 14.16it/s, est. speed input: 14950.42 toks/s, output: 14.60 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:47<00:23, 14.17it/s, est. speed input: 14945.55 toks/s, output: 14.60 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:47<00:22, 14.19it/s, est. speed input: 14941.01 toks/s, output: 14.59 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:48<00:22, 14.17it/s, est. speed input: 14935.34 toks/s, output: 14.59 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:48<00:21, 14.21it/s, est. speed input: 14932.10 toks/s, output: 14.58 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:49<00:21, 14.19it/s, est. speed input: 14927.02 toks/s, output: 14.58 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:50<00:20, 14.19it/s, est. speed input: 14922.71 toks/s, output: 14.57 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:50<00:20, 14.17it/s, est. speed input: 14917.48 toks/s, output: 14.57 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:51<00:19, 14.20it/s, est. speed input: 14914.00 toks/s, output: 14.56 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:51<00:19, 14.18it/s, est. speed input: 14909.35 toks/s, output: 14.56 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:52<00:18, 14.19it/s, est. speed input: 14905.44 toks/s, output: 14.56 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:52<00:17, 14.17it/s, est. speed input: 14900.82 toks/s, output: 14.55 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:53<00:17, 14.20it/s, est. speed input: 14897.63 toks/s, output: 14.55 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:54<00:16, 14.19it/s, est. speed input: 14893.67 toks/s, output: 14.54 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:54<00:16, 14.18it/s, est. speed input: 14889.55 toks/s, output: 14.54 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:55<00:15, 14.21it/s, est. speed input: 14886.69 toks/s, output: 14.54 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:55<00:15, 14.19it/s, est. speed input: 14882.57 toks/s, output: 14.53 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:56<00:14, 14.21it/s, est. speed input: 14879.96 toks/s, output: 14.53 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:56<00:13, 14.16it/s, est. speed input: 14874.89 toks/s, output: 14.53 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:57<00:13, 14.21it/s, est. speed input: 14872.88 toks/s, output: 14.52 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:57<00:12, 14.18it/s, est. speed input: 14868.86 toks/s, output: 14.52 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:58<00:12, 14.19it/s, est. speed input: 14865.89 toks/s, output: 14.52 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:59<00:11, 14.17it/s, est. speed input: 14862.05 toks/s, output: 14.51 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:59<00:11, 14.20it/s, est. speed input: 14859.56 toks/s, output: 14.51 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [01:00<00:10, 14.21it/s, est. speed input: 14856.92 toks/s, output: 14.51 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [01:00<00:09, 14.22it/s, est. speed input: 14854.44 toks/s, output: 14.51 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [01:01<00:09, 14.22it/s, est. speed input: 14851.67 toks/s, output: 14.50 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [01:01<00:08, 14.23it/s, est. speed input: 14849.44 toks/s, output: 14.50 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [01:02<00:08, 14.22it/s, est. speed input: 14846.69 toks/s, output: 14.50 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [01:03<00:07, 14.23it/s, est. speed input: 14844.24 toks/s, output: 14.50 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [01:03<00:07, 14.17it/s, est. speed input: 14839.95 toks/s, output: 14.49 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [01:04<00:06, 14.20it/s, est. speed input: 14837.96 toks/s, output: 14.49 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [01:04<00:05, 14.70it/s, est. speed input: 14850.06 toks/s, output: 14.50 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [01:05<00:05, 14.53it/s, est. speed input: 14846.93 toks/s, output: 14.50 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [01:05<00:04, 14.44it/s, est. speed input: 14844.66 toks/s, output: 14.50 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [01:06<00:04, 14.33it/s, est. speed input: 14841.08 toks/s, output: 14.49 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [01:06<00:03, 14.29it/s, est. speed input: 14838.46 toks/s, output: 14.49 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [01:07<00:03, 14.21it/s, est. speed input: 14834.54 toks/s, output: 14.49 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [01:08<00:02, 14.68it/s, est. speed input: 14845.22 toks/s, output: 14.50 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [01:08<00:02, 14.50it/s, est. speed input: 14841.75 toks/s, output: 14.49 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [01:09<00:01, 14.41it/s, est. speed input: 14839.34 toks/s, output: 14.49 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [01:09<00:00, 14.33it/s, est. speed input: 14836.52 toks/s, output: 14.49 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [01:10<00:00, 14.82it/s, est. speed input: 14848.19 toks/s, output: 14.50 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:10<00:00, 14.82it/s, est. speed input: 14935.67 toks/s, output: 14.59 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:10<00:00, 14.59it/s, est. speed input: 14935.67 toks/s, output: 14.59 toks/s]
[rank0]:[W128 08:30:21.442945115 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 110.9s

测试结果:
  Requests/s:   14.19
  Tokens/s:     14547.23
  Total Reqs:   1024
  Elapsed:      72.15s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     14533.03

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:30:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:30:32 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3693893) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3693893) WARNING 01-28 08:31:03 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.23 requests/s, 14584.27 total tokens/s, 14.23 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 08:30:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:30:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:30:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:30:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:30:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:30:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:30:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:30:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:30:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:30:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:30:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:30:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:30:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:30:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:30:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:30:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:30:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:30:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3693893) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3693893) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.41s/it]
(EngineCore_DP0 pid=3693893) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.41s/it]
(EngineCore_DP0 pid=3693893) 
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3693893) [2026-01-28 08:30:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3693893) 2026-01-28 08:31:01,904 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3693893) 2026-01-28 08:31:01,974 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 60/2048 [00:00<00:03, 597.86it/s]
Adding requests:   6%|▌         | 120/2048 [00:00<00:03, 597.68it/s]
Adding requests:   9%|▉         | 180/2048 [00:00<00:03, 553.50it/s]
Adding requests:  12%|█▏        | 236/2048 [00:00<00:03, 550.26it/s]
Adding requests:  14%|█▍        | 292/2048 [00:00<00:03, 534.60it/s]
Adding requests:  17%|█▋        | 346/2048 [00:00<00:03, 532.79it/s]
Adding requests:  20%|█▉        | 400/2048 [00:00<00:03, 532.52it/s]
Adding requests:  22%|██▏       | 455/2048 [00:00<00:02, 535.87it/s]
Adding requests:  25%|██▍       | 509/2048 [00:00<00:02, 530.18it/s]
Adding requests:  27%|██▋       | 563/2048 [00:01<00:02, 532.67it/s]
Adding requests:  30%|███       | 617/2048 [00:01<00:02, 528.92it/s]
Adding requests:  33%|███▎      | 673/2048 [00:01<00:02, 538.18it/s]
Adding requests:  36%|███▌      | 729/2048 [00:01<00:02, 540.89it/s]
Adding requests:  38%|███▊      | 784/2048 [00:01<00:02, 516.60it/s]
Adding requests:  41%|████      | 836/2048 [00:01<00:02, 510.22it/s]
Adding requests:  43%|████▎     | 888/2048 [00:02<00:05, 226.83it/s]
Adding requests:  46%|████▌     | 941/2048 [00:02<00:04, 272.99it/s]
Adding requests:  48%|████▊     | 993/2048 [00:02<00:03, 317.23it/s]
Adding requests:  51%|█████     | 1047/2048 [00:02<00:02, 362.75it/s]
Adding requests:  54%|█████▎    | 1098/2048 [00:02<00:02, 395.56it/s]
Adding requests:  56%|█████▌    | 1150/2048 [00:02<00:02, 425.87it/s]
Adding requests:  59%|█████▉    | 1207/2048 [00:02<00:01, 463.08it/s]
Adding requests:  61%|██████▏   | 1259/2048 [00:02<00:01, 476.70it/s]
Adding requests:  64%|██████▍   | 1313/2048 [00:02<00:01, 493.34it/s]
Adding requests:  67%|██████▋   | 1369/2048 [00:03<00:01, 511.21it/s]
Adding requests:  70%|██████▉   | 1425/2048 [00:03<00:01, 522.82it/s]
Adding requests:  72%|███████▏  | 1481/2048 [00:03<00:01, 531.93it/s]
Adding requests:  75%|███████▌  | 1536/2048 [00:03<00:00, 537.10it/s]
Adding requests:  78%|███████▊  | 1593/2048 [00:03<00:00, 545.11it/s]
Adding requests:  81%|████████  | 1650/2048 [00:03<00:00, 549.23it/s]
Adding requests:  83%|████████▎ | 1706/2048 [00:03<00:00, 545.03it/s]
Adding requests:  86%|████████▌ | 1761/2048 [00:03<00:00, 537.62it/s]
Adding requests:  89%|████████▊ | 1817/2048 [00:03<00:00, 543.66it/s]
Adding requests:  91%|█████████▏| 1872/2048 [00:03<00:00, 539.63it/s]
Adding requests:  94%|█████████▍| 1928/2048 [00:04<00:00, 543.12it/s]
Adding requests:  97%|█████████▋| 1983/2048 [00:04<00:00, 536.41it/s]
Adding requests:  99%|█████████▉| 2037/2048 [00:04<00:00, 515.35it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 479.08it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 50/2048 [00:00<00:16, 119.51it/s, est. speed input: 122386.96 toks/s, output: 119.52 toks/s]
Processed prompts:   3%|▎         | 66/2048 [00:01<00:55, 35.86it/s, est. speed input: 43668.25 toks/s, output: 42.64 toks/s]   
Processed prompts:   4%|▍         | 82/2048 [00:02<01:20, 24.34it/s, est. speed input: 31375.45 toks/s, output: 30.64 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:03<01:37, 19.96it/s, est. speed input: 26381.39 toks/s, output: 25.76 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:04<01:49, 17.74it/s, est. speed input: 23655.72 toks/s, output: 23.10 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:06<01:56, 16.50it/s, est. speed input: 21968.15 toks/s, output: 21.45 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:07<02:00, 15.74it/s, est. speed input: 20810.03 toks/s, output: 20.32 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:08<02:03, 15.23it/s, est. speed input: 19955.42 toks/s, output: 19.49 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:09<02:05, 14.91it/s, est. speed input: 19310.04 toks/s, output: 18.86 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:10<02:06, 14.69it/s, est. speed input: 18802.05 toks/s, output: 18.36 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:11<02:06, 14.54it/s, est. speed input: 18390.21 toks/s, output: 17.96 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:12<02:06, 14.44it/s, est. speed input: 18054.87 toks/s, output: 17.63 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:13<02:05, 14.37it/s, est. speed input: 17770.34 toks/s, output: 17.35 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:15<02:05, 14.31it/s, est. speed input: 17525.93 toks/s, output: 17.12 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:16<02:04, 14.27it/s, est. speed input: 17316.78 toks/s, output: 16.91 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:17<02:03, 14.25it/s, est. speed input: 17136.39 toks/s, output: 16.73 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:18<02:02, 14.23it/s, est. speed input: 16977.02 toks/s, output: 16.58 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:19<02:01, 14.21it/s, est. speed input: 16834.38 toks/s, output: 16.44 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:20<01:58, 14.39it/s, est. speed input: 16748.64 toks/s, output: 16.36 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:21<01:58, 14.34it/s, est. speed input: 16635.18 toks/s, output: 16.25 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:22<01:57, 14.28it/s, est. speed input: 16528.40 toks/s, output: 16.14 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:24<01:56, 14.25it/s, est. speed input: 16434.18 toks/s, output: 16.05 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:25<01:55, 14.23it/s, est. speed input: 16348.45 toks/s, output: 15.97 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:26<01:54, 14.21it/s, est. speed input: 16269.71 toks/s, output: 15.89 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:27<01:53, 14.19it/s, est. speed input: 16196.12 toks/s, output: 15.82 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:28<01:50, 14.43it/s, est. speed input: 16165.83 toks/s, output: 15.79 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:29<01:50, 14.35it/s, est. speed input: 16102.92 toks/s, output: 15.73 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:30<01:49, 14.28it/s, est. speed input: 16041.81 toks/s, output: 15.67 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:31<01:48, 14.25it/s, est. speed input: 15987.96 toks/s, output: 15.61 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:33<01:47, 14.23it/s, est. speed input: 15937.45 toks/s, output: 15.56 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:34<01:46, 14.21it/s, est. speed input: 15890.88 toks/s, output: 15.52 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:35<01:45, 14.20it/s, est. speed input: 15846.72 toks/s, output: 15.48 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:36<01:44, 14.18it/s, est. speed input: 15804.37 toks/s, output: 15.43 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:37<01:43, 14.18it/s, est. speed input: 15765.71 toks/s, output: 15.40 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:38<01:42, 14.18it/s, est. speed input: 15729.12 toks/s, output: 15.36 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:39<01:41, 14.19it/s, est. speed input: 15696.08 toks/s, output: 15.33 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:40<01:40, 14.19it/s, est. speed input: 15664.15 toks/s, output: 15.30 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:42<01:39, 14.18it/s, est. speed input: 15632.90 toks/s, output: 15.27 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:43<01:38, 14.18it/s, est. speed input: 15603.47 toks/s, output: 15.24 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:44<01:36, 14.18it/s, est. speed input: 15576.21 toks/s, output: 15.21 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:45<01:35, 14.18it/s, est. speed input: 15550.03 toks/s, output: 15.19 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:46<01:34, 14.19it/s, est. speed input: 15525.85 toks/s, output: 15.16 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:47<01:33, 14.18it/s, est. speed input: 15501.23 toks/s, output: 15.14 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:48<01:32, 14.18it/s, est. speed input: 15478.53 toks/s, output: 15.12 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:49<01:31, 14.16it/s, est. speed input: 15455.73 toks/s, output: 15.09 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:51<01:30, 14.15it/s, est. speed input: 15433.21 toks/s, output: 15.07 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:52<01:29, 14.16it/s, est. speed input: 15413.50 toks/s, output: 15.05 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:53<01:28, 14.16it/s, est. speed input: 15394.19 toks/s, output: 15.03 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:54<01:26, 14.16it/s, est. speed input: 15375.75 toks/s, output: 15.02 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:55<01:25, 14.17it/s, est. speed input: 15358.34 toks/s, output: 15.00 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:56<01:24, 14.16it/s, est. speed input: 15341.04 toks/s, output: 14.98 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:57<01:23, 14.16it/s, est. speed input: 15324.37 toks/s, output: 14.97 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:58<01:22, 14.17it/s, est. speed input: 15309.14 toks/s, output: 14.95 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [01:00<01:21, 14.16it/s, est. speed input: 15293.78 toks/s, output: 14.94 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [01:01<01:20, 14.16it/s, est. speed input: 15279.31 toks/s, output: 14.92 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [01:02<01:17, 14.42it/s, est. speed input: 15281.48 toks/s, output: 14.92 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [01:03<01:16, 14.33it/s, est. speed input: 15267.23 toks/s, output: 14.91 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [01:04<01:16, 14.29it/s, est. speed input: 15254.31 toks/s, output: 14.90 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [01:05<01:13, 14.51it/s, est. speed input: 15256.84 toks/s, output: 14.90 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [01:06<01:13, 14.41it/s, est. speed input: 15244.39 toks/s, output: 14.89 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [01:07<01:12, 14.31it/s, est. speed input: 15230.42 toks/s, output: 14.87 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [01:09<01:11, 14.25it/s, est. speed input: 15218.01 toks/s, output: 14.86 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [01:10<01:10, 14.22it/s, est. speed input: 15205.95 toks/s, output: 14.85 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [01:11<01:09, 14.21it/s, est. speed input: 15195.53 toks/s, output: 14.84 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [01:12<01:08, 14.18it/s, est. speed input: 15183.87 toks/s, output: 14.83 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [01:13<01:07, 14.18it/s, est. speed input: 15173.52 toks/s, output: 14.82 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [01:14<01:06, 14.18it/s, est. speed input: 15163.78 toks/s, output: 14.81 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [01:15<01:05, 14.18it/s, est. speed input: 15153.95 toks/s, output: 14.80 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [01:16<01:04, 14.16it/s, est. speed input: 15143.58 toks/s, output: 14.79 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [01:18<01:01, 14.44it/s, est. speed input: 15148.34 toks/s, output: 14.79 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [01:19<01:01, 14.35it/s, est. speed input: 15139.09 toks/s, output: 14.78 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [01:20<01:00, 14.30it/s, est. speed input: 15130.37 toks/s, output: 14.78 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [01:21<00:59, 14.26it/s, est. speed input: 15121.51 toks/s, output: 14.77 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [01:22<00:58, 14.22it/s, est. speed input: 15112.45 toks/s, output: 14.76 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [01:23<00:57, 14.18it/s, est. speed input: 15103.28 toks/s, output: 14.75 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [01:24<00:56, 14.15it/s, est. speed input: 15094.33 toks/s, output: 14.74 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [01:25<00:54, 14.44it/s, est. speed input: 15099.52 toks/s, output: 14.75 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [01:26<00:53, 14.36it/s, est. speed input: 15091.89 toks/s, output: 14.74 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [01:28<00:51, 14.57it/s, est. speed input: 15096.12 toks/s, output: 14.74 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [01:29<00:50, 14.45it/s, est. speed input: 15088.68 toks/s, output: 14.74 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [01:30<00:50, 14.35it/s, est. speed input: 15080.91 toks/s, output: 14.73 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [01:31<00:49, 14.29it/s, est. speed input: 15073.84 toks/s, output: 14.72 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [01:32<00:48, 14.25it/s, est. speed input: 15066.55 toks/s, output: 14.71 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [01:33<00:47, 14.22it/s, est. speed input: 15059.69 toks/s, output: 14.71 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [01:34<00:46, 14.20it/s, est. speed input: 15052.97 toks/s, output: 14.70 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [01:35<00:44, 14.20it/s, est. speed input: 15046.83 toks/s, output: 14.69 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [01:37<00:43, 14.18it/s, est. speed input: 15040.08 toks/s, output: 14.69 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [01:38<00:42, 14.17it/s, est. speed input: 15033.68 toks/s, output: 14.68 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [01:39<00:41, 14.15it/s, est. speed input: 15027.18 toks/s, output: 14.67 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [01:40<00:40, 14.16it/s, est. speed input: 15021.44 toks/s, output: 14.67 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [01:41<00:39, 14.17it/s, est. speed input: 15016.01 toks/s, output: 14.66 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [01:42<00:38, 14.15it/s, est. speed input: 15009.79 toks/s, output: 14.66 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [01:43<00:37, 14.15it/s, est. speed input: 15004.12 toks/s, output: 14.65 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [01:45<00:36, 14.15it/s, est. speed input: 14998.46 toks/s, output: 14.65 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [01:46<00:34, 14.15it/s, est. speed input: 14993.19 toks/s, output: 14.64 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [01:47<00:33, 14.16it/s, est. speed input: 14988.18 toks/s, output: 14.64 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [01:48<00:31, 14.45it/s, est. speed input: 14993.43 toks/s, output: 14.64 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [01:49<00:31, 14.35it/s, est. speed input: 14988.12 toks/s, output: 14.64 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [01:50<00:30, 14.29it/s, est. speed input: 14982.88 toks/s, output: 14.63 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [01:51<00:29, 14.24it/s, est. speed input: 14977.59 toks/s, output: 14.63 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [01:52<00:27, 14.47it/s, est. speed input: 14981.69 toks/s, output: 14.63 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [01:53<00:26, 14.37it/s, est. speed input: 14976.55 toks/s, output: 14.63 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [01:55<00:25, 14.31it/s, est. speed input: 14972.06 toks/s, output: 14.62 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [01:56<00:24, 14.26it/s, est. speed input: 14967.39 toks/s, output: 14.62 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [01:57<00:23, 14.21it/s, est. speed input: 14962.26 toks/s, output: 14.61 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [01:58<00:22, 14.21it/s, est. speed input: 14958.26 toks/s, output: 14.61 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [01:59<00:21, 14.19it/s, est. speed input: 14953.61 toks/s, output: 14.60 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [02:00<00:20, 14.17it/s, est. speed input: 14949.08 toks/s, output: 14.60 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [02:01<00:19, 14.16it/s, est. speed input: 14944.80 toks/s, output: 14.59 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [02:02<00:17, 14.15it/s, est. speed input: 14940.21 toks/s, output: 14.59 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [02:04<00:16, 14.11it/s, est. speed input: 14935.03 toks/s, output: 14.58 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [02:05<00:15, 14.11it/s, est. speed input: 14930.66 toks/s, output: 14.58 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [02:06<00:14, 14.13it/s, est. speed input: 14927.00 toks/s, output: 14.58 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [02:07<00:13, 14.13it/s, est. speed input: 14922.85 toks/s, output: 14.57 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [02:08<00:12, 14.39it/s, est. speed input: 14926.90 toks/s, output: 14.58 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [02:09<00:11, 14.31it/s, est. speed input: 14922.67 toks/s, output: 14.57 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [02:10<00:09, 14.27it/s, est. speed input: 14919.37 toks/s, output: 14.57 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [02:11<00:08, 14.23it/s, est. speed input: 14915.61 toks/s, output: 14.57 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [02:13<00:07, 14.20it/s, est. speed input: 14911.72 toks/s, output: 14.56 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [02:14<00:06, 14.46it/s, est. speed input: 14916.01 toks/s, output: 14.57 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [02:15<00:05, 14.37it/s, est. speed input: 14912.60 toks/s, output: 14.56 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [02:16<00:04, 14.58it/s, est. speed input: 14916.84 toks/s, output: 14.57 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [02:17<00:03, 14.45it/s, est. speed input: 14913.38 toks/s, output: 14.56 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [02:18<00:02, 14.35it/s, est. speed input: 14909.62 toks/s, output: 14.56 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [02:19<00:00, 14.55it/s, est. speed input: 14913.54 toks/s, output: 14.56 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:19<00:00, 14.55it/s, est. speed input: 15016.17 toks/s, output: 14.66 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:19<00:00, 14.66it/s, est. speed input: 15016.17 toks/s, output: 14.66 toks/s]
[rank0]:[W128 08:33:27.765822658 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 186.4s

测试结果:
  Requests/s:   14.23
  Tokens/s:     14584.27
  Total Reqs:   2048
  Elapsed:      143.94s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     14570.04

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:33:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:33:44 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3696749) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3696749) WARNING 01-28 08:34:18 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.33 requests/s, 14690.33 total tokens/s, 14.33 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 08:33:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:33:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:33:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:33:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:33:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:33:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:33:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:33:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:33:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:33:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:33:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:33:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:33:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:33:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:33:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:33:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:33:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3696749) [2026-01-28 08:33:49] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3696749) [2026-01-28 08:33:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3696749) [2026-01-28 08:33:49] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3696749) [2026-01-28 08:33:49] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3696749) [2026-01-28 08:33:49] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3696749) [2026-01-28 08:33:49] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3696749) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3696749) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.50s/it]
(EngineCore_DP0 pid=3696749) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.50s/it]
(EngineCore_DP0 pid=3696749) 
(EngineCore_DP0 pid=3696749) [2026-01-28 08:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3696749) [2026-01-28 08:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3696749) [2026-01-28 08:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3696749) [2026-01-28 08:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3696749) [2026-01-28 08:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3696749) [2026-01-28 08:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3696749) [2026-01-28 08:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3696749) [2026-01-28 08:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3696749) 2026-01-28 08:34:15,513 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3696749) 2026-01-28 08:34:15,804 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 59/4096 [00:00<00:06, 586.49it/s]
Adding requests:   3%|▎         | 118/4096 [00:00<00:07, 556.84it/s]
Adding requests:   4%|▍         | 174/4096 [00:00<00:07, 526.75it/s]
Adding requests:   6%|▌         | 227/4096 [00:00<00:07, 519.98it/s]
Adding requests:   7%|▋         | 281/4096 [00:00<00:07, 524.13it/s]
Adding requests:   8%|▊         | 334/4096 [00:00<00:07, 523.33it/s]
Adding requests:   9%|▉         | 387/4096 [00:00<00:07, 522.59it/s]
Adding requests:  11%|█         | 440/4096 [00:00<00:07, 513.35it/s]
Adding requests:  12%|█▏        | 492/4096 [00:00<00:07, 514.29it/s]
Adding requests:  13%|█▎        | 544/4096 [00:01<00:07, 499.32it/s]
Adding requests:  15%|█▍        | 596/4096 [00:01<00:06, 505.38it/s]
Adding requests:  16%|█▌        | 649/4096 [00:01<00:06, 512.27it/s]
Adding requests:  17%|█▋        | 703/4096 [00:01<00:06, 519.25it/s]
Adding requests:  18%|█▊        | 755/4096 [00:01<00:06, 515.11it/s]
Adding requests:  20%|█▉        | 807/4096 [00:01<00:06, 510.28it/s]
Adding requests:  21%|██        | 859/4096 [00:01<00:06, 507.36it/s]
Adding requests:  22%|██▏       | 914/4096 [00:01<00:06, 517.75it/s]
Adding requests:  24%|██▎       | 968/4096 [00:01<00:05, 523.41it/s]
Adding requests:  25%|██▍       | 1021/4096 [00:01<00:05, 516.72it/s]
Adding requests:  26%|██▌       | 1073/4096 [00:02<00:05, 515.12it/s]
Adding requests:  27%|██▋       | 1125/4096 [00:02<00:05, 511.67it/s]
Adding requests:  29%|██▉       | 1180/4096 [00:02<00:05, 521.76it/s]
Adding requests:  30%|███       | 1233/4096 [00:02<00:05, 495.54it/s]
Adding requests:  31%|███▏      | 1283/4096 [00:02<00:05, 491.50it/s]
Adding requests:  33%|███▎      | 1336/4096 [00:02<00:05, 501.15it/s]
Adding requests:  34%|███▍      | 1389/4096 [00:02<00:05, 508.52it/s]
Adding requests:  35%|███▌      | 1440/4096 [00:02<00:05, 504.59it/s]
Adding requests:  36%|███▋      | 1493/4096 [00:02<00:05, 511.32it/s]
Adding requests:  38%|███▊      | 1545/4096 [00:03<00:04, 513.09it/s]
Adding requests:  39%|███▉      | 1599/4096 [00:03<00:04, 520.15it/s]
Adding requests:  40%|████      | 1654/4096 [00:03<00:04, 526.97it/s]
Adding requests:  42%|████▏     | 1707/4096 [00:03<00:04, 522.71it/s]
Adding requests:  43%|████▎     | 1760/4096 [00:03<00:04, 521.09it/s]
Adding requests:  44%|████▍     | 1813/4096 [00:03<00:04, 518.18it/s]
Adding requests:  46%|████▌     | 1865/4096 [00:03<00:04, 507.81it/s]
Adding requests:  47%|████▋     | 1916/4096 [00:03<00:04, 506.43it/s]
Adding requests:  48%|████▊     | 1967/4096 [00:03<00:04, 505.92it/s]
Adding requests:  49%|████▉     | 2021/4096 [00:03<00:04, 514.46it/s]
Adding requests:  51%|█████     | 2076/4096 [00:04<00:03, 523.17it/s]
Adding requests:  52%|█████▏    | 2129/4096 [00:04<00:03, 512.69it/s]
Adding requests:  53%|█████▎    | 2181/4096 [00:04<00:03, 506.98it/s]
Adding requests:  55%|█████▍    | 2234/4096 [00:04<00:03, 513.59it/s]
Adding requests:  56%|█████▌    | 2286/4096 [00:04<00:03, 513.53it/s]
Adding requests:  57%|█████▋    | 2338/4096 [00:04<00:03, 510.10it/s]
Adding requests:  58%|█████▊    | 2390/4096 [00:04<00:03, 509.16it/s]
Adding requests:  60%|█████▉    | 2441/4096 [00:04<00:03, 489.63it/s]
Adding requests:  61%|██████    | 2494/4096 [00:04<00:03, 499.44it/s]
Adding requests:  62%|██████▏   | 2545/4096 [00:04<00:03, 501.56it/s]
Adding requests:  63%|██████▎   | 2598/4096 [00:05<00:02, 507.51it/s]
Adding requests:  65%|██████▍   | 2655/4096 [00:05<00:02, 522.92it/s]
Adding requests:  66%|██████▌   | 2708/4096 [00:05<00:02, 518.48it/s]
Adding requests:  67%|██████▋   | 2762/4096 [00:05<00:02, 524.04it/s]
Adding requests:  69%|██████▊   | 2815/4096 [00:05<00:02, 520.58it/s]
Adding requests:  70%|███████   | 2868/4096 [00:05<00:02, 522.53it/s]
Adding requests:  71%|███████▏  | 2921/4096 [00:05<00:02, 520.03it/s]
Adding requests:  73%|███████▎  | 2974/4096 [00:05<00:02, 518.77it/s]
Adding requests:  74%|███████▍  | 3026/4096 [00:05<00:02, 517.71it/s]
Adding requests:  75%|███████▌  | 3078/4096 [00:05<00:01, 517.95it/s]
Adding requests:  76%|███████▋  | 3131/4096 [00:06<00:01, 519.26it/s]
Adding requests:  78%|███████▊  | 3184/4096 [00:06<00:01, 520.81it/s]
Adding requests:  79%|███████▉  | 3240/4096 [00:06<00:01, 529.65it/s]
Adding requests:  80%|████████  | 3293/4096 [00:06<00:01, 528.73it/s]
Adding requests:  82%|████████▏ | 3346/4096 [00:06<00:01, 523.24it/s]
Adding requests:  83%|████████▎ | 3399/4096 [00:06<00:01, 523.77it/s]
Adding requests:  84%|████████▍ | 3452/4096 [00:06<00:01, 524.12it/s]
Adding requests:  86%|████████▌ | 3505/4096 [00:06<00:01, 519.58it/s]
Adding requests:  87%|████████▋ | 3558/4096 [00:06<00:01, 520.70it/s]
Adding requests:  88%|████████▊ | 3611/4096 [00:07<00:00, 514.24it/s]
Adding requests:  89%|████████▉ | 3664/4096 [00:07<00:00, 517.71it/s]
Adding requests:  91%|█████████ | 3716/4096 [00:07<00:00, 517.90it/s]
Adding requests:  92%|█████████▏| 3772/4096 [00:07<00:00, 529.46it/s]
Adding requests:  93%|█████████▎| 3825/4096 [00:07<00:00, 507.26it/s]
Adding requests:  95%|█████████▍| 3879/4096 [00:07<00:00, 514.36it/s]
Adding requests:  96%|█████████▌| 3931/4096 [00:07<00:00, 513.78it/s]
Adding requests:  97%|█████████▋| 3985/4096 [00:07<00:00, 519.38it/s]
Adding requests:  99%|█████████▊| 4038/4096 [00:07<00:00, 519.38it/s]
Adding requests: 100%|█████████▉| 4093/4096 [00:07<00:00, 525.99it/s]
Adding requests: 100%|██████████| 4096/4096 [00:07<00:00, 516.10it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 98/4096 [00:01<00:50, 78.77it/s, est. speed input: 80665.01 toks/s, output: 78.77 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:03<02:02, 32.29it/s, est. speed input: 38159.38 toks/s, output: 37.26 toks/s]
Processed prompts:   4%|▍         | 162/4096 [00:05<02:49, 23.15it/s, est. speed input: 28941.41 toks/s, output: 28.26 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:07<03:20, 19.43it/s, est. speed input: 24916.71 toks/s, output: 24.33 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:10<03:41, 17.50it/s, est. speed input: 22660.17 toks/s, output: 22.13 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:12<03:54, 16.37it/s, est. speed input: 21210.44 toks/s, output: 20.71 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:14<04:02, 15.67it/s, est. speed input: 20204.64 toks/s, output: 19.73 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:16<04:06, 15.31it/s, est. speed input: 19509.59 toks/s, output: 19.05 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:19<04:10, 14.96it/s, est. speed input: 18928.22 toks/s, output: 18.48 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:21<04:11, 14.73it/s, est. speed input: 18470.43 toks/s, output: 18.04 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:23<04:12, 14.59it/s, est. speed input: 18102.92 toks/s, output: 17.68 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:25<04:09, 14.58it/s, est. speed input: 17833.19 toks/s, output: 17.42 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:28<04:09, 14.49it/s, est. speed input: 17575.49 toks/s, output: 17.16 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:30<04:08, 14.43it/s, est. speed input: 17357.82 toks/s, output: 16.95 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:32<04:07, 14.36it/s, est. speed input: 17164.19 toks/s, output: 16.76 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:34<04:05, 14.32it/s, est. speed input: 16995.53 toks/s, output: 16.60 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:37<04:03, 14.30it/s, est. speed input: 16850.73 toks/s, output: 16.46 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:39<04:01, 14.28it/s, est. speed input: 16721.30 toks/s, output: 16.33 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:41<03:59, 14.26it/s, est. speed input: 16604.18 toks/s, output: 16.21 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:43<03:57, 14.26it/s, est. speed input: 16500.65 toks/s, output: 16.11 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:46<03:55, 14.26it/s, est. speed input: 16408.77 toks/s, output: 16.02 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:48<03:53, 14.25it/s, est. speed input: 16322.96 toks/s, output: 15.94 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:50<03:51, 14.25it/s, est. speed input: 16245.54 toks/s, output: 15.86 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:52<03:49, 14.24it/s, est. speed input: 16173.37 toks/s, output: 15.79 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:55<03:46, 14.24it/s, est. speed input: 16108.92 toks/s, output: 15.73 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:57<03:44, 14.24it/s, est. speed input: 16048.25 toks/s, output: 15.67 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:59<03:40, 14.35it/s, est. speed input: 16009.31 toks/s, output: 15.63 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [01:01<03:37, 14.44it/s, est. speed input: 15973.27 toks/s, output: 15.60 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [01:03<03:36, 14.36it/s, est. speed input: 15921.83 toks/s, output: 15.55 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [01:06<03:34, 14.33it/s, est. speed input: 15877.06 toks/s, output: 15.50 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [01:08<03:32, 14.31it/s, est. speed input: 15835.05 toks/s, output: 15.46 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [01:10<03:30, 14.29it/s, est. speed input: 15795.68 toks/s, output: 15.43 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [01:12<03:28, 14.28it/s, est. speed input: 15758.82 toks/s, output: 15.39 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [01:15<03:24, 14.38it/s, est. speed input: 15735.65 toks/s, output: 15.37 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [01:17<03:23, 14.33it/s, est. speed input: 15701.20 toks/s, output: 15.33 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [01:19<03:21, 14.29it/s, est. speed input: 15668.53 toks/s, output: 15.30 toks/s]
Processed prompts:  31%|███       | 1250/4096 [01:21<03:17, 14.40it/s, est. speed input: 15650.65 toks/s, output: 15.28 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [01:23<03:14, 14.46it/s, est. speed input: 15632.68 toks/s, output: 15.27 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [01:26<03:13, 14.38it/s, est. speed input: 15604.16 toks/s, output: 15.24 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [01:28<03:11, 14.34it/s, est. speed input: 15578.18 toks/s, output: 15.21 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [01:30<03:09, 14.31it/s, est. speed input: 15553.58 toks/s, output: 15.19 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [01:32<03:08, 14.28it/s, est. speed input: 15529.68 toks/s, output: 15.17 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [01:35<03:05, 14.27it/s, est. speed input: 15507.40 toks/s, output: 15.14 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [01:37<03:03, 14.25it/s, est. speed input: 15485.10 toks/s, output: 15.12 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [01:39<03:01, 14.25it/s, est. speed input: 15465.14 toks/s, output: 15.10 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [01:41<02:59, 14.24it/s, est. speed input: 15445.09 toks/s, output: 15.08 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [01:44<02:55, 14.35it/s, est. speed input: 15435.12 toks/s, output: 15.07 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [01:46<02:54, 14.32it/s, est. speed input: 15417.36 toks/s, output: 15.06 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [01:48<02:50, 14.40it/s, est. speed input: 15407.69 toks/s, output: 15.05 toks/s]
Processed prompts:  41%|████      | 1666/4096 [01:50<02:49, 14.35it/s, est. speed input: 15390.68 toks/s, output: 15.03 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [01:53<02:47, 14.32it/s, est. speed input: 15374.66 toks/s, output: 15.01 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [01:55<02:45, 14.29it/s, est. speed input: 15358.69 toks/s, output: 15.00 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [01:57<02:43, 14.27it/s, est. speed input: 15343.61 toks/s, output: 14.98 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [01:59<02:41, 14.25it/s, est. speed input: 15328.41 toks/s, output: 14.97 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [02:02<02:39, 14.25it/s, est. speed input: 15315.21 toks/s, output: 14.96 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [02:04<02:35, 14.35it/s, est. speed input: 15308.62 toks/s, output: 14.95 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [02:06<02:34, 14.31it/s, est. speed input: 15295.27 toks/s, output: 14.94 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [02:08<02:32, 14.27it/s, est. speed input: 15281.84 toks/s, output: 14.92 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [02:10<02:29, 14.37it/s, est. speed input: 15276.02 toks/s, output: 14.92 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [02:13<02:26, 14.44it/s, est. speed input: 15270.79 toks/s, output: 14.91 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [02:15<02:24, 14.38it/s, est. speed input: 15259.67 toks/s, output: 14.90 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [02:17<02:22, 14.33it/s, est. speed input: 15248.24 toks/s, output: 14.89 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [02:19<02:20, 14.30it/s, est. speed input: 15237.28 toks/s, output: 14.88 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [02:22<02:18, 14.28it/s, est. speed input: 15226.68 toks/s, output: 14.87 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [02:24<02:16, 14.25it/s, est. speed input: 15215.84 toks/s, output: 14.86 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [02:26<02:14, 14.24it/s, est. speed input: 15205.79 toks/s, output: 14.85 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [02:28<02:10, 14.47it/s, est. speed input: 15208.68 toks/s, output: 14.85 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [02:31<02:08, 14.40it/s, est. speed input: 15199.34 toks/s, output: 14.84 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [02:33<02:06, 14.46it/s, est. speed input: 15195.49 toks/s, output: 14.84 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [02:35<02:04, 14.39it/s, est. speed input: 15186.53 toks/s, output: 14.83 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [02:37<02:01, 14.45it/s, est. speed input: 15183.33 toks/s, output: 14.83 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [02:39<01:57, 14.65it/s, est. speed input: 15187.57 toks/s, output: 14.83 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [02:41<01:55, 14.65it/s, est. speed input: 15184.76 toks/s, output: 14.83 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [02:44<01:54, 14.52it/s, est. speed input: 15176.60 toks/s, output: 14.82 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [02:46<01:52, 14.44it/s, est. speed input: 15168.72 toks/s, output: 14.81 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [02:48<01:50, 14.51it/s, est. speed input: 15166.74 toks/s, output: 14.81 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [02:50<01:48, 14.42it/s, est. speed input: 15159.00 toks/s, output: 14.80 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [02:53<01:45, 14.48it/s, est. speed input: 15156.68 toks/s, output: 14.80 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [02:55<01:44, 14.40it/s, est. speed input: 15149.02 toks/s, output: 14.79 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [02:57<01:42, 14.34it/s, est. speed input: 15141.32 toks/s, output: 14.79 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [02:59<01:40, 14.31it/s, est. speed input: 15134.07 toks/s, output: 14.78 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [03:02<01:37, 14.39it/s, est. speed input: 15131.95 toks/s, output: 14.78 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [03:04<01:35, 14.34it/s, est. speed input: 15124.82 toks/s, output: 14.77 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [03:06<01:33, 14.30it/s, est. speed input: 15118.10 toks/s, output: 14.76 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [03:08<01:31, 14.28it/s, est. speed input: 15111.40 toks/s, output: 14.76 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [03:11<01:29, 14.26it/s, est. speed input: 15104.84 toks/s, output: 14.75 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [03:13<01:26, 14.35it/s, est. speed input: 15102.93 toks/s, output: 14.75 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [03:15<01:24, 14.32it/s, est. speed input: 15096.85 toks/s, output: 14.74 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [03:17<01:22, 14.30it/s, est. speed input: 15091.03 toks/s, output: 14.74 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [03:19<01:20, 14.28it/s, est. speed input: 15085.32 toks/s, output: 14.73 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [03:22<01:18, 14.27it/s, est. speed input: 15079.71 toks/s, output: 14.73 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [03:24<01:16, 14.25it/s, est. speed input: 15074.02 toks/s, output: 14.72 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [03:26<01:14, 14.24it/s, est. speed input: 15068.41 toks/s, output: 14.72 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [03:28<01:11, 14.24it/s, est. speed input: 15063.03 toks/s, output: 14.71 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [03:31<01:08, 14.51it/s, est. speed input: 15067.85 toks/s, output: 14.71 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [03:33<01:05, 14.54it/s, est. speed input: 15066.81 toks/s, output: 14.71 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [03:35<01:04, 14.44it/s, est. speed input: 15061.44 toks/s, output: 14.71 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [03:37<01:02, 14.38it/s, est. speed input: 15056.41 toks/s, output: 14.70 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [03:39<00:59, 14.46it/s, est. speed input: 15055.87 toks/s, output: 14.70 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [03:42<00:57, 14.39it/s, est. speed input: 15050.86 toks/s, output: 14.70 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [03:44<00:55, 14.34it/s, est. speed input: 15046.19 toks/s, output: 14.69 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [03:46<00:53, 14.31it/s, est. speed input: 15041.40 toks/s, output: 14.69 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [03:48<00:51, 14.27it/s, est. speed input: 15036.41 toks/s, output: 14.68 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [03:51<00:49, 14.26it/s, est. speed input: 15031.80 toks/s, output: 14.68 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [03:53<00:46, 14.37it/s, est. speed input: 15031.47 toks/s, output: 14.68 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [03:55<00:44, 14.33it/s, est. speed input: 15027.20 toks/s, output: 14.67 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [03:57<00:41, 14.57it/s, est. speed input: 15031.61 toks/s, output: 14.68 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [03:59<00:39, 14.47it/s, est. speed input: 15027.41 toks/s, output: 14.68 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [04:02<00:37, 14.40it/s, est. speed input: 15023.31 toks/s, output: 14.67 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [04:04<00:35, 14.35it/s, est. speed input: 15019.29 toks/s, output: 14.67 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [04:06<00:33, 14.32it/s, est. speed input: 15015.24 toks/s, output: 14.66 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [04:08<00:31, 14.30it/s, est. speed input: 15011.39 toks/s, output: 14.66 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [04:11<00:29, 14.27it/s, est. speed input: 15007.30 toks/s, output: 14.66 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [04:13<00:26, 14.37it/s, est. speed input: 15006.78 toks/s, output: 14.66 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [04:15<00:24, 14.33it/s, est. speed input: 15002.94 toks/s, output: 14.65 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [04:17<00:22, 14.29it/s, est. speed input: 14998.87 toks/s, output: 14.65 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [04:20<00:20, 14.27it/s, est. speed input: 14995.16 toks/s, output: 14.64 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [04:22<00:17, 14.37it/s, est. speed input: 14994.85 toks/s, output: 14.64 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [04:24<00:15, 14.32it/s, est. speed input: 14991.13 toks/s, output: 14.64 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [04:26<00:13, 14.30it/s, est. speed input: 14987.70 toks/s, output: 14.64 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [04:29<00:11, 14.28it/s, est. speed input: 14984.33 toks/s, output: 14.63 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [04:31<00:08, 14.27it/s, est. speed input: 14980.92 toks/s, output: 14.63 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [04:33<00:06, 14.24it/s, est. speed input: 14977.13 toks/s, output: 14.63 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [04:35<00:04, 14.63it/s, est. speed input: 14984.78 toks/s, output: 14.63 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [04:37<00:02, 14.63it/s, est. speed input: 14984.74 toks/s, output: 14.63 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:37<00:00, 14.63it/s, est. speed input: 15095.28 toks/s, output: 14.74 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:37<00:00, 14.74it/s, est. speed input: 15095.28 toks/s, output: 14.74 toks/s]
[rank0]:[W128 08:39:04.787304500 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 337.0s

测试结果:
  Requests/s:   14.33
  Tokens/s:     14690.33
  Total Reqs:   4096
  Elapsed:      285.79s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     14676.00


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,25.7932,13231.8945,4.9626
1024,1024,1,128,128,14.3967,14756.6472,8.8909
2048,1024,2,256,128,15.1440,15522.5965,16.9044
4096,1024,4,512,128,14.1570,14510.9442,36.1658
8192,1024,8,1024,128,14.1924,14547.2263,72.1512
16384,1024,16,2048,128,14.2286,14584.2712,143.9359
32768,1024,32,4096,128,14.3320,14690.3317,285.7934

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_073123.log
[SUCCESS] bitnet1.58-2b-int8 Prefill 完成 (4074.4s)

------------------------------------------------------------
  Prefill Benchmark: bitnet1.58-2b-fp8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-fp8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[WARNING] GPU 架构不被 Triton 支持: GB10 (sm_121a) is not yet supported by Triton/ptxas
[WARNING] 将使用 eager mode (禁用 torch.compile)
[WARNING] 检测到不支持 torch.compile 的 GPU 架构
[WARNING] 自动启用 eager mode

============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA GB10                               ││
│ GPU (short):      GB10                                      │
│ Memory:           119.7 GB                                    │
│ CC:               cc121 (Blackwell)                            │
│ SM Code:          sm_121                                    │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.95.05                                 │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✗ GB10 (sm_121a) is not yet supp          ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-fp8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['prefill']
  M_prefill:        [512, 1024, 2048, 4096, 8192, 16384, 32768]
  M_decode:         [512, 1024, 2048, 4096, 8192, 16384, 32768]
  GPU 内存利用率:   0.8
  编译模式:         Eager

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_083917.log


============================================================
  BitNet-2B-FP8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:39:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:39:21 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3701590) WARNING 01-28 08:39:44 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 27.20 requests/s, 13952.68 total tokens/s, 27.20 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:39:21] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:39:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:39:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:39:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:39:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:39:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:39:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:39:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:39:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:39:24] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:39:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:39:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:39:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:39:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:39:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:39:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:39:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:39:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3701590) [2026-01-28 08:39:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3701590) [2026-01-28 08:39:25] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3701590) [2026-01-28 08:39:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3701590) [2026-01-28 08:39:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3701590) [2026-01-28 08:39:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3701590) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3701590) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.21s/it]
(EngineCore_DP0 pid=3701590) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.21s/it]
(EngineCore_DP0 pid=3701590) 
(EngineCore_DP0 pid=3701590) 2026-01-28 08:39:44,183 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3701590) 2026-01-28 08:39:44,200 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  87%|████████▋ | 111/128 [00:00<00:00, 1103.28it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1107.60it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:08, 14.32it/s, est. speed input: 7332.14 toks/s, output: 14.32 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:07, 16.50it/s, est. speed input: 8258.28 toks/s, output: 16.13 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:06, 18.99it/s, est. speed input: 9261.05 toks/s, output: 18.09 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:05, 20.00it/s, est. speed input: 9714.44 toks/s, output: 18.97 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:05, 22.50it/s, est. speed input: 10496.67 toks/s, output: 20.50 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:04, 25.16it/s, est. speed input: 11339.97 toks/s, output: 22.15 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:04, 26.41it/s, est. speed input: 11785.95 toks/s, output: 23.02 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:03, 27.30it/s, est. speed input: 12135.05 toks/s, output: 23.70 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:03, 26.02it/s, est. speed input: 12124.31 toks/s, output: 23.68 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:04, 24.40it/s, est. speed input: 11989.80 toks/s, output: 23.42 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:03, 25.71it/s, est. speed input: 12220.89 toks/s, output: 23.87 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:03, 26.78it/s, est. speed input: 12427.64 toks/s, output: 24.27 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:03, 27.55it/s, est. speed input: 12605.02 toks/s, output: 24.62 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:03, 27.57it/s, est. speed input: 12706.39 toks/s, output: 24.82 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 28.16it/s, est. speed input: 12847.87 toks/s, output: 25.09 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 28.53it/s, est. speed input: 12969.71 toks/s, output: 25.33 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 28.68it/s, est. speed input: 13070.22 toks/s, output: 25.53 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:02, 28.99it/s, est. speed input: 13175.67 toks/s, output: 25.73 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:02<00:02, 29.23it/s, est. speed input: 13272.75 toks/s, output: 25.92 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:02<00:02, 29.45it/s, est. speed input: 13364.52 toks/s, output: 26.10 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:02<00:02, 29.59it/s, est. speed input: 13447.48 toks/s, output: 26.26 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:02, 29.42it/s, est. speed input: 13507.24 toks/s, output: 26.38 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 29.67it/s, est. speed input: 13604.96 toks/s, output: 26.57 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:01, 29.19it/s, est. speed input: 13634.29 toks/s, output: 26.63 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 29.57it/s, est. speed input: 13722.31 toks/s, output: 26.80 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 29.66it/s, est. speed input: 13776.57 toks/s, output: 26.91 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:03<00:01, 29.69it/s, est. speed input: 13824.80 toks/s, output: 27.00 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:03<00:01, 29.70it/s, est. speed input: 13869.93 toks/s, output: 27.09 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:03<00:01, 29.77it/s, est. speed input: 13915.12 toks/s, output: 27.18 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:03<00:01, 29.74it/s, est. speed input: 13953.78 toks/s, output: 27.25 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:03<00:01, 29.73it/s, est. speed input: 13990.38 toks/s, output: 27.32 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:01, 29.65it/s, est. speed input: 14021.82 toks/s, output: 27.39 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:03<00:00, 29.71it/s, est. speed input: 14056.66 toks/s, output: 27.45 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:03<00:00, 29.29it/s, est. speed input: 14069.42 toks/s, output: 27.48 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:03<00:00, 29.37it/s, est. speed input: 14097.58 toks/s, output: 27.53 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:03<00:00, 29.60it/s, est. speed input: 14140.25 toks/s, output: 27.62 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:04<00:00, 29.54it/s, est. speed input: 14162.67 toks/s, output: 27.66 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:04<00:00, 29.41it/s, est. speed input: 14180.55 toks/s, output: 27.70 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:04<00:00, 29.76it/s, est. speed input: 14223.22 toks/s, output: 27.78 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:04<00:00, 29.75it/s, est. speed input: 14246.03 toks/s, output: 27.82 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:04<00:00, 29.73it/s, est. speed input: 14267.31 toks/s, output: 27.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 29.73it/s, est. speed input: 14278.95 toks/s, output: 27.89 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 27.89it/s, est. speed input: 14278.95 toks/s, output: 27.89 toks/s]
[rank0]:[W128 08:39:49.700412765 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.5s

测试结果:
  Requests/s:   27.20
  Tokens/s:     13952.68
  Total Reqs:   128
  Elapsed:      4.71s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     13925.48

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:39:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:39:56 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3702266) WARNING 01-28 08:40:18 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.70 requests/s, 16089.84 total tokens/s, 15.70 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:39:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:39:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:39:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:39:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:39:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:39:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:39:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:39:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:39:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:39:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:39:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:39:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:39:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:39:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:39:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:39:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:39:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:39:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:39:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3702266) [2026-01-28 08:40:00] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3702266) [2026-01-28 08:40:00] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3702266) [2026-01-28 08:40:00] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3702266) [2026-01-28 08:40:00] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3702266) [2026-01-28 08:40:00] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3702266) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3702266) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.78s/it]
(EngineCore_DP0 pid=3702266) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.78s/it]
(EngineCore_DP0 pid=3702266) 
(EngineCore_DP0 pid=3702266) 2026-01-28 08:40:18,276 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3702266) 2026-01-28 08:40:18,304 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  48%|████▊     | 61/128 [00:00<00:00, 599.05it/s]
Adding requests:  95%|█████████▍| 121/128 [00:00<00:00, 580.00it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 585.51it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 39.72it/s, est. speed input: 40677.34 toks/s, output: 39.72 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:05, 21.49it/s, est. speed input: 23637.10 toks/s, output: 23.08 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 16.29it/s, est. speed input: 18732.34 toks/s, output: 18.29 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:07, 16.16it/s, est. speed input: 18285.66 toks/s, output: 17.86 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:06, 16.20it/s, est. speed input: 18058.97 toks/s, output: 17.64 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:06, 16.11it/s, est. speed input: 17829.28 toks/s, output: 17.41 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 15.80it/s, est. speed input: 17543.17 toks/s, output: 17.13 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 15.80it/s, est. speed input: 17404.42 toks/s, output: 17.00 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 15.80it/s, est. speed input: 17289.29 toks/s, output: 16.88 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 15.84it/s, est. speed input: 17207.70 toks/s, output: 16.80 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 15.92it/s, est. speed input: 17153.72 toks/s, output: 16.75 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:06, 15.93it/s, est. speed input: 17095.57 toks/s, output: 16.69 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:06, 15.90it/s, est. speed input: 17034.18 toks/s, output: 16.63 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 15.92it/s, est. speed input: 16991.56 toks/s, output: 16.59 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 15.78it/s, est. speed input: 16919.66 toks/s, output: 16.52 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 15.96it/s, est. speed input: 16913.37 toks/s, output: 16.52 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.04it/s, est. speed input: 16897.84 toks/s, output: 16.50 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.04it/s, est. speed input: 16875.14 toks/s, output: 16.48 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.11it/s, est. speed input: 16865.45 toks/s, output: 16.47 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:05, 16.08it/s, est. speed input: 16843.09 toks/s, output: 16.45 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:05, 16.00it/s, est. speed input: 16815.37 toks/s, output: 16.42 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 16.00it/s, est. speed input: 16796.98 toks/s, output: 16.40 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 16.10it/s, est. speed input: 16794.06 toks/s, output: 16.40 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 15.80it/s, est. speed input: 16742.09 toks/s, output: 16.35 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 15.93it/s, est. speed input: 16737.70 toks/s, output: 16.35 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 15.88it/s, est. speed input: 16715.97 toks/s, output: 16.32 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 15.93it/s, est. speed input: 16706.09 toks/s, output: 16.31 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 16.05it/s, est. speed input: 16707.53 toks/s, output: 16.32 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:04, 15.97it/s, est. speed input: 16689.61 toks/s, output: 16.30 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 15.97it/s, est. speed input: 16679.45 toks/s, output: 16.29 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 16.02it/s, est. speed input: 16674.88 toks/s, output: 16.28 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 15.77it/s, est. speed input: 16640.82 toks/s, output: 16.25 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 15.76it/s, est. speed input: 16625.18 toks/s, output: 16.24 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 15.75it/s, est. speed input: 16610.97 toks/s, output: 16.22 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 15.95it/s, est. speed input: 16616.17 toks/s, output: 16.23 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 15.90it/s, est. speed input: 16604.72 toks/s, output: 16.22 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:03, 15.98it/s, est. speed input: 16602.88 toks/s, output: 16.21 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 15.93it/s, est. speed input: 16593.12 toks/s, output: 16.20 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 15.93it/s, est. speed input: 16585.93 toks/s, output: 16.20 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 15.82it/s, est. speed input: 16569.92 toks/s, output: 16.18 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 15.99it/s, est. speed input: 16575.26 toks/s, output: 16.19 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 15.95it/s, est. speed input: 16567.40 toks/s, output: 16.18 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.00it/s, est. speed input: 16565.74 toks/s, output: 16.18 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 15.98it/s, est. speed input: 16560.44 toks/s, output: 16.17 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:02, 16.03it/s, est. speed input: 16560.04 toks/s, output: 16.17 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 16.09it/s, est. speed input: 16561.64 toks/s, output: 16.17 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 16.12it/s, est. speed input: 16561.81 toks/s, output: 16.17 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 16.14it/s, est. speed input: 16561.89 toks/s, output: 16.17 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 15.88it/s, est. speed input: 16543.42 toks/s, output: 16.16 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 15.82it/s, est. speed input: 16533.87 toks/s, output: 16.15 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 15.86it/s, est. speed input: 16530.26 toks/s, output: 16.14 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 15.91it/s, est. speed input: 16527.97 toks/s, output: 16.14 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.04it/s, est. speed input: 16532.13 toks/s, output: 16.14 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 16.05it/s, est. speed input: 16530.56 toks/s, output: 16.14 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 16.06it/s, est. speed input: 16529.42 toks/s, output: 16.14 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 16.07it/s, est. speed input: 16528.56 toks/s, output: 16.14 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 15.91it/s, est. speed input: 16518.09 toks/s, output: 16.13 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 15.92it/s, est. speed input: 16514.64 toks/s, output: 16.13 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 16.06it/s, est. speed input: 16519.18 toks/s, output: 16.13 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 16.04it/s, est. speed input: 16516.64 toks/s, output: 16.13 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 16.02it/s, est. speed input: 16514.44 toks/s, output: 16.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.02it/s, est. speed input: 16519.61 toks/s, output: 16.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.13it/s, est. speed input: 16519.61 toks/s, output: 16.13 toks/s]
[rank0]:[W128 08:40:27.182358423 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 37.2s

测试结果:
  Requests/s:   15.70
  Tokens/s:     16089.84
  Total Reqs:   128
  Elapsed:      8.15s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     16074.14

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:40:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:40:33 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3702973) WARNING 01-28 08:40:56 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.64 requests/s, 17060.67 total tokens/s, 16.64 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 08:40:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:40:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:40:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:40:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:40:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:40:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:40:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:40:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:40:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:40:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:40:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:40:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:40:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:40:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:40:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:40:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:40:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:40:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:40:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:40:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:40:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:40:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:40:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:40:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:40:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:40:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:40:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:40:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3702973) [2026-01-28 08:40:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3702973) [2026-01-28 08:40:37] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3702973) [2026-01-28 08:40:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3702973) [2026-01-28 08:40:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3702973) [2026-01-28 08:40:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3702973) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3702973) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.65s/it]
(EngineCore_DP0 pid=3702973) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.65s/it]
(EngineCore_DP0 pid=3702973) 
(EngineCore_DP0 pid=3702973) 2026-01-28 08:40:55,388 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3702973) 2026-01-28 08:40:55,420 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 57/256 [00:00<00:00, 566.72it/s]
Adding requests:  45%|████▍     | 114/256 [00:00<00:00, 559.13it/s]
Adding requests:  66%|██████▋   | 170/256 [00:00<00:00, 521.33it/s]
Adding requests:  88%|████████▊ | 226/256 [00:00<00:00, 532.58it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 541.55it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 8/256 [00:00<00:06, 38.44it/s, est. speed input: 39372.67 toks/s, output: 38.44 toks/s]
Processed prompts:   5%|▍         | 12/256 [00:00<00:09, 24.61it/s, est. speed input: 27155.37 toks/s, output: 26.52 toks/s]
Processed prompts:   6%|▌         | 15/256 [00:00<00:09, 24.87it/s, est. speed input: 26933.60 toks/s, output: 26.30 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:12, 19.28it/s, est. speed input: 22892.76 toks/s, output: 22.36 toks/s]
Processed prompts:   8%|▊         | 21/256 [00:00<00:11, 20.79it/s, est. speed input: 23282.45 toks/s, output: 22.74 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:01<00:13, 17.38it/s, est. speed input: 21162.15 toks/s, output: 20.67 toks/s]
Processed prompts:  10%|█         | 26/256 [00:01<00:13, 17.28it/s, est. speed input: 20807.07 toks/s, output: 20.32 toks/s]
Processed prompts:  11%|█         | 28/256 [00:01<00:13, 17.15it/s, est. speed input: 20496.02 toks/s, output: 20.02 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:01<00:13, 16.94it/s, est. speed input: 20197.64 toks/s, output: 19.72 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:01<00:13, 16.91it/s, est. speed input: 19983.19 toks/s, output: 19.51 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:01<00:13, 16.89it/s, est. speed input: 19797.46 toks/s, output: 19.33 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:01<00:12, 16.95it/s, est. speed input: 19653.67 toks/s, output: 19.19 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:01<00:12, 16.85it/s, est. speed input: 19495.66 toks/s, output: 19.04 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:02<00:12, 16.80it/s, est. speed input: 19357.83 toks/s, output: 18.90 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:02<00:12, 16.89it/s, est. speed input: 19261.96 toks/s, output: 18.81 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:02<00:12, 16.98it/s, est. speed input: 19180.31 toks/s, output: 18.73 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:02<00:12, 16.95it/s, est. speed input: 19088.30 toks/s, output: 18.64 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:02<00:12, 16.68it/s, est. speed input: 18962.49 toks/s, output: 18.52 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:02<00:12, 16.83it/s, est. speed input: 18903.85 toks/s, output: 18.46 toks/s]
Processed prompts:  20%|██        | 52/256 [00:02<00:12, 16.94it/s, est. speed input: 18851.47 toks/s, output: 18.41 toks/s]
Processed prompts:  21%|██        | 54/256 [00:02<00:11, 16.95it/s, est. speed input: 18792.12 toks/s, output: 18.35 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:03<00:11, 16.97it/s, est. speed input: 18740.18 toks/s, output: 18.30 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:03<00:11, 16.95it/s, est. speed input: 18686.45 toks/s, output: 18.25 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:03<00:11, 16.99it/s, est. speed input: 18644.36 toks/s, output: 18.21 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:03<00:11, 16.96it/s, est. speed input: 18597.53 toks/s, output: 18.16 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:03<00:11, 16.80it/s, est. speed input: 18536.58 toks/s, output: 18.10 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:03<00:11, 16.71it/s, est. speed input: 18483.07 toks/s, output: 18.05 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:03<00:11, 16.83it/s, est. speed input: 18452.84 toks/s, output: 18.02 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:03<00:10, 16.93it/s, est. speed input: 18426.80 toks/s, output: 17.99 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:04<00:10, 16.96it/s, est. speed input: 18398.24 toks/s, output: 17.97 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:04<00:10, 16.87it/s, est. speed input: 18359.44 toks/s, output: 17.93 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:04<00:10, 16.97it/s, est. speed input: 18339.28 toks/s, output: 17.91 toks/s]
Processed prompts:  30%|███       | 78/256 [00:04<00:10, 17.06it/s, est. speed input: 18321.42 toks/s, output: 17.89 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:04<00:10, 16.92it/s, est. speed input: 18285.86 toks/s, output: 17.86 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:04<00:10, 16.68it/s, est. speed input: 18238.86 toks/s, output: 17.81 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:04<00:10, 16.71it/s, est. speed input: 18211.98 toks/s, output: 17.79 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:04<00:10, 16.76it/s, est. speed input: 18189.69 toks/s, output: 17.76 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:04<00:09, 16.89it/s, est. speed input: 18176.37 toks/s, output: 17.75 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:05<00:09, 16.91it/s, est. speed input: 18157.07 toks/s, output: 17.73 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:05<00:09, 16.89it/s, est. speed input: 18136.32 toks/s, output: 17.71 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:05<00:09, 16.95it/s, est. speed input: 18122.69 toks/s, output: 17.70 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:05<00:09, 16.99it/s, est. speed input: 18109.17 toks/s, output: 17.68 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:05<00:09, 16.95it/s, est. speed input: 18090.53 toks/s, output: 17.67 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:05<00:09, 16.69it/s, est. speed input: 18056.33 toks/s, output: 17.63 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:05<00:09, 16.69it/s, est. speed input: 18036.10 toks/s, output: 17.61 toks/s]
Processed prompts:  41%|████      | 104/256 [00:05<00:09, 16.75it/s, est. speed input: 18021.19 toks/s, output: 17.60 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:06<00:08, 16.76it/s, est. speed input: 18004.90 toks/s, output: 17.58 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:06<00:08, 16.87it/s, est. speed input: 17995.72 toks/s, output: 17.57 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:06<00:08, 16.82it/s, est. speed input: 17979.04 toks/s, output: 17.56 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:06<00:08, 16.85it/s, est. speed input: 17966.62 toks/s, output: 17.55 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:06<00:08, 16.87it/s, est. speed input: 17954.83 toks/s, output: 17.53 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:06<00:08, 16.82it/s, est. speed input: 17939.76 toks/s, output: 17.52 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:06<00:08, 16.63it/s, est. speed input: 17914.82 toks/s, output: 17.49 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:06<00:08, 16.63it/s, est. speed input: 17899.49 toks/s, output: 17.48 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:06<00:07, 16.79it/s, est. speed input: 17894.32 toks/s, output: 17.47 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:07<00:07, 16.79it/s, est. speed input: 17882.67 toks/s, output: 17.46 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:07<00:07, 16.84it/s, est. speed input: 17874.32 toks/s, output: 17.46 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:07<00:07, 16.89it/s, est. speed input: 17866.61 toks/s, output: 17.45 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:07<00:07, 16.83it/s, est. speed input: 17854.16 toks/s, output: 17.44 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:07<00:07, 16.80it/s, est. speed input: 17842.92 toks/s, output: 17.42 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:07<00:07, 16.95it/s, est. speed input: 17841.05 toks/s, output: 17.42 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:07<00:07, 16.63it/s, est. speed input: 17816.48 toks/s, output: 17.40 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:07<00:07, 16.65it/s, est. speed input: 17805.47 toks/s, output: 17.39 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:08<00:06, 16.70it/s, est. speed input: 17796.72 toks/s, output: 17.38 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:08<00:06, 16.80it/s, est. speed input: 17791.70 toks/s, output: 17.37 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:08<00:06, 16.89it/s, est. speed input: 17788.03 toks/s, output: 17.37 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:08<00:06, 16.86it/s, est. speed input: 17779.63 toks/s, output: 17.36 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:08<00:06, 16.93it/s, est. speed input: 17775.90 toks/s, output: 17.36 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:08<00:06, 16.99it/s, est. speed input: 17772.60 toks/s, output: 17.36 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:08<00:06, 16.98it/s, est. speed input: 17767.16 toks/s, output: 17.35 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:08<00:06, 16.68it/s, est. speed input: 17747.80 toks/s, output: 17.33 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:09<00:05, 16.73it/s, est. speed input: 17741.21 toks/s, output: 17.33 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:09<00:05, 16.66it/s, est. speed input: 17730.20 toks/s, output: 17.31 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:09<00:05, 16.84it/s, est. speed input: 17729.49 toks/s, output: 17.31 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:09<00:05, 16.81it/s, est. speed input: 17722.28 toks/s, output: 17.31 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:09<00:05, 16.81it/s, est. speed input: 17715.70 toks/s, output: 17.30 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:09<00:05, 16.81it/s, est. speed input: 17709.45 toks/s, output: 17.29 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:09<00:05, 16.87it/s, est. speed input: 17705.98 toks/s, output: 17.29 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:09<00:05, 16.71it/s, est. speed input: 17694.10 toks/s, output: 17.28 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:09<00:05, 16.60it/s, est. speed input: 17682.28 toks/s, output: 17.27 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:10<00:04, 16.68it/s, est. speed input: 17677.57 toks/s, output: 17.26 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:10<00:04, 16.79it/s, est. speed input: 17674.90 toks/s, output: 17.26 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:10<00:04, 16.81it/s, est. speed input: 17670.34 toks/s, output: 17.26 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:10<00:04, 16.77it/s, est. speed input: 17663.38 toks/s, output: 17.25 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:10<00:04, 16.78it/s, est. speed input: 17658.41 toks/s, output: 17.24 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:10<00:04, 16.81it/s, est. speed input: 17654.09 toks/s, output: 17.24 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:10<00:04, 16.81it/s, est. speed input: 17649.23 toks/s, output: 17.24 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:10<00:04, 16.65it/s, est. speed input: 17638.29 toks/s, output: 17.22 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:11<00:03, 16.62it/s, est. speed input: 17630.66 toks/s, output: 17.22 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:11<00:03, 16.72it/s, est. speed input: 17627.86 toks/s, output: 17.21 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:11<00:03, 16.78it/s, est. speed input: 17624.92 toks/s, output: 17.21 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:11<00:03, 16.87it/s, est. speed input: 17623.35 toks/s, output: 17.21 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:11<00:03, 16.93it/s, est. speed input: 17622.10 toks/s, output: 17.21 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:11<00:03, 16.84it/s, est. speed input: 17615.94 toks/s, output: 17.20 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:11<00:03, 16.81it/s, est. speed input: 17611.27 toks/s, output: 17.20 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:11<00:03, 16.87it/s, est. speed input: 17609.28 toks/s, output: 17.20 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:11<00:02, 16.68it/s, est. speed input: 17599.52 toks/s, output: 17.19 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:12<00:02, 16.73it/s, est. speed input: 17595.93 toks/s, output: 17.18 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:12<00:02, 16.77it/s, est. speed input: 17592.76 toks/s, output: 17.18 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:12<00:02, 16.74it/s, est. speed input: 17587.85 toks/s, output: 17.18 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:12<00:02, 16.82it/s, est. speed input: 17585.99 toks/s, output: 17.17 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:12<00:02, 16.81it/s, est. speed input: 17582.44 toks/s, output: 17.17 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:12<00:02, 16.83it/s, est. speed input: 17579.55 toks/s, output: 17.17 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:12<00:02, 16.78it/s, est. speed input: 17574.64 toks/s, output: 17.16 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:12<00:02, 16.81it/s, est. speed input: 17571.97 toks/s, output: 17.16 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:13<00:01, 16.64it/s, est. speed input: 17563.43 toks/s, output: 17.15 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:13<00:01, 16.69it/s, est. speed input: 17560.17 toks/s, output: 17.15 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:13<00:01, 16.84it/s, est. speed input: 17560.57 toks/s, output: 17.15 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:13<00:01, 16.88it/s, est. speed input: 17559.08 toks/s, output: 17.15 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:13<00:01, 16.89it/s, est. speed input: 17556.86 toks/s, output: 17.15 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:13<00:01, 16.93it/s, est. speed input: 17555.84 toks/s, output: 17.14 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:13<00:01, 16.96it/s, est. speed input: 17554.71 toks/s, output: 17.14 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:13<00:01, 16.91it/s, est. speed input: 17551.66 toks/s, output: 17.14 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:14<00:00, 16.80it/s, est. speed input: 17546.55 toks/s, output: 17.14 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:14<00:00, 16.51it/s, est. speed input: 17534.88 toks/s, output: 17.12 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:14<00:00, 16.55it/s, est. speed input: 17530.77 toks/s, output: 17.12 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:14<00:00, 16.70it/s, est. speed input: 17530.43 toks/s, output: 17.12 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:14<00:00, 16.72it/s, est. speed input: 17527.43 toks/s, output: 17.12 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:14<00:00, 16.70it/s, est. speed input: 17523.56 toks/s, output: 17.11 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:14<00:00, 16.77it/s, est. speed input: 17522.02 toks/s, output: 17.11 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:14<00:00, 16.85it/s, est. speed input: 17521.59 toks/s, output: 17.11 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:14<00:00, 16.85it/s, est. speed input: 17585.68 toks/s, output: 17.17 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:14<00:00, 17.17it/s, est. speed input: 17585.68 toks/s, output: 17.17 toks/s]
[rank0]:[W128 08:41:11.616965541 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.4s

测试结果:
  Requests/s:   16.64
  Tokens/s:     17060.67
  Total Reqs:   256
  Elapsed:      15.38s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     17044.02

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:41:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:41:18 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3703788) WARNING 01-28 08:41:41 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.17 requests/s, 16570.28 total tokens/s, 16.17 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 08:41:18] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:41:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:41:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:41:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:41:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:41:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:41:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:41:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:41:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:41:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:41:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:41:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:41:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:41:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:41:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:41:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:41:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:41:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:41:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:41:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:41:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:41:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:41:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:41:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:41:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:41:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:41:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:41:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3703788) [2026-01-28 08:41:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3703788) [2026-01-28 08:41:23] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3703788) [2026-01-28 08:41:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3703788) [2026-01-28 08:41:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3703788) [2026-01-28 08:41:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3703788) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3703788) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.55s/it]
(EngineCore_DP0 pid=3703788) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.55s/it]
(EngineCore_DP0 pid=3703788) 
(EngineCore_DP0 pid=3703788) 2026-01-28 08:41:40,714 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3703788) 2026-01-28 08:41:40,756 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  12%|█▏        | 60/512 [00:00<00:00, 593.72it/s]
Adding requests:  23%|██▎       | 120/512 [00:00<00:00, 556.74it/s]
Adding requests:  34%|███▍      | 176/512 [00:00<00:00, 531.52it/s]
Adding requests:  45%|████▍     | 230/512 [00:00<00:00, 532.45it/s]
Adding requests:  55%|█████▌    | 284/512 [00:00<00:00, 528.17it/s]
Adding requests:  66%|██████▌   | 338/512 [00:00<00:00, 529.72it/s]
Adding requests:  77%|███████▋  | 392/512 [00:00<00:00, 525.20it/s]
Adding requests:  87%|████████▋ | 445/512 [00:00<00:00, 524.65it/s]
Adding requests:  97%|█████████▋| 498/512 [00:00<00:00, 522.82it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 529.95it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 14/512 [00:00<00:06, 77.27it/s, est. speed input: 79130.94 toks/s, output: 77.27 toks/s]
Processed prompts:   4%|▍         | 22/512 [00:00<00:17, 28.51it/s, est. speed input: 33196.98 toks/s, output: 32.42 toks/s]
Processed prompts:   5%|▌         | 27/512 [00:00<00:19, 25.50it/s, est. speed input: 29849.62 toks/s, output: 29.15 toks/s]
Processed prompts:   6%|▌         | 31/512 [00:01<00:21, 22.31it/s, est. speed input: 27035.38 toks/s, output: 26.40 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:01<00:25, 19.00it/s, est. speed input: 24509.59 toks/s, output: 23.93 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:01<00:26, 18.08it/s, est. speed input: 23315.85 toks/s, output: 22.77 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:01<00:26, 17.43it/s, est. speed input: 22415.61 toks/s, output: 21.89 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:02<00:27, 16.99it/s, est. speed input: 21723.24 toks/s, output: 21.21 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:02<00:27, 16.84it/s, est. speed input: 21238.05 toks/s, output: 20.74 toks/s]
Processed prompts:  11%|█         | 54/512 [00:02<00:27, 16.67it/s, est. speed input: 20813.20 toks/s, output: 20.33 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:02<00:27, 16.42it/s, est. speed input: 20417.86 toks/s, output: 19.94 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:03<00:27, 16.35it/s, est. speed input: 20117.27 toks/s, output: 19.65 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:03<00:27, 16.38it/s, est. speed input: 19882.88 toks/s, output: 19.42 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:03<00:27, 16.36it/s, est. speed input: 19669.96 toks/s, output: 19.21 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:03<00:26, 16.23it/s, est. speed input: 19453.36 toks/s, output: 19.00 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:04<00:26, 16.25it/s, est. speed input: 19288.30 toks/s, output: 18.84 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:04<00:26, 16.25it/s, est. speed input: 19139.68 toks/s, output: 18.69 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:04<00:26, 16.24it/s, est. speed input: 19005.84 toks/s, output: 18.56 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:04<00:26, 16.17it/s, est. speed input: 18871.20 toks/s, output: 18.43 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:05<00:25, 16.16it/s, est. speed input: 18757.76 toks/s, output: 18.32 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:05<00:25, 16.20it/s, est. speed input: 18662.77 toks/s, output: 18.23 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:05<00:25, 16.19it/s, est. speed input: 18569.77 toks/s, output: 18.13 toks/s]
Processed prompts:  21%|██        | 106/512 [00:05<00:25, 16.23it/s, est. speed input: 18492.17 toks/s, output: 18.06 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:06<00:24, 16.10it/s, est. speed input: 18396.72 toks/s, output: 17.97 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:06<00:24, 16.11it/s, est. speed input: 18324.69 toks/s, output: 17.90 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:06<00:24, 16.16it/s, est. speed input: 18263.28 toks/s, output: 17.84 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:06<00:24, 16.23it/s, est. speed input: 18209.84 toks/s, output: 17.78 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:07<00:24, 16.08it/s, est. speed input: 18135.85 toks/s, output: 17.71 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:07<00:23, 16.17it/s, est. speed input: 18090.45 toks/s, output: 17.67 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:07<00:23, 16.14it/s, est. speed input: 18037.62 toks/s, output: 17.61 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:07<00:23, 16.18it/s, est. speed input: 17993.90 toks/s, output: 17.57 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:08<00:23, 16.06it/s, est. speed input: 17936.67 toks/s, output: 17.52 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:08<00:22, 16.12it/s, est. speed input: 17899.11 toks/s, output: 17.48 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:08<00:22, 16.18it/s, est. speed input: 17864.93 toks/s, output: 17.45 toks/s]
Processed prompts:  30%|███       | 154/512 [00:08<00:22, 16.23it/s, est. speed input: 17833.96 toks/s, output: 17.42 toks/s]
Processed prompts:  31%|███       | 158/512 [00:09<00:21, 16.10it/s, est. speed input: 17787.77 toks/s, output: 17.37 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:09<00:21, 16.17it/s, est. speed input: 17759.97 toks/s, output: 17.34 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:09<00:21, 16.25it/s, est. speed input: 17737.05 toks/s, output: 17.32 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:09<00:21, 16.23it/s, est. speed input: 17707.38 toks/s, output: 17.29 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:10<00:20, 16.24it/s, est. speed input: 17681.43 toks/s, output: 17.27 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:10<00:20, 16.13it/s, est. speed input: 17646.78 toks/s, output: 17.23 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:10<00:20, 16.16it/s, est. speed input: 17623.06 toks/s, output: 17.21 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:10<00:20, 16.22it/s, est. speed input: 17603.38 toks/s, output: 17.19 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:11<00:19, 16.19it/s, est. speed input: 17579.21 toks/s, output: 17.17 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:11<00:19, 16.11it/s, est. speed input: 17550.96 toks/s, output: 17.14 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:11<00:19, 16.13it/s, est. speed input: 17529.67 toks/s, output: 17.12 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:11<00:19, 16.13it/s, est. speed input: 17508.41 toks/s, output: 17.10 toks/s]
Processed prompts:  40%|████      | 206/512 [00:12<00:18, 16.17it/s, est. speed input: 17491.19 toks/s, output: 17.08 toks/s]
Processed prompts:  41%|████      | 210/512 [00:12<00:18, 16.11it/s, est. speed input: 17468.06 toks/s, output: 17.06 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:12<00:18, 16.10it/s, est. speed input: 17448.09 toks/s, output: 17.04 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:12<00:18, 16.15it/s, est. speed input: 17432.73 toks/s, output: 17.02 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:13<00:17, 16.24it/s, est. speed input: 17422.21 toks/s, output: 17.01 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:13<00:17, 16.20it/s, est. speed input: 17404.53 toks/s, output: 17.00 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:13<00:17, 16.15it/s, est. speed input: 17386.17 toks/s, output: 16.98 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:13<00:17, 16.16it/s, est. speed input: 17372.12 toks/s, output: 16.96 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:14<00:16, 16.13it/s, est. speed input: 17355.47 toks/s, output: 16.95 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:14<00:16, 16.19it/s, est. speed input: 17344.73 toks/s, output: 16.94 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:14<00:16, 16.09it/s, est. speed input: 17325.47 toks/s, output: 16.92 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:14<00:16, 16.17it/s, est. speed input: 17315.87 toks/s, output: 16.91 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:15<00:15, 16.14it/s, est. speed input: 17301.50 toks/s, output: 16.90 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:15<00:15, 16.18it/s, est. speed input: 17291.25 toks/s, output: 16.89 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:15<00:15, 16.05it/s, est. speed input: 17272.41 toks/s, output: 16.87 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:15<00:15, 16.09it/s, est. speed input: 17261.67 toks/s, output: 16.86 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:16<00:15, 16.10it/s, est. speed input: 17250.07 toks/s, output: 16.85 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:16<00:14, 16.14it/s, est. speed input: 17240.54 toks/s, output: 16.84 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:16<00:14, 16.11it/s, est. speed input: 17228.18 toks/s, output: 16.82 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:16<00:14, 16.12it/s, est. speed input: 17218.11 toks/s, output: 16.81 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:17<00:14, 16.14it/s, est. speed input: 17208.44 toks/s, output: 16.81 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:17<00:13, 16.15it/s, est. speed input: 17199.57 toks/s, output: 16.80 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:17<00:13, 16.17it/s, est. speed input: 17190.80 toks/s, output: 16.79 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:17<00:13, 16.07it/s, est. speed input: 17177.26 toks/s, output: 16.77 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:18<00:13, 16.11it/s, est. speed input: 17169.30 toks/s, output: 16.77 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:18<00:12, 16.13it/s, est. speed input: 17161.27 toks/s, output: 16.76 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:18<00:12, 16.19it/s, est. speed input: 17155.32 toks/s, output: 16.75 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:18<00:12, 16.10it/s, est. speed input: 17143.74 toks/s, output: 16.74 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:19<00:12, 16.14it/s, est. speed input: 17136.82 toks/s, output: 16.74 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:19<00:11, 16.18it/s, est. speed input: 17130.97 toks/s, output: 16.73 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:19<00:11, 16.20it/s, est. speed input: 17124.56 toks/s, output: 16.72 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:19<00:11, 16.08it/s, est. speed input: 17112.76 toks/s, output: 16.71 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:19<00:11, 16.14it/s, est. speed input: 17107.26 toks/s, output: 16.71 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:20<00:10, 16.22it/s, est. speed input: 17103.62 toks/s, output: 16.70 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:20<00:09, 17.36it/s, est. speed input: 17142.85 toks/s, output: 16.74 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:20<00:09, 17.03it/s, est. speed input: 17137.63 toks/s, output: 16.74 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:20<00:09, 16.68it/s, est. speed input: 17127.40 toks/s, output: 16.73 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:21<00:09, 16.57it/s, est. speed input: 17122.69 toks/s, output: 16.72 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:21<00:09, 16.47it/s, est. speed input: 17117.05 toks/s, output: 16.72 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:21<00:09, 16.38it/s, est. speed input: 17110.64 toks/s, output: 16.71 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:21<00:08, 16.23it/s, est. speed input: 17100.97 toks/s, output: 16.70 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:22<00:08, 16.25it/s, est. speed input: 17096.53 toks/s, output: 16.70 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:22<00:08, 16.21it/s, est. speed input: 17089.82 toks/s, output: 16.69 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:22<00:08, 16.19it/s, est. speed input: 17083.69 toks/s, output: 16.68 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:22<00:08, 16.05it/s, est. speed input: 17072.75 toks/s, output: 16.67 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:23<00:07, 16.10it/s, est. speed input: 17067.82 toks/s, output: 16.67 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:23<00:07, 16.13it/s, est. speed input: 17062.73 toks/s, output: 16.66 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:23<00:07, 16.16it/s, est. speed input: 17058.09 toks/s, output: 16.66 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:23<00:07, 16.07it/s, est. speed input: 17049.71 toks/s, output: 16.65 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:24<00:06, 16.10it/s, est. speed input: 17044.64 toks/s, output: 16.65 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:24<00:06, 16.15it/s, est. speed input: 17040.60 toks/s, output: 16.64 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:24<00:06, 16.12it/s, est. speed input: 17034.54 toks/s, output: 16.64 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:24<00:06, 16.13it/s, est. speed input: 17029.60 toks/s, output: 16.63 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:25<00:05, 16.04it/s, est. speed input: 17021.46 toks/s, output: 16.62 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:25<00:05, 16.14it/s, est. speed input: 17018.96 toks/s, output: 16.62 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:25<00:05, 16.17it/s, est. speed input: 17015.26 toks/s, output: 16.62 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:25<00:05, 16.18it/s, est. speed input: 17011.33 toks/s, output: 16.61 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:26<00:04, 16.04it/s, est. speed input: 17002.27 toks/s, output: 16.60 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:26<00:04, 16.09it/s, est. speed input: 16998.61 toks/s, output: 16.60 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:26<00:04, 16.16it/s, est. speed input: 16996.10 toks/s, output: 16.60 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:26<00:04, 16.16it/s, est. speed input: 16992.03 toks/s, output: 16.59 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:27<00:03, 17.44it/s, est. speed input: 17025.86 toks/s, output: 16.63 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:27<00:03, 17.05it/s, est. speed input: 17021.87 toks/s, output: 16.62 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:27<00:03, 16.76it/s, est. speed input: 17017.37 toks/s, output: 16.62 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:27<00:03, 16.57it/s, est. speed input: 17012.94 toks/s, output: 16.61 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:28<00:02, 16.50it/s, est. speed input: 17010.38 toks/s, output: 16.61 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:28<00:02, 16.31it/s, est. speed input: 17003.80 toks/s, output: 16.61 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:28<00:02, 16.31it/s, est. speed input: 17001.33 toks/s, output: 16.60 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:28<00:02, 16.32it/s, est. speed input: 16999.09 toks/s, output: 16.60 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:29<00:01, 16.33it/s, est. speed input: 16996.82 toks/s, output: 16.60 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:29<00:01, 16.25it/s, est. speed input: 16992.31 toks/s, output: 16.59 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:29<00:01, 16.24it/s, est. speed input: 16989.06 toks/s, output: 16.59 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:29<00:01, 16.24it/s, est. speed input: 16985.93 toks/s, output: 16.59 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:30<00:00, 16.31it/s, est. speed input: 16984.98 toks/s, output: 16.59 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:30<00:00, 16.17it/s, est. speed input: 16978.90 toks/s, output: 16.58 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:30<00:00, 16.17it/s, est. speed input: 16975.39 toks/s, output: 16.58 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:30<00:00, 17.59it/s, est. speed input: 17008.90 toks/s, output: 16.61 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:30<00:00, 17.59it/s, est. speed input: 17075.55 toks/s, output: 16.68 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:30<00:00, 16.68it/s, est. speed input: 17075.55 toks/s, output: 16.68 toks/s]
[rank0]:[W128 08:42:13.445482125 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 61.9s

测试结果:
  Requests/s:   16.17
  Tokens/s:     16570.28
  Total Reqs:   512
  Elapsed:      31.67s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     16554.12

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:42:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:42:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3704861) WARNING 01-28 08:42:45 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.75 requests/s, 16148.67 total tokens/s, 15.75 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 08:42:21] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:42:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:42:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:42:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:42:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:42:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:42:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:42:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:42:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:42:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:42:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:42:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:42:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:42:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:42:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:42:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:42:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:42:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:42:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:42:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:42:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:42:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:42:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:42:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:42:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:42:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:42:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:42:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3704861) [2026-01-28 08:42:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3704861) [2026-01-28 08:42:26] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3704861) [2026-01-28 08:42:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3704861) [2026-01-28 08:42:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3704861) [2026-01-28 08:42:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3704861) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3704861) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.66s/it]
(EngineCore_DP0 pid=3704861) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.66s/it]
(EngineCore_DP0 pid=3704861) 
(EngineCore_DP0 pid=3704861) 2026-01-28 08:42:44,170 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3704861) 2026-01-28 08:42:44,226 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:01, 627.94it/s]
Adding requests:  12%|█▏        | 126/1024 [00:00<00:01, 617.68it/s]
Adding requests:  18%|█▊        | 188/1024 [00:00<00:01, 557.55it/s]
Adding requests:  24%|██▍       | 245/1024 [00:00<00:01, 542.74it/s]
Adding requests:  29%|██▉       | 300/1024 [00:00<00:01, 526.04it/s]
Adding requests:  34%|███▍      | 353/1024 [00:00<00:01, 524.73it/s]
Adding requests:  40%|███▉      | 406/1024 [00:00<00:01, 513.08it/s]
Adding requests:  45%|████▍     | 458/1024 [00:00<00:01, 512.54it/s]
Adding requests:  50%|████▉     | 510/1024 [00:00<00:01, 508.30it/s]
Adding requests:  55%|█████▍    | 561/1024 [00:01<00:00, 489.10it/s]
Adding requests:  60%|█████▉    | 612/1024 [00:01<00:00, 493.52it/s]
Adding requests:  65%|██████▍   | 662/1024 [00:01<00:00, 486.19it/s]
Adding requests:  70%|██████▉   | 715/1024 [00:01<00:00, 498.01it/s]
Adding requests:  75%|███████▍  | 766/1024 [00:01<00:00, 499.24it/s]
Adding requests:  80%|███████▉  | 816/1024 [00:01<00:00, 495.51it/s]
Adding requests:  85%|████████▍ | 867/1024 [00:01<00:00, 497.89it/s]
Adding requests:  90%|████████▉ | 918/1024 [00:01<00:00, 499.83it/s]
Adding requests:  95%|█████████▍| 969/1024 [00:01<00:00, 501.66it/s]
Adding requests: 100%|█████████▉| 1022/1024 [00:01<00:00, 507.91it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 512.39it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 26/1024 [00:00<00:07, 138.75it/s, est. speed input: 142104.89 toks/s, output: 138.76 toks/s]
Processed prompts:   4%|▍         | 40/1024 [00:00<00:19, 50.48it/s, est. speed input: 59013.66 toks/s, output: 57.63 toks/s]   
Processed prompts:   5%|▍         | 48/1024 [00:01<00:30, 32.18it/s, est. speed input: 41079.16 toks/s, output: 40.12 toks/s]
Processed prompts:   5%|▌         | 53/1024 [00:01<00:43, 22.48it/s, est. speed input: 31820.96 toks/s, output: 31.08 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:02<00:54, 17.70it/s, est. speed input: 26873.70 toks/s, output: 26.24 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:02<00:56, 17.02it/s, est. speed input: 24872.89 toks/s, output: 24.29 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:03<00:57, 16.63it/s, est. speed input: 23519.33 toks/s, output: 22.97 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:03<00:57, 16.31it/s, est. speed input: 22492.78 toks/s, output: 21.97 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:04<00:57, 16.15it/s, est. speed input: 21738.97 toks/s, output: 21.23 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:04<00:57, 16.03it/s, est. speed input: 21142.07 toks/s, output: 20.65 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:05<00:57, 15.97it/s, est. speed input: 20669.45 toks/s, output: 20.18 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:05<00:56, 15.97it/s, est. speed input: 20294.10 toks/s, output: 19.82 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:06<00:56, 15.91it/s, est. speed input: 19957.45 toks/s, output: 19.49 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:06<00:56, 15.90it/s, est. speed input: 19682.99 toks/s, output: 19.22 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:07<00:55, 15.84it/s, est. speed input: 19430.95 toks/s, output: 18.98 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:07<00:55, 15.85it/s, est. speed input: 19224.09 toks/s, output: 18.77 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:08<00:55, 15.80it/s, est. speed input: 19030.43 toks/s, output: 18.58 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:08<00:54, 15.84it/s, est. speed input: 18874.21 toks/s, output: 18.43 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:09<00:54, 15.78it/s, est. speed input: 18716.71 toks/s, output: 18.28 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:09<00:53, 15.82it/s, est. speed input: 18592.12 toks/s, output: 18.16 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:10<00:53, 15.80it/s, est. speed input: 18471.90 toks/s, output: 18.04 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:10<00:52, 15.83it/s, est. speed input: 18369.65 toks/s, output: 17.94 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:11<00:52, 15.80it/s, est. speed input: 18268.42 toks/s, output: 17.84 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:11<00:51, 15.82it/s, est. speed input: 18182.59 toks/s, output: 17.76 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:12<00:51, 15.77it/s, est. speed input: 18093.25 toks/s, output: 17.67 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:12<00:50, 15.81it/s, est. speed input: 18022.67 toks/s, output: 17.60 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:13<00:50, 15.80it/s, est. speed input: 17951.15 toks/s, output: 17.53 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:13<00:49, 15.81it/s, est. speed input: 17888.61 toks/s, output: 17.47 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:14<00:49, 15.78it/s, est. speed input: 17824.08 toks/s, output: 17.41 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:14<00:48, 15.79it/s, est. speed input: 17768.60 toks/s, output: 17.35 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:15<00:48, 15.76it/s, est. speed input: 17712.27 toks/s, output: 17.30 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:15<00:47, 15.77it/s, est. speed input: 17663.20 toks/s, output: 17.25 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:16<00:47, 15.73it/s, est. speed input: 17611.99 toks/s, output: 17.20 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:16<00:46, 15.74it/s, est. speed input: 17567.90 toks/s, output: 17.16 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:17<00:46, 15.76it/s, est. speed input: 17527.41 toks/s, output: 17.12 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:17<00:45, 15.72it/s, est. speed input: 17483.88 toks/s, output: 17.07 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:18<00:45, 15.74it/s, est. speed input: 17447.71 toks/s, output: 17.04 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:18<00:44, 15.71it/s, est. speed input: 17408.56 toks/s, output: 17.00 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:19<00:44, 15.74it/s, est. speed input: 17377.41 toks/s, output: 16.97 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:19<00:42, 16.24it/s, est. speed input: 17391.05 toks/s, output: 16.98 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:20<00:42, 16.12it/s, est. speed input: 17362.37 toks/s, output: 16.96 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:20<00:41, 15.98it/s, est. speed input: 17330.14 toks/s, output: 16.92 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:21<00:41, 15.94it/s, est. speed input: 17303.59 toks/s, output: 16.90 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:21<00:41, 15.87it/s, est. speed input: 17275.16 toks/s, output: 16.87 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:22<00:40, 15.85it/s, est. speed input: 17250.41 toks/s, output: 16.85 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:22<00:40, 15.78it/s, est. speed input: 17223.01 toks/s, output: 16.82 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:23<00:39, 15.80it/s, est. speed input: 17201.22 toks/s, output: 16.80 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:23<00:39, 15.76it/s, est. speed input: 17176.65 toks/s, output: 16.77 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:24<00:38, 15.76it/s, est. speed input: 17155.31 toks/s, output: 16.75 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:24<00:38, 15.77it/s, est. speed input: 17135.02 toks/s, output: 16.73 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:25<00:37, 15.78it/s, est. speed input: 17116.45 toks/s, output: 16.72 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:25<00:37, 15.74it/s, est. speed input: 17094.53 toks/s, output: 16.69 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:26<00:36, 15.76it/s, est. speed input: 17077.25 toks/s, output: 16.68 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:26<00:35, 16.36it/s, est. speed input: 17099.11 toks/s, output: 16.70 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:27<00:35, 16.14it/s, est. speed input: 17078.82 toks/s, output: 16.68 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:27<00:34, 16.05it/s, est. speed input: 17063.61 toks/s, output: 16.66 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:28<00:34, 15.97it/s, est. speed input: 17047.43 toks/s, output: 16.65 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:28<00:34, 15.92it/s, est. speed input: 17032.73 toks/s, output: 16.63 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:29<00:33, 15.87it/s, est. speed input: 17017.24 toks/s, output: 16.62 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:30<00:33, 15.60it/s, est. speed input: 16987.51 toks/s, output: 16.59 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:30<00:35, 14.50it/s, est. speed input: 16899.13 toks/s, output: 16.50 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:31<00:34, 14.87it/s, est. speed input: 16887.72 toks/s, output: 16.49 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:31<00:33, 15.10it/s, est. speed input: 16874.15 toks/s, output: 16.48 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:32<00:32, 15.35it/s, est. speed input: 16865.73 toks/s, output: 16.47 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:32<00:31, 15.44it/s, est. speed input: 16852.69 toks/s, output: 16.46 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:33<00:30, 15.55it/s, est. speed input: 16842.59 toks/s, output: 16.45 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:33<00:30, 15.58it/s, est. speed input: 16830.39 toks/s, output: 16.44 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:34<00:29, 15.66it/s, est. speed input: 16821.50 toks/s, output: 16.43 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:34<00:28, 15.66it/s, est. speed input: 16809.66 toks/s, output: 16.42 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:35<00:28, 15.72it/s, est. speed input: 16801.85 toks/s, output: 16.41 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:35<00:27, 15.70it/s, est. speed input: 16790.49 toks/s, output: 16.40 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:36<00:27, 15.76it/s, est. speed input: 16783.53 toks/s, output: 16.39 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:36<00:26, 15.73it/s, est. speed input: 16773.11 toks/s, output: 16.38 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:37<00:26, 15.75it/s, est. speed input: 16765.03 toks/s, output: 16.37 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:37<00:25, 15.76it/s, est. speed input: 16756.93 toks/s, output: 16.36 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:38<00:25, 15.76it/s, est. speed input: 16748.74 toks/s, output: 16.36 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:38<00:24, 15.73it/s, est. speed input: 16739.45 toks/s, output: 16.35 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:39<00:24, 15.75it/s, est. speed input: 16732.11 toks/s, output: 16.34 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:39<00:23, 15.72it/s, est. speed input: 16722.87 toks/s, output: 16.33 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:40<00:23, 15.75it/s, est. speed input: 16716.64 toks/s, output: 16.32 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:40<00:22, 15.74it/s, est. speed input: 16708.56 toks/s, output: 16.32 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:41<00:22, 15.72it/s, est. speed input: 16700.68 toks/s, output: 16.31 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:41<00:21, 15.75it/s, est. speed input: 16694.63 toks/s, output: 16.30 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:42<00:21, 15.72it/s, est. speed input: 16686.30 toks/s, output: 16.30 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:42<00:20, 15.74it/s, est. speed input: 16680.17 toks/s, output: 16.29 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:43<00:20, 15.71it/s, est. speed input: 16672.38 toks/s, output: 16.28 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:43<00:19, 15.75it/s, est. speed input: 16667.28 toks/s, output: 16.28 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:44<00:19, 15.69it/s, est. speed input: 16658.75 toks/s, output: 16.27 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:44<00:18, 15.74it/s, est. speed input: 16653.93 toks/s, output: 16.26 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:45<00:18, 15.70it/s, est. speed input: 16646.54 toks/s, output: 16.26 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:45<00:17, 15.73it/s, est. speed input: 16641.38 toks/s, output: 16.25 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:46<00:17, 15.73it/s, est. speed input: 16635.25 toks/s, output: 16.25 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:46<00:16, 15.79it/s, est. speed input: 16631.98 toks/s, output: 16.24 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:47<00:16, 15.74it/s, est. speed input: 16625.04 toks/s, output: 16.24 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:47<00:15, 15.78it/s, est. speed input: 16621.19 toks/s, output: 16.23 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:48<00:15, 15.72it/s, est. speed input: 16614.27 toks/s, output: 16.22 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:48<00:14, 15.76it/s, est. speed input: 16610.24 toks/s, output: 16.22 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:49<00:14, 15.76it/s, est. speed input: 16605.33 toks/s, output: 16.22 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:49<00:13, 15.77it/s, est. speed input: 16601.16 toks/s, output: 16.21 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:50<00:13, 15.72it/s, est. speed input: 16594.75 toks/s, output: 16.21 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:50<00:12, 15.76it/s, est. speed input: 16591.04 toks/s, output: 16.20 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:51<00:12, 15.74it/s, est. speed input: 16585.96 toks/s, output: 16.20 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:51<00:11, 15.73it/s, est. speed input: 16581.21 toks/s, output: 16.19 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:52<00:11, 15.77it/s, est. speed input: 16577.71 toks/s, output: 16.19 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:53<00:10, 15.74it/s, est. speed input: 16572.83 toks/s, output: 16.18 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:53<00:10, 15.78it/s, est. speed input: 16569.66 toks/s, output: 16.18 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:54<00:09, 15.76it/s, est. speed input: 16565.12 toks/s, output: 16.18 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:54<00:08, 15.79it/s, est. speed input: 16562.13 toks/s, output: 16.17 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:55<00:08, 15.74it/s, est. speed input: 16556.81 toks/s, output: 16.17 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:55<00:07, 15.76it/s, est. speed input: 16553.43 toks/s, output: 16.17 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:56<00:07, 15.73it/s, est. speed input: 16548.70 toks/s, output: 16.16 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:56<00:06, 15.76it/s, est. speed input: 16545.59 toks/s, output: 16.16 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:57<00:06, 15.70it/s, est. speed input: 16540.22 toks/s, output: 16.15 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:57<00:05, 15.74it/s, est. speed input: 16537.46 toks/s, output: 16.15 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:58<00:05, 16.28it/s, est. speed input: 16549.60 toks/s, output: 16.16 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:58<00:04, 16.12it/s, est. speed input: 16546.05 toks/s, output: 16.16 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:59<00:04, 15.96it/s, est. speed input: 16541.17 toks/s, output: 16.15 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:59<00:03, 15.93it/s, est. speed input: 16538.45 toks/s, output: 16.15 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [01:00<00:03, 15.83it/s, est. speed input: 16533.86 toks/s, output: 16.15 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [01:00<00:02, 15.82it/s, est. speed input: 16530.76 toks/s, output: 16.14 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [01:01<00:02, 16.41it/s, est. speed input: 16544.55 toks/s, output: 16.16 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [01:01<00:01, 16.18it/s, est. speed input: 16540.28 toks/s, output: 16.15 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [01:02<00:01, 16.06it/s, est. speed input: 16537.30 toks/s, output: 16.15 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [01:02<00:00, 15.94it/s, est. speed input: 16533.12 toks/s, output: 16.15 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [01:02<00:00, 16.55it/s, est. speed input: 16547.65 toks/s, output: 16.16 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:02<00:00, 16.55it/s, est. speed input: 16645.15 toks/s, output: 16.26 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:02<00:00, 16.26it/s, est. speed input: 16645.15 toks/s, output: 16.26 toks/s]
[rank0]:[W128 08:43:50.361432478 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 96.9s

测试结果:
  Requests/s:   15.75
  Tokens/s:     16148.67
  Total Reqs:   1024
  Elapsed:      65.00s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     16132.92

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:44:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:44:01 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3706435) WARNING 01-28 08:44:26 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.86 requests/s, 16260.24 total tokens/s, 15.86 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 08:44:01] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:44:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:44:01] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:44:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:44:01] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:44:01] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:44:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:44:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:44:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:44:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:44:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:44:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:44:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:44:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:44:05] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:44:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:44:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:44:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:44:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:44:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:44:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:44:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:44:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:44:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:44:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:44:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:44:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:44:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3706435) [2026-01-28 08:44:06] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3706435) [2026-01-28 08:44:06] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3706435) [2026-01-28 08:44:06] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3706435) [2026-01-28 08:44:06] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3706435) [2026-01-28 08:44:06] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3706435) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3706435) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.70s/it]
(EngineCore_DP0 pid=3706435) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.70s/it]
(EngineCore_DP0 pid=3706435) 
(EngineCore_DP0 pid=3706435) 2026-01-28 08:44:24,563 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3706435) 2026-01-28 08:44:24,647 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 59/2048 [00:00<00:03, 584.54it/s]
Adding requests:   6%|▌         | 118/2048 [00:00<00:03, 551.33it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:03, 506.98it/s]
Adding requests:  11%|█         | 226/2048 [00:00<00:03, 500.40it/s]
Adding requests:  14%|█▎        | 277/2048 [00:00<00:03, 498.24it/s]
Adding requests:  16%|█▌        | 327/2048 [00:00<00:03, 494.01it/s]
Adding requests:  18%|█▊        | 378/2048 [00:00<00:03, 496.99it/s]
Adding requests:  21%|██        | 428/2048 [00:00<00:03, 494.97it/s]
Adding requests:  23%|██▎       | 478/2048 [00:00<00:03, 484.85it/s]
Adding requests:  26%|██▌       | 527/2048 [00:01<00:03, 474.74it/s]
Adding requests:  28%|██▊       | 579/2048 [00:01<00:03, 483.78it/s]
Adding requests:  31%|███       | 628/2048 [00:01<00:02, 480.05it/s]
Adding requests:  33%|███▎      | 678/2048 [00:01<00:02, 485.78it/s]
Adding requests:  36%|███▌      | 729/2048 [00:01<00:02, 492.21it/s]
Adding requests:  38%|███▊      | 779/2048 [00:01<00:02, 465.87it/s]
Adding requests:  40%|████      | 826/2048 [00:01<00:02, 462.15it/s]
Adding requests:  43%|████▎     | 873/2048 [00:02<00:07, 167.74it/s]
Adding requests:  45%|████▍     | 921/2048 [00:02<00:05, 207.83it/s]
Adding requests:  47%|████▋     | 971/2048 [00:02<00:04, 252.74it/s]
Adding requests:  50%|████▉     | 1022/2048 [00:02<00:03, 298.63it/s]
Adding requests:  52%|█████▏    | 1070/2048 [00:02<00:02, 334.95it/s]
Adding requests:  55%|█████▍    | 1119/2048 [00:02<00:02, 368.74it/s]
Adding requests:  57%|█████▋    | 1169/2048 [00:03<00:02, 400.27it/s]
Adding requests:  60%|█████▉    | 1221/2048 [00:03<00:01, 430.28it/s]
Adding requests:  62%|██████▏   | 1270/2048 [00:03<00:01, 436.46it/s]
Adding requests:  64%|██████▍   | 1318/2048 [00:03<00:01, 446.59it/s]
Adding requests:  67%|██████▋   | 1366/2048 [00:03<00:01, 453.66it/s]
Adding requests:  69%|██████▉   | 1415/2048 [00:03<00:01, 461.82it/s]
Adding requests:  71%|███████▏  | 1463/2048 [00:03<00:01, 465.37it/s]
Adding requests:  74%|███████▍  | 1512/2048 [00:03<00:01, 470.15it/s]
Adding requests:  76%|███████▌  | 1561/2048 [00:03<00:01, 473.23it/s]
Adding requests:  79%|███████▊  | 1611/2048 [00:03<00:00, 478.55it/s]
Adding requests:  81%|████████  | 1660/2048 [00:04<00:00, 481.71it/s]
Adding requests:  83%|████████▎ | 1710/2048 [00:04<00:00, 486.48it/s]
Adding requests:  86%|████████▌ | 1759/2048 [00:04<00:00, 484.23it/s]
Adding requests:  88%|████████▊ | 1808/2048 [00:04<00:00, 482.50it/s]
Adding requests:  91%|█████████ | 1857/2048 [00:04<00:00, 483.80it/s]
Adding requests:  93%|█████████▎| 1906/2048 [00:04<00:00, 485.38it/s]
Adding requests:  95%|█████████▌| 1955/2048 [00:04<00:00, 480.83it/s]
Adding requests:  98%|█████████▊| 2007/2048 [00:04<00:00, 489.14it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 421.80it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 66/2048 [00:00<00:10, 187.55it/s, est. speed input: 192090.82 toks/s, output: 187.56 toks/s]
Processed prompts:   4%|▍         | 85/2048 [00:01<00:37, 51.93it/s, est. speed input: 63952.54 toks/s, output: 62.45 toks/s]   
Processed prompts:   5%|▍         | 98/2048 [00:02<01:02, 31.06it/s, est. speed input: 42335.33 toks/s, output: 41.34 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:03<01:18, 24.48it/s, est. speed input: 34549.31 toks/s, output: 33.74 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:04<01:30, 21.18it/s, est. speed input: 30331.60 toks/s, output: 29.62 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:05<01:38, 19.29it/s, est. speed input: 27693.19 toks/s, output: 27.04 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:06<01:43, 18.17it/s, est. speed input: 25905.00 toks/s, output: 25.30 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:07<01:47, 17.43it/s, est. speed input: 24594.39 toks/s, output: 24.02 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:08<01:49, 16.95it/s, est. speed input: 23605.83 toks/s, output: 23.05 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:09<01:50, 16.63it/s, est. speed input: 22824.83 toks/s, output: 22.29 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:10<01:51, 16.39it/s, est. speed input: 22187.43 toks/s, output: 21.67 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:11<01:51, 16.22it/s, est. speed input: 21659.80 toks/s, output: 21.15 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:12<01:51, 16.10it/s, est. speed input: 21218.49 toks/s, output: 20.72 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:13<01:50, 16.03it/s, est. speed input: 20843.73 toks/s, output: 20.36 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:14<01:50, 15.97it/s, est. speed input: 20521.09 toks/s, output: 20.04 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:15<01:49, 15.96it/s, est. speed input: 20247.96 toks/s, output: 19.77 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:16<01:48, 15.96it/s, est. speed input: 20011.26 toks/s, output: 19.54 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:17<01:45, 16.23it/s, est. speed input: 19863.55 toks/s, output: 19.40 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:18<01:45, 16.09it/s, est. speed input: 19659.64 toks/s, output: 19.20 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:19<01:44, 16.00it/s, est. speed input: 19478.30 toks/s, output: 19.02 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:20<01:44, 15.97it/s, est. speed input: 19319.97 toks/s, output: 18.87 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:21<01:43, 15.91it/s, est. speed input: 19171.44 toks/s, output: 18.72 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:22<01:42, 15.90it/s, est. speed input: 19040.60 toks/s, output: 18.59 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:23<01:41, 15.87it/s, est. speed input: 18916.84 toks/s, output: 18.47 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:24<01:38, 16.17it/s, est. speed input: 18855.20 toks/s, output: 18.41 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:25<01:38, 16.06it/s, est. speed input: 18749.58 toks/s, output: 18.31 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:26<01:37, 16.00it/s, est. speed input: 18653.13 toks/s, output: 18.22 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:27<01:42, 15.08it/s, est. speed input: 18435.11 toks/s, output: 18.00 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:28<01:40, 15.30it/s, est. speed input: 18357.11 toks/s, output: 17.93 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:29<01:38, 15.46it/s, est. speed input: 18284.73 toks/s, output: 17.86 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:30<01:36, 15.56it/s, est. speed input: 18215.33 toks/s, output: 17.79 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:31<01:34, 15.65it/s, est. speed input: 18152.78 toks/s, output: 17.73 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:32<01:33, 15.70it/s, est. speed input: 18091.62 toks/s, output: 17.67 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:33<01:32, 15.73it/s, est. speed input: 18034.65 toks/s, output: 17.61 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:34<01:31, 15.76it/s, est. speed input: 17981.46 toks/s, output: 17.56 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:35<01:30, 15.77it/s, est. speed input: 17930.70 toks/s, output: 17.51 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:36<01:29, 15.79it/s, est. speed input: 17883.09 toks/s, output: 17.46 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:37<01:27, 15.82it/s, est. speed input: 17840.65 toks/s, output: 17.42 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:38<01:26, 15.84it/s, est. speed input: 17799.61 toks/s, output: 17.38 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:39<01:25, 15.84it/s, est. speed input: 17759.44 toks/s, output: 17.34 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:40<01:24, 15.82it/s, est. speed input: 17719.75 toks/s, output: 17.30 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:41<01:23, 15.84it/s, est. speed input: 17684.68 toks/s, output: 17.27 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:42<01:22, 15.83it/s, est. speed input: 17649.48 toks/s, output: 17.24 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:43<01:21, 15.84it/s, est. speed input: 17616.96 toks/s, output: 17.20 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:44<01:20, 15.84it/s, est. speed input: 17585.22 toks/s, output: 17.17 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:45<01:19, 15.82it/s, est. speed input: 17553.89 toks/s, output: 17.14 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:46<01:18, 15.83it/s, est. speed input: 17525.07 toks/s, output: 17.11 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:47<01:17, 15.81it/s, est. speed input: 17496.09 toks/s, output: 17.09 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:48<01:16, 15.83it/s, est. speed input: 17470.21 toks/s, output: 17.06 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:49<01:15, 15.83it/s, est. speed input: 17444.87 toks/s, output: 17.04 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:50<01:14, 15.82it/s, est. speed input: 17419.38 toks/s, output: 17.01 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:51<01:13, 15.82it/s, est. speed input: 17395.99 toks/s, output: 16.99 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:52<01:12, 15.83it/s, est. speed input: 17373.63 toks/s, output: 16.97 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:53<01:11, 15.84it/s, est. speed input: 17352.52 toks/s, output: 16.95 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:54<01:09, 16.15it/s, est. speed input: 17352.27 toks/s, output: 16.95 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:55<01:08, 16.09it/s, est. speed input: 17334.19 toks/s, output: 16.93 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:56<01:07, 16.00it/s, est. speed input: 17313.41 toks/s, output: 16.91 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:57<01:05, 16.29it/s, est. speed input: 17314.76 toks/s, output: 16.91 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:58<01:05, 16.16it/s, est. speed input: 17296.72 toks/s, output: 16.89 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:59<01:06, 15.53it/s, est. speed input: 17245.97 toks/s, output: 16.84 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [01:00<01:05, 15.61it/s, est. speed input: 17228.17 toks/s, output: 16.82 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [01:01<01:04, 15.68it/s, est. speed input: 17211.83 toks/s, output: 16.81 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [01:03<01:02, 15.73it/s, est. speed input: 17196.14 toks/s, output: 16.79 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [01:04<01:01, 15.75it/s, est. speed input: 17180.20 toks/s, output: 16.78 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [01:05<01:00, 15.77it/s, est. speed input: 17164.77 toks/s, output: 16.76 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [01:06<00:59, 15.78it/s, est. speed input: 17149.78 toks/s, output: 16.75 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [01:07<00:58, 15.79it/s, est. speed input: 17135.17 toks/s, output: 16.73 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [01:08<00:57, 15.79it/s, est. speed input: 17121.06 toks/s, output: 16.72 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [01:09<00:55, 16.13it/s, est. speed input: 17124.38 toks/s, output: 16.72 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [01:10<00:54, 16.01it/s, est. speed input: 17109.90 toks/s, output: 16.71 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [01:11<00:53, 15.97it/s, est. speed input: 17097.65 toks/s, output: 16.70 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [01:12<00:53, 15.92it/s, est. speed input: 17084.91 toks/s, output: 16.68 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [01:13<00:52, 15.90it/s, est. speed input: 17073.40 toks/s, output: 16.67 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [01:14<00:51, 15.90it/s, est. speed input: 17062.66 toks/s, output: 16.66 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [01:15<00:50, 15.88it/s, est. speed input: 17051.16 toks/s, output: 16.65 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [01:16<00:48, 16.17it/s, est. speed input: 17054.00 toks/s, output: 16.65 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [01:17<00:47, 16.06it/s, est. speed input: 17042.68 toks/s, output: 16.64 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [01:17<00:45, 16.32it/s, est. speed input: 17046.85 toks/s, output: 16.65 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [01:18<00:45, 16.18it/s, est. speed input: 17036.56 toks/s, output: 16.64 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [01:19<00:44, 16.07it/s, est. speed input: 17025.89 toks/s, output: 16.63 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [01:21<00:43, 15.99it/s, est. speed input: 17015.20 toks/s, output: 16.62 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [01:22<00:43, 15.93it/s, est. speed input: 17005.05 toks/s, output: 16.61 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [01:23<00:42, 15.88it/s, est. speed input: 16994.48 toks/s, output: 16.60 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [01:24<00:41, 15.85it/s, est. speed input: 16984.57 toks/s, output: 16.59 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [01:25<00:40, 15.83it/s, est. speed input: 16974.79 toks/s, output: 16.58 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [01:26<00:39, 15.84it/s, est. speed input: 16965.95 toks/s, output: 16.57 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [01:27<00:38, 15.81it/s, est. speed input: 16955.98 toks/s, output: 16.56 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [01:28<00:37, 15.83it/s, est. speed input: 16948.03 toks/s, output: 16.55 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [01:29<00:36, 15.82it/s, est. speed input: 16939.18 toks/s, output: 16.54 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [01:30<00:35, 15.80it/s, est. speed input: 16930.37 toks/s, output: 16.53 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [01:31<00:34, 15.81it/s, est. speed input: 16922.45 toks/s, output: 16.53 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [01:32<00:34, 15.26it/s, est. speed input: 16892.04 toks/s, output: 16.50 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [01:33<00:33, 15.42it/s, est. speed input: 16884.22 toks/s, output: 16.49 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [01:34<00:31, 15.55it/s, est. speed input: 16877.49 toks/s, output: 16.48 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [01:35<00:30, 15.65it/s, est. speed input: 16870.99 toks/s, output: 16.48 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [01:36<00:28, 16.00it/s, est. speed input: 16875.29 toks/s, output: 16.48 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [01:37<00:27, 15.94it/s, est. speed input: 16867.91 toks/s, output: 16.47 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [01:38<00:27, 15.91it/s, est. speed input: 16861.10 toks/s, output: 16.47 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [01:39<00:26, 15.87it/s, est. speed input: 16854.00 toks/s, output: 16.46 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [01:40<00:24, 16.15it/s, est. speed input: 16857.60 toks/s, output: 16.46 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [01:41<00:23, 16.03it/s, est. speed input: 16850.53 toks/s, output: 16.46 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [01:42<00:22, 15.97it/s, est. speed input: 16844.29 toks/s, output: 16.45 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [01:43<00:21, 15.92it/s, est. speed input: 16837.78 toks/s, output: 16.44 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [01:44<00:21, 15.87it/s, est. speed input: 16830.95 toks/s, output: 16.44 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [01:45<00:20, 15.87it/s, est. speed input: 16825.41 toks/s, output: 16.43 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [01:46<00:19, 15.84it/s, est. speed input: 16819.01 toks/s, output: 16.42 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [01:47<00:18, 15.84it/s, est. speed input: 16813.14 toks/s, output: 16.42 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [01:48<00:17, 15.82it/s, est. speed input: 16807.24 toks/s, output: 16.41 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [01:49<00:16, 15.83it/s, est. speed input: 16801.85 toks/s, output: 16.41 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [01:50<00:15, 15.82it/s, est. speed input: 16796.12 toks/s, output: 16.40 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [01:51<00:14, 15.83it/s, est. speed input: 16790.86 toks/s, output: 16.40 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [01:52<00:13, 15.82it/s, est. speed input: 16785.53 toks/s, output: 16.39 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [01:53<00:11, 15.84it/s, est. speed input: 16780.90 toks/s, output: 16.39 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [01:54<00:10, 16.15it/s, est. speed input: 16785.47 toks/s, output: 16.39 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [01:55<00:09, 16.05it/s, est. speed input: 16780.43 toks/s, output: 16.39 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [01:56<00:08, 15.99it/s, est. speed input: 16775.53 toks/s, output: 16.38 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [01:57<00:07, 15.93it/s, est. speed input: 16770.37 toks/s, output: 16.38 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [01:58<00:06, 15.89it/s, est. speed input: 16765.25 toks/s, output: 16.37 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [01:59<00:05, 16.19it/s, est. speed input: 16769.89 toks/s, output: 16.38 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [02:00<00:04, 16.06it/s, est. speed input: 16764.51 toks/s, output: 16.37 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [02:01<00:03, 16.30it/s, est. speed input: 16768.70 toks/s, output: 16.38 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [02:02<00:02, 16.14it/s, est. speed input: 16763.48 toks/s, output: 16.37 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [02:03<00:01, 16.06it/s, est. speed input: 16759.34 toks/s, output: 16.37 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [02:04<00:00, 16.32it/s, est. speed input: 16764.03 toks/s, output: 16.37 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:04<00:00, 16.32it/s, est. speed input: 16879.39 toks/s, output: 16.48 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:04<00:00, 16.48it/s, est. speed input: 16879.39 toks/s, output: 16.48 toks/s]
[rank0]:[W128 08:46:35.422479112 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 165.0s

测试结果:
  Requests/s:   15.86
  Tokens/s:     16260.24
  Total Reqs:   2048
  Elapsed:      129.10s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     16244.37

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:46:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:46:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3708956) WARNING 01-28 08:47:18 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.98 requests/s, 16378.41 total tokens/s, 15.98 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 08:46:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:46:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:46:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:46:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:46:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:46:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:46:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:46:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:46:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:46:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:46:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:46:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:46:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:46:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:46:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:46:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:46:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:46:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:46:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:46:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:46:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:46:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:46:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:46:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:46:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:46:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:46:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:46:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3708956) [2026-01-28 08:46:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3708956) [2026-01-28 08:46:56] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3708956) [2026-01-28 08:46:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3708956) [2026-01-28 08:46:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3708956) [2026-01-28 08:46:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3708956) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3708956) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.52s/it]
(EngineCore_DP0 pid=3708956) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.52s/it]
(EngineCore_DP0 pid=3708956) 
(EngineCore_DP0 pid=3708956) 2026-01-28 08:47:16,053 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3708956) 2026-01-28 08:47:16,200 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 61/4096 [00:00<00:06, 607.96it/s]
Adding requests:   3%|▎         | 122/4096 [00:00<00:07, 556.28it/s]
Adding requests:   4%|▍         | 178/4096 [00:00<00:07, 524.95it/s]
Adding requests:   6%|▌         | 231/4096 [00:00<00:07, 516.43it/s]
Adding requests:   7%|▋         | 283/4096 [00:00<00:07, 500.64it/s]
Adding requests:   8%|▊         | 334/4096 [00:00<00:07, 491.97it/s]
Adding requests:   9%|▉         | 386/4096 [00:00<00:07, 498.24it/s]
Adding requests:  11%|█         | 436/4096 [00:00<00:07, 498.44it/s]
Adding requests:  12%|█▏        | 487/4096 [00:00<00:07, 499.48it/s]
Adding requests:  13%|█▎        | 537/4096 [00:01<00:07, 489.46it/s]
Adding requests:  14%|█▍        | 589/4096 [00:01<00:07, 498.42it/s]
Adding requests:  16%|█▌        | 639/4096 [00:01<00:06, 494.69it/s]
Adding requests:  17%|█▋        | 690/4096 [00:01<00:06, 498.83it/s]
Adding requests:  18%|█▊        | 740/4096 [00:01<00:06, 496.25it/s]
Adding requests:  19%|█▉        | 791/4096 [00:01<00:06, 498.31it/s]
Adding requests:  21%|██        | 841/4096 [00:01<00:06, 485.19it/s]
Adding requests:  22%|██▏       | 894/4096 [00:01<00:06, 496.23it/s]
Adding requests:  23%|██▎       | 944/4096 [00:01<00:06, 491.86it/s]
Adding requests:  24%|██▍       | 995/4096 [00:01<00:06, 496.91it/s]
Adding requests:  26%|██▌       | 1046/4096 [00:02<00:06, 500.54it/s]
Adding requests:  27%|██▋       | 1097/4096 [00:02<00:06, 494.35it/s]
Adding requests:  28%|██▊       | 1147/4096 [00:02<00:06, 479.50it/s]
Adding requests:  29%|██▉       | 1198/4096 [00:02<00:05, 486.81it/s]
Adding requests:  30%|███       | 1247/4096 [00:02<00:05, 487.31it/s]
Adding requests:  32%|███▏      | 1296/4096 [00:02<00:05, 472.05it/s]
Adding requests:  33%|███▎      | 1346/4096 [00:02<00:05, 478.45it/s]
Adding requests:  34%|███▍      | 1397/4096 [00:02<00:05, 486.65it/s]
Adding requests:  35%|███▌      | 1447/4096 [00:02<00:05, 489.76it/s]
Adding requests:  37%|███▋      | 1500/4096 [00:03<00:05, 500.80it/s]
Adding requests:  38%|███▊      | 1552/4096 [00:03<00:05, 505.49it/s]
Adding requests:  39%|███▉      | 1604/4096 [00:03<00:04, 508.75it/s]
Adding requests:  40%|████      | 1655/4096 [00:03<00:04, 506.87it/s]
Adding requests:  42%|████▏     | 1706/4096 [00:03<00:04, 500.75it/s]
Adding requests:  43%|████▎     | 1757/4096 [00:03<00:04, 496.04it/s]
Adding requests:  44%|████▍     | 1807/4096 [00:03<00:04, 495.54it/s]
Adding requests:  45%|████▌     | 1857/4096 [00:03<00:04, 493.80it/s]
Adding requests:  47%|████▋     | 1907/4096 [00:03<00:04, 487.61it/s]
Adding requests:  48%|████▊     | 1956/4096 [00:03<00:04, 485.42it/s]
Adding requests:  49%|████▉     | 2005/4096 [00:04<00:04, 484.63it/s]
Adding requests:  50%|█████     | 2054/4096 [00:04<00:04, 475.54it/s]
Adding requests:  51%|█████▏    | 2103/4096 [00:04<00:04, 479.30it/s]
Adding requests:  53%|█████▎    | 2151/4096 [00:04<00:04, 468.57it/s]
Adding requests:  54%|█████▎    | 2198/4096 [00:04<00:04, 465.60it/s]
Adding requests:  55%|█████▍    | 2248/4096 [00:04<00:03, 474.94it/s]
Adding requests:  56%|█████▌    | 2297/4096 [00:04<00:03, 479.21it/s]
Adding requests:  57%|█████▋    | 2347/4096 [00:04<00:03, 482.73it/s]
Adding requests:  58%|█████▊    | 2396/4096 [00:04<00:03, 484.86it/s]
Adding requests:  60%|█████▉    | 2445/4096 [00:04<00:03, 483.78it/s]
Adding requests:  61%|██████    | 2494/4096 [00:05<00:03, 481.28it/s]
Adding requests:  62%|██████▏   | 2543/4096 [00:05<00:03, 455.99it/s]
Adding requests:  63%|██████▎   | 2591/4096 [00:05<00:03, 460.91it/s]
Adding requests:  64%|██████▍   | 2641/4096 [00:05<00:03, 470.04it/s]
Adding requests:  66%|██████▌   | 2689/4096 [00:05<00:02, 469.76it/s]
Adding requests:  67%|██████▋   | 2738/4096 [00:05<00:02, 474.78it/s]
Adding requests:  68%|██████▊   | 2786/4096 [00:05<00:02, 469.37it/s]
Adding requests:  69%|██████▉   | 2836/4096 [00:05<00:02, 477.72it/s]
Adding requests:  70%|███████   | 2886/4096 [00:05<00:02, 481.70it/s]
Adding requests:  72%|███████▏  | 2935/4096 [00:06<00:02, 472.23it/s]
Adding requests:  73%|███████▎  | 2985/4096 [00:06<00:02, 477.31it/s]
Adding requests:  74%|███████▍  | 3033/4096 [00:06<00:02, 476.62it/s]
Adding requests:  75%|███████▌  | 3081/4096 [00:06<00:02, 474.97it/s]
Adding requests:  76%|███████▋  | 3129/4096 [00:06<00:02, 472.79it/s]
Adding requests:  78%|███████▊  | 3178/4096 [00:06<00:01, 476.68it/s]
Adding requests:  79%|███████▉  | 3227/4096 [00:06<00:01, 478.13it/s]
Adding requests:  80%|████████  | 3277/4096 [00:06<00:01, 484.44it/s]
Adding requests:  81%|████████  | 3326/4096 [00:06<00:01, 485.75it/s]
Adding requests:  82%|████████▏ | 3375/4096 [00:06<00:01, 483.04it/s]
Adding requests:  84%|████████▎ | 3426/4096 [00:07<00:01, 488.56it/s]
Adding requests:  85%|████████▍ | 3475/4096 [00:07<00:01, 475.75it/s]
Adding requests:  86%|████████▌ | 3525/4096 [00:07<00:01, 482.64it/s]
Adding requests:  87%|████████▋ | 3574/4096 [00:07<00:01, 483.23it/s]
Adding requests:  88%|████████▊ | 3624/4096 [00:07<00:00, 486.49it/s]
Adding requests:  90%|████████▉ | 3676/4096 [00:07<00:00, 495.72it/s]
Adding requests:  91%|█████████ | 3726/4096 [00:07<00:00, 486.29it/s]
Adding requests:  92%|█████████▏| 3776/4096 [00:07<00:00, 488.94it/s]
Adding requests:  93%|█████████▎| 3825/4096 [00:07<00:00, 475.59it/s]
Adding requests:  95%|█████████▍| 3878/4096 [00:07<00:00, 489.48it/s]
Adding requests:  96%|█████████▌| 3928/4096 [00:08<00:00, 461.27it/s]
Adding requests:  97%|█████████▋| 3978/4096 [00:08<00:00, 472.04it/s]
Adding requests:  98%|█████████▊| 4026/4096 [00:08<00:00, 473.05it/s]
Adding requests:  99%|█████████▉| 4074/4096 [00:08<00:00, 469.08it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 485.71it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:01<00:56, 70.54it/s, est. speed input: 72235.67 toks/s, output: 70.54 toks/s]
Processed prompts:   4%|▍         | 162/4096 [00:03<01:45, 37.21it/s, est. speed input: 42992.92 toks/s, output: 41.98 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:05<02:22, 27.31it/s, est. speed input: 33848.82 toks/s, output: 33.06 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:07<02:50, 22.73it/s, est. speed input: 29346.24 toks/s, output: 28.66 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:09<03:09, 20.24it/s, est. speed input: 26681.53 toks/s, output: 26.06 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:11<03:23, 18.74it/s, est. speed input: 24915.50 toks/s, output: 24.33 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:13<03:29, 17.98it/s, est. speed input: 23766.56 toks/s, output: 23.21 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:15<03:36, 17.32it/s, est. speed input: 22821.84 toks/s, output: 22.29 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:17<03:39, 16.87it/s, est. speed input: 22082.69 toks/s, output: 21.57 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:19<03:42, 16.55it/s, est. speed input: 21488.55 toks/s, output: 20.98 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:21<03:41, 16.43it/s, est. speed input: 21040.99 toks/s, output: 20.55 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:23<03:42, 16.25it/s, est. speed input: 20633.58 toks/s, output: 20.15 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:25<03:42, 16.13it/s, est. speed input: 20292.42 toks/s, output: 19.82 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:27<03:41, 16.05it/s, est. speed input: 19999.64 toks/s, output: 19.53 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:29<03:40, 15.98it/s, est. speed input: 19742.60 toks/s, output: 19.28 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:31<03:38, 15.95it/s, est. speed input: 19524.94 toks/s, output: 19.07 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:34<03:37, 15.91it/s, est. speed input: 19327.42 toks/s, output: 18.87 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:36<03:35, 15.89it/s, est. speed input: 19153.86 toks/s, output: 18.70 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:38<03:33, 15.87it/s, est. speed input: 18997.32 toks/s, output: 18.55 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:40<03:31, 15.88it/s, est. speed input: 18859.60 toks/s, output: 18.42 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:42<03:29, 15.87it/s, est. speed input: 18733.86 toks/s, output: 18.29 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:44<03:27, 15.86it/s, est. speed input: 18619.30 toks/s, output: 18.18 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:46<03:25, 15.86it/s, est. speed input: 18513.96 toks/s, output: 18.08 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:48<03:23, 15.85it/s, est. speed input: 18417.69 toks/s, output: 17.99 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:50<03:21, 15.84it/s, est. speed input: 18328.30 toks/s, output: 17.90 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:52<03:17, 15.99it/s, est. speed input: 18269.08 toks/s, output: 17.84 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:54<03:14, 16.08it/s, est. speed input: 18211.57 toks/s, output: 17.78 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:56<03:13, 16.03it/s, est. speed input: 18142.52 toks/s, output: 17.72 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:58<03:12, 15.98it/s, est. speed input: 18076.79 toks/s, output: 17.65 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [01:00<03:10, 15.94it/s, est. speed input: 18014.57 toks/s, output: 17.59 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [01:02<03:08, 15.91it/s, est. speed input: 17956.24 toks/s, output: 17.54 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [01:04<03:07, 15.89it/s, est. speed input: 17902.11 toks/s, output: 17.48 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [01:06<03:03, 16.04it/s, est. speed input: 17869.70 toks/s, output: 17.45 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [01:08<03:02, 15.98it/s, est. speed input: 17820.01 toks/s, output: 17.40 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [01:10<03:00, 15.94it/s, est. speed input: 17774.31 toks/s, output: 17.36 toks/s]
Processed prompts:  31%|███       | 1250/4096 [01:12<02:57, 16.06it/s, est. speed input: 17746.71 toks/s, output: 17.33 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [01:14<02:53, 16.21it/s, est. speed input: 17726.40 toks/s, output: 17.31 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [01:16<02:52, 16.09it/s, est. speed input: 17685.65 toks/s, output: 17.27 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [01:18<02:51, 16.01it/s, est. speed input: 17647.46 toks/s, output: 17.23 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [01:20<02:50, 15.94it/s, est. speed input: 17609.71 toks/s, output: 17.20 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [01:22<02:48, 15.92it/s, est. speed input: 17576.05 toks/s, output: 17.16 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [01:24<02:47, 15.89it/s, est. speed input: 17543.06 toks/s, output: 17.13 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [01:26<02:45, 15.88it/s, est. speed input: 17512.25 toks/s, output: 17.10 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [01:28<02:43, 15.87it/s, est. speed input: 17483.46 toks/s, output: 17.07 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [01:30<02:41, 15.87it/s, est. speed input: 17455.93 toks/s, output: 17.05 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [01:32<02:37, 16.01it/s, est. speed input: 17440.72 toks/s, output: 17.03 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [01:34<02:36, 15.97it/s, est. speed input: 17415.06 toks/s, output: 17.01 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [01:36<02:32, 16.10it/s, est. speed input: 17402.70 toks/s, output: 16.99 toks/s]
Processed prompts:  41%|████      | 1666/4096 [01:38<02:31, 16.02it/s, est. speed input: 17378.67 toks/s, output: 16.97 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [01:40<02:30, 15.97it/s, est. speed input: 17355.45 toks/s, output: 16.95 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [01:42<02:28, 15.93it/s, est. speed input: 17333.15 toks/s, output: 16.93 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [01:44<02:26, 15.90it/s, est. speed input: 17311.42 toks/s, output: 16.91 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [01:46<02:24, 15.90it/s, est. speed input: 17291.58 toks/s, output: 16.89 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [01:48<02:22, 15.88it/s, est. speed input: 17271.85 toks/s, output: 16.87 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [01:50<02:19, 16.01it/s, est. speed input: 17261.56 toks/s, output: 16.86 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [01:52<02:18, 15.94it/s, est. speed input: 17242.06 toks/s, output: 16.84 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [01:54<02:16, 15.91it/s, est. speed input: 17223.75 toks/s, output: 16.82 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [01:56<02:13, 16.07it/s, est. speed input: 17217.59 toks/s, output: 16.81 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [01:58<02:10, 16.15it/s, est. speed input: 17209.42 toks/s, output: 16.81 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [02:00<02:09, 16.05it/s, est. speed input: 17192.74 toks/s, output: 16.79 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [02:02<02:07, 16.00it/s, est. speed input: 17177.18 toks/s, output: 16.77 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [02:04<02:06, 15.95it/s, est. speed input: 17161.78 toks/s, output: 16.76 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [02:06<02:04, 15.91it/s, est. speed input: 17145.92 toks/s, output: 16.74 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [02:08<02:02, 15.90it/s, est. speed input: 17132.13 toks/s, output: 16.73 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [02:10<02:00, 15.88it/s, est. speed input: 17117.82 toks/s, output: 16.72 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [02:12<01:56, 16.17it/s, est. speed input: 17120.60 toks/s, output: 16.72 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [02:14<01:55, 16.07it/s, est. speed input: 17107.07 toks/s, output: 16.71 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [02:16<01:52, 16.14it/s, est. speed input: 17101.29 toks/s, output: 16.70 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [02:18<01:51, 16.05it/s, est. speed input: 17088.29 toks/s, output: 16.69 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [02:20<01:48, 16.16it/s, est. speed input: 17084.54 toks/s, output: 16.68 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [02:22<01:45, 16.39it/s, est. speed input: 17087.96 toks/s, output: 16.69 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [02:23<01:43, 16.41it/s, est. speed input: 17084.77 toks/s, output: 16.68 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [02:25<01:42, 16.23it/s, est. speed input: 17072.74 toks/s, output: 16.67 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [02:28<01:41, 16.12it/s, est. speed input: 17061.26 toks/s, output: 16.66 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [02:29<01:38, 16.20it/s, est. speed input: 17057.97 toks/s, output: 16.66 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [02:31<01:37, 16.10it/s, est. speed input: 17046.98 toks/s, output: 16.65 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [02:33<01:34, 16.18it/s, est. speed input: 17043.72 toks/s, output: 16.64 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [02:35<01:33, 16.08it/s, est. speed input: 17032.88 toks/s, output: 16.63 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [02:37<01:31, 16.00it/s, est. speed input: 17022.39 toks/s, output: 16.62 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [02:39<01:30, 15.96it/s, est. speed input: 17012.40 toks/s, output: 16.61 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [02:41<01:27, 16.09it/s, est. speed input: 17010.07 toks/s, output: 16.61 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [02:43<01:25, 16.02it/s, est. speed input: 17000.49 toks/s, output: 16.60 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [02:45<01:24, 15.97it/s, est. speed input: 16991.11 toks/s, output: 16.59 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [02:48<01:22, 15.92it/s, est. speed input: 16981.29 toks/s, output: 16.58 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [02:50<01:20, 15.89it/s, est. speed input: 16972.22 toks/s, output: 16.57 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [02:51<01:17, 16.05it/s, est. speed input: 16970.57 toks/s, output: 16.57 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [02:53<01:15, 15.98it/s, est. speed input: 16961.51 toks/s, output: 16.56 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [02:56<01:14, 15.94it/s, est. speed input: 16953.16 toks/s, output: 16.56 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [02:58<01:12, 15.90it/s, est. speed input: 16944.48 toks/s, output: 16.55 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [03:00<01:10, 15.89it/s, est. speed input: 16936.64 toks/s, output: 16.54 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [03:02<01:08, 15.89it/s, est. speed input: 16929.17 toks/s, output: 16.53 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [03:04<01:06, 15.85it/s, est. speed input: 16920.59 toks/s, output: 16.52 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [03:06<01:04, 15.84it/s, est. speed input: 16912.61 toks/s, output: 16.52 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [03:08<01:01, 16.12it/s, est. speed input: 16915.70 toks/s, output: 16.52 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [03:09<00:59, 16.19it/s, est. speed input: 16914.07 toks/s, output: 16.52 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [03:11<00:57, 16.09it/s, est. speed input: 16906.90 toks/s, output: 16.51 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [03:14<00:55, 16.00it/s, est. speed input: 16899.17 toks/s, output: 16.50 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [03:15<00:53, 16.20it/s, est. speed input: 16901.09 toks/s, output: 16.50 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [03:17<00:51, 16.09it/s, est. speed input: 16894.24 toks/s, output: 16.50 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [03:19<00:49, 16.01it/s, est. speed input: 16887.36 toks/s, output: 16.49 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [03:22<00:48, 15.96it/s, est. speed input: 16880.50 toks/s, output: 16.48 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [03:24<00:46, 15.91it/s, est. speed input: 16873.57 toks/s, output: 16.48 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [03:26<00:44, 15.88it/s, est. speed input: 16866.82 toks/s, output: 16.47 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [03:28<00:41, 16.04it/s, est. speed input: 16866.29 toks/s, output: 16.47 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [03:30<00:39, 15.96it/s, est. speed input: 16859.45 toks/s, output: 16.46 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [03:31<00:37, 16.24it/s, est. speed input: 16863.69 toks/s, output: 16.47 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [03:33<00:35, 16.10it/s, est. speed input: 16856.98 toks/s, output: 16.46 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [03:35<00:33, 16.01it/s, est. speed input: 16850.68 toks/s, output: 16.46 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [03:37<00:31, 15.95it/s, est. speed input: 16844.70 toks/s, output: 16.45 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [03:40<00:30, 15.92it/s, est. speed input: 16838.97 toks/s, output: 16.44 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [03:42<00:28, 15.88it/s, est. speed input: 16832.88 toks/s, output: 16.44 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [03:44<00:26, 15.86it/s, est. speed input: 16827.04 toks/s, output: 16.43 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [03:46<00:23, 16.04it/s, est. speed input: 16827.49 toks/s, output: 16.43 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [03:48<00:21, 15.97it/s, est. speed input: 16821.75 toks/s, output: 16.43 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [03:50<00:19, 15.94it/s, est. speed input: 16816.84 toks/s, output: 16.42 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [03:52<00:17, 15.90it/s, est. speed input: 16811.29 toks/s, output: 16.42 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [03:54<00:15, 16.06it/s, est. speed input: 16811.65 toks/s, output: 16.42 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [03:56<00:13, 16.00it/s, est. speed input: 16806.60 toks/s, output: 16.41 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [03:58<00:11, 15.94it/s, est. speed input: 16801.35 toks/s, output: 16.41 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [04:00<00:09, 15.90it/s, est. speed input: 16796.03 toks/s, output: 16.40 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [04:02<00:07, 15.87it/s, est. speed input: 16790.87 toks/s, output: 16.40 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [04:04<00:05, 15.86it/s, est. speed input: 16786.05 toks/s, output: 16.39 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [04:05<00:03, 16.33it/s, est. speed input: 16794.85 toks/s, output: 16.40 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [04:07<00:01, 16.37it/s, est. speed input: 16795.29 toks/s, output: 16.40 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:07<00:00, 16.37it/s, est. speed input: 16919.19 toks/s, output: 16.52 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:07<00:00, 16.52it/s, est. speed input: 16919.19 toks/s, output: 16.52 toks/s]
[rank0]:[W128 08:51:35.282530820 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 299.9s

测试结果:
  Requests/s:   15.98
  Tokens/s:     16378.41
  Total Reqs:   4096
  Elapsed:      256.34s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     16362.43


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,27.1982,13952.6792,4.7062
1024,1024,1,128,128,15.6974,16089.8410,8.1542
2048,1024,2,256,128,16.6446,17060.6677,15.3804
4096,1024,4,512,128,16.1661,16570.2824,31.6712
8192,1024,8,1024,128,15.7548,16148.6719,64.9961
16384,1024,16,2048,128,15.8636,16260.2358,129.1002
32768,1024,32,4096,128,15.9789,16378.4127,256.3374

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:51:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:51:41 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3713099) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3713099) WARNING 01-28 08:52:04 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 29.26 requests/s, 15009.61 total tokens/s, 29.26 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:51:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:51:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:51:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:51:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:51:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:51:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:51:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:51:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:51:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:51:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:51:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:51:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:51:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:51:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:51:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:51:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:51:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:51:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:51:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:51:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:51:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:51:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:51:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:51:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:51:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:51:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:51:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:51:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3713099) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3713099) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.12s/it]
(EngineCore_DP0 pid=3713099) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.12s/it]
(EngineCore_DP0 pid=3713099) 
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3713099) [2026-01-28 08:51:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3713099) 2026-01-28 08:52:04,183 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3713099) 2026-01-28 08:52:04,197 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  90%|████████▉ | 115/128 [00:00<00:00, 1142.77it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1159.01it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:09, 13.90it/s, est. speed input: 7119.50 toks/s, output: 13.90 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:06, 18.90it/s, est. speed input: 9276.76 toks/s, output: 18.12 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:05, 20.97it/s, est. speed input: 10164.79 toks/s, output: 19.85 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:05, 21.73it/s, est. speed input: 10549.27 toks/s, output: 20.60 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:05, 22.09it/s, est. speed input: 10764.07 toks/s, output: 21.02 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:04, 22.36it/s, est. speed input: 10920.34 toks/s, output: 21.33 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:04, 22.65it/s, est. speed input: 11057.94 toks/s, output: 21.60 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:04, 23.84it/s, est. speed input: 11346.12 toks/s, output: 22.16 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:03, 26.18it/s, est. speed input: 11853.84 toks/s, output: 23.15 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:03, 28.34it/s, est. speed input: 12343.89 toks/s, output: 24.11 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:03, 29.75it/s, est. speed input: 12736.99 toks/s, output: 24.88 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:02, 30.48it/s, est. speed input: 13038.61 toks/s, output: 25.47 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 28.43it/s, est. speed input: 13006.53 toks/s, output: 25.40 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 29.82it/s, est. speed input: 13278.47 toks/s, output: 25.93 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 30.86it/s, est. speed input: 13517.28 toks/s, output: 26.40 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:02<00:02, 31.51it/s, est. speed input: 13718.91 toks/s, output: 26.79 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:02<00:02, 31.81it/s, est. speed input: 13884.89 toks/s, output: 27.12 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:02<00:02, 31.97it/s, est. speed input: 14029.21 toks/s, output: 27.40 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:02<00:01, 32.34it/s, est. speed input: 14177.45 toks/s, output: 27.69 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:01, 32.54it/s, est. speed input: 14307.50 toks/s, output: 27.94 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:01, 32.81it/s, est. speed input: 14434.17 toks/s, output: 28.19 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 32.77it/s, est. speed input: 14535.23 toks/s, output: 28.39 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 33.06it/s, est. speed input: 14647.75 toks/s, output: 28.61 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:03<00:01, 33.15it/s, est. speed input: 14744.15 toks/s, output: 28.80 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:03<00:01, 33.02it/s, est. speed input: 14822.47 toks/s, output: 28.95 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:03<00:01, 32.51it/s, est. speed input: 14870.58 toks/s, output: 29.04 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:03<00:00, 32.47it/s, est. speed input: 14933.05 toks/s, output: 29.17 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:03<00:00, 32.95it/s, est. speed input: 15017.57 toks/s, output: 29.33 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:03<00:00, 33.15it/s, est. speed input: 15089.65 toks/s, output: 29.47 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:03<00:00, 33.25it/s, est. speed input: 15154.89 toks/s, output: 29.60 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:03<00:00, 33.26it/s, est. speed input: 15213.74 toks/s, output: 29.71 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 33.21it/s, est. speed input: 15266.19 toks/s, output: 29.82 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:04<00:00, 33.35it/s, est. speed input: 15323.20 toks/s, output: 29.93 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:04<00:00, 33.29it/s, est. speed input: 15370.29 toks/s, output: 30.02 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 33.29it/s, est. speed input: 15372.67 toks/s, output: 30.02 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 30.02it/s, est. speed input: 15372.67 toks/s, output: 30.02 toks/s]
[rank0]:[W128 08:52:09.453217295 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.1s

测试结果:
  Requests/s:   29.26
  Tokens/s:     15009.61
  Total Reqs:   128
  Elapsed:      4.37s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     14980.35

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:52:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:52:15 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3713780) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3713780) WARNING 01-28 08:52:38 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.78 requests/s, 17196.48 total tokens/s, 16.78 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:52:15] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:52:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:52:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:52:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:52:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:52:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:52:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:52:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:52:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:52:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:52:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:52:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:52:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:52:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:52:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:52:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:52:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:52:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3713780) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3713780) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.67s/it]
(EngineCore_DP0 pid=3713780) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.67s/it]
(EngineCore_DP0 pid=3713780) 
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3713780) [2026-01-28 08:52:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3713780) 2026-01-28 08:52:37,888 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3713780) 2026-01-28 08:52:37,901 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  38%|███▊      | 49/128 [00:00<00:00, 484.12it/s]
Adding requests:  91%|█████████▏| 117/128 [00:00<00:00, 597.41it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 590.76it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:08, 15.27it/s, est. speed input: 15639.84 toks/s, output: 15.27 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:09, 13.71it/s, est. speed input: 14258.77 toks/s, output: 13.92 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:09, 13.07it/s, est. speed input: 13689.87 toks/s, output: 13.37 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:09, 12.86it/s, est. speed input: 13473.28 toks/s, output: 13.16 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:09, 12.70it/s, est. speed input: 13317.86 toks/s, output: 13.01 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:08, 13.16it/s, est. speed input: 13500.44 toks/s, output: 13.18 toks/s]
Processed prompts:  11%|█         | 14/128 [00:01<00:07, 14.39it/s, est. speed input: 14020.27 toks/s, output: 13.69 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:01<00:07, 15.34it/s, est. speed input: 14441.12 toks/s, output: 14.10 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:06, 16.05it/s, est. speed input: 14787.52 toks/s, output: 14.44 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 16.57it/s, est. speed input: 15077.83 toks/s, output: 14.72 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 16.81it/s, est. speed input: 15290.77 toks/s, output: 14.93 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:06, 17.13it/s, est. speed input: 15504.18 toks/s, output: 15.14 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:05, 17.30it/s, est. speed input: 15679.76 toks/s, output: 15.31 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 17.40it/s, est. speed input: 15827.94 toks/s, output: 15.46 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 17.55it/s, est. speed input: 15973.92 toks/s, output: 15.60 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:02<00:05, 17.66it/s, est. speed input: 16103.58 toks/s, output: 15.73 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:02<00:05, 17.80it/s, est. speed input: 16229.88 toks/s, output: 15.85 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:02<00:05, 17.78it/s, est. speed input: 16326.66 toks/s, output: 15.94 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 17.82it/s, est. speed input: 16421.81 toks/s, output: 16.04 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:04, 17.69it/s, est. speed input: 16486.40 toks/s, output: 16.10 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:04, 17.69it/s, est. speed input: 16556.59 toks/s, output: 16.17 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:04, 17.67it/s, est. speed input: 16618.97 toks/s, output: 16.23 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 17.72it/s, est. speed input: 16684.25 toks/s, output: 16.29 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 17.89it/s, est. speed input: 16760.70 toks/s, output: 16.37 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:03<00:04, 17.88it/s, est. speed input: 16817.34 toks/s, output: 16.42 toks/s]
Processed prompts:  41%|████      | 52/128 [00:03<00:04, 17.91it/s, est. speed input: 16873.34 toks/s, output: 16.48 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 17.94it/s, est. speed input: 16926.70 toks/s, output: 16.53 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:03, 18.02it/s, est. speed input: 16983.05 toks/s, output: 16.58 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:03, 18.01it/s, est. speed input: 17028.30 toks/s, output: 16.63 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:03, 17.72it/s, est. speed input: 17043.72 toks/s, output: 16.64 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 17.69it/s, est. speed input: 17073.83 toks/s, output: 16.67 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:03, 17.76it/s, est. speed input: 17111.62 toks/s, output: 16.71 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 17.83it/s, est. speed input: 17148.88 toks/s, output: 16.75 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:04<00:03, 17.93it/s, est. speed input: 17188.39 toks/s, output: 16.79 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:04<00:03, 17.91it/s, est. speed input: 17217.90 toks/s, output: 16.81 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 17.86it/s, est. speed input: 17242.72 toks/s, output: 16.84 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 17.96it/s, est. speed input: 17277.81 toks/s, output: 16.87 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:02, 17.90it/s, est. speed input: 17300.86 toks/s, output: 16.90 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:02, 17.68it/s, est. speed input: 17307.81 toks/s, output: 16.90 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:02, 17.63it/s, est. speed input: 17322.91 toks/s, output: 16.92 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 17.75it/s, est. speed input: 17349.28 toks/s, output: 16.94 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:04<00:02, 17.85it/s, est. speed input: 17375.82 toks/s, output: 16.97 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:05<00:02, 17.82it/s, est. speed input: 17393.34 toks/s, output: 16.99 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 17.84it/s, est. speed input: 17413.64 toks/s, output: 17.01 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 17.81it/s, est. speed input: 17429.65 toks/s, output: 17.02 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 17.83it/s, est. speed input: 17447.79 toks/s, output: 17.04 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:01, 17.87it/s, est. speed input: 17466.91 toks/s, output: 17.06 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:01, 17.89it/s, est. speed input: 17484.93 toks/s, output: 17.08 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 17.71it/s, est. speed input: 17489.53 toks/s, output: 17.08 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:05<00:01, 17.77it/s, est. speed input: 17505.74 toks/s, output: 17.10 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:05<00:01, 17.83it/s, est. speed input: 17522.53 toks/s, output: 17.11 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:06<00:01, 17.87it/s, est. speed input: 17538.48 toks/s, output: 17.13 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 17.95it/s, est. speed input: 17556.91 toks/s, output: 17.15 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 17.96it/s, est. speed input: 17572.10 toks/s, output: 17.16 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:01, 17.98it/s, est. speed input: 17587.52 toks/s, output: 17.18 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:06<00:00, 18.05it/s, est. speed input: 17605.60 toks/s, output: 17.19 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:06<00:00, 18.06it/s, est. speed input: 17620.74 toks/s, output: 17.21 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:06<00:00, 17.66it/s, est. speed input: 17613.21 toks/s, output: 17.20 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:06<00:00, 17.79it/s, est. speed input: 17628.18 toks/s, output: 17.21 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:06<00:00, 17.76it/s, est. speed input: 17636.17 toks/s, output: 17.22 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:07<00:00, 17.80it/s, est. speed input: 17647.17 toks/s, output: 17.23 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:07<00:00, 17.91it/s, est. speed input: 17661.54 toks/s, output: 17.25 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:07<00:00, 17.93it/s, est. speed input: 17673.22 toks/s, output: 17.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.95it/s, est. speed input: 17684.27 toks/s, output: 17.27 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.95it/s, est. speed input: 17684.27 toks/s, output: 17.27 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.27it/s, est. speed input: 17684.27 toks/s, output: 17.27 toks/s]
[rank0]:[W128 08:52:46.283354445 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.8s

测试结果:
  Requests/s:   16.78
  Tokens/s:     17196.48
  Total Reqs:   128
  Elapsed:      7.63s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     17179.70

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:52:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:52:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3714475) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3714475) WARNING 01-28 08:53:15 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 17.87 requests/s, 18317.49 total tokens/s, 17.87 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 08:52:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:52:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:52:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:52:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:52:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:52:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:52:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:52:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:52:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:52:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:52:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:52:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:52:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:52:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:52:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:52:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:52:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:52:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:52:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3714475) [2026-01-28 08:52:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3714475) [2026-01-28 08:52:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3714475) [2026-01-28 08:52:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3714475) [2026-01-28 08:52:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3714475) [2026-01-28 08:52:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3714475) [2026-01-28 08:52:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3714475) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3714475) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.49s/it]
(EngineCore_DP0 pid=3714475) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.49s/it]
(EngineCore_DP0 pid=3714475) 
(EngineCore_DP0 pid=3714475) [2026-01-28 08:53:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3714475) [2026-01-28 08:53:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3714475) [2026-01-28 08:53:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3714475) [2026-01-28 08:53:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3714475) [2026-01-28 08:53:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3714475) [2026-01-28 08:53:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3714475) [2026-01-28 08:53:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3714475) [2026-01-28 08:53:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3714475) 2026-01-28 08:53:14,627 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3714475) 2026-01-28 08:53:14,638 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  27%|██▋       | 68/256 [00:00<00:00, 667.76it/s]
Adding requests:  53%|█████▎    | 135/256 [00:00<00:00, 625.08it/s]
Adding requests:  77%|███████▋  | 198/256 [00:00<00:00, 584.88it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 594.60it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 8/256 [00:00<00:06, 39.27it/s, est. speed input: 40217.92 toks/s, output: 39.27 toks/s]
Processed prompts:   5%|▍         | 12/256 [00:00<00:09, 26.28it/s, est. speed input: 28819.13 toks/s, output: 28.14 toks/s]
Processed prompts:   6%|▌         | 15/256 [00:00<00:09, 26.33it/s, est. speed input: 28450.05 toks/s, output: 27.78 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:11, 20.49it/s, est. speed input: 24257.91 toks/s, output: 23.69 toks/s]
Processed prompts:   8%|▊         | 21/256 [00:00<00:10, 22.16it/s, est. speed input: 24715.00 toks/s, output: 24.14 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:01<00:12, 18.56it/s, est. speed input: 22504.54 toks/s, output: 21.98 toks/s]
Processed prompts:  11%|█         | 27/256 [00:01<00:11, 20.46it/s, est. speed input: 22963.12 toks/s, output: 22.42 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:01<00:12, 17.62it/s, est. speed input: 21497.10 toks/s, output: 20.99 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:01<00:12, 17.72it/s, est. speed input: 21282.52 toks/s, output: 20.78 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:01<00:12, 17.81it/s, est. speed input: 21097.29 toks/s, output: 20.60 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:01<00:12, 17.94it/s, est. speed input: 20953.34 toks/s, output: 20.46 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:01<00:12, 18.00it/s, est. speed input: 20813.77 toks/s, output: 20.33 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:01<00:11, 18.03it/s, est. speed input: 20688.81 toks/s, output: 20.20 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:02<00:11, 18.06it/s, est. speed input: 20577.41 toks/s, output: 20.10 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:02<00:11, 18.07it/s, est. speed input: 20474.87 toks/s, output: 19.99 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:02<00:11, 18.00it/s, est. speed input: 20366.64 toks/s, output: 19.89 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:02<00:11, 18.12it/s, est. speed input: 20299.02 toks/s, output: 19.82 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:02<00:11, 18.08it/s, est. speed input: 20216.55 toks/s, output: 19.74 toks/s]
Processed prompts:  20%|██        | 52/256 [00:02<00:11, 18.05it/s, est. speed input: 20140.06 toks/s, output: 19.67 toks/s]
Processed prompts:  21%|██        | 54/256 [00:02<00:11, 18.08it/s, est. speed input: 20078.63 toks/s, output: 19.61 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:02<00:11, 18.10it/s, est. speed input: 20020.92 toks/s, output: 19.55 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:02<00:10, 18.21it/s, est. speed input: 19980.68 toks/s, output: 19.51 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:03<00:10, 18.21it/s, est. speed input: 19933.60 toks/s, output: 19.47 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:03<00:10, 18.14it/s, est. speed input: 19880.45 toks/s, output: 19.41 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:03<00:10, 18.22it/s, est. speed input: 19846.68 toks/s, output: 19.38 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:03<00:10, 17.97it/s, est. speed input: 19778.72 toks/s, output: 19.32 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:03<00:10, 18.14it/s, est. speed input: 19754.66 toks/s, output: 19.29 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:03<00:10, 18.11it/s, est. speed input: 19716.04 toks/s, output: 19.25 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:03<00:10, 18.07it/s, est. speed input: 19676.75 toks/s, output: 19.22 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:03<00:10, 18.20it/s, est. speed input: 19656.56 toks/s, output: 19.20 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:03<00:09, 18.18it/s, est. speed input: 19625.85 toks/s, output: 19.17 toks/s]
Processed prompts:  30%|███       | 78/256 [00:04<00:09, 18.17it/s, est. speed input: 19597.60 toks/s, output: 19.14 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:04<00:09, 18.11it/s, est. speed input: 19566.05 toks/s, output: 19.11 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:04<00:09, 18.08it/s, est. speed input: 19537.30 toks/s, output: 19.08 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:04<00:09, 17.82it/s, est. speed input: 19487.71 toks/s, output: 19.03 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:04<00:09, 17.93it/s, est. speed input: 19466.91 toks/s, output: 19.01 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:04<00:09, 17.97it/s, est. speed input: 19443.45 toks/s, output: 18.99 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:04<00:09, 18.12it/s, est. speed input: 19431.64 toks/s, output: 18.98 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:04<00:09, 18.21it/s, est. speed input: 19418.99 toks/s, output: 18.96 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:04<00:08, 18.16it/s, est. speed input: 19398.33 toks/s, output: 18.94 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:05<00:08, 18.24it/s, est. speed input: 19386.46 toks/s, output: 18.93 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:05<00:08, 18.24it/s, est. speed input: 19371.95 toks/s, output: 18.92 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:05<00:08, 18.14it/s, est. speed input: 19349.82 toks/s, output: 18.90 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:05<00:08, 18.14it/s, est. speed input: 19333.97 toks/s, output: 18.88 toks/s]
Processed prompts:  41%|████      | 104/256 [00:05<00:08, 17.92it/s, est. speed input: 19303.43 toks/s, output: 18.85 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:05<00:08, 18.07it/s, est. speed input: 19294.68 toks/s, output: 18.84 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:05<00:08, 18.08it/s, est. speed input: 19280.44 toks/s, output: 18.83 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:05<00:08, 18.13it/s, est. speed input: 19269.32 toks/s, output: 18.82 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:05<00:07, 18.22it/s, est. speed input: 19261.78 toks/s, output: 18.81 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:06<00:07, 18.16it/s, est. speed input: 19247.45 toks/s, output: 18.80 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:06<00:07, 18.10it/s, est. speed input: 19231.86 toks/s, output: 18.78 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:06<00:07, 18.16it/s, est. speed input: 19223.31 toks/s, output: 18.77 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:06<00:07, 18.15it/s, est. speed input: 19212.15 toks/s, output: 18.76 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:06<00:07, 17.98it/s, est. speed input: 19191.41 toks/s, output: 18.74 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:06<00:07, 18.00it/s, est. speed input: 19179.20 toks/s, output: 18.73 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:06<00:07, 18.08it/s, est. speed input: 19171.40 toks/s, output: 18.72 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:06<00:07, 18.06it/s, est. speed input: 19159.61 toks/s, output: 18.71 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:06<00:06, 18.12it/s, est. speed input: 19152.18 toks/s, output: 18.70 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:07<00:06, 18.09it/s, est. speed input: 19141.72 toks/s, output: 18.69 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:07<00:06, 18.18it/s, est. speed input: 19136.90 toks/s, output: 18.69 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:07<00:06, 18.15it/s, est. speed input: 19127.67 toks/s, output: 18.68 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:07<00:06, 18.23it/s, est. speed input: 19123.60 toks/s, output: 18.68 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:07<00:06, 18.06it/s, est. speed input: 19108.24 toks/s, output: 18.66 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:07<00:06, 17.79it/s, est. speed input: 19085.15 toks/s, output: 18.64 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:07<00:06, 17.84it/s, est. speed input: 19075.17 toks/s, output: 18.63 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:07<00:06, 17.98it/s, est. speed input: 19070.82 toks/s, output: 18.62 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:07<00:05, 18.16it/s, est. speed input: 19070.21 toks/s, output: 18.62 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:08<00:05, 18.06it/s, est. speed input: 19058.91 toks/s, output: 18.61 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:08<00:05, 18.11it/s, est. speed input: 19053.81 toks/s, output: 18.61 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:08<00:05, 18.17it/s, est. speed input: 19049.78 toks/s, output: 18.60 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:08<00:05, 18.25it/s, est. speed input: 19047.72 toks/s, output: 18.60 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:08<00:05, 18.15it/s, est. speed input: 19038.69 toks/s, output: 18.59 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:08<00:05, 17.90it/s, est. speed input: 19021.29 toks/s, output: 18.58 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:08<00:05, 17.88it/s, est. speed input: 19011.55 toks/s, output: 18.57 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:08<00:05, 18.08it/s, est. speed input: 19011.39 toks/s, output: 18.57 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:08<00:04, 18.09it/s, est. speed input: 19005.71 toks/s, output: 18.56 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:09<00:04, 18.09it/s, est. speed input: 18999.86 toks/s, output: 18.55 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:09<00:04, 18.06it/s, est. speed input: 18992.70 toks/s, output: 18.55 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:09<00:04, 18.12it/s, est. speed input: 18989.36 toks/s, output: 18.54 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:09<00:04, 18.14it/s, est. speed input: 18985.13 toks/s, output: 18.54 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:09<00:04, 18.05it/s, est. speed input: 18976.61 toks/s, output: 18.53 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:09<00:04, 18.13it/s, est. speed input: 18974.33 toks/s, output: 18.53 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:09<00:04, 17.76it/s, est. speed input: 18954.72 toks/s, output: 18.51 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:09<00:04, 17.90it/s, est. speed input: 18951.50 toks/s, output: 18.51 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:09<00:04, 17.99it/s, est. speed input: 18947.95 toks/s, output: 18.50 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:10<00:03, 18.03it/s, est. speed input: 18943.87 toks/s, output: 18.50 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:10<00:03, 17.98it/s, est. speed input: 18936.73 toks/s, output: 18.49 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:10<00:03, 17.98it/s, est. speed input: 18930.90 toks/s, output: 18.49 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:10<00:03, 17.95it/s, est. speed input: 18924.29 toks/s, output: 18.48 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:10<00:03, 18.01it/s, est. speed input: 18920.69 toks/s, output: 18.48 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:10<00:03, 17.93it/s, est. speed input: 18912.96 toks/s, output: 18.47 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:10<00:03, 17.75it/s, est. speed input: 18900.36 toks/s, output: 18.46 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:10<00:03, 17.78it/s, est. speed input: 18894.00 toks/s, output: 18.45 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:10<00:03, 17.91it/s, est. speed input: 18891.74 toks/s, output: 18.45 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:11<00:02, 17.99it/s, est. speed input: 18888.94 toks/s, output: 18.45 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:11<00:02, 18.03it/s, est. speed input: 18885.69 toks/s, output: 18.44 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:11<00:02, 17.99it/s, est. speed input: 18880.26 toks/s, output: 18.44 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:11<00:02, 17.99it/s, est. speed input: 18875.72 toks/s, output: 18.43 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:11<00:02, 18.01it/s, est. speed input: 18872.15 toks/s, output: 18.43 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:11<00:02, 18.05it/s, est. speed input: 18869.10 toks/s, output: 18.43 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:11<00:02, 18.07it/s, est. speed input: 18866.25 toks/s, output: 18.42 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:11<00:02, 17.85it/s, est. speed input: 18855.63 toks/s, output: 18.41 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:11<00:02, 17.96it/s, est. speed input: 18853.83 toks/s, output: 18.41 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:12<00:01, 17.96it/s, est. speed input: 18849.39 toks/s, output: 18.41 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:12<00:01, 18.01it/s, est. speed input: 18846.73 toks/s, output: 18.40 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:12<00:01, 18.05it/s, est. speed input: 18844.38 toks/s, output: 18.40 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:12<00:01, 18.05it/s, est. speed input: 18841.15 toks/s, output: 18.40 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:12<00:01, 18.04it/s, est. speed input: 18837.74 toks/s, output: 18.40 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:12<00:01, 18.01it/s, est. speed input: 18833.71 toks/s, output: 18.39 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:12<00:01, 18.03it/s, est. speed input: 18830.79 toks/s, output: 18.39 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:12<00:01, 17.92it/s, est. speed input: 18824.19 toks/s, output: 18.38 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:12<00:01, 17.97it/s, est. speed input: 18821.63 toks/s, output: 18.38 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:13<00:00, 18.02it/s, est. speed input: 18819.50 toks/s, output: 18.38 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:13<00:00, 18.02it/s, est. speed input: 18816.50 toks/s, output: 18.38 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:13<00:00, 18.12it/s, est. speed input: 18816.30 toks/s, output: 18.38 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:13<00:00, 18.13it/s, est. speed input: 18814.47 toks/s, output: 18.37 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:13<00:00, 18.10it/s, est. speed input: 18811.57 toks/s, output: 18.37 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:13<00:00, 18.10it/s, est. speed input: 18809.45 toks/s, output: 18.37 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:13<00:00, 17.96it/s, est. speed input: 18803.29 toks/s, output: 18.36 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:13<00:00, 18.05it/s, est. speed input: 18802.43 toks/s, output: 18.36 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 18.05it/s, est. speed input: 18868.20 toks/s, output: 18.43 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 18.43it/s, est. speed input: 18868.20 toks/s, output: 18.43 toks/s]
[rank0]:[W128 08:53:29.831889182 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.6s

测试结果:
  Requests/s:   17.87
  Tokens/s:     18317.49
  Total Reqs:   256
  Elapsed:      14.33s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     18299.62

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:53:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:53:37 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3715279) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3715279) WARNING 01-28 08:53:59 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.65 requests/s, 17061.26 total tokens/s, 16.65 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 08:53:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:53:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:53:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:53:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:53:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:53:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:53:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:53:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:53:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:53:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:53:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:53:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:53:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:53:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:53:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:53:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:53:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:53:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:53:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:53:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:53:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:53:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:53:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:53:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:53:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:53:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:53:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:53:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:41] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:41] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:41] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:41] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:41] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3715279) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3715279) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.40s/it]
(EngineCore_DP0 pid=3715279) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.40s/it]
(EngineCore_DP0 pid=3715279) 
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3715279) [2026-01-28 08:53:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3715279) 2026-01-28 08:53:58,851 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3715279) 2026-01-28 08:53:58,867 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 65/512 [00:00<00:00, 642.26it/s]
Adding requests:  25%|██▌       | 130/512 [00:00<00:00, 614.60it/s]
Adding requests:  38%|███▊      | 192/512 [00:00<00:00, 572.07it/s]
Adding requests:  49%|████▉     | 250/512 [00:00<00:00, 574.63it/s]
Adding requests:  60%|██████    | 308/512 [00:00<00:00, 558.52it/s]
Adding requests:  71%|███████▏  | 365/512 [00:00<00:00, 560.18it/s]
Adding requests:  82%|████████▏ | 422/512 [00:00<00:00, 548.65it/s]
Adding requests:  93%|█████████▎| 477/512 [00:00<00:00, 548.92it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 562.11it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 14/512 [00:00<00:08, 59.82it/s, est. speed input: 61260.60 toks/s, output: 59.82 toks/s]
Processed prompts:   4%|▍         | 20/512 [00:00<00:12, 39.17it/s, est. speed input: 43248.93 toks/s, output: 42.24 toks/s]
Processed prompts:   5%|▍         | 24/512 [00:00<00:16, 28.90it/s, est. speed input: 34500.55 toks/s, output: 33.69 toks/s]
Processed prompts:   5%|▌         | 27/512 [00:00<00:21, 22.51it/s, est. speed input: 29129.02 toks/s, output: 28.45 toks/s]
Processed prompts:   6%|▌         | 30/512 [00:01<00:25, 18.84it/s, est. speed input: 25808.29 toks/s, output: 25.20 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:01<00:26, 18.12it/s, est. speed input: 24355.12 toks/s, output: 23.78 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:01<00:26, 17.74it/s, est. speed input: 23360.61 toks/s, output: 22.81 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:01<00:26, 17.48it/s, est. speed input: 22609.59 toks/s, output: 22.08 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:02<00:27, 17.16it/s, est. speed input: 21959.59 toks/s, output: 21.44 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:02<00:27, 17.05it/s, est. speed input: 21485.19 toks/s, output: 20.98 toks/s]
Processed prompts:  11%|█         | 54/512 [00:02<00:26, 16.96it/s, est. speed input: 21092.65 toks/s, output: 20.60 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:02<00:26, 16.89it/s, est. speed input: 20759.82 toks/s, output: 20.27 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:03<00:26, 16.83it/s, est. speed input: 20476.44 toks/s, output: 20.00 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:03<00:26, 16.74it/s, est. speed input: 20220.66 toks/s, output: 19.75 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:03<00:26, 16.71it/s, est. speed input: 20008.26 toks/s, output: 19.54 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:03<00:26, 16.73it/s, est. speed input: 19830.39 toks/s, output: 19.37 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:04<00:25, 16.70it/s, est. speed input: 19664.22 toks/s, output: 19.20 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:04<00:25, 16.71it/s, est. speed input: 19525.26 toks/s, output: 19.07 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:04<00:25, 16.72it/s, est. speed input: 19398.93 toks/s, output: 18.94 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:04<00:25, 16.76it/s, est. speed input: 19292.56 toks/s, output: 18.84 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:05<00:24, 16.77it/s, est. speed input: 19193.68 toks/s, output: 18.74 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:05<00:24, 16.79it/s, est. speed input: 19105.11 toks/s, output: 18.66 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:05<00:24, 16.70it/s, est. speed input: 19006.95 toks/s, output: 18.56 toks/s]
Processed prompts:  21%|██        | 106/512 [00:05<00:24, 16.77it/s, est. speed input: 18938.11 toks/s, output: 18.49 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:05<00:23, 16.78it/s, est. speed input: 18869.16 toks/s, output: 18.43 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:06<00:23, 16.77it/s, est. speed input: 18803.66 toks/s, output: 18.36 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:06<00:23, 16.67it/s, est. speed input: 18729.14 toks/s, output: 18.29 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:06<00:23, 16.70it/s, est. speed input: 18674.02 toks/s, output: 18.24 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:06<00:23, 16.71it/s, est. speed input: 18621.26 toks/s, output: 18.18 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:07<00:22, 16.78it/s, est. speed input: 18578.50 toks/s, output: 18.14 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:07<00:22, 16.72it/s, est. speed input: 18527.03 toks/s, output: 18.09 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:07<00:22, 16.71it/s, est. speed input: 18481.82 toks/s, output: 18.05 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:07<00:22, 16.70it/s, est. speed input: 18439.08 toks/s, output: 18.01 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:08<00:21, 16.73it/s, est. speed input: 18403.38 toks/s, output: 17.97 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:08<00:21, 16.71it/s, est. speed input: 18364.96 toks/s, output: 17.93 toks/s]
Processed prompts:  30%|███       | 154/512 [00:08<00:21, 16.64it/s, est. speed input: 18322.51 toks/s, output: 17.89 toks/s]
Processed prompts:  31%|███       | 158/512 [00:08<00:21, 16.64it/s, est. speed input: 18287.91 toks/s, output: 17.86 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:09<00:20, 16.68it/s, est. speed input: 18258.68 toks/s, output: 17.83 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:09<00:20, 16.73it/s, est. speed input: 18232.76 toks/s, output: 17.81 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:09<00:20, 16.64it/s, est. speed input: 18197.02 toks/s, output: 17.77 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:09<00:20, 16.67it/s, est. speed input: 18171.17 toks/s, output: 17.75 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:10<00:19, 16.72it/s, est. speed input: 18149.55 toks/s, output: 17.72 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:10<00:19, 16.74it/s, est. speed input: 18127.03 toks/s, output: 17.70 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:10<00:19, 16.73it/s, est. speed input: 18103.99 toks/s, output: 17.68 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:10<00:19, 16.62it/s, est. speed input: 18073.87 toks/s, output: 17.65 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:11<00:19, 16.64it/s, est. speed input: 18052.12 toks/s, output: 17.63 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:11<00:18, 16.68it/s, est. speed input: 18033.40 toks/s, output: 17.61 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:11<00:18, 16.72it/s, est. speed input: 18016.52 toks/s, output: 17.59 toks/s]
Processed prompts:  40%|████      | 206/512 [00:11<00:18, 16.63it/s, est. speed input: 17991.67 toks/s, output: 17.57 toks/s]
Processed prompts:  41%|████      | 210/512 [00:11<00:18, 16.63it/s, est. speed input: 17972.60 toks/s, output: 17.55 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:12<00:17, 16.66it/s, est. speed input: 17955.76 toks/s, output: 17.53 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:12<00:17, 16.67it/s, est. speed input: 17939.05 toks/s, output: 17.52 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:12<00:17, 16.71it/s, est. speed input: 17925.19 toks/s, output: 17.51 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:12<00:17, 16.56it/s, est. speed input: 17900.30 toks/s, output: 17.48 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:13<00:16, 16.65it/s, est. speed input: 17888.84 toks/s, output: 17.47 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:13<00:16, 16.67it/s, est. speed input: 17874.84 toks/s, output: 17.46 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:13<00:16, 16.70it/s, est. speed input: 17862.74 toks/s, output: 17.44 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:13<00:16, 16.57it/s, est. speed input: 17841.82 toks/s, output: 17.42 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:14<00:16, 16.62it/s, est. speed input: 17829.75 toks/s, output: 17.41 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:14<00:15, 16.64it/s, est. speed input: 17817.20 toks/s, output: 17.40 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:14<00:15, 16.68it/s, est. speed input: 17806.80 toks/s, output: 17.39 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:14<00:15, 16.65it/s, est. speed input: 17793.66 toks/s, output: 17.38 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:15<00:15, 16.61it/s, est. speed input: 17779.50 toks/s, output: 17.36 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:15<00:14, 16.66it/s, est. speed input: 17770.13 toks/s, output: 17.35 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:15<00:14, 16.65it/s, est. speed input: 17758.69 toks/s, output: 17.34 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:15<00:14, 16.73it/s, est. speed input: 17751.97 toks/s, output: 17.34 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:16<00:14, 16.60it/s, est. speed input: 17735.93 toks/s, output: 17.32 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:16<00:13, 16.64it/s, est. speed input: 17727.38 toks/s, output: 17.31 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:16<00:13, 16.73it/s, est. speed input: 17721.76 toks/s, output: 17.31 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:16<00:13, 16.81it/s, est. speed input: 17717.19 toks/s, output: 17.30 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:17<00:13, 16.74it/s, est. speed input: 17707.02 toks/s, output: 17.29 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:17<00:12, 16.67it/s, est. speed input: 17695.66 toks/s, output: 17.28 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:17<00:12, 16.72it/s, est. speed input: 17689.64 toks/s, output: 17.28 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:17<00:12, 16.71it/s, est. speed input: 17681.42 toks/s, output: 17.27 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:17<00:12, 16.75it/s, est. speed input: 17675.74 toks/s, output: 17.26 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:18<00:11, 16.64it/s, est. speed input: 17663.49 toks/s, output: 17.25 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:18<00:11, 16.68it/s, est. speed input: 17657.19 toks/s, output: 17.24 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:18<00:11, 16.70it/s, est. speed input: 17650.98 toks/s, output: 17.24 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:18<00:11, 16.70it/s, est. speed input: 17644.03 toks/s, output: 17.23 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:19<00:10, 16.63it/s, est. speed input: 17634.19 toks/s, output: 17.22 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:19<00:10, 16.65it/s, est. speed input: 17627.28 toks/s, output: 17.21 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:19<00:10, 16.76it/s, est. speed input: 17624.90 toks/s, output: 17.21 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:19<00:09, 17.40it/s, est. speed input: 17645.45 toks/s, output: 17.23 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:20<00:09, 17.19it/s, est. speed input: 17639.11 toks/s, output: 17.23 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:20<00:09, 16.96it/s, est. speed input: 17629.64 toks/s, output: 17.22 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:20<00:09, 16.87it/s, est. speed input: 17623.13 toks/s, output: 17.21 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:20<00:09, 16.87it/s, est. speed input: 17619.22 toks/s, output: 17.21 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:21<00:08, 16.82it/s, est. speed input: 17613.25 toks/s, output: 17.20 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:21<00:08, 16.70it/s, est. speed input: 17604.09 toks/s, output: 17.19 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:21<00:08, 16.65it/s, est. speed input: 17596.45 toks/s, output: 17.18 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:21<00:08, 16.71it/s, est. speed input: 17592.89 toks/s, output: 17.18 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:22<00:08, 16.66it/s, est. speed input: 17585.72 toks/s, output: 17.17 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:22<00:07, 16.68it/s, est. speed input: 17580.62 toks/s, output: 17.17 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:22<00:07, 16.60it/s, est. speed input: 17572.42 toks/s, output: 17.16 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:22<00:07, 16.66it/s, est. speed input: 17568.48 toks/s, output: 17.16 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:22<00:07, 16.65it/s, est. speed input: 17562.92 toks/s, output: 17.15 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:23<00:06, 16.66it/s, est. speed input: 17557.99 toks/s, output: 17.15 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:23<00:06, 16.57it/s, est. speed input: 17549.57 toks/s, output: 17.14 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:23<00:06, 16.63it/s, est. speed input: 17545.72 toks/s, output: 17.13 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:23<00:06, 16.69it/s, est. speed input: 17542.64 toks/s, output: 17.13 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:24<00:05, 16.65it/s, est. speed input: 17536.73 toks/s, output: 17.13 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:24<00:05, 16.68it/s, est. speed input: 17533.18 toks/s, output: 17.12 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:24<00:05, 16.62it/s, est. speed input: 17526.49 toks/s, output: 17.12 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:24<00:05, 16.66it/s, est. speed input: 17522.99 toks/s, output: 17.11 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:25<00:04, 16.71it/s, est. speed input: 17520.14 toks/s, output: 17.11 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:25<00:04, 16.78it/s, est. speed input: 17518.59 toks/s, output: 17.11 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:25<00:04, 16.68it/s, est. speed input: 17512.10 toks/s, output: 17.10 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:25<00:04, 16.65it/s, est. speed input: 17507.29 toks/s, output: 17.10 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:26<00:03, 16.69it/s, est. speed input: 17504.37 toks/s, output: 17.09 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:26<00:03, 17.62it/s, est. speed input: 17528.73 toks/s, output: 17.12 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:26<00:03, 17.28it/s, est. speed input: 17523.28 toks/s, output: 17.11 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:26<00:03, 17.05it/s, est. speed input: 17517.85 toks/s, output: 17.11 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:27<00:02, 16.92it/s, est. speed input: 17513.47 toks/s, output: 17.10 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:27<00:02, 16.91it/s, est. speed input: 17511.52 toks/s, output: 17.10 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:27<00:02, 16.84it/s, est. speed input: 17507.86 toks/s, output: 17.10 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:27<00:02, 16.75it/s, est. speed input: 17502.83 toks/s, output: 17.09 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:27<00:02, 16.70it/s, est. speed input: 17498.25 toks/s, output: 17.09 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:28<00:01, 16.73it/s, est. speed input: 17495.83 toks/s, output: 17.09 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:28<00:01, 16.77it/s, est. speed input: 17493.83 toks/s, output: 17.08 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:28<00:01, 16.72it/s, est. speed input: 17489.77 toks/s, output: 17.08 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:28<00:01, 16.62it/s, est. speed input: 17483.90 toks/s, output: 17.07 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:29<00:00, 16.67it/s, est. speed input: 17481.54 toks/s, output: 17.07 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:29<00:00, 16.71it/s, est. speed input: 17479.30 toks/s, output: 17.07 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:29<00:00, 16.70it/s, est. speed input: 17476.13 toks/s, output: 17.07 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:29<00:00, 17.60it/s, est. speed input: 17497.00 toks/s, output: 17.09 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:29<00:00, 17.60it/s, est. speed input: 17565.55 toks/s, output: 17.15 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:29<00:00, 17.15it/s, est. speed input: 17565.55 toks/s, output: 17.15 toks/s]
[rank0]:[W128 08:54:30.551155912 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 60.7s

测试结果:
  Requests/s:   16.65
  Tokens/s:     17061.26
  Total Reqs:   512
  Elapsed:      30.76s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     17044.61

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:54:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:54:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3716334) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3716334) WARNING 01-28 08:55:02 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.86 requests/s, 17285.40 total tokens/s, 16.86 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 08:54:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:54:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:54:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:54:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:54:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:54:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:54:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:54:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:54:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:54:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:54:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:54:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:54:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:54:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:54:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:54:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:54:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:54:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:54:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:54:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:54:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:54:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:54:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:54:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:54:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:54:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:54:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:54:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:43] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:43] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:43] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:43] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:43] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3716334) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3716334) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.48s/it]
(EngineCore_DP0 pid=3716334) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.48s/it]
(EngineCore_DP0 pid=3716334) 
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3716334) [2026-01-28 08:54:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3716334) 2026-01-28 08:55:01,692 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3716334) 2026-01-28 08:55:01,746 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 58/1024 [00:00<00:01, 573.63it/s]
Adding requests:  11%|█▏        | 116/1024 [00:00<00:01, 558.84it/s]
Adding requests:  17%|█▋        | 172/1024 [00:00<00:01, 514.67it/s]
Adding requests:  22%|██▏       | 224/1024 [00:00<00:01, 504.81it/s]
Adding requests:  27%|██▋       | 276/1024 [00:00<00:01, 508.19it/s]
Adding requests:  32%|███▏      | 327/1024 [00:00<00:01, 507.01it/s]
Adding requests:  37%|███▋      | 378/1024 [00:00<00:01, 500.72it/s]
Adding requests:  42%|████▏     | 429/1024 [00:00<00:01, 499.34it/s]
Adding requests:  47%|████▋     | 479/1024 [00:00<00:01, 498.81it/s]
Adding requests:  52%|█████▏    | 529/1024 [00:01<00:01, 491.11it/s]
Adding requests:  57%|█████▋    | 581/1024 [00:01<00:00, 496.35it/s]
Adding requests:  62%|██████▏   | 631/1024 [00:01<00:00, 482.59it/s]
Adding requests:  67%|██████▋   | 682/1024 [00:01<00:00, 489.07it/s]
Adding requests:  72%|███████▏  | 735/1024 [00:01<00:00, 498.80it/s]
Adding requests:  77%|███████▋  | 785/1024 [00:01<00:00, 493.39it/s]
Adding requests:  82%|████████▏ | 835/1024 [00:01<00:00, 479.77it/s]
Adding requests:  86%|████████▋ | 885/1024 [00:01<00:00, 484.48it/s]
Adding requests:  91%|█████████▏| 936/1024 [00:01<00:00, 490.15it/s]
Adding requests:  96%|█████████▋| 986/1024 [00:01<00:00, 492.14it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 498.04it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 34/1024 [00:00<00:14, 67.78it/s, est. speed input: 69409.36 toks/s, output: 67.78 toks/s]
Processed prompts:   4%|▍         | 42/1024 [00:00<00:25, 38.64it/s, est. speed input: 44182.74 toks/s, output: 43.15 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:01<00:33, 28.74it/s, est. speed input: 35301.47 toks/s, output: 34.47 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:01<00:40, 24.14it/s, est. speed input: 30901.65 toks/s, output: 30.18 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:02<00:44, 21.56it/s, est. speed input: 28227.11 toks/s, output: 27.57 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:02<00:47, 19.97it/s, est. speed input: 26419.00 toks/s, output: 25.80 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:03<00:49, 19.01it/s, est. speed input: 25150.58 toks/s, output: 24.56 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:03<00:50, 18.32it/s, est. speed input: 24164.75 toks/s, output: 23.60 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:04<00:51, 17.88it/s, est. speed input: 23409.94 toks/s, output: 22.86 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:04<00:52, 17.53it/s, est. speed input: 22784.33 toks/s, output: 22.25 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:05<00:52, 17.34it/s, est. speed input: 22288.89 toks/s, output: 21.77 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:05<00:52, 17.16it/s, est. speed input: 21859.32 toks/s, output: 21.35 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:06<00:52, 17.08it/s, est. speed input: 21511.89 toks/s, output: 21.01 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:06<00:51, 17.05it/s, est. speed input: 21220.41 toks/s, output: 20.72 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:07<00:51, 16.96it/s, est. speed input: 20946.82 toks/s, output: 20.46 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:07<00:51, 16.95it/s, est. speed input: 20722.08 toks/s, output: 20.24 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:08<00:51, 16.90it/s, est. speed input: 20513.74 toks/s, output: 20.03 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:08<00:50, 16.90it/s, est. speed input: 20336.73 toks/s, output: 19.86 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:09<00:50, 16.85it/s, est. speed input: 20167.07 toks/s, output: 19.69 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:09<00:49, 16.88it/s, est. speed input: 20027.48 toks/s, output: 19.56 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:09<00:49, 16.85it/s, est. speed input: 19891.68 toks/s, output: 19.43 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:10<00:48, 16.86it/s, est. speed input: 19773.95 toks/s, output: 19.31 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:10<00:48, 16.88it/s, est. speed input: 19668.82 toks/s, output: 19.21 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:11<00:47, 16.86it/s, est. speed input: 19566.23 toks/s, output: 19.11 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:11<00:47, 16.91it/s, est. speed input: 19482.37 toks/s, output: 19.03 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:12<00:46, 16.87it/s, est. speed input: 19393.17 toks/s, output: 18.94 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:12<00:46, 16.88it/s, est. speed input: 19315.66 toks/s, output: 18.86 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:13<00:45, 16.84it/s, est. speed input: 19238.37 toks/s, output: 18.79 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:13<00:45, 16.87it/s, est. speed input: 19173.18 toks/s, output: 18.72 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:14<00:45, 16.83it/s, est. speed input: 19104.84 toks/s, output: 18.66 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:14<00:44, 16.86it/s, est. speed input: 19048.51 toks/s, output: 18.60 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:15<00:43, 16.88it/s, est. speed input: 18995.24 toks/s, output: 18.55 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:15<00:43, 16.84it/s, est. speed input: 18939.28 toks/s, output: 18.50 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:16<00:43, 16.87it/s, est. speed input: 18892.30 toks/s, output: 18.45 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:16<00:42, 16.81it/s, est. speed input: 18840.39 toks/s, output: 18.40 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:17<00:42, 16.88it/s, est. speed input: 18802.55 toks/s, output: 18.36 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:17<00:41, 16.83it/s, est. speed input: 18755.75 toks/s, output: 18.32 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:18<00:41, 16.85it/s, est. speed input: 18718.24 toks/s, output: 18.28 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:18<00:39, 17.16it/s, est. speed input: 18709.46 toks/s, output: 18.27 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:18<00:39, 17.09it/s, est. speed input: 18674.82 toks/s, output: 18.24 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:19<00:39, 17.05it/s, est. speed input: 18643.39 toks/s, output: 18.21 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:19<00:39, 16.94it/s, est. speed input: 18606.28 toks/s, output: 18.17 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:20<00:38, 16.94it/s, est. speed input: 18577.12 toks/s, output: 18.14 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:20<00:38, 16.88it/s, est. speed input: 18544.17 toks/s, output: 18.11 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:21<00:37, 16.90it/s, est. speed input: 18518.01 toks/s, output: 18.08 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:21<00:37, 16.85it/s, est. speed input: 18487.84 toks/s, output: 18.05 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:22<00:36, 16.87it/s, est. speed input: 18462.61 toks/s, output: 18.03 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:22<00:36, 16.84it/s, est. speed input: 18435.96 toks/s, output: 18.00 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:23<00:35, 16.86it/s, est. speed input: 18412.66 toks/s, output: 17.98 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:23<00:35, 16.88it/s, est. speed input: 18391.28 toks/s, output: 17.96 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:24<00:34, 16.87it/s, est. speed input: 18368.64 toks/s, output: 17.94 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:24<00:34, 16.88it/s, est. speed input: 18348.47 toks/s, output: 17.92 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:25<00:33, 17.22it/s, est. speed input: 18351.37 toks/s, output: 17.92 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:25<00:33, 17.13it/s, est. speed input: 18332.20 toks/s, output: 17.90 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:26<00:32, 17.02it/s, est. speed input: 18311.00 toks/s, output: 17.88 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:26<00:32, 16.99it/s, est. speed input: 18293.33 toks/s, output: 17.86 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:27<00:32, 16.93it/s, est. speed input: 18274.01 toks/s, output: 17.85 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:27<00:31, 16.92it/s, est. speed input: 18257.57 toks/s, output: 17.83 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:27<00:31, 16.91it/s, est. speed input: 18241.07 toks/s, output: 17.81 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:28<00:30, 16.86it/s, est. speed input: 18222.64 toks/s, output: 17.80 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:28<00:30, 16.87it/s, est. speed input: 18207.47 toks/s, output: 17.78 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:29<00:29, 16.85it/s, est. speed input: 18191.45 toks/s, output: 17.77 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:29<00:29, 16.87it/s, est. speed input: 18177.81 toks/s, output: 17.75 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:30<00:28, 16.83it/s, est. speed input: 18161.52 toks/s, output: 17.74 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:30<00:28, 16.86it/s, est. speed input: 18148.92 toks/s, output: 17.72 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:31<00:27, 16.83it/s, est. speed input: 18133.56 toks/s, output: 17.71 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:31<00:27, 16.84it/s, est. speed input: 18120.97 toks/s, output: 17.70 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:32<00:26, 16.86it/s, est. speed input: 18109.07 toks/s, output: 17.68 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:32<00:26, 16.84it/s, est. speed input: 18095.51 toks/s, output: 17.67 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:33<00:25, 16.87it/s, est. speed input: 18085.21 toks/s, output: 17.66 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:33<00:25, 16.82it/s, est. speed input: 18071.33 toks/s, output: 17.65 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:34<00:25, 16.87it/s, est. speed input: 18061.78 toks/s, output: 17.64 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:34<00:24, 16.83it/s, est. speed input: 18048.96 toks/s, output: 17.63 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:35<00:24, 16.86it/s, est. speed input: 18039.60 toks/s, output: 17.62 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:35<00:23, 16.85it/s, est. speed input: 18028.76 toks/s, output: 17.61 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:36<00:23, 16.89it/s, est. speed input: 18020.58 toks/s, output: 17.60 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:36<00:22, 16.90it/s, est. speed input: 18011.64 toks/s, output: 17.59 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:36<00:22, 16.86it/s, est. speed input: 18000.66 toks/s, output: 17.58 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:37<00:21, 16.89it/s, est. speed input: 17992.79 toks/s, output: 17.57 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:37<00:21, 16.84it/s, est. speed input: 17981.84 toks/s, output: 17.56 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:38<00:20, 16.87it/s, est. speed input: 17973.81 toks/s, output: 17.55 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:38<00:20, 16.83it/s, est. speed input: 17963.67 toks/s, output: 17.54 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:39<00:19, 16.86it/s, est. speed input: 17956.13 toks/s, output: 17.54 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:39<00:19, 16.82it/s, est. speed input: 17946.02 toks/s, output: 17.53 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:40<00:18, 16.86it/s, est. speed input: 17939.24 toks/s, output: 17.52 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:40<00:18, 16.88it/s, est. speed input: 17932.21 toks/s, output: 17.51 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:41<00:17, 16.82it/s, est. speed input: 17922.55 toks/s, output: 17.50 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:41<00:17, 16.87it/s, est. speed input: 17916.53 toks/s, output: 17.50 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:42<00:16, 16.83it/s, est. speed input: 17907.72 toks/s, output: 17.49 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:42<00:16, 16.87it/s, est. speed input: 17901.95 toks/s, output: 17.48 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:43<00:16, 16.84it/s, est. speed input: 17893.67 toks/s, output: 17.47 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:43<00:15, 16.85it/s, est. speed input: 17887.12 toks/s, output: 17.47 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:44<00:15, 16.81it/s, est. speed input: 17878.72 toks/s, output: 17.46 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:44<00:14, 16.85it/s, est. speed input: 17873.22 toks/s, output: 17.45 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:45<00:14, 16.88it/s, est. speed input: 17867.85 toks/s, output: 17.45 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:45<00:13, 16.87it/s, est. speed input: 17861.25 toks/s, output: 17.44 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:45<00:13, 16.87it/s, est. speed input: 17855.32 toks/s, output: 17.44 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:46<00:12, 16.88it/s, est. speed input: 17849.80 toks/s, output: 17.43 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:46<00:12, 16.88it/s, est. speed input: 17844.12 toks/s, output: 17.43 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:47<00:11, 16.84it/s, est. speed input: 17837.07 toks/s, output: 17.42 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:47<00:11, 16.86it/s, est. speed input: 17831.90 toks/s, output: 17.41 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:48<00:10, 16.83it/s, est. speed input: 17825.37 toks/s, output: 17.41 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:48<00:10, 16.86it/s, est. speed input: 17820.48 toks/s, output: 17.40 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:49<00:09, 16.88it/s, est. speed input: 17815.77 toks/s, output: 17.40 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:49<00:09, 16.83it/s, est. speed input: 17809.07 toks/s, output: 17.39 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:50<00:08, 16.88it/s, est. speed input: 17805.18 toks/s, output: 17.39 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:50<00:08, 16.84it/s, est. speed input: 17799.18 toks/s, output: 17.38 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:51<00:07, 16.86it/s, est. speed input: 17794.47 toks/s, output: 17.38 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:51<00:07, 16.82it/s, est. speed input: 17788.54 toks/s, output: 17.37 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:52<00:07, 16.85it/s, est. speed input: 17784.37 toks/s, output: 17.37 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:52<00:06, 16.82it/s, est. speed input: 17778.61 toks/s, output: 17.36 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:53<00:06, 16.86it/s, est. speed input: 17774.83 toks/s, output: 17.36 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:53<00:05, 16.88it/s, est. speed input: 17771.00 toks/s, output: 17.35 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:54<00:04, 17.42it/s, est. speed input: 17782.84 toks/s, output: 17.37 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:54<00:04, 17.25it/s, est. speed input: 17778.43 toks/s, output: 17.36 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:54<00:04, 17.09it/s, est. speed input: 17772.83 toks/s, output: 17.36 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:55<00:03, 17.05it/s, est. speed input: 17769.37 toks/s, output: 17.35 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:55<00:03, 16.95it/s, est. speed input: 17763.65 toks/s, output: 17.35 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:56<00:02, 16.96it/s, est. speed input: 17760.64 toks/s, output: 17.34 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:56<00:02, 17.58it/s, est. speed input: 17774.59 toks/s, output: 17.36 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:57<00:01, 17.31it/s, est. speed input: 17769.20 toks/s, output: 17.35 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:57<00:01, 17.20it/s, est. speed input: 17765.82 toks/s, output: 17.35 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:58<00:00, 17.08it/s, est. speed input: 17761.21 toks/s, output: 17.34 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:58<00:00, 17.47it/s, est. speed input: 17769.56 toks/s, output: 17.35 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:58<00:00, 17.47it/s, est. speed input: 17874.25 toks/s, output: 17.46 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:58<00:00, 17.46it/s, est. speed input: 17874.25 toks/s, output: 17.46 toks/s]
[rank0]:[W128 08:56:03.594419514 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 93.0s

测试结果:
  Requests/s:   16.86
  Tokens/s:     17285.40
  Total Reqs:   1024
  Elapsed:      60.72s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     17268.54

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:56:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:56:15 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3717843) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3717843) WARNING 01-28 08:56:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 17.11 requests/s, 17540.62 total tokens/s, 17.11 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 08:56:15] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:56:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:56:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:56:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:56:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:56:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:56:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:56:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:56:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:56:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:56:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:56:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:56:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:56:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:56:18] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:56:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:56:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:56:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:56:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:56:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:56:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:56:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:56:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:56:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:56:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:56:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:56:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:56:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:19] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3717843) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3717843) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.50s/it]
(EngineCore_DP0 pid=3717843) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.50s/it]
(EngineCore_DP0 pid=3717843) 
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3717843) [2026-01-28 08:56:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3717843) 2026-01-28 08:56:37,943 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3717843) 2026-01-28 08:56:38,045 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 66/2048 [00:00<00:03, 653.18it/s]
Adding requests:   6%|▋         | 132/2048 [00:00<00:03, 611.12it/s]
Adding requests:   9%|▉         | 194/2048 [00:00<00:03, 546.62it/s]
Adding requests:  12%|█▏        | 250/2048 [00:00<00:03, 541.64it/s]
Adding requests:  15%|█▍        | 305/2048 [00:00<00:03, 530.02it/s]
Adding requests:  18%|█▊        | 359/2048 [00:00<00:03, 532.71it/s]
Adding requests:  20%|██        | 413/2048 [00:00<00:03, 527.84it/s]
Adding requests:  23%|██▎       | 467/2048 [00:00<00:02, 529.81it/s]
Adding requests:  25%|██▌       | 521/2048 [00:00<00:02, 513.18it/s]
Adding requests:  28%|██▊       | 573/2048 [00:01<00:02, 513.26it/s]
Adding requests:  31%|███       | 626/2048 [00:01<00:02, 513.94it/s]
Adding requests:  33%|███▎      | 678/2048 [00:01<00:02, 510.81it/s]
Adding requests:  36%|███▌      | 733/2048 [00:01<00:02, 519.37it/s]
Adding requests:  38%|███▊      | 785/2048 [00:01<00:02, 494.27it/s]
Adding requests:  41%|████      | 835/2048 [00:01<00:02, 491.44it/s]
Adding requests:  43%|████▎     | 885/2048 [00:02<00:06, 175.66it/s]
Adding requests:  46%|████▌     | 935/2048 [00:02<00:05, 217.03it/s]
Adding requests:  48%|████▊     | 984/2048 [00:02<00:04, 259.12it/s]
Adding requests:  51%|█████     | 1037/2048 [00:02<00:03, 307.01it/s]
Adding requests:  53%|█████▎    | 1088/2048 [00:02<00:02, 347.84it/s]
Adding requests:  56%|█████▌    | 1139/2048 [00:02<00:02, 382.80it/s]
Adding requests:  58%|█████▊    | 1195/2048 [00:02<00:02, 424.49it/s]
Adding requests:  61%|██████    | 1247/2048 [00:03<00:01, 448.17it/s]
Adding requests:  63%|██████▎   | 1298/2048 [00:03<00:01, 455.86it/s]
Adding requests:  66%|██████▌   | 1348/2048 [00:03<00:01, 463.69it/s]
Adding requests:  68%|██████▊   | 1399/2048 [00:03<00:01, 475.84it/s]
Adding requests:  71%|███████   | 1449/2048 [00:03<00:01, 481.92it/s]
Adding requests:  73%|███████▎  | 1502/2048 [00:03<00:01, 494.31it/s]
Adding requests:  76%|███████▌  | 1553/2048 [00:03<00:01, 494.22it/s]
Adding requests:  78%|███████▊  | 1607/2048 [00:03<00:00, 506.22it/s]
Adding requests:  81%|████████  | 1659/2048 [00:03<00:00, 502.17it/s]
Adding requests:  83%|████████▎ | 1710/2048 [00:03<00:00, 501.67it/s]
Adding requests:  86%|████████▌ | 1761/2048 [00:04<00:00, 500.18it/s]
Adding requests:  89%|████████▊ | 1813/2048 [00:04<00:00, 505.55it/s]
Adding requests:  91%|█████████ | 1864/2048 [00:04<00:00, 501.96it/s]
Adding requests:  94%|█████████▎| 1915/2048 [00:04<00:00, 504.19it/s]
Adding requests:  96%|█████████▌| 1966/2048 [00:04<00:00, 498.00it/s]
Adding requests:  98%|█████████▊| 2016/2048 [00:04<00:00, 485.33it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 440.50it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 66/2048 [00:00<00:07, 276.94it/s, est. speed input: 283620.43 toks/s, output: 276.95 toks/s]
Processed prompts:   5%|▍         | 94/2048 [00:01<00:29, 67.23it/s, est. speed input: 81907.29 toks/s, output: 79.99 toks/s]   
Processed prompts:   5%|▌         | 108/2048 [00:02<00:50, 38.61it/s, est. speed input: 52397.59 toks/s, output: 51.17 toks/s]
Processed prompts:   6%|▌         | 116/2048 [00:03<01:15, 25.43it/s, est. speed input: 39015.13 toks/s, output: 38.10 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:03<01:29, 21.38it/s, est. speed input: 33429.52 toks/s, output: 32.65 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:04<01:35, 19.87it/s, est. speed input: 30410.05 toks/s, output: 29.70 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:05<01:39, 18.93it/s, est. speed input: 28345.18 toks/s, output: 27.68 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:06<01:41, 18.37it/s, est. speed input: 26870.41 toks/s, output: 26.24 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:07<01:43, 17.95it/s, est. speed input: 25728.82 toks/s, output: 25.13 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:08<01:44, 17.66it/s, est. speed input: 24826.60 toks/s, output: 24.24 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:09<01:44, 17.49it/s, est. speed input: 24113.47 toks/s, output: 23.55 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:10<01:44, 17.36it/s, est. speed input: 23522.75 toks/s, output: 22.97 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:11<01:43, 17.28it/s, est. speed input: 23032.32 toks/s, output: 22.49 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:12<01:43, 17.21it/s, est. speed input: 22611.37 toks/s, output: 22.08 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:13<01:42, 17.19it/s, est. speed input: 22257.65 toks/s, output: 21.74 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:14<01:41, 17.21it/s, est. speed input: 21959.67 toks/s, output: 21.44 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:15<01:40, 17.16it/s, est. speed input: 21681.17 toks/s, output: 21.17 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:16<01:39, 17.27it/s, est. speed input: 21470.37 toks/s, output: 20.97 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:17<01:38, 17.22it/s, est. speed input: 21253.47 toks/s, output: 20.76 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:17<01:37, 17.18it/s, est. speed input: 21057.66 toks/s, output: 20.56 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:18<01:36, 17.16it/s, est. speed input: 20884.04 toks/s, output: 20.39 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:19<01:36, 17.14it/s, est. speed input: 20723.41 toks/s, output: 20.24 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:20<01:35, 17.12it/s, est. speed input: 20577.29 toks/s, output: 20.09 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:21<01:34, 17.11it/s, est. speed input: 20444.51 toks/s, output: 19.97 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:22<01:32, 17.21it/s, est. speed input: 20340.10 toks/s, output: 19.86 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:23<01:32, 17.20it/s, est. speed input: 20231.04 toks/s, output: 19.76 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:24<01:31, 17.17it/s, est. speed input: 20126.98 toks/s, output: 19.66 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:25<01:30, 17.15it/s, est. speed input: 20031.36 toks/s, output: 19.56 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:26<01:29, 17.14it/s, est. speed input: 19942.53 toks/s, output: 19.48 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:27<01:28, 17.12it/s, est. speed input: 19858.68 toks/s, output: 19.39 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:28<01:27, 17.10it/s, est. speed input: 19778.76 toks/s, output: 19.32 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:29<01:26, 17.12it/s, est. speed input: 19708.89 toks/s, output: 19.25 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:30<01:26, 17.08it/s, est. speed input: 19636.39 toks/s, output: 19.18 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:31<01:25, 17.08it/s, est. speed input: 19571.28 toks/s, output: 19.11 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:32<01:24, 17.08it/s, est. speed input: 19511.00 toks/s, output: 19.05 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:32<01:23, 17.05it/s, est. speed input: 19450.41 toks/s, output: 18.99 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:33<01:22, 17.07it/s, est. speed input: 19397.62 toks/s, output: 18.94 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:34<01:21, 17.08it/s, est. speed input: 19347.07 toks/s, output: 18.89 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:35<01:20, 17.09it/s, est. speed input: 19298.88 toks/s, output: 18.85 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:36<01:19, 17.09it/s, est. speed input: 19253.40 toks/s, output: 18.80 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:37<01:18, 17.05it/s, est. speed input: 19206.14 toks/s, output: 18.76 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:38<01:17, 17.06it/s, est. speed input: 19164.05 toks/s, output: 18.71 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:39<01:16, 17.04it/s, est. speed input: 19122.44 toks/s, output: 18.67 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:40<01:15, 17.05it/s, est. speed input: 19084.37 toks/s, output: 18.64 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:41<01:14, 17.08it/s, est. speed input: 19049.99 toks/s, output: 18.60 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:42<01:13, 17.08it/s, est. speed input: 19015.37 toks/s, output: 18.57 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:43<01:13, 17.06it/s, est. speed input: 18981.10 toks/s, output: 18.54 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:44<01:12, 17.07it/s, est. speed input: 18949.37 toks/s, output: 18.51 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:45<01:11, 17.03it/s, est. speed input: 18916.15 toks/s, output: 18.47 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:46<01:10, 17.05it/s, est. speed input: 18887.66 toks/s, output: 18.44 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:47<01:09, 17.05it/s, est. speed input: 18858.92 toks/s, output: 18.42 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:47<01:08, 17.08it/s, est. speed input: 18833.52 toks/s, output: 18.39 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:48<01:07, 17.03it/s, est. speed input: 18804.43 toks/s, output: 18.36 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:49<01:06, 17.05it/s, est. speed input: 18779.80 toks/s, output: 18.34 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:50<01:04, 17.36it/s, est. speed input: 18776.33 toks/s, output: 18.34 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:51<01:03, 17.25it/s, est. speed input: 18751.54 toks/s, output: 18.31 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:52<01:03, 17.18it/s, est. speed input: 18728.01 toks/s, output: 18.29 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:53<01:01, 17.43it/s, est. speed input: 18723.42 toks/s, output: 18.28 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:54<01:00, 17.32it/s, est. speed input: 18702.24 toks/s, output: 18.26 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:55<01:00, 17.25it/s, est. speed input: 18681.63 toks/s, output: 18.24 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:56<00:59, 17.21it/s, est. speed input: 18662.37 toks/s, output: 18.22 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:57<00:58, 17.14it/s, est. speed input: 18641.61 toks/s, output: 18.20 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:58<00:57, 17.11it/s, est. speed input: 18622.38 toks/s, output: 18.19 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:59<00:57, 17.09it/s, est. speed input: 18603.50 toks/s, output: 18.17 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [01:00<00:56, 17.07it/s, est. speed input: 18585.44 toks/s, output: 18.15 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [01:00<00:55, 17.09it/s, est. speed input: 18569.23 toks/s, output: 18.13 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [01:01<00:54, 17.06it/s, est. speed input: 18551.43 toks/s, output: 18.12 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [01:02<00:53, 17.06it/s, est. speed input: 18535.55 toks/s, output: 18.10 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [01:03<00:51, 17.36it/s, est. speed input: 18535.38 toks/s, output: 18.10 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [01:04<00:50, 17.25it/s, est. speed input: 18519.13 toks/s, output: 18.09 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [01:05<00:50, 17.20it/s, est. speed input: 18504.30 toks/s, output: 18.07 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [01:06<00:49, 17.17it/s, est. speed input: 18490.63 toks/s, output: 18.06 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [01:07<00:48, 17.12it/s, est. speed input: 18475.60 toks/s, output: 18.04 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [01:08<00:47, 17.10it/s, est. speed input: 18461.54 toks/s, output: 18.03 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [01:09<00:46, 17.09it/s, est. speed input: 18448.12 toks/s, output: 18.02 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [01:10<00:44, 17.38it/s, est. speed input: 18449.22 toks/s, output: 18.02 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [01:11<00:44, 17.26it/s, est. speed input: 18435.20 toks/s, output: 18.00 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [01:12<00:42, 17.47it/s, est. speed input: 18434.93 toks/s, output: 18.00 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [01:13<00:42, 17.35it/s, est. speed input: 18422.67 toks/s, output: 17.99 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [01:13<00:41, 17.23it/s, est. speed input: 18409.06 toks/s, output: 17.98 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [01:14<00:40, 17.13it/s, est. speed input: 18395.43 toks/s, output: 17.96 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [01:15<00:40, 17.12it/s, est. speed input: 18384.47 toks/s, output: 17.95 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [01:16<00:39, 17.07it/s, est. speed input: 18371.61 toks/s, output: 17.94 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [01:17<00:38, 17.02it/s, est. speed input: 18358.96 toks/s, output: 17.93 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [01:18<00:37, 17.04it/s, est. speed input: 18348.59 toks/s, output: 17.92 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [01:19<00:36, 17.04it/s, est. speed input: 18338.14 toks/s, output: 17.91 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [01:20<00:35, 17.02it/s, est. speed input: 18326.93 toks/s, output: 17.90 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [01:21<00:34, 17.02it/s, est. speed input: 18316.67 toks/s, output: 17.89 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [01:22<00:33, 17.05it/s, est. speed input: 18307.73 toks/s, output: 17.88 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [01:23<00:32, 17.04it/s, est. speed input: 18297.82 toks/s, output: 17.87 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [01:24<00:31, 17.03it/s, est. speed input: 18287.79 toks/s, output: 17.86 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [01:25<00:30, 16.99it/s, est. speed input: 18277.04 toks/s, output: 17.85 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [01:26<00:30, 16.99it/s, est. speed input: 18267.40 toks/s, output: 17.84 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [01:27<00:29, 17.02it/s, est. speed input: 18258.99 toks/s, output: 17.83 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [01:28<00:28, 16.98it/s, est. speed input: 18248.78 toks/s, output: 17.82 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [01:28<00:26, 17.30it/s, est. speed input: 18251.55 toks/s, output: 17.82 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [01:29<00:25, 17.21it/s, est. speed input: 18242.84 toks/s, output: 17.82 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [01:30<00:25, 17.18it/s, est. speed input: 18235.31 toks/s, output: 17.81 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [01:31<00:24, 17.11it/s, est. speed input: 18226.37 toks/s, output: 17.80 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [01:32<00:22, 17.38it/s, est. speed input: 18228.80 toks/s, output: 17.80 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [01:33<00:22, 17.29it/s, est. speed input: 18221.50 toks/s, output: 17.79 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [01:34<00:21, 17.22it/s, est. speed input: 18213.97 toks/s, output: 17.79 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [01:35<00:20, 17.18it/s, est. speed input: 18206.90 toks/s, output: 17.78 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [01:36<00:19, 17.13it/s, est. speed input: 18199.27 toks/s, output: 17.77 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [01:37<00:18, 17.10it/s, est. speed input: 18192.07 toks/s, output: 17.77 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [01:38<00:17, 17.09it/s, est. speed input: 18185.14 toks/s, output: 17.76 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [01:39<00:16, 17.09it/s, est. speed input: 18178.80 toks/s, output: 17.75 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [01:40<00:15, 17.10it/s, est. speed input: 18172.81 toks/s, output: 17.75 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [01:41<00:14, 17.09it/s, est. speed input: 18166.43 toks/s, output: 17.74 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [01:42<00:13, 17.05it/s, est. speed input: 18158.94 toks/s, output: 17.73 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [01:43<00:13, 17.05it/s, est. speed input: 18152.54 toks/s, output: 17.73 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [01:43<00:12, 17.05it/s, est. speed input: 18146.36 toks/s, output: 17.72 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [01:44<00:11, 17.02it/s, est. speed input: 18139.28 toks/s, output: 17.71 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [01:45<00:10, 17.31it/s, est. speed input: 18141.88 toks/s, output: 17.72 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [01:46<00:09, 17.22it/s, est. speed input: 18135.64 toks/s, output: 17.71 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [01:47<00:08, 17.16it/s, est. speed input: 18129.50 toks/s, output: 17.70 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [01:48<00:07, 17.10it/s, est. speed input: 18122.99 toks/s, output: 17.70 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [01:49<00:06, 17.10it/s, est. speed input: 18117.65 toks/s, output: 17.69 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [01:50<00:05, 17.39it/s, est. speed input: 18121.01 toks/s, output: 17.70 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [01:51<00:04, 17.27it/s, est. speed input: 18114.90 toks/s, output: 17.69 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [01:52<00:03, 17.49it/s, est. speed input: 18117.57 toks/s, output: 17.69 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [01:53<00:02, 17.33it/s, est. speed input: 18111.45 toks/s, output: 17.69 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [01:54<00:01, 17.24it/s, est. speed input: 18105.85 toks/s, output: 17.68 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [01:55<00:00, 17.42it/s, est. speed input: 18107.39 toks/s, output: 17.68 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [01:55<00:00, 17.42it/s, est. speed input: 18231.99 toks/s, output: 17.80 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [01:55<00:00, 17.80it/s, est. speed input: 18231.99 toks/s, output: 17.80 toks/s]
[rank0]:[W128 08:58:39.719190807 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 156.1s

测试结果:
  Requests/s:   17.11
  Tokens/s:     17540.62
  Total Reqs:   2048
  Elapsed:      119.68s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     17523.51

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:58:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 08:58:56 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3720238) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3720238) WARNING 01-28 08:59:22 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.81 requests/s, 17225.16 total tokens/s, 16.81 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 08:58:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:58:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:58:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:58:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:58:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:58:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:58:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:58:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:58:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:58:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 08:58:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:58:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:58:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:58:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:58:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:58:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:58:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:58:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:00] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:00] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:00] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:00] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:00] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3720238) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3720238) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.62s/it]
(EngineCore_DP0 pid=3720238) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.62s/it]
(EngineCore_DP0 pid=3720238) 
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3720238) [2026-01-28 08:59:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3720238) 2026-01-28 08:59:20,375 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3720238) 2026-01-28 08:59:20,500 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 65/4096 [00:00<00:06, 649.14it/s]
Adding requests:   3%|▎         | 130/4096 [00:00<00:06, 625.91it/s]
Adding requests:   5%|▍         | 193/4096 [00:00<00:07, 545.66it/s]
Adding requests:   6%|▌         | 249/4096 [00:00<00:07, 524.86it/s]
Adding requests:   7%|▋         | 303/4096 [00:00<00:07, 512.46it/s]
Adding requests:   9%|▊         | 356/4096 [00:00<00:07, 515.55it/s]
Adding requests:  10%|▉         | 408/4096 [00:00<00:07, 515.82it/s]
Adding requests:  11%|█         | 460/4096 [00:00<00:07, 511.66it/s]
Adding requests:  12%|█▎        | 512/4096 [00:00<00:07, 502.28it/s]
Adding requests:  14%|█▎        | 563/4096 [00:01<00:07, 495.81it/s]
Adding requests:  15%|█▍        | 613/4096 [00:01<00:07, 491.23it/s]
Adding requests:  16%|█▌        | 663/4096 [00:01<00:06, 492.79it/s]
Adding requests:  17%|█▋        | 713/4096 [00:01<00:06, 488.52it/s]
Adding requests:  19%|█▊        | 762/4096 [00:01<00:06, 476.79it/s]
Adding requests:  20%|█▉        | 810/4096 [00:01<00:06, 476.06it/s]
Adding requests:  21%|██        | 858/4096 [00:01<00:06, 473.30it/s]
Adding requests:  22%|██▏       | 909/4096 [00:01<00:06, 483.94it/s]
Adding requests:  23%|██▎       | 958/4096 [00:01<00:06, 475.28it/s]
Adding requests:  25%|██▍       | 1006/4096 [00:02<00:06, 470.66it/s]
Adding requests:  26%|██▌       | 1055/4096 [00:02<00:06, 474.50it/s]
Adding requests:  27%|██▋       | 1103/4096 [00:02<00:06, 472.96it/s]
Adding requests:  28%|██▊       | 1151/4096 [00:02<00:06, 471.37it/s]
Adding requests:  29%|██▉       | 1200/4096 [00:02<00:06, 474.99it/s]
Adding requests:  30%|███       | 1248/4096 [00:02<00:06, 473.20it/s]
Adding requests:  32%|███▏      | 1296/4096 [00:02<00:06, 444.83it/s]
Adding requests:  33%|███▎      | 1346/4096 [00:02<00:05, 459.33it/s]
Adding requests:  34%|███▍      | 1394/4096 [00:02<00:05, 465.05it/s]
Adding requests:  35%|███▌      | 1441/4096 [00:02<00:05, 463.39it/s]
Adding requests:  36%|███▋      | 1492/4096 [00:03<00:05, 474.53it/s]
Adding requests:  38%|███▊      | 1541/4096 [00:03<00:05, 476.19it/s]
Adding requests:  39%|███▉      | 1591/4096 [00:03<00:05, 481.06it/s]
Adding requests:  40%|████      | 1640/4096 [00:03<00:05, 480.57it/s]
Adding requests:  41%|████      | 1689/4096 [00:03<00:05, 476.55it/s]
Adding requests:  42%|████▏     | 1737/4096 [00:03<00:04, 475.45it/s]
Adding requests:  44%|████▎     | 1785/4096 [00:03<00:04, 471.74it/s]
Adding requests:  45%|████▍     | 1834/4096 [00:03<00:04, 476.66it/s]
Adding requests:  46%|████▌     | 1882/4096 [00:03<00:04, 476.18it/s]
Adding requests:  47%|████▋     | 1930/4096 [00:03<00:04, 470.91it/s]
Adding requests:  48%|████▊     | 1978/4096 [00:04<00:04, 469.27it/s]
Adding requests:  49%|████▉     | 2026/4096 [00:04<00:04, 471.42it/s]
Adding requests:  51%|█████     | 2075/4096 [00:04<00:04, 474.99it/s]
Adding requests:  52%|█████▏    | 2123/4096 [00:04<00:04, 475.32it/s]
Adding requests:  53%|█████▎    | 2171/4096 [00:04<00:04, 472.80it/s]
Adding requests:  54%|█████▍    | 2219/4096 [00:04<00:03, 472.44it/s]
Adding requests:  55%|█████▌    | 2269/4096 [00:04<00:03, 478.37it/s]
Adding requests:  57%|█████▋    | 2319/4096 [00:04<00:03, 482.70it/s]
Adding requests:  58%|█████▊    | 2368/4096 [00:04<00:03, 483.22it/s]
Adding requests:  59%|█████▉    | 2417/4096 [00:04<00:03, 483.00it/s]
Adding requests:  60%|██████    | 2466/4096 [00:05<00:03, 477.87it/s]
Adding requests:  61%|██████▏   | 2516/4096 [00:05<00:03, 481.95it/s]
Adding requests:  63%|██████▎   | 2565/4096 [00:05<00:03, 443.60it/s]
Adding requests:  64%|██████▎   | 2611/4096 [00:05<00:03, 446.75it/s]
Adding requests:  65%|██████▍   | 2661/4096 [00:05<00:03, 461.70it/s]
Adding requests:  66%|██████▌   | 2710/4096 [00:05<00:02, 467.90it/s]
Adding requests:  67%|██████▋   | 2761/4096 [00:05<00:02, 476.80it/s]
Adding requests:  69%|██████▊   | 2811/4096 [00:05<00:02, 481.03it/s]
Adding requests:  70%|██████▉   | 2860/4096 [00:05<00:02, 477.73it/s]
Adding requests:  71%|███████   | 2908/4096 [00:06<00:02, 477.37it/s]
Adding requests:  72%|███████▏  | 2958/4096 [00:06<00:02, 481.83it/s]
Adding requests:  73%|███████▎  | 3007/4096 [00:06<00:02, 483.85it/s]
Adding requests:  75%|███████▍  | 3059/4096 [00:06<00:02, 492.52it/s]
Adding requests:  76%|███████▌  | 3109/4096 [00:06<00:02, 489.41it/s]
Adding requests:  77%|███████▋  | 3158/4096 [00:06<00:01, 486.15it/s]
Adding requests:  78%|███████▊  | 3211/4096 [00:06<00:01, 499.03it/s]
Adding requests:  80%|███████▉  | 3263/4096 [00:06<00:01, 503.14it/s]
Adding requests:  81%|████████  | 3315/4096 [00:06<00:01, 506.05it/s]
Adding requests:  82%|████████▏ | 3367/4096 [00:06<00:01, 507.40it/s]
Adding requests:  83%|████████▎ | 3418/4096 [00:07<00:01, 501.02it/s]
Adding requests:  85%|████████▍ | 3469/4096 [00:07<00:01, 493.27it/s]
Adding requests:  86%|████████▌ | 3519/4096 [00:07<00:01, 486.26it/s]
Adding requests:  87%|████████▋ | 3568/4096 [00:07<00:01, 479.71it/s]
Adding requests:  88%|████████▊ | 3617/4096 [00:07<00:00, 480.01it/s]
Adding requests:  90%|████████▉ | 3668/4096 [00:07<00:00, 486.58it/s]
Adding requests:  91%|█████████ | 3717/4096 [00:07<00:00, 486.49it/s]
Adding requests:  92%|█████████▏| 3771/4096 [00:07<00:00, 500.39it/s]
Adding requests:  93%|█████████▎| 3822/4096 [00:07<00:00, 493.90it/s]
Adding requests:  95%|█████████▍| 3875/4096 [00:07<00:00, 503.84it/s]
Adding requests:  96%|█████████▌| 3926/4096 [00:08<00:00, 467.69it/s]
Adding requests:  97%|█████████▋| 3977/4096 [00:08<00:00, 478.25it/s]
Adding requests:  98%|█████████▊| 4026/4096 [00:08<00:00, 479.16it/s]
Adding requests: 100%|█████████▉| 4077/4096 [00:08<00:00, 486.40it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 483.95it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:01<00:36, 109.37it/s, est. speed input: 111994.54 toks/s, output: 109.37 toks/s]
Processed prompts:   4%|▍         | 162/4096 [00:03<01:27, 45.20it/s, est. speed input: 53895.34 toks/s, output: 52.63 toks/s]   
Processed prompts:   5%|▍         | 194/4096 [00:04<02:05, 31.12it/s, est. speed input: 39988.18 toks/s, output: 39.05 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:06<02:33, 25.25it/s, est. speed input: 33768.84 toks/s, output: 32.98 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:08<02:53, 22.13it/s, est. speed input: 30203.52 toks/s, output: 29.50 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:10<03:07, 20.31it/s, est. speed input: 27905.65 toks/s, output: 27.25 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:12<03:16, 19.25it/s, est. speed input: 26348.53 toks/s, output: 25.73 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:14<03:21, 18.53it/s, est. speed input: 25180.86 toks/s, output: 24.59 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:16<03:25, 18.02it/s, est. speed input: 24270.51 toks/s, output: 23.70 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:18<03:28, 17.67it/s, est. speed input: 23543.70 toks/s, output: 22.99 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:20<03:27, 17.54it/s, est. speed input: 22999.27 toks/s, output: 22.46 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:21<03:28, 17.35it/s, est. speed input: 22508.81 toks/s, output: 21.98 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:23<03:28, 17.21it/s, est. speed input: 22093.77 toks/s, output: 21.58 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:25<03:27, 17.09it/s, est. speed input: 21734.29 toks/s, output: 21.22 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:27<03:26, 17.03it/s, est. speed input: 21428.55 toks/s, output: 20.93 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:29<03:25, 16.98it/s, est. speed input: 21161.58 toks/s, output: 20.67 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:31<03:28, 16.58it/s, est. speed input: 20835.46 toks/s, output: 20.35 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:33<03:25, 16.67it/s, est. speed input: 20634.17 toks/s, output: 20.15 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:35<03:22, 16.75it/s, est. speed input: 20459.31 toks/s, output: 19.98 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:37<03:19, 16.80it/s, est. speed input: 20300.08 toks/s, output: 19.82 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:39<03:17, 16.84it/s, est. speed input: 20157.43 toks/s, output: 19.68 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:41<03:15, 16.85it/s, est. speed input: 20024.29 toks/s, output: 19.55 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:42<03:13, 16.89it/s, est. speed input: 19907.50 toks/s, output: 19.44 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:44<03:11, 16.89it/s, est. speed input: 19796.92 toks/s, output: 19.33 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:46<03:09, 16.92it/s, est. speed input: 19700.26 toks/s, output: 19.24 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:48<03:05, 17.05it/s, est. speed input: 19626.87 toks/s, output: 19.17 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:50<03:02, 17.15it/s, est. speed input: 19560.92 toks/s, output: 19.10 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:52<03:01, 17.09it/s, est. speed input: 19480.99 toks/s, output: 19.02 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:54<03:00, 17.05it/s, est. speed input: 19407.35 toks/s, output: 18.95 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:56<02:58, 16.99it/s, est. speed input: 19334.12 toks/s, output: 18.88 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:57<02:57, 16.96it/s, est. speed input: 19267.19 toks/s, output: 18.82 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:59<02:55, 16.94it/s, est. speed input: 19205.03 toks/s, output: 18.75 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [01:01<02:52, 17.06it/s, est. speed input: 19161.86 toks/s, output: 18.71 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [01:03<02:51, 17.01it/s, est. speed input: 19106.57 toks/s, output: 18.66 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [01:05<02:51, 16.79it/s, est. speed input: 19034.39 toks/s, output: 18.59 toks/s]
Processed prompts:  31%|███       | 1250/4096 [01:07<02:47, 16.94it/s, est. speed input: 18998.06 toks/s, output: 18.55 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [01:09<02:45, 17.05it/s, est. speed input: 18964.13 toks/s, output: 18.52 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [01:11<02:43, 17.01it/s, est. speed input: 18920.00 toks/s, output: 18.48 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [01:13<02:42, 16.97it/s, est. speed input: 18877.52 toks/s, output: 18.44 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [01:14<02:40, 16.94it/s, est. speed input: 18837.31 toks/s, output: 18.40 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [01:16<02:38, 16.93it/s, est. speed input: 18799.19 toks/s, output: 18.36 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [01:18<02:36, 16.94it/s, est. speed input: 18765.36 toks/s, output: 18.33 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [01:20<02:34, 16.95it/s, est. speed input: 18733.09 toks/s, output: 18.29 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [01:22<02:32, 16.94it/s, est. speed input: 18700.50 toks/s, output: 18.26 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [01:24<02:30, 16.94it/s, est. speed input: 18670.48 toks/s, output: 18.23 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [01:26<02:28, 17.05it/s, est. speed input: 18650.42 toks/s, output: 18.21 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [01:28<02:26, 17.02it/s, est. speed input: 18622.12 toks/s, output: 18.19 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [01:29<02:24, 17.09it/s, est. speed input: 18602.60 toks/s, output: 18.17 toks/s]
Processed prompts:  41%|████      | 1666/4096 [01:31<02:22, 17.04it/s, est. speed input: 18576.75 toks/s, output: 18.14 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [01:33<02:21, 17.01it/s, est. speed input: 18551.48 toks/s, output: 18.12 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [01:35<02:19, 16.99it/s, est. speed input: 18528.27 toks/s, output: 18.09 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [01:37<02:18, 16.81it/s, est. speed input: 18493.14 toks/s, output: 18.06 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [01:39<02:16, 16.81it/s, est. speed input: 18468.96 toks/s, output: 18.04 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [01:41<02:14, 16.83it/s, est. speed input: 18446.56 toks/s, output: 18.01 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [01:43<02:11, 16.98it/s, est. speed input: 18434.30 toks/s, output: 18.00 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [01:45<02:10, 16.95it/s, est. speed input: 18413.56 toks/s, output: 17.98 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [01:47<02:08, 16.93it/s, est. speed input: 18393.59 toks/s, output: 17.96 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [01:48<02:05, 17.03it/s, est. speed input: 18381.77 toks/s, output: 17.95 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [01:50<02:03, 17.11it/s, est. speed input: 18370.44 toks/s, output: 17.94 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [01:52<02:01, 17.06it/s, est. speed input: 18353.33 toks/s, output: 17.92 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [01:54<02:00, 17.02it/s, est. speed input: 18336.29 toks/s, output: 17.91 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [01:56<01:58, 17.00it/s, est. speed input: 18320.33 toks/s, output: 17.89 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [01:58<01:56, 16.99it/s, est. speed input: 18305.21 toks/s, output: 17.88 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [02:00<01:54, 16.98it/s, est. speed input: 18290.74 toks/s, output: 17.86 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [02:02<01:53, 16.95it/s, est. speed input: 18275.24 toks/s, output: 17.85 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [02:03<01:49, 17.19it/s, est. speed input: 18274.25 toks/s, output: 17.85 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [02:05<01:48, 17.09it/s, est. speed input: 18258.96 toks/s, output: 17.83 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [02:07<01:46, 17.17it/s, est. speed input: 18252.19 toks/s, output: 17.82 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [02:09<01:45, 16.89it/s, est. speed input: 18227.92 toks/s, output: 17.80 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [02:11<01:43, 17.02it/s, est. speed input: 18221.07 toks/s, output: 17.79 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [02:13<01:39, 17.27it/s, est. speed input: 18222.17 toks/s, output: 17.80 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [02:15<01:37, 17.30it/s, est. speed input: 18216.31 toks/s, output: 17.79 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [02:16<01:36, 17.17it/s, est. speed input: 18203.47 toks/s, output: 17.78 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [02:18<01:35, 17.08it/s, est. speed input: 18190.68 toks/s, output: 17.76 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [02:20<01:33, 17.15it/s, est. speed input: 18184.63 toks/s, output: 17.76 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [02:22<01:31, 17.08it/s, est. speed input: 18173.22 toks/s, output: 17.75 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [02:24<01:29, 17.16it/s, est. speed input: 18167.90 toks/s, output: 17.74 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [02:26<01:28, 17.06it/s, est. speed input: 18156.10 toks/s, output: 17.73 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [02:28<01:26, 17.03it/s, est. speed input: 18146.03 toks/s, output: 17.72 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [02:30<01:24, 16.97it/s, est. speed input: 18134.43 toks/s, output: 17.71 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [02:31<01:22, 17.07it/s, est. speed input: 18129.29 toks/s, output: 17.70 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [02:33<01:20, 17.02it/s, est. speed input: 18119.16 toks/s, output: 17.69 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [02:35<01:19, 16.97it/s, est. speed input: 18108.76 toks/s, output: 17.68 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [02:37<01:17, 16.95it/s, est. speed input: 18099.22 toks/s, output: 17.68 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [02:39<01:15, 16.93it/s, est. speed input: 18089.63 toks/s, output: 17.67 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [02:41<01:13, 17.05it/s, est. speed input: 18085.61 toks/s, output: 17.66 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [02:43<01:11, 16.99it/s, est. speed input: 18076.09 toks/s, output: 17.65 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [02:45<01:09, 16.97it/s, est. speed input: 18067.47 toks/s, output: 17.64 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [02:47<01:07, 16.95it/s, est. speed input: 18059.00 toks/s, output: 17.64 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [02:48<01:05, 16.95it/s, est. speed input: 18051.12 toks/s, output: 17.63 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [02:50<01:04, 16.93it/s, est. speed input: 18042.66 toks/s, output: 17.62 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [02:52<01:02, 16.93it/s, est. speed input: 18034.95 toks/s, output: 17.61 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [02:54<01:00, 16.94it/s, est. speed input: 18027.92 toks/s, output: 17.61 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [02:56<00:57, 17.21it/s, est. speed input: 18030.64 toks/s, output: 17.61 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [02:58<00:55, 17.26it/s, est. speed input: 18028.10 toks/s, output: 17.61 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [03:00<00:53, 17.16it/s, est. speed input: 18020.80 toks/s, output: 17.60 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [03:02<00:52, 17.06it/s, est. speed input: 18012.70 toks/s, output: 17.59 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [03:03<00:50, 17.12it/s, est. speed input: 18009.38 toks/s, output: 17.59 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [03:05<00:48, 17.09it/s, est. speed input: 18003.40 toks/s, output: 17.58 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [03:07<00:46, 17.04it/s, est. speed input: 17996.71 toks/s, output: 17.57 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [03:09<00:45, 17.02it/s, est. speed input: 17990.44 toks/s, output: 17.57 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [03:11<00:43, 16.99it/s, est. speed input: 17983.83 toks/s, output: 17.56 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [03:13<00:41, 16.86it/s, est. speed input: 17973.65 toks/s, output: 17.55 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [03:15<00:39, 17.00it/s, est. speed input: 17971.72 toks/s, output: 17.55 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [03:17<00:37, 16.97it/s, est. speed input: 17965.15 toks/s, output: 17.54 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [03:18<00:35, 17.23it/s, est. speed input: 17968.34 toks/s, output: 17.55 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [03:20<00:33, 17.14it/s, est. speed input: 17962.39 toks/s, output: 17.54 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [03:22<00:31, 17.07it/s, est. speed input: 17956.34 toks/s, output: 17.54 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [03:24<00:29, 17.05it/s, est. speed input: 17951.18 toks/s, output: 17.53 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [03:26<00:28, 16.98it/s, est. speed input: 17944.50 toks/s, output: 17.52 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [03:28<00:26, 16.96it/s, est. speed input: 17938.72 toks/s, output: 17.52 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [03:30<00:24, 16.93it/s, est. speed input: 17932.80 toks/s, output: 17.51 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [03:32<00:22, 17.07it/s, est. speed input: 17931.73 toks/s, output: 17.51 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [03:33<00:20, 17.05it/s, est. speed input: 17927.18 toks/s, output: 17.51 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [03:35<00:18, 17.02it/s, est. speed input: 17922.33 toks/s, output: 17.50 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [03:37<00:16, 16.99it/s, est. speed input: 17917.06 toks/s, output: 17.50 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [03:39<00:14, 17.11it/s, est. speed input: 17916.27 toks/s, output: 17.50 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [03:41<00:13, 17.03it/s, est. speed input: 17910.49 toks/s, output: 17.49 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [03:43<00:11, 17.00it/s, est. speed input: 17905.67 toks/s, output: 17.49 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [03:45<00:09, 16.87it/s, est. speed input: 17897.76 toks/s, output: 17.48 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [03:47<00:07, 16.88it/s, est. speed input: 17892.74 toks/s, output: 17.47 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [03:49<00:05, 16.90it/s, est. speed input: 17888.27 toks/s, output: 17.47 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [03:52<00:04, 13.64it/s, est. speed input: 17767.63 toks/s, output: 17.35 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [03:55<00:02, 12.93it/s, est. speed input: 17697.05 toks/s, output: 17.28 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [03:55<00:00, 12.93it/s, est. speed input: 17827.58 toks/s, output: 17.41 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [03:55<00:00, 17.41it/s, est. speed input: 17827.58 toks/s, output: 17.41 toks/s]
[rank0]:[W128 09:03:26.879465512 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 287.3s

测试结果:
  Requests/s:   16.81
  Tokens/s:     17225.16
  Total Reqs:   4096
  Elapsed:      243.74s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     17208.36


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,29.2585,15009.6114,4.3748
1024,1024,1,128,128,16.7771,17196.4812,7.6295
2048,1024,2,256,128,17.8707,18317.4884,14.3251
4096,1024,4,512,128,16.6451,17061.2553,30.7598
8192,1024,8,1024,128,16.8638,17285.3993,60.7218
16384,1024,16,2048,128,17.1128,17540.6218,119.6765
32768,1024,32,4096,128,16.8050,17225.1630,243.7364

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:03:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:03:33 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3724204) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3724204) WARNING 01-28 09:03:59 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 27.69 requests/s, 14205.57 total tokens/s, 27.69 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:03:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:03:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:03:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:03:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:03:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:03:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:03:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:03:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:03:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:03:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:03:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:03:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:03:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:03:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:03:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:03:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:03:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:03:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3724204) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3724204) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.27s/it]
(EngineCore_DP0 pid=3724204) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.27s/it]
(EngineCore_DP0 pid=3724204) 
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3724204) [2026-01-28 09:03:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3724204) 2026-01-28 09:03:59,025 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3724204) 2026-01-28 09:03:59,039 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1296.39it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:19,  6.64it/s, est. speed input: 3400.12 toks/s, output: 6.64 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:07, 17.71it/s, est. speed input: 8061.82 toks/s, output: 15.74 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:05, 22.38it/s, est. speed input: 10022.31 toks/s, output: 19.57 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:04, 24.88it/s, est. speed input: 11115.93 toks/s, output: 21.71 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 26.47it/s, est. speed input: 11837.55 toks/s, output: 23.12 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:04, 27.57it/s, est. speed input: 12359.08 toks/s, output: 24.14 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:03, 28.12it/s, est. speed input: 12711.60 toks/s, output: 24.83 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:03, 27.90it/s, est. speed input: 12879.22 toks/s, output: 25.15 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 28.36it/s, est. speed input: 13106.59 toks/s, output: 25.60 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:03, 28.64it/s, est. speed input: 13285.89 toks/s, output: 25.95 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:03, 28.86it/s, est. speed input: 13436.50 toks/s, output: 26.24 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:03, 28.96it/s, est. speed input: 13557.36 toks/s, output: 26.48 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:03, 28.98it/s, est. speed input: 13654.19 toks/s, output: 26.67 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:03, 29.06it/s, est. speed input: 13744.96 toks/s, output: 26.85 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 29.11it/s, est. speed input: 13823.96 toks/s, output: 27.00 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 29.14it/s, est. speed input: 13891.92 toks/s, output: 27.13 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 29.25it/s, est. speed input: 13960.94 toks/s, output: 27.27 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 29.01it/s, est. speed input: 13994.88 toks/s, output: 27.33 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:02<00:02, 28.81it/s, est. speed input: 14022.21 toks/s, output: 27.39 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:02<00:02, 29.03it/s, est. speed input: 14075.43 toks/s, output: 27.49 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:02, 29.03it/s, est. speed input: 14112.52 toks/s, output: 27.56 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:02<00:02, 29.06it/s, est. speed input: 14148.17 toks/s, output: 27.63 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:02<00:02, 29.13it/s, est. speed input: 14184.41 toks/s, output: 27.70 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:02<00:01, 29.21it/s, est. speed input: 14219.59 toks/s, output: 27.77 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 29.28it/s, est. speed input: 14252.66 toks/s, output: 27.84 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 29.29it/s, est. speed input: 14281.14 toks/s, output: 27.89 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 29.26it/s, est. speed input: 14305.14 toks/s, output: 27.94 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 29.15it/s, est. speed input: 14322.80 toks/s, output: 27.97 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:03<00:01, 28.69it/s, est. speed input: 14316.86 toks/s, output: 27.96 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:03<00:01, 28.71it/s, est. speed input: 14330.56 toks/s, output: 27.99 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:03<00:01, 28.76it/s, est. speed input: 14345.05 toks/s, output: 28.02 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:03<00:01, 28.93it/s, est. speed input: 14365.73 toks/s, output: 28.06 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:01, 29.05it/s, est. speed input: 14385.02 toks/s, output: 28.10 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:03<00:00, 29.00it/s, est. speed input: 14396.68 toks/s, output: 28.12 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:03<00:00, 28.97it/s, est. speed input: 14408.01 toks/s, output: 28.14 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:03<00:00, 29.09it/s, est. speed input: 14425.11 toks/s, output: 28.17 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 28.93it/s, est. speed input: 14430.63 toks/s, output: 28.18 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:03<00:00, 29.12it/s, est. speed input: 14448.92 toks/s, output: 28.22 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:04<00:00, 28.65it/s, est. speed input: 14440.32 toks/s, output: 28.20 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:04<00:00, 28.95it/s, est. speed input: 14458.79 toks/s, output: 28.24 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:04<00:00, 29.06it/s, est. speed input: 14472.09 toks/s, output: 28.27 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:04<00:00, 28.87it/s, est. speed input: 14474.06 toks/s, output: 28.27 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:04<00:00, 28.94it/s, est. speed input: 14483.93 toks/s, output: 28.29 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 28.94it/s, est. speed input: 14491.83 toks/s, output: 28.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 28.30it/s, est. speed input: 14491.83 toks/s, output: 28.30 toks/s]
[rank0]:[W128 09:04:04.525394958 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 37.5s

测试结果:
  Requests/s:   27.69
  Tokens/s:     14205.57
  Total Reqs:   128
  Elapsed:      4.62s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     14177.88

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:04:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:04:10 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3724936) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3724936) WARNING 01-28 09:04:36 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.10 requests/s, 15481.17 total tokens/s, 15.10 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:04:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:04:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:04:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:04:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:04:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:04:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:04:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:04:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:04:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:04:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:04:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:04:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:04:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:04:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:04:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:04:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:04:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:04:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3724936) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3724936) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.53s/it]
(EngineCore_DP0 pid=3724936) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.53s/it]
(EngineCore_DP0 pid=3724936) 
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3724936) [2026-01-28 09:04:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3724936) 2026-01-28 09:04:35,690 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3724936) 2026-01-28 09:04:35,704 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  57%|█████▋    | 73/128 [00:00<00:00, 726.14it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 729.11it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:14,  8.63it/s, est. speed input: 8841.73 toks/s, output: 8.63 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:09, 12.86it/s, est. speed input: 12551.33 toks/s, output: 12.26 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:08, 14.12it/s, est. speed input: 13718.66 toks/s, output: 13.40 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:08, 14.63it/s, est. speed input: 14246.76 toks/s, output: 13.91 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:07, 15.01it/s, est. speed input: 14610.09 toks/s, output: 14.27 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 15.25it/s, est. speed input: 14862.34 toks/s, output: 14.51 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:07, 15.42it/s, est. speed input: 15046.70 toks/s, output: 14.69 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 15.30it/s, est. speed input: 15095.01 toks/s, output: 14.74 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:07, 15.22it/s, est. speed input: 15130.51 toks/s, output: 14.78 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:07, 15.31it/s, est. speed input: 15208.43 toks/s, output: 14.85 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 15.47it/s, est. speed input: 15298.32 toks/s, output: 14.94 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 15.45it/s, est. speed input: 15339.63 toks/s, output: 14.98 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 15.59it/s, est. speed input: 15411.38 toks/s, output: 15.05 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 15.71it/s, est. speed input: 15478.34 toks/s, output: 15.12 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:06, 15.60it/s, est. speed input: 15495.86 toks/s, output: 15.13 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:06, 15.57it/s, est. speed input: 15519.80 toks/s, output: 15.16 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:06, 15.34it/s, est. speed input: 15499.84 toks/s, output: 15.14 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:06, 15.49it/s, est. speed input: 15538.46 toks/s, output: 15.17 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 15.43it/s, est. speed input: 15545.70 toks/s, output: 15.18 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 15.51it/s, est. speed input: 15571.67 toks/s, output: 15.21 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 15.64it/s, est. speed input: 15607.34 toks/s, output: 15.24 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 15.63it/s, est. speed input: 15624.09 toks/s, output: 15.26 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:05, 15.66it/s, est. speed input: 15645.40 toks/s, output: 15.28 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:05, 15.73it/s, est. speed input: 15671.11 toks/s, output: 15.30 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:05, 15.41it/s, est. speed input: 15645.09 toks/s, output: 15.28 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 15.52it/s, est. speed input: 15665.24 toks/s, output: 15.30 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 15.53it/s, est. speed input: 15674.99 toks/s, output: 15.31 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 15.60it/s, est. speed input: 15691.53 toks/s, output: 15.32 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 15.52it/s, est. speed input: 15691.94 toks/s, output: 15.32 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 15.48it/s, est. speed input: 15694.50 toks/s, output: 15.33 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 15.58it/s, est. speed input: 15710.83 toks/s, output: 15.34 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:04<00:04, 15.56it/s, est. speed input: 15715.85 toks/s, output: 15.35 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:04, 15.32it/s, est. speed input: 15697.35 toks/s, output: 15.33 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 15.36it/s, est. speed input: 15701.48 toks/s, output: 15.33 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 15.38it/s, est. speed input: 15704.46 toks/s, output: 15.34 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 15.50it/s, est. speed input: 15716.90 toks/s, output: 15.35 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 15.51it/s, est. speed input: 15722.30 toks/s, output: 15.35 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 15.53it/s, est. speed input: 15728.14 toks/s, output: 15.36 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:05<00:03, 15.51it/s, est. speed input: 15730.95 toks/s, output: 15.36 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:05<00:03, 15.51it/s, est. speed input: 15734.58 toks/s, output: 15.37 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:05<00:03, 15.42it/s, est. speed input: 15731.02 toks/s, output: 15.36 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 15.32it/s, est. speed input: 15724.19 toks/s, output: 15.36 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 15.37it/s, est. speed input: 15726.89 toks/s, output: 15.36 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 15.52it/s, est. speed input: 15739.28 toks/s, output: 15.37 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 15.61it/s, est. speed input: 15749.60 toks/s, output: 15.38 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 15.67it/s, est. speed input: 15758.63 toks/s, output: 15.39 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:06<00:02, 15.69it/s, est. speed input: 15765.99 toks/s, output: 15.40 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:06<00:02, 15.66it/s, est. speed input: 15770.50 toks/s, output: 15.40 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:06<00:01, 15.55it/s, est. speed input: 15767.87 toks/s, output: 15.40 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 15.49it/s, est. speed input: 15767.48 toks/s, output: 15.40 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 15.58it/s, est. speed input: 15775.23 toks/s, output: 15.41 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 15.64it/s, est. speed input: 15782.39 toks/s, output: 15.41 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 15.67it/s, est. speed input: 15788.40 toks/s, output: 15.42 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 15.51it/s, est. speed input: 15783.45 toks/s, output: 15.41 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:07<00:01, 15.45it/s, est. speed input: 15781.44 toks/s, output: 15.41 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:07<00:01, 15.59it/s, est. speed input: 15790.89 toks/s, output: 15.42 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:07<00:00, 15.35it/s, est. speed input: 15779.51 toks/s, output: 15.41 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 15.38it/s, est. speed input: 15780.10 toks/s, output: 15.41 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 15.46it/s, est. speed input: 15784.20 toks/s, output: 15.41 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 15.52it/s, est. speed input: 15788.47 toks/s, output: 15.42 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 15.53it/s, est. speed input: 15790.76 toks/s, output: 15.42 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 15.60it/s, est. speed input: 15796.25 toks/s, output: 15.43 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:08<00:00, 15.48it/s, est. speed input: 15792.48 toks/s, output: 15.42 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:08<00:00, 15.44it/s, est. speed input: 15791.66 toks/s, output: 15.42 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 15.44it/s, est. speed input: 15795.00 toks/s, output: 15.42 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 15.42it/s, est. speed input: 15795.00 toks/s, output: 15.42 toks/s]
[rank0]:[W128 09:04:44.957443910 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.4s

测试结果:
  Requests/s:   15.10
  Tokens/s:     15481.17
  Total Reqs:   128
  Elapsed:      8.47s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     15466.06

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:04:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:04:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3725693) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3725693) WARNING 01-28 09:05:16 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.44 requests/s, 15828.01 total tokens/s, 15.44 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 09:04:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:04:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:04:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:04:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:04:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:04:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:04:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:04:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:04:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:04:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:04:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:04:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:04:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:04:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:04:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:04:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:04:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:04:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3725693) [2026-01-28 09:04:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3725693) [2026-01-28 09:04:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3725693) [2026-01-28 09:04:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3725693) [2026-01-28 09:04:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3725693) [2026-01-28 09:04:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3725693) [2026-01-28 09:04:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3725693) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3725693) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.34s/it]
(EngineCore_DP0 pid=3725693) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.34s/it]
(EngineCore_DP0 pid=3725693) 
(EngineCore_DP0 pid=3725693) [2026-01-28 09:05:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3725693) [2026-01-28 09:05:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3725693) [2026-01-28 09:05:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3725693) [2026-01-28 09:05:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3725693) [2026-01-28 09:05:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3725693) [2026-01-28 09:05:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3725693) [2026-01-28 09:05:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3725693) [2026-01-28 09:05:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3725693) 2026-01-28 09:05:16,251 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3725693) 2026-01-28 09:05:16,263 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  26%|██▌       | 67/256 [00:00<00:00, 667.60it/s]
Adding requests:  52%|█████▏    | 134/256 [00:00<00:00, 642.81it/s]
Adding requests:  78%|███████▊  | 199/256 [00:00<00:00, 590.19it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 597.14it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 6/256 [00:00<00:06, 38.44it/s, est. speed input: 39376.04 toks/s, output: 38.45 toks/s]
Processed prompts:   4%|▍         | 10/256 [00:00<00:11, 21.97it/s, est. speed input: 24380.96 toks/s, output: 23.81 toks/s]
Processed prompts:   5%|▌         | 13/256 [00:00<00:10, 22.37it/s, est. speed input: 24227.35 toks/s, output: 23.66 toks/s]
Processed prompts:   6%|▋         | 16/256 [00:00<00:13, 17.26it/s, est. speed input: 20369.88 toks/s, output: 19.89 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:14, 16.79it/s, est. speed input: 19738.49 toks/s, output: 19.28 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:01<00:14, 16.47it/s, est. speed input: 19284.68 toks/s, output: 18.83 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:01<00:14, 16.23it/s, est. speed input: 18927.31 toks/s, output: 18.48 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:01<00:14, 15.90it/s, est. speed input: 18579.49 toks/s, output: 18.14 toks/s]
Processed prompts:  10%|█         | 26/256 [00:01<00:14, 15.74it/s, est. speed input: 18323.63 toks/s, output: 17.89 toks/s]
Processed prompts:  11%|█         | 28/256 [00:01<00:14, 15.75it/s, est. speed input: 18147.96 toks/s, output: 17.72 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:01<00:14, 15.70it/s, est. speed input: 17984.64 toks/s, output: 17.56 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:01<00:14, 15.74it/s, est. speed input: 17861.17 toks/s, output: 17.44 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:01<00:14, 15.71it/s, est. speed input: 17741.73 toks/s, output: 17.33 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:02<00:13, 15.77it/s, est. speed input: 17655.20 toks/s, output: 17.24 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:02<00:13, 15.72it/s, est. speed input: 17558.41 toks/s, output: 17.15 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:02<00:13, 15.62it/s, est. speed input: 17458.26 toks/s, output: 17.05 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:02<00:13, 15.48it/s, est. speed input: 17354.56 toks/s, output: 16.95 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:02<00:13, 15.51it/s, est. speed input: 17285.53 toks/s, output: 16.88 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:02<00:13, 15.57it/s, est. speed input: 17229.46 toks/s, output: 16.83 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:02<00:13, 15.57it/s, est. speed input: 17172.28 toks/s, output: 16.77 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:02<00:13, 15.59it/s, est. speed input: 17123.11 toks/s, output: 16.72 toks/s]
Processed prompts:  20%|██        | 52/256 [00:03<00:13, 15.66it/s, est. speed input: 17085.36 toks/s, output: 16.68 toks/s]
Processed prompts:  21%|██        | 54/256 [00:03<00:12, 15.65it/s, est. speed input: 17042.40 toks/s, output: 16.64 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:03<00:12, 15.65it/s, est. speed input: 17003.92 toks/s, output: 16.61 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:03<00:12, 15.52it/s, est. speed input: 16951.10 toks/s, output: 16.55 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:03<00:12, 15.57it/s, est. speed input: 16920.06 toks/s, output: 16.52 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:03<00:12, 15.62it/s, est. speed input: 16892.61 toks/s, output: 16.50 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:03<00:12, 15.62it/s, est. speed input: 16863.34 toks/s, output: 16.47 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:04<00:12, 15.55it/s, est. speed input: 16828.01 toks/s, output: 16.43 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:04<00:12, 15.62it/s, est. speed input: 16807.69 toks/s, output: 16.41 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:04<00:11, 15.61it/s, est. speed input: 16782.54 toks/s, output: 16.39 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:04<00:11, 15.58it/s, est. speed input: 16755.82 toks/s, output: 16.36 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:04<00:11, 15.44it/s, est. speed input: 16718.55 toks/s, output: 16.33 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:04<00:11, 15.54it/s, est. speed input: 16702.99 toks/s, output: 16.31 toks/s]
Processed prompts:  30%|███       | 78/256 [00:04<00:11, 15.63it/s, est. speed input: 16690.64 toks/s, output: 16.30 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:04<00:11, 15.63it/s, est. speed input: 16672.58 toks/s, output: 16.28 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:05<00:11, 15.63it/s, est. speed input: 16655.56 toks/s, output: 16.27 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:05<00:10, 15.65it/s, est. speed input: 16641.20 toks/s, output: 16.25 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:05<00:10, 15.63it/s, est. speed input: 16624.59 toks/s, output: 16.23 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:05<00:10, 15.67it/s, est. speed input: 16613.19 toks/s, output: 16.22 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:05<00:10, 15.45it/s, est. speed input: 16582.10 toks/s, output: 16.19 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:05<00:10, 15.46it/s, est. speed input: 16565.41 toks/s, output: 16.18 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:05<00:10, 15.55it/s, est. speed input: 16556.55 toks/s, output: 16.17 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:05<00:10, 15.66it/s, est. speed input: 16551.40 toks/s, output: 16.16 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:06<00:10, 15.69it/s, est. speed input: 16542.56 toks/s, output: 16.15 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:06<00:09, 15.64it/s, est. speed input: 16529.28 toks/s, output: 16.14 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:06<00:09, 15.70it/s, est. speed input: 16523.39 toks/s, output: 16.14 toks/s]
Processed prompts:  41%|████      | 104/256 [00:06<00:09, 15.66it/s, est. speed input: 16511.58 toks/s, output: 16.12 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:06<00:09, 15.54it/s, est. speed input: 16493.77 toks/s, output: 16.11 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:06<00:09, 15.45it/s, est. speed input: 16476.74 toks/s, output: 16.09 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:06<00:09, 15.48it/s, est. speed input: 16466.69 toks/s, output: 16.08 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:06<00:09, 15.47it/s, est. speed input: 16454.11 toks/s, output: 16.07 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:07<00:09, 15.46it/s, est. speed input: 16442.39 toks/s, output: 16.06 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:07<00:09, 15.49it/s, est. speed input: 16433.60 toks/s, output: 16.05 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:07<00:08, 15.56it/s, est. speed input: 16427.92 toks/s, output: 16.04 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:07<00:08, 15.61it/s, est. speed input: 16422.60 toks/s, output: 16.04 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:07<00:08, 15.68it/s, est. speed input: 16419.40 toks/s, output: 16.03 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:07<00:08, 15.47it/s, est. speed input: 16401.24 toks/s, output: 16.02 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:07<00:08, 15.47it/s, est. speed input: 16391.69 toks/s, output: 16.01 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:07<00:08, 15.59it/s, est. speed input: 16389.44 toks/s, output: 16.01 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:08<00:08, 15.63it/s, est. speed input: 16385.22 toks/s, output: 16.00 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:08<00:07, 15.69it/s, est. speed input: 16382.64 toks/s, output: 16.00 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:08<00:07, 15.73it/s, est. speed input: 16380.06 toks/s, output: 16.00 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:08<00:07, 15.69it/s, est. speed input: 16373.93 toks/s, output: 15.99 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:08<00:07, 15.68it/s, est. speed input: 16368.84 toks/s, output: 15.99 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:08<00:07, 15.50it/s, est. speed input: 16355.11 toks/s, output: 15.97 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:08<00:07, 15.56it/s, est. speed input: 16351.11 toks/s, output: 15.97 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:09<00:07, 15.59it/s, est. speed input: 16346.64 toks/s, output: 15.96 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:09<00:07, 15.62it/s, est. speed input: 16342.76 toks/s, output: 15.96 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:09<00:06, 15.60it/s, est. speed input: 16336.89 toks/s, output: 15.95 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:09<00:06, 15.67it/s, est. speed input: 16335.14 toks/s, output: 15.95 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:09<00:06, 15.62it/s, est. speed input: 16329.14 toks/s, output: 15.95 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:09<00:06, 15.69it/s, est. speed input: 16327.64 toks/s, output: 15.94 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:09<00:06, 15.49it/s, est. speed input: 16315.03 toks/s, output: 15.93 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:09<00:06, 15.52it/s, est. speed input: 16310.79 toks/s, output: 15.93 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:10<00:06, 15.53it/s, est. speed input: 16305.79 toks/s, output: 15.92 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:10<00:06, 15.53it/s, est. speed input: 16300.59 toks/s, output: 15.92 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:10<00:05, 15.61it/s, est. speed input: 16299.13 toks/s, output: 15.92 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:10<00:05, 15.63it/s, est. speed input: 16296.00 toks/s, output: 15.91 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:10<00:05, 15.62it/s, est. speed input: 16292.31 toks/s, output: 15.91 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:10<00:05, 15.67it/s, est. speed input: 16290.87 toks/s, output: 15.91 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:10<00:05, 15.53it/s, est. speed input: 16281.96 toks/s, output: 15.90 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:10<00:05, 15.61it/s, est. speed input: 16280.77 toks/s, output: 15.90 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:11<00:05, 15.55it/s, est. speed input: 16274.79 toks/s, output: 15.89 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:11<00:05, 15.60it/s, est. speed input: 16272.85 toks/s, output: 15.89 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:11<00:04, 15.60it/s, est. speed input: 16269.70 toks/s, output: 15.89 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:11<00:04, 15.68it/s, est. speed input: 16269.24 toks/s, output: 15.89 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:11<00:04, 15.69it/s, est. speed input: 16267.41 toks/s, output: 15.89 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:11<00:04, 15.75it/s, est. speed input: 16267.57 toks/s, output: 15.89 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:11<00:04, 15.66it/s, est. speed input: 16262.47 toks/s, output: 15.88 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:11<00:04, 15.39it/s, est. speed input: 16249.91 toks/s, output: 15.87 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:12<00:04, 15.49it/s, est. speed input: 16248.64 toks/s, output: 15.87 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:12<00:03, 15.58it/s, est. speed input: 16247.88 toks/s, output: 15.87 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:12<00:03, 15.56it/s, est. speed input: 16244.14 toks/s, output: 15.86 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:12<00:03, 15.56it/s, est. speed input: 16240.85 toks/s, output: 15.86 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:12<00:03, 15.64it/s, est. speed input: 16240.48 toks/s, output: 15.86 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:12<00:03, 15.73it/s, est. speed input: 16241.38 toks/s, output: 15.86 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:12<00:03, 15.67it/s, est. speed input: 16237.97 toks/s, output: 15.86 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:12<00:03, 15.40it/s, est. speed input: 16226.68 toks/s, output: 15.85 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:13<00:03, 15.55it/s, est. speed input: 16227.42 toks/s, output: 15.85 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:13<00:02, 15.51it/s, est. speed input: 16223.21 toks/s, output: 15.84 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:13<00:02, 15.57it/s, est. speed input: 16221.85 toks/s, output: 15.84 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:13<00:02, 15.62it/s, est. speed input: 16220.78 toks/s, output: 15.84 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:13<00:02, 15.60it/s, est. speed input: 16217.96 toks/s, output: 15.84 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:13<00:02, 15.63it/s, est. speed input: 16216.78 toks/s, output: 15.84 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:13<00:02, 15.60it/s, est. speed input: 16213.94 toks/s, output: 15.83 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:14<00:02, 15.43it/s, est. speed input: 16206.27 toks/s, output: 15.83 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:14<00:02, 15.49it/s, est. speed input: 16204.42 toks/s, output: 15.82 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:14<00:01, 15.47it/s, est. speed input: 16200.84 toks/s, output: 15.82 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:14<00:01, 15.51it/s, est. speed input: 16198.85 toks/s, output: 15.82 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:14<00:01, 15.58it/s, est. speed input: 16198.05 toks/s, output: 15.82 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:14<00:01, 15.63it/s, est. speed input: 16197.37 toks/s, output: 15.82 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:14<00:01, 15.64it/s, est. speed input: 16196.00 toks/s, output: 15.82 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:14<00:01, 15.70it/s, est. speed input: 16196.19 toks/s, output: 15.82 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:15<00:01, 15.53it/s, est. speed input: 16190.33 toks/s, output: 15.81 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:15<00:01, 15.48it/s, est. speed input: 16186.32 toks/s, output: 15.81 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:15<00:00, 15.56it/s, est. speed input: 16185.74 toks/s, output: 15.81 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:15<00:00, 15.61it/s, est. speed input: 16185.11 toks/s, output: 15.81 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:15<00:00, 15.65it/s, est. speed input: 16184.68 toks/s, output: 15.81 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:15<00:00, 15.59it/s, est. speed input: 16181.56 toks/s, output: 15.80 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:15<00:00, 15.61it/s, est. speed input: 16180.38 toks/s, output: 15.80 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:15<00:00, 15.55it/s, est. speed input: 16177.02 toks/s, output: 15.80 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:16<00:00, 15.59it/s, est. speed input: 16176.31 toks/s, output: 15.80 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 15.59it/s, est. speed input: 16233.65 toks/s, output: 15.85 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 15.85it/s, est. speed input: 16233.65 toks/s, output: 15.85 toks/s]
[rank0]:[W128 09:05:33.759401795 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 48.8s

测试结果:
  Requests/s:   15.44
  Tokens/s:     15828.01
  Total Reqs:   256
  Elapsed:      16.58s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     15812.56

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:05:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:05:40 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3726577) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3726577) WARNING 01-28 09:06:06 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.52 requests/s, 14887.26 total tokens/s, 14.52 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 09:05:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:05:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:05:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:05:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:05:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:05:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:05:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:05:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:05:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:05:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:05:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:05:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:05:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:05:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:05:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:05:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:05:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:05:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3726577) [2026-01-28 09:05:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3726577) [2026-01-28 09:05:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3726577) [2026-01-28 09:05:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3726577) [2026-01-28 09:05:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3726577) [2026-01-28 09:05:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3726577) [2026-01-28 09:05:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3726577) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3726577) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.36s/it]
(EngineCore_DP0 pid=3726577) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.36s/it]
(EngineCore_DP0 pid=3726577) 
(EngineCore_DP0 pid=3726577) [2026-01-28 09:06:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3726577) [2026-01-28 09:06:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3726577) [2026-01-28 09:06:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3726577) [2026-01-28 09:06:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3726577) [2026-01-28 09:06:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3726577) [2026-01-28 09:06:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3726577) [2026-01-28 09:06:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3726577) [2026-01-28 09:06:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3726577) 2026-01-28 09:06:05,990 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3726577) 2026-01-28 09:06:06,002 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  12%|█▏        | 62/512 [00:00<00:00, 615.54it/s]
Adding requests:  24%|██▍       | 124/512 [00:00<00:00, 583.27it/s]
Adding requests:  36%|███▌      | 183/512 [00:00<00:00, 558.07it/s]
Adding requests:  47%|████▋     | 241/512 [00:00<00:00, 564.69it/s]
Adding requests:  58%|█████▊    | 298/512 [00:00<00:00, 553.31it/s]
Adding requests:  69%|██████▉   | 355/512 [00:00<00:00, 555.46it/s]
Adding requests:  80%|████████  | 411/512 [00:00<00:00, 553.23it/s]
Adding requests:  91%|█████████ | 467/512 [00:00<00:00, 548.94it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 553.66it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 14/512 [00:00<00:12, 39.99it/s, est. speed input: 40953.09 toks/s, output: 39.99 toks/s]
Processed prompts:   4%|▎         | 18/512 [00:00<00:18, 26.51it/s, est. speed input: 29464.93 toks/s, output: 28.77 toks/s]
Processed prompts:   4%|▍         | 22/512 [00:00<00:22, 21.50it/s, est. speed input: 25124.60 toks/s, output: 24.54 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:01<00:25, 18.96it/s, est. speed input: 22812.71 toks/s, output: 22.28 toks/s]
Processed prompts:   6%|▌         | 30/512 [00:01<00:27, 17.50it/s, est. speed input: 21368.72 toks/s, output: 20.87 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:01<00:29, 16.42it/s, est. speed input: 20279.27 toks/s, output: 19.80 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:01<00:29, 15.83it/s, est. speed input: 19546.67 toks/s, output: 19.09 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:02<00:30, 15.47it/s, est. speed input: 19005.93 toks/s, output: 18.56 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:02<00:30, 15.25it/s, est. speed input: 18589.66 toks/s, output: 18.15 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:02<00:30, 14.99it/s, est. speed input: 18209.19 toks/s, output: 17.78 toks/s]
Processed prompts:  11%|█         | 54/512 [00:03<00:30, 14.90it/s, est. speed input: 17929.97 toks/s, output: 17.51 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:03<00:30, 14.85it/s, est. speed input: 17702.07 toks/s, output: 17.29 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:03<00:30, 14.80it/s, est. speed input: 17499.92 toks/s, output: 17.09 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:03<00:30, 14.68it/s, est. speed input: 17306.15 toks/s, output: 16.90 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:04<00:30, 14.66it/s, est. speed input: 17151.99 toks/s, output: 16.75 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:04<00:29, 14.70it/s, est. speed input: 17031.09 toks/s, output: 16.63 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:04<00:29, 14.69it/s, est. speed input: 16914.22 toks/s, output: 16.52 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:04<00:29, 14.62it/s, est. speed input: 16797.29 toks/s, output: 16.40 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:05<00:29, 14.64it/s, est. speed input: 16707.59 toks/s, output: 16.32 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:05<00:28, 14.69it/s, est. speed input: 16630.92 toks/s, output: 16.24 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:05<00:28, 14.63it/s, est. speed input: 16545.61 toks/s, output: 16.16 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:06<00:28, 14.67it/s, est. speed input: 16482.92 toks/s, output: 16.10 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:06<00:27, 14.67it/s, est. speed input: 16419.38 toks/s, output: 16.03 toks/s]
Processed prompts:  21%|██        | 106/512 [00:06<00:27, 14.64it/s, est. speed input: 16357.16 toks/s, output: 15.97 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:06<00:27, 14.58it/s, est. speed input: 16294.66 toks/s, output: 15.91 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:07<00:27, 14.59it/s, est. speed input: 16244.26 toks/s, output: 15.86 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:07<00:26, 14.66it/s, est. speed input: 16205.39 toks/s, output: 15.83 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:07<00:26, 14.68it/s, est. speed input: 16165.46 toks/s, output: 15.79 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:08<00:26, 14.67it/s, est. speed input: 16125.88 toks/s, output: 15.75 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:08<00:26, 14.69it/s, est. speed input: 16091.79 toks/s, output: 15.71 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:08<00:25, 14.63it/s, est. speed input: 16051.73 toks/s, output: 15.68 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:08<00:25, 14.65it/s, est. speed input: 16020.67 toks/s, output: 15.65 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:09<00:25, 14.57it/s, est. speed input: 15980.90 toks/s, output: 15.61 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:09<00:25, 14.63it/s, est. speed input: 15956.76 toks/s, output: 15.58 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:09<00:24, 14.67it/s, est. speed input: 15932.51 toks/s, output: 15.56 toks/s]
Processed prompts:  30%|███       | 154/512 [00:09<00:24, 14.68it/s, est. speed input: 15908.87 toks/s, output: 15.54 toks/s]
Processed prompts:  31%|███       | 158/512 [00:10<00:24, 14.58it/s, est. speed input: 15875.76 toks/s, output: 15.50 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:10<00:24, 14.58it/s, est. speed input: 15851.16 toks/s, output: 15.48 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:10<00:23, 14.60it/s, est. speed input: 15829.06 toks/s, output: 15.46 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:11<00:23, 14.66it/s, est. speed input: 15812.77 toks/s, output: 15.44 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:11<00:23, 14.60it/s, est. speed input: 15788.55 toks/s, output: 15.42 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:11<00:22, 14.62it/s, est. speed input: 15770.52 toks/s, output: 15.40 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:11<00:22, 14.66it/s, est. speed input: 15755.02 toks/s, output: 15.39 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:12<00:22, 14.61it/s, est. speed input: 15734.25 toks/s, output: 15.37 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:12<00:22, 14.56it/s, est. speed input: 15713.32 toks/s, output: 15.35 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:12<00:21, 14.64it/s, est. speed input: 15702.26 toks/s, output: 15.33 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:12<00:21, 14.68it/s, est. speed input: 15689.70 toks/s, output: 15.32 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:13<00:21, 14.71it/s, est. speed input: 15678.86 toks/s, output: 15.31 toks/s]
Processed prompts:  40%|████      | 206/512 [00:13<00:20, 14.64it/s, est. speed input: 15660.92 toks/s, output: 15.29 toks/s]
Processed prompts:  41%|████      | 210/512 [00:13<00:20, 14.68it/s, est. speed input: 15650.33 toks/s, output: 15.28 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:14<00:20, 14.68it/s, est. speed input: 15638.59 toks/s, output: 15.27 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:14<00:19, 14.72it/s, est. speed input: 15629.57 toks/s, output: 15.26 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:14<00:19, 14.59it/s, est. speed input: 15610.85 toks/s, output: 15.24 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:14<00:19, 14.64it/s, est. speed input: 15601.37 toks/s, output: 15.24 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:15<00:19, 14.61it/s, est. speed input: 15588.73 toks/s, output: 15.22 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:15<00:19, 14.56it/s, est. speed input: 15574.08 toks/s, output: 15.21 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:15<00:18, 14.62it/s, est. speed input: 15566.13 toks/s, output: 15.20 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:15<00:18, 14.67it/s, est. speed input: 15559.04 toks/s, output: 15.19 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:16<00:18, 14.67it/s, est. speed input: 15549.79 toks/s, output: 15.19 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:16<00:17, 14.56it/s, est. speed input: 15534.58 toks/s, output: 15.17 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:16<00:17, 14.60it/s, est. speed input: 15526.47 toks/s, output: 15.16 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:17<00:17, 14.59it/s, est. speed input: 15516.74 toks/s, output: 15.15 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:17<00:17, 14.63it/s, est. speed input: 15510.14 toks/s, output: 15.15 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:17<00:16, 14.54it/s, est. speed input: 15497.03 toks/s, output: 15.13 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:17<00:16, 14.56it/s, est. speed input: 15488.52 toks/s, output: 15.13 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:18<00:16, 14.56it/s, est. speed input: 15479.85 toks/s, output: 15.12 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:18<00:16, 14.62it/s, est. speed input: 15474.67 toks/s, output: 15.11 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:18<00:15, 14.58it/s, est. speed input: 15464.94 toks/s, output: 15.10 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:18<00:15, 14.60it/s, est. speed input: 15458.29 toks/s, output: 15.10 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:19<00:15, 14.63it/s, est. speed input: 15452.35 toks/s, output: 15.09 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:19<00:14, 14.69it/s, est. speed input: 15449.05 toks/s, output: 15.09 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:19<00:14, 14.61it/s, est. speed input: 15439.24 toks/s, output: 15.08 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:20<00:14, 14.62it/s, est. speed input: 15433.06 toks/s, output: 15.07 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:20<00:14, 14.67it/s, est. speed input: 15429.45 toks/s, output: 15.07 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:20<00:13, 14.65it/s, est. speed input: 15423.02 toks/s, output: 15.06 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:20<00:13, 14.60it/s, est. speed input: 15415.15 toks/s, output: 15.05 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:21<00:13, 14.65it/s, est. speed input: 15411.49 toks/s, output: 15.05 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:21<00:12, 14.65it/s, est. speed input: 15406.10 toks/s, output: 15.05 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:21<00:12, 14.65it/s, est. speed input: 15401.26 toks/s, output: 15.04 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:21<00:12, 14.55it/s, est. speed input: 15391.58 toks/s, output: 15.03 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:22<00:12, 14.63it/s, est. speed input: 15389.25 toks/s, output: 15.03 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:22<00:11, 14.62it/s, est. speed input: 15383.70 toks/s, output: 15.02 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:22<00:11, 15.25it/s, est. speed input: 15404.38 toks/s, output: 15.04 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:23<00:11, 14.94it/s, est. speed input: 15394.56 toks/s, output: 15.03 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:23<00:10, 14.89it/s, est. speed input: 15391.34 toks/s, output: 15.03 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:23<00:10, 14.80it/s, est. speed input: 15386.23 toks/s, output: 15.03 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:23<00:10, 14.83it/s, est. speed input: 15384.79 toks/s, output: 15.02 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:24<00:10, 14.69it/s, est. speed input: 15377.05 toks/s, output: 15.02 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:24<00:09, 14.70it/s, est. speed input: 15373.82 toks/s, output: 15.01 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:24<00:09, 14.70it/s, est. speed input: 15370.27 toks/s, output: 15.01 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:24<00:09, 14.61it/s, est. speed input: 15363.38 toks/s, output: 15.00 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:25<00:09, 14.59it/s, est. speed input: 15358.24 toks/s, output: 15.00 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:25<00:08, 14.65it/s, est. speed input: 15355.85 toks/s, output: 15.00 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:25<00:08, 14.67it/s, est. speed input: 15352.73 toks/s, output: 14.99 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:26<00:08, 14.61it/s, est. speed input: 15347.36 toks/s, output: 14.99 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:26<00:08, 14.63it/s, est. speed input: 15343.97 toks/s, output: 14.98 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:26<00:07, 14.60it/s, est. speed input: 15339.12 toks/s, output: 14.98 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:26<00:07, 14.62it/s, est. speed input: 15335.87 toks/s, output: 14.98 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:27<00:07, 14.51it/s, est. speed input: 15328.15 toks/s, output: 14.97 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:27<00:06, 14.60it/s, est. speed input: 15326.66 toks/s, output: 14.97 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:27<00:06, 14.63it/s, est. speed input: 15324.14 toks/s, output: 14.96 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:27<00:06, 14.69it/s, est. speed input: 15322.56 toks/s, output: 14.96 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:28<00:06, 14.56it/s, est. speed input: 15315.70 toks/s, output: 14.96 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:28<00:05, 14.56it/s, est. speed input: 15311.72 toks/s, output: 14.95 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:28<00:05, 14.59it/s, est. speed input: 15308.75 toks/s, output: 14.95 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:29<00:05, 14.59it/s, est. speed input: 15305.49 toks/s, output: 14.95 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:29<00:05, 14.59it/s, est. speed input: 15301.83 toks/s, output: 14.94 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:29<00:04, 14.59it/s, est. speed input: 15298.48 toks/s, output: 14.94 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:29<00:04, 14.62it/s, est. speed input: 15296.07 toks/s, output: 14.94 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:30<00:04, 15.48it/s, est. speed input: 15318.89 toks/s, output: 14.96 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:30<00:03, 15.15it/s, est. speed input: 15313.92 toks/s, output: 14.95 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:30<00:03, 14.98it/s, est. speed input: 15310.72 toks/s, output: 14.95 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:30<00:03, 14.88it/s, est. speed input: 15308.02 toks/s, output: 14.95 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:31<00:03, 14.78it/s, est. speed input: 15304.52 toks/s, output: 14.95 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:31<00:03, 12.91it/s, est. speed input: 15239.68 toks/s, output: 14.88 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:31<00:02, 13.05it/s, est. speed input: 15225.23 toks/s, output: 14.87 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:32<00:02, 13.55it/s, est. speed input: 15225.32 toks/s, output: 14.87 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:32<00:02, 13.33it/s, est. speed input: 15205.43 toks/s, output: 14.85 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:32<00:01, 13.65it/s, est. speed input: 15201.99 toks/s, output: 14.85 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:33<00:01, 13.94it/s, est. speed input: 15200.59 toks/s, output: 14.84 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:33<00:01, 14.15it/s, est. speed input: 15199.01 toks/s, output: 14.84 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:33<00:00, 14.22it/s, est. speed input: 15195.19 toks/s, output: 14.84 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:33<00:00, 14.37it/s, est. speed input: 15194.40 toks/s, output: 14.84 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:34<00:00, 14.43it/s, est. speed input: 15192.12 toks/s, output: 14.84 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:34<00:00, 15.40it/s, est. speed input: 15214.49 toks/s, output: 14.86 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:34<00:00, 15.40it/s, est. speed input: 15274.10 toks/s, output: 14.92 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:34<00:00, 14.92it/s, est. speed input: 15274.10 toks/s, output: 14.92 toks/s]
[rank0]:[W128 09:06:42.274208660 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 68.6s

测试结果:
  Requests/s:   14.52
  Tokens/s:     14887.26
  Total Reqs:   512
  Elapsed:      35.25s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     14872.73

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:06:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:06:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3727749) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3727749) WARNING 01-28 09:07:17 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.63 requests/s, 14992.23 total tokens/s, 14.63 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 09:06:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:06:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:06:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:06:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:06:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:06:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:06:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:06:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:06:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:06:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:06:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:06:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:06:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:06:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:06:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:06:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:06:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:06:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3727749) [2026-01-28 09:06:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3727749) [2026-01-28 09:06:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3727749) [2026-01-28 09:06:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3727749) [2026-01-28 09:06:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3727749) [2026-01-28 09:06:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3727749) [2026-01-28 09:06:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3727749) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3727749) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.68s/it]
(EngineCore_DP0 pid=3727749) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.68s/it]
(EngineCore_DP0 pid=3727749) 
(EngineCore_DP0 pid=3727749) [2026-01-28 09:07:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3727749) [2026-01-28 09:07:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3727749) [2026-01-28 09:07:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3727749) [2026-01-28 09:07:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3727749) [2026-01-28 09:07:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3727749) [2026-01-28 09:07:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3727749) [2026-01-28 09:07:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3727749) [2026-01-28 09:07:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3727749) 2026-01-28 09:07:16,771 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3727749) 2026-01-28 09:07:16,824 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 60/1024 [00:00<00:01, 593.10it/s]
Adding requests:  12%|█▏        | 120/1024 [00:00<00:01, 566.52it/s]
Adding requests:  17%|█▋        | 177/1024 [00:00<00:01, 519.25it/s]
Adding requests:  22%|██▏       | 230/1024 [00:00<00:01, 516.35it/s]
Adding requests:  28%|██▊       | 282/1024 [00:00<00:01, 504.93it/s]
Adding requests:  33%|███▎      | 333/1024 [00:00<00:01, 497.02it/s]
Adding requests:  37%|███▋      | 383/1024 [00:00<00:01, 496.03it/s]
Adding requests:  42%|████▏     | 433/1024 [00:00<00:01, 494.71it/s]
Adding requests:  47%|████▋     | 483/1024 [00:00<00:01, 484.82it/s]
Adding requests:  52%|█████▏    | 532/1024 [00:01<00:01, 465.85it/s]
Adding requests:  57%|█████▋    | 582/1024 [00:01<00:00, 474.90it/s]
Adding requests:  62%|██████▏   | 632/1024 [00:01<00:00, 481.79it/s]
Adding requests:  67%|██████▋   | 683/1024 [00:01<00:00, 487.90it/s]
Adding requests:  72%|███████▏  | 733/1024 [00:01<00:00, 489.94it/s]
Adding requests:  76%|███████▋  | 783/1024 [00:01<00:00, 480.15it/s]
Adding requests:  81%|████████▏ | 832/1024 [00:01<00:00, 467.15it/s]
Adding requests:  86%|████████▋ | 884/1024 [00:01<00:00, 479.77it/s]
Adding requests:  91%|█████████ | 934/1024 [00:01<00:00, 485.45it/s]
Adding requests:  96%|█████████▌| 983/1024 [00:02<00:00, 482.60it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 490.96it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 26/1024 [00:00<00:10, 94.59it/s, est. speed input: 96865.66 toks/s, output: 94.59 toks/s]
Processed prompts:   4%|▎         | 36/1024 [00:00<00:25, 38.44it/s, est. speed input: 45172.99 toks/s, output: 44.11 toks/s]
Processed prompts:   4%|▍         | 42/1024 [00:01<00:40, 24.23it/s, est. speed input: 31512.45 toks/s, output: 30.77 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:01<00:48, 20.23it/s, est. speed input: 26847.82 toks/s, output: 26.22 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:02<00:53, 18.12it/s, est. speed input: 24207.25 toks/s, output: 23.64 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:02<00:56, 16.99it/s, est. speed input: 22590.30 toks/s, output: 22.06 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:03<00:58, 16.19it/s, est. speed input: 21408.83 toks/s, output: 20.91 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:04<00:59, 15.77it/s, est. speed input: 20592.28 toks/s, output: 20.11 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:04<01:00, 15.40it/s, est. speed input: 19927.44 toks/s, output: 19.46 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:05<01:00, 15.20it/s, est. speed input: 19421.48 toks/s, output: 18.97 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:05<01:01, 15.01it/s, est. speed input: 18990.13 toks/s, output: 18.54 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:06<01:01, 14.92it/s, est. speed input: 18648.21 toks/s, output: 18.21 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:06<01:00, 14.82it/s, est. speed input: 18349.62 toks/s, output: 17.92 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:07<01:00, 14.78it/s, est. speed input: 18105.49 toks/s, output: 17.68 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:07<00:59, 14.78it/s, est. speed input: 17902.34 toks/s, output: 17.48 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:08<00:59, 14.77it/s, est. speed input: 17720.70 toks/s, output: 17.31 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:08<00:58, 14.75it/s, est. speed input: 17560.71 toks/s, output: 17.15 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:09<00:58, 14.76it/s, est. speed input: 17422.71 toks/s, output: 17.01 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:10<00:57, 14.73it/s, est. speed input: 17291.34 toks/s, output: 16.89 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:10<00:57, 14.72it/s, est. speed input: 17177.21 toks/s, output: 16.77 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:11<00:57, 14.69it/s, est. speed input: 17068.65 toks/s, output: 16.67 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:11<00:56, 14.74it/s, est. speed input: 16982.48 toks/s, output: 16.58 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:12<00:55, 14.69it/s, est. speed input: 16890.46 toks/s, output: 16.49 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:12<00:55, 14.68it/s, est. speed input: 16809.81 toks/s, output: 16.42 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:13<00:54, 14.66it/s, est. speed input: 16735.22 toks/s, output: 16.34 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:13<00:54, 14.71it/s, est. speed input: 16674.24 toks/s, output: 16.28 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:14<00:53, 14.69it/s, est. speed input: 16610.84 toks/s, output: 16.22 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:14<00:53, 14.70it/s, est. speed input: 16555.24 toks/s, output: 16.17 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:15<00:52, 14.65it/s, est. speed input: 16495.87 toks/s, output: 16.11 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:16<00:52, 14.67it/s, est. speed input: 16448.12 toks/s, output: 16.06 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:16<00:51, 14.65it/s, est. speed input: 16398.98 toks/s, output: 16.01 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:17<00:51, 14.66it/s, est. speed input: 16355.30 toks/s, output: 15.97 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:17<00:50, 14.65it/s, est. speed input: 16313.33 toks/s, output: 15.93 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:18<00:50, 14.61it/s, est. speed input: 16269.23 toks/s, output: 15.89 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:18<00:49, 14.65it/s, est. speed input: 16235.04 toks/s, output: 15.85 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:19<00:49, 14.65it/s, est. speed input: 16200.76 toks/s, output: 15.82 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:19<00:48, 14.68it/s, est. speed input: 16170.38 toks/s, output: 15.79 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:20<00:47, 14.65it/s, est. speed input: 16137.51 toks/s, output: 15.76 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:20<00:47, 14.66it/s, est. speed input: 16109.19 toks/s, output: 15.73 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:21<00:46, 14.89it/s, est. speed input: 16101.85 toks/s, output: 15.72 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:22<00:45, 14.85it/s, est. speed input: 16077.68 toks/s, output: 15.70 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:22<00:45, 14.77it/s, est. speed input: 16049.70 toks/s, output: 15.67 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:23<00:44, 14.77it/s, est. speed input: 16028.51 toks/s, output: 15.65 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:23<00:44, 14.71it/s, est. speed input: 16002.73 toks/s, output: 15.63 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:24<00:43, 14.72it/s, est. speed input: 15982.44 toks/s, output: 15.61 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:24<00:43, 14.70it/s, est. speed input: 15960.59 toks/s, output: 15.59 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:25<00:42, 14.70it/s, est. speed input: 15941.30 toks/s, output: 15.57 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:25<00:42, 14.67it/s, est. speed input: 15920.45 toks/s, output: 15.55 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:26<00:41, 14.71it/s, est. speed input: 15904.68 toks/s, output: 15.53 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:26<00:41, 14.69it/s, est. speed input: 15886.06 toks/s, output: 15.51 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:27<00:40, 14.70it/s, est. speed input: 15870.08 toks/s, output: 15.50 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:28<00:40, 14.70it/s, est. speed input: 15854.04 toks/s, output: 15.48 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:28<00:39, 14.73it/s, est. speed input: 15840.71 toks/s, output: 15.47 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:29<00:38, 15.08it/s, est. speed input: 15849.38 toks/s, output: 15.48 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:29<00:38, 14.73it/s, est. speed input: 15819.95 toks/s, output: 15.45 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:30<00:41, 13.33it/s, est. speed input: 15707.54 toks/s, output: 15.34 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:30<00:40, 13.70it/s, est. speed input: 15695.47 toks/s, output: 15.33 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:31<00:38, 13.96it/s, est. speed input: 15682.28 toks/s, output: 15.31 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:32<00:37, 14.19it/s, est. speed input: 15672.94 toks/s, output: 15.31 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:32<00:36, 14.27it/s, est. speed input: 15658.29 toks/s, output: 15.29 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:33<00:35, 14.39it/s, est. speed input: 15647.75 toks/s, output: 15.28 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:33<00:35, 14.50it/s, est. speed input: 15639.07 toks/s, output: 15.27 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:34<00:34, 14.52it/s, est. speed input: 15627.35 toks/s, output: 15.26 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:34<00:33, 14.54it/s, est. speed input: 15616.81 toks/s, output: 15.25 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:35<00:33, 14.57it/s, est. speed input: 15607.16 toks/s, output: 15.24 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:35<00:32, 14.60it/s, est. speed input: 15598.20 toks/s, output: 15.23 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:36<00:32, 14.60it/s, est. speed input: 15588.18 toks/s, output: 15.22 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:36<00:31, 14.66it/s, est. speed input: 15581.94 toks/s, output: 15.22 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:37<00:30, 14.65it/s, est. speed input: 15573.30 toks/s, output: 15.21 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:38<00:30, 14.64it/s, est. speed input: 15564.54 toks/s, output: 15.20 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:38<00:29, 14.64it/s, est. speed input: 15556.36 toks/s, output: 15.19 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:39<00:29, 14.67it/s, est. speed input: 15550.20 toks/s, output: 15.19 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:39<00:28, 14.63it/s, est. speed input: 15540.65 toks/s, output: 15.18 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:40<00:28, 14.67it/s, est. speed input: 15534.97 toks/s, output: 15.17 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:40<00:27, 14.69it/s, est. speed input: 15529.04 toks/s, output: 15.17 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:41<00:27, 14.72it/s, est. speed input: 15524.28 toks/s, output: 15.16 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:41<00:26, 14.70it/s, est. speed input: 15517.36 toks/s, output: 15.15 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:42<00:25, 14.73it/s, est. speed input: 15512.57 toks/s, output: 15.15 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:42<00:25, 14.69it/s, est. speed input: 15505.70 toks/s, output: 15.14 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:43<00:24, 14.72it/s, est. speed input: 15501.12 toks/s, output: 15.14 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:44<00:24, 14.67it/s, est. speed input: 15493.44 toks/s, output: 15.13 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:44<00:23, 14.67it/s, est. speed input: 15487.85 toks/s, output: 15.12 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:45<00:23, 14.66it/s, est. speed input: 15481.71 toks/s, output: 15.12 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:45<00:22, 14.66it/s, est. speed input: 15476.12 toks/s, output: 15.11 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:46<00:22, 14.63it/s, est. speed input: 15469.34 toks/s, output: 15.11 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:46<00:21, 14.67it/s, est. speed input: 15465.20 toks/s, output: 15.10 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:47<00:21, 14.62it/s, est. speed input: 15458.25 toks/s, output: 15.10 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:47<00:20, 14.65it/s, est. speed input: 15454.00 toks/s, output: 15.09 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:48<00:20, 14.66it/s, est. speed input: 15449.29 toks/s, output: 15.09 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:48<00:19, 14.70it/s, est. speed input: 15445.87 toks/s, output: 15.08 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:49<00:18, 14.67it/s, est. speed input: 15440.50 toks/s, output: 15.08 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:50<00:18, 14.69it/s, est. speed input: 15436.59 toks/s, output: 15.07 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:50<00:17, 14.66it/s, est. speed input: 15431.23 toks/s, output: 15.07 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:51<00:17, 14.69it/s, est. speed input: 15427.85 toks/s, output: 15.07 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:51<00:16, 14.66it/s, est. speed input: 15422.85 toks/s, output: 15.06 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:52<00:16, 14.67it/s, est. speed input: 15418.70 toks/s, output: 15.06 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:52<00:15, 14.63it/s, est. speed input: 15413.11 toks/s, output: 15.05 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:53<00:15, 14.67it/s, est. speed input: 15410.33 toks/s, output: 15.05 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:53<00:14, 14.66it/s, est. speed input: 15405.94 toks/s, output: 15.04 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:54<00:14, 14.62it/s, est. speed input: 15400.60 toks/s, output: 15.04 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:54<00:13, 14.64it/s, est. speed input: 15396.97 toks/s, output: 15.04 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:55<00:12, 14.63it/s, est. speed input: 15392.65 toks/s, output: 15.03 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:56<00:12, 14.69it/s, est. speed input: 15390.62 toks/s, output: 15.03 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:56<00:11, 14.66it/s, est. speed input: 15386.28 toks/s, output: 15.03 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:57<00:11, 14.70it/s, est. speed input: 15384.09 toks/s, output: 15.02 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:57<00:10, 14.64it/s, est. speed input: 15378.80 toks/s, output: 15.02 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:58<00:10, 14.66it/s, est. speed input: 15375.82 toks/s, output: 15.02 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:58<00:09, 14.60it/s, est. speed input: 15370.67 toks/s, output: 15.01 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:59<00:09, 14.62it/s, est. speed input: 15367.41 toks/s, output: 15.01 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:59<00:08, 14.62it/s, est. speed input: 15363.72 toks/s, output: 15.00 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [01:00<00:08, 14.62it/s, est. speed input: 15360.15 toks/s, output: 15.00 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [01:00<00:07, 14.60it/s, est. speed input: 15355.92 toks/s, output: 15.00 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [01:01<00:06, 14.61it/s, est. speed input: 15352.69 toks/s, output: 14.99 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [01:02<00:06, 14.58it/s, est. speed input: 15348.40 toks/s, output: 14.99 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [01:02<00:05, 14.87it/s, est. speed input: 15353.48 toks/s, output: 14.99 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [01:03<00:05, 14.75it/s, est. speed input: 15348.78 toks/s, output: 14.99 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [01:03<00:04, 14.72it/s, est. speed input: 15345.91 toks/s, output: 14.99 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [01:04<00:04, 14.68it/s, est. speed input: 15342.21 toks/s, output: 14.98 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [01:04<00:03, 14.65it/s, est. speed input: 15338.69 toks/s, output: 14.98 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [01:05<00:03, 14.62it/s, est. speed input: 15335.14 toks/s, output: 14.98 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [01:05<00:02, 15.15it/s, est. speed input: 15347.07 toks/s, output: 14.99 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [01:06<00:02, 15.00it/s, est. speed input: 15344.21 toks/s, output: 14.98 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [01:06<00:01, 14.91it/s, est. speed input: 15341.98 toks/s, output: 14.98 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [01:07<00:00, 14.82it/s, est. speed input: 15338.92 toks/s, output: 14.98 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [01:07<00:00, 15.18it/s, est. speed input: 15347.34 toks/s, output: 14.99 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:07<00:00, 15.18it/s, est. speed input: 15437.76 toks/s, output: 15.08 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:07<00:00, 15.08it/s, est. speed input: 15437.76 toks/s, output: 15.08 toks/s]
[rank0]:[W128 09:08:28.160891811 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 105.9s

测试结果:
  Requests/s:   14.63
  Tokens/s:     14992.23
  Total Reqs:   1024
  Elapsed:      70.01s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     14977.61

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:08:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:08:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3729442) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3729442) WARNING 01-28 09:09:07 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.93 requests/s, 15301.94 total tokens/s, 14.93 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 09:08:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:08:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:08:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:08:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:08:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:08:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:08:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:08:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:08:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:08:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:08:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:08:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:08:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:08:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:08:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:08:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:08:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:08:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:43] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:43] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:43] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:43] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:43] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3729442) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3729442) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.66s/it]
(EngineCore_DP0 pid=3729442) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.66s/it]
(EngineCore_DP0 pid=3729442) 
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3729442) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3729442) 2026-01-28 09:09:05,793 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3729442) 2026-01-28 09:09:05,863 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 66/2048 [00:00<00:03, 654.01it/s]
Adding requests:   6%|▋         | 132/2048 [00:00<00:03, 604.79it/s]
Adding requests:   9%|▉         | 193/2048 [00:00<00:03, 543.45it/s]
Adding requests:  12%|█▏        | 248/2048 [00:00<00:03, 540.96it/s]
Adding requests:  15%|█▍        | 303/2048 [00:00<00:03, 527.60it/s]
Adding requests:  17%|█▋        | 357/2048 [00:00<00:03, 529.33it/s]
Adding requests:  20%|██        | 411/2048 [00:00<00:03, 518.15it/s]
Adding requests:  23%|██▎       | 464/2048 [00:00<00:03, 520.36it/s]
Adding requests:  25%|██▌       | 517/2048 [00:00<00:03, 502.76it/s]
Adding requests:  28%|██▊       | 568/2048 [00:01<00:02, 497.78it/s]
Adding requests:  30%|███       | 620/2048 [00:01<00:02, 504.07it/s]
Adding requests:  33%|███▎      | 671/2048 [00:01<00:02, 505.48it/s]
Adding requests:  35%|███▌      | 724/2048 [00:01<00:02, 511.00it/s]
Adding requests:  38%|███▊      | 776/2048 [00:01<00:02, 486.98it/s]
Adding requests:  40%|████      | 825/2048 [00:01<00:02, 481.83it/s]
Adding requests:  43%|████▎     | 874/2048 [00:02<00:06, 195.38it/s]
Adding requests:  45%|████▌     | 925/2048 [00:02<00:04, 239.77it/s]
Adding requests:  48%|████▊     | 975/2048 [00:02<00:03, 282.60it/s]
Adding requests:  50%|█████     | 1028/2048 [00:02<00:03, 330.03it/s]
Adding requests:  53%|█████▎    | 1079/2048 [00:02<00:02, 368.54it/s]
Adding requests:  55%|█████▌    | 1131/2048 [00:02<00:02, 403.90it/s]
Adding requests:  58%|█████▊    | 1184/2048 [00:02<00:01, 435.15it/s]
Adding requests:  60%|██████    | 1235/2048 [00:02<00:01, 453.51it/s]
Adding requests:  63%|██████▎   | 1286/2048 [00:03<00:01, 460.97it/s]
Adding requests:  65%|██████▌   | 1338/2048 [00:03<00:01, 476.51it/s]
Adding requests:  68%|██████▊   | 1391/2048 [00:03<00:01, 490.57it/s]
Adding requests:  70%|███████   | 1442/2048 [00:03<00:01, 492.66it/s]
Adding requests:  73%|███████▎  | 1494/2048 [00:03<00:01, 499.67it/s]
Adding requests:  75%|███████▌  | 1545/2048 [00:03<00:01, 502.58it/s]
Adding requests:  78%|███████▊  | 1600/2048 [00:03<00:00, 514.73it/s]
Adding requests:  81%|████████  | 1653/2048 [00:03<00:00, 517.66it/s]
Adding requests:  83%|████████▎ | 1706/2048 [00:03<00:00, 509.75it/s]
Adding requests:  86%|████████▌ | 1758/2048 [00:03<00:00, 510.82it/s]
Adding requests:  88%|████████▊ | 1812/2048 [00:04<00:00, 517.94it/s]
Adding requests:  91%|█████████ | 1864/2048 [00:04<00:00, 512.67it/s]
Adding requests:  94%|█████████▎| 1917/2048 [00:04<00:00, 516.41it/s]
Adding requests:  96%|█████████▌| 1969/2048 [00:04<00:00, 511.10it/s]
Adding requests:  99%|█████████▊| 2021/2048 [00:04<00:00, 493.73it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 451.92it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 66/2048 [00:01<00:30, 65.11it/s, est. speed input: 66679.76 toks/s, output: 65.11 toks/s]
Processed prompts:   4%|▍         | 82/2048 [00:02<00:56, 34.98it/s, est. speed input: 40325.58 toks/s, output: 39.38 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:03<01:15, 25.69it/s, est. speed input: 31803.04 toks/s, output: 31.06 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:04<01:30, 21.40it/s, est. speed input: 27613.79 toks/s, output: 26.97 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:05<01:40, 19.07it/s, est. speed input: 25129.20 toks/s, output: 24.54 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:06<01:47, 17.68it/s, est. speed input: 23489.42 toks/s, output: 22.94 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:07<01:52, 16.80it/s, est. speed input: 22319.05 toks/s, output: 21.80 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:08<01:55, 16.20it/s, est. speed input: 21429.97 toks/s, output: 20.93 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:09<01:57, 15.81it/s, est. speed input: 20748.18 toks/s, output: 20.26 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:10<01:58, 15.55it/s, est. speed input: 20203.86 toks/s, output: 19.73 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:11<01:58, 15.37it/s, est. speed input: 19757.08 toks/s, output: 19.29 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:12<01:58, 15.25it/s, est. speed input: 19387.76 toks/s, output: 18.93 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:13<01:58, 15.17it/s, est. speed input: 19074.76 toks/s, output: 18.63 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:14<01:57, 15.12it/s, est. speed input: 18810.01 toks/s, output: 18.37 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:15<01:56, 15.06it/s, est. speed input: 18572.88 toks/s, output: 18.14 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:17<01:56, 15.02it/s, est. speed input: 18365.47 toks/s, output: 17.94 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:18<01:55, 14.99it/s, est. speed input: 18183.42 toks/s, output: 17.76 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:19<01:53, 15.10it/s, est. speed input: 18050.45 toks/s, output: 17.63 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:20<01:52, 15.04it/s, est. speed input: 17903.03 toks/s, output: 17.48 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:21<01:51, 15.01it/s, est. speed input: 17771.43 toks/s, output: 17.35 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:22<01:50, 14.98it/s, est. speed input: 17650.91 toks/s, output: 17.24 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:23<01:49, 14.97it/s, est. speed input: 17544.01 toks/s, output: 17.13 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:24<01:49, 14.95it/s, est. speed input: 17444.22 toks/s, output: 17.04 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:25<01:48, 14.94it/s, est. speed input: 17353.60 toks/s, output: 16.95 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:26<01:45, 15.10it/s, est. speed input: 17295.80 toks/s, output: 16.89 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:27<01:51, 14.22it/s, est. speed input: 17090.29 toks/s, output: 16.69 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:28<01:48, 14.43it/s, est. speed input: 17025.09 toks/s, output: 16.63 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:30<01:46, 14.58it/s, est. speed input: 16963.53 toks/s, output: 16.57 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:31<01:44, 14.67it/s, est. speed input: 16904.58 toks/s, output: 16.51 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:32<01:42, 14.75it/s, est. speed input: 16850.33 toks/s, output: 16.46 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:33<01:41, 14.80it/s, est. speed input: 16799.91 toks/s, output: 16.41 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:34<01:40, 14.83it/s, est. speed input: 16751.29 toks/s, output: 16.36 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:35<01:39, 14.84it/s, est. speed input: 16705.31 toks/s, output: 16.31 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:36<01:37, 14.88it/s, est. speed input: 16664.74 toks/s, output: 16.27 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:37<01:36, 14.89it/s, est. speed input: 16625.41 toks/s, output: 16.24 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:38<01:35, 14.91it/s, est. speed input: 16588.44 toks/s, output: 16.20 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:39<01:34, 14.90it/s, est. speed input: 16552.55 toks/s, output: 16.16 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:40<01:33, 14.92it/s, est. speed input: 16519.66 toks/s, output: 16.13 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:41<01:31, 14.94it/s, est. speed input: 16489.78 toks/s, output: 16.10 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:42<01:30, 14.93it/s, est. speed input: 16459.08 toks/s, output: 16.07 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:44<01:29, 14.92it/s, est. speed input: 16429.86 toks/s, output: 16.04 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:45<01:28, 14.91it/s, est. speed input: 16401.64 toks/s, output: 16.02 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:46<01:27, 14.90it/s, est. speed input: 16374.68 toks/s, output: 15.99 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:47<01:26, 14.91it/s, est. speed input: 16349.55 toks/s, output: 15.97 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:48<01:25, 14.92it/s, est. speed input: 16326.56 toks/s, output: 15.94 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:49<01:24, 14.91it/s, est. speed input: 16303.17 toks/s, output: 15.92 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:50<01:23, 14.91it/s, est. speed input: 16280.94 toks/s, output: 15.90 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:51<01:22, 14.93it/s, est. speed input: 16261.57 toks/s, output: 15.88 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:52<01:21, 14.92it/s, est. speed input: 16240.63 toks/s, output: 15.86 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:53<01:20, 14.91it/s, est. speed input: 16221.16 toks/s, output: 15.84 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:54<01:19, 14.92it/s, est. speed input: 16203.24 toks/s, output: 15.82 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:55<01:18, 14.92it/s, est. speed input: 16185.09 toks/s, output: 15.81 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:56<01:17, 14.92it/s, est. speed input: 16168.10 toks/s, output: 15.79 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:57<01:16, 14.91it/s, est. speed input: 16150.99 toks/s, output: 15.77 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:58<01:13, 15.13it/s, est. speed input: 16149.16 toks/s, output: 15.77 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [01:00<01:14, 14.76it/s, est. speed input: 16113.65 toks/s, output: 15.74 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [01:01<01:13, 14.80it/s, est. speed input: 16098.51 toks/s, output: 15.72 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [01:02<01:11, 15.05it/s, est. speed input: 16097.70 toks/s, output: 15.72 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [01:03<01:10, 15.01it/s, est. speed input: 16083.70 toks/s, output: 15.71 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [01:04<01:09, 14.97it/s, est. speed input: 16069.47 toks/s, output: 15.69 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [01:05<01:08, 14.95it/s, est. speed input: 16056.52 toks/s, output: 15.68 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [01:06<01:07, 14.94it/s, est. speed input: 16044.02 toks/s, output: 15.67 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [01:07<01:06, 14.92it/s, est. speed input: 16031.01 toks/s, output: 15.66 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [01:08<01:05, 14.92it/s, est. speed input: 16019.01 toks/s, output: 15.64 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [01:09<01:04, 14.91it/s, est. speed input: 16007.08 toks/s, output: 15.63 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [01:10<01:03, 14.91it/s, est. speed input: 15995.82 toks/s, output: 15.62 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [01:11<01:02, 14.90it/s, est. speed input: 15984.76 toks/s, output: 15.61 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [01:12<01:01, 14.90it/s, est. speed input: 15973.86 toks/s, output: 15.60 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [01:13<00:59, 15.14it/s, est. speed input: 15975.48 toks/s, output: 15.60 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [01:15<00:58, 15.08it/s, est. speed input: 15965.78 toks/s, output: 15.59 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [01:16<00:57, 15.02it/s, est. speed input: 15955.61 toks/s, output: 15.58 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [01:17<00:56, 14.97it/s, est. speed input: 15945.28 toks/s, output: 15.57 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [01:18<00:55, 14.96it/s, est. speed input: 15936.26 toks/s, output: 15.56 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [01:19<00:54, 14.94it/s, est. speed input: 15927.17 toks/s, output: 15.55 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [01:20<00:53, 14.93it/s, est. speed input: 15918.48 toks/s, output: 15.55 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [01:21<00:51, 15.16it/s, est. speed input: 15920.39 toks/s, output: 15.55 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [01:22<00:50, 15.07it/s, est. speed input: 15911.53 toks/s, output: 15.54 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [01:23<00:49, 15.26it/s, est. speed input: 15913.96 toks/s, output: 15.54 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [01:24<00:48, 15.15it/s, est. speed input: 15905.49 toks/s, output: 15.53 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [01:25<00:47, 15.07it/s, est. speed input: 15897.28 toks/s, output: 15.52 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [01:26<00:46, 15.02it/s, est. speed input: 15889.27 toks/s, output: 15.52 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [01:27<00:45, 14.98it/s, est. speed input: 15881.49 toks/s, output: 15.51 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [01:28<00:44, 14.95it/s, est. speed input: 15873.60 toks/s, output: 15.50 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [01:29<00:43, 14.92it/s, est. speed input: 15865.78 toks/s, output: 15.49 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [01:31<00:42, 14.92it/s, est. speed input: 15858.80 toks/s, output: 15.49 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [01:32<00:41, 14.83it/s, est. speed input: 15848.27 toks/s, output: 15.48 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [01:33<00:40, 14.85it/s, est. speed input: 15841.28 toks/s, output: 15.47 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [01:34<00:39, 14.85it/s, est. speed input: 15834.12 toks/s, output: 15.46 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [01:35<00:38, 14.87it/s, est. speed input: 15827.91 toks/s, output: 15.46 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [01:36<00:37, 14.87it/s, est. speed input: 15821.26 toks/s, output: 15.45 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [01:37<00:36, 14.88it/s, est. speed input: 15814.92 toks/s, output: 15.44 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [01:38<00:35, 14.89it/s, est. speed input: 15808.95 toks/s, output: 15.44 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [01:39<00:34, 14.89it/s, est. speed input: 15802.84 toks/s, output: 15.43 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [01:40<00:33, 14.88it/s, est. speed input: 15796.79 toks/s, output: 15.43 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [01:41<00:32, 14.89it/s, est. speed input: 15790.97 toks/s, output: 15.42 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [01:42<00:30, 15.12it/s, est. speed input: 15793.92 toks/s, output: 15.42 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [01:43<00:29, 15.06it/s, est. speed input: 15788.62 toks/s, output: 15.42 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [01:44<00:28, 15.01it/s, est. speed input: 15783.16 toks/s, output: 15.41 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [01:46<00:27, 14.97it/s, est. speed input: 15777.50 toks/s, output: 15.41 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [01:47<00:26, 15.18it/s, est. speed input: 15780.35 toks/s, output: 15.41 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [01:48<00:25, 15.08it/s, est. speed input: 15774.72 toks/s, output: 15.40 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [01:49<00:24, 15.03it/s, est. speed input: 15769.72 toks/s, output: 15.40 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [01:50<00:23, 14.98it/s, est. speed input: 15764.43 toks/s, output: 15.39 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [01:51<00:22, 14.96it/s, est. speed input: 15759.49 toks/s, output: 15.39 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [01:52<00:21, 14.93it/s, est. speed input: 15754.47 toks/s, output: 15.39 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [01:53<00:20, 14.94it/s, est. speed input: 15750.40 toks/s, output: 15.38 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [01:54<00:19, 14.93it/s, est. speed input: 15745.88 toks/s, output: 15.38 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [01:55<00:18, 14.93it/s, est. speed input: 15741.52 toks/s, output: 15.37 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [01:56<00:17, 14.92it/s, est. speed input: 15736.96 toks/s, output: 15.37 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [01:57<00:15, 14.91it/s, est. speed input: 15732.65 toks/s, output: 15.36 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [01:58<00:14, 14.91it/s, est. speed input: 15728.50 toks/s, output: 15.36 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [01:59<00:13, 14.90it/s, est. speed input: 15724.12 toks/s, output: 15.36 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [02:01<00:12, 14.90it/s, est. speed input: 15719.83 toks/s, output: 15.35 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [02:02<00:11, 15.13it/s, est. speed input: 15722.94 toks/s, output: 15.35 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [02:03<00:10, 15.06it/s, est. speed input: 15718.78 toks/s, output: 15.35 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [02:04<00:09, 15.01it/s, est. speed input: 15714.72 toks/s, output: 15.35 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [02:05<00:08, 14.91it/s, est. speed input: 15708.84 toks/s, output: 15.34 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [02:06<00:07, 14.90it/s, est. speed input: 15704.66 toks/s, output: 15.34 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [02:07<00:06, 15.13it/s, est. speed input: 15707.64 toks/s, output: 15.34 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [02:08<00:05, 15.06it/s, est. speed input: 15703.77 toks/s, output: 15.34 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [02:09<00:04, 15.27it/s, est. speed input: 15707.35 toks/s, output: 15.34 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [02:10<00:03, 15.15it/s, est. speed input: 15703.51 toks/s, output: 15.34 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [02:11<00:01, 15.07it/s, est. speed input: 15699.87 toks/s, output: 15.33 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [02:12<00:00, 15.21it/s, est. speed input: 15701.45 toks/s, output: 15.33 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:12<00:00, 15.21it/s, est. speed input: 15809.50 toks/s, output: 15.44 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:12<00:00, 15.44it/s, est. speed input: 15809.50 toks/s, output: 15.44 toks/s]
[rank0]:[W128 09:11:25.073991550 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 176.9s

测试结果:
  Requests/s:   14.93
  Tokens/s:     15301.94
  Total Reqs:   2048
  Elapsed:      137.19s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     15287.01

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:11:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:11:42 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3732122) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3732122) WARNING 01-28 09:12:12 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.78 requests/s, 15152.57 total tokens/s, 14.78 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 09:11:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:11:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:11:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:11:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:11:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:11:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:11:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:11:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:11:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:11:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:11:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:11:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:11:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:11:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:11:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:11:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:11:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:11:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3732122) [2026-01-28 09:11:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3732122) [2026-01-28 09:11:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3732122) [2026-01-28 09:11:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3732122) [2026-01-28 09:11:46] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3732122) [2026-01-28 09:11:46] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3732122) [2026-01-28 09:11:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3732122) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3732122) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.53s/it]
(EngineCore_DP0 pid=3732122) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.53s/it]
(EngineCore_DP0 pid=3732122) 
(EngineCore_DP0 pid=3732122) [2026-01-28 09:12:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3732122) [2026-01-28 09:12:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3732122) [2026-01-28 09:12:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3732122) [2026-01-28 09:12:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3732122) [2026-01-28 09:12:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3732122) [2026-01-28 09:12:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3732122) [2026-01-28 09:12:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3732122) [2026-01-28 09:12:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3732122) 2026-01-28 09:12:09,373 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3732122) 2026-01-28 09:12:09,686 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 62/4096 [00:00<00:06, 615.13it/s]
Adding requests:   3%|▎         | 124/4096 [00:00<00:07, 558.68it/s]
Adding requests:   4%|▍         | 181/4096 [00:00<00:07, 521.61it/s]
Adding requests:   6%|▌         | 234/4096 [00:00<00:07, 518.51it/s]
Adding requests:   7%|▋         | 287/4096 [00:00<00:07, 513.72it/s]
Adding requests:   8%|▊         | 339/4096 [00:00<00:07, 511.14it/s]
Adding requests:  10%|▉         | 392/4096 [00:00<00:07, 515.53it/s]
Adding requests:  11%|█         | 445/4096 [00:00<00:07, 516.40it/s]
Adding requests:  12%|█▏        | 497/4096 [00:00<00:07, 512.55it/s]
Adding requests:  13%|█▎        | 549/4096 [00:01<00:06, 508.68it/s]
Adding requests:  15%|█▍        | 602/4096 [00:01<00:06, 514.76it/s]
Adding requests:  16%|█▌        | 657/4096 [00:01<00:06, 522.03it/s]
Adding requests:  17%|█▋        | 712/4096 [00:01<00:06, 527.78it/s]
Adding requests:  19%|█▊        | 765/4096 [00:01<00:06, 521.91it/s]
Adding requests:  20%|█▉        | 818/4096 [00:01<00:06, 510.27it/s]
Adding requests:  21%|██        | 870/4096 [00:01<00:06, 504.83it/s]
Adding requests:  23%|██▎       | 923/4096 [00:01<00:06, 510.77it/s]
Adding requests:  24%|██▍       | 976/4096 [00:01<00:06, 514.36it/s]
Adding requests:  25%|██▌       | 1029/4096 [00:01<00:05, 518.81it/s]
Adding requests:  26%|██▋       | 1081/4096 [00:02<00:05, 518.21it/s]
Adding requests:  28%|██▊       | 1133/4096 [00:02<00:05, 510.20it/s]
Adding requests:  29%|██▉       | 1185/4096 [00:02<00:05, 489.22it/s]
Adding requests:  30%|███       | 1235/4096 [00:02<00:05, 484.47it/s]
Adding requests:  31%|███▏      | 1286/4096 [00:02<00:05, 489.90it/s]
Adding requests:  33%|███▎      | 1338/4096 [00:02<00:05, 497.02it/s]
Adding requests:  34%|███▍      | 1391/4096 [00:02<00:05, 506.29it/s]
Adding requests:  35%|███▌      | 1444/4096 [00:02<00:05, 512.38it/s]
Adding requests:  37%|███▋      | 1497/4096 [00:02<00:05, 515.83it/s]
Adding requests:  38%|███▊      | 1550/4096 [00:03<00:04, 519.41it/s]
Adding requests:  39%|███▉      | 1604/4096 [00:03<00:04, 524.09it/s]
Adding requests:  40%|████      | 1657/4096 [00:03<00:04, 525.02it/s]
Adding requests:  42%|████▏     | 1710/4096 [00:03<00:04, 525.80it/s]
Adding requests:  43%|████▎     | 1764/4096 [00:03<00:04, 527.51it/s]
Adding requests:  44%|████▍     | 1819/4096 [00:03<00:04, 532.60it/s]
Adding requests:  46%|████▌     | 1873/4096 [00:03<00:04, 521.12it/s]
Adding requests:  47%|████▋     | 1926/4096 [00:03<00:04, 512.12it/s]
Adding requests:  48%|████▊     | 1978/4096 [00:03<00:04, 507.88it/s]
Adding requests:  50%|████▉     | 2033/4096 [00:03<00:03, 517.62it/s]
Adding requests:  51%|█████     | 2088/4096 [00:04<00:03, 524.14it/s]
Adding requests:  52%|█████▏    | 2141/4096 [00:04<00:03, 521.71it/s]
Adding requests:  54%|█████▎    | 2194/4096 [00:04<00:03, 521.28it/s]
Adding requests:  55%|█████▍    | 2249/4096 [00:04<00:03, 527.02it/s]
Adding requests:  56%|█████▌    | 2302/4096 [00:04<00:03, 519.89it/s]
Adding requests:  57%|█████▋    | 2355/4096 [00:04<00:03, 510.03it/s]
Adding requests:  59%|█████▉    | 2407/4096 [00:04<00:03, 512.10it/s]
Adding requests:  60%|██████    | 2459/4096 [00:04<00:03, 476.15it/s]
Adding requests:  61%|██████▏   | 2511/4096 [00:04<00:03, 487.51it/s]
Adding requests:  63%|██████▎   | 2562/4096 [00:04<00:03, 492.91it/s]
Adding requests:  64%|██████▍   | 2617/4096 [00:05<00:02, 507.32it/s]
Adding requests:  65%|██████▌   | 2672/4096 [00:05<00:02, 519.28it/s]
Adding requests:  67%|██████▋   | 2725/4096 [00:05<00:02, 516.15it/s]
Adding requests:  68%|██████▊   | 2777/4096 [00:05<00:02, 513.27it/s]
Adding requests:  69%|██████▉   | 2829/4096 [00:05<00:02, 512.24it/s]
Adding requests:  70%|███████   | 2884/4096 [00:05<00:02, 520.78it/s]
Adding requests:  72%|███████▏  | 2937/4096 [00:05<00:02, 514.77it/s]
Adding requests:  73%|███████▎  | 2989/4096 [00:05<00:02, 515.16it/s]
Adding requests:  74%|███████▍  | 3041/4096 [00:05<00:02, 502.88it/s]
Adding requests:  76%|███████▌  | 3093/4096 [00:06<00:01, 506.69it/s]
Adding requests:  77%|███████▋  | 3145/4096 [00:06<00:01, 508.19it/s]
Adding requests:  78%|███████▊  | 3199/4096 [00:06<00:01, 516.37it/s]
Adding requests:  79%|███████▉  | 3252/4096 [00:06<00:01, 520.15it/s]
Adding requests:  81%|████████  | 3305/4096 [00:06<00:01, 517.73it/s]
Adding requests:  82%|████████▏ | 3358/4096 [00:06<00:01, 520.37it/s]
Adding requests:  83%|████████▎ | 3412/4096 [00:06<00:01, 524.02it/s]
Adding requests:  85%|████████▍ | 3465/4096 [00:06<00:01, 515.81it/s]
Adding requests:  86%|████████▌ | 3517/4096 [00:06<00:01, 513.67it/s]
Adding requests:  87%|████████▋ | 3569/4096 [00:06<00:01, 509.64it/s]
Adding requests:  88%|████████▊ | 3621/4096 [00:07<00:00, 512.57it/s]
Adding requests:  90%|████████▉ | 3676/4096 [00:07<00:00, 521.24it/s]
Adding requests:  91%|█████████ | 3730/4096 [00:07<00:00, 524.14it/s]
Adding requests:  92%|█████████▏| 3788/4096 [00:07<00:00, 539.87it/s]
Adding requests:  94%|█████████▍| 3843/4096 [00:07<00:00, 494.74it/s]
Adding requests:  95%|█████████▌| 3897/4096 [00:07<00:00, 506.16it/s]
Adding requests:  96%|█████████▋| 3952/4096 [00:07<00:00, 517.01it/s]
Adding requests:  98%|█████████▊| 4007/4096 [00:07<00:00, 524.33it/s]
Adding requests:  99%|█████████▉| 4060/4096 [00:07<00:00, 511.25it/s]
Adding requests: 100%|██████████| 4096/4096 [00:07<00:00, 514.45it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 98/4096 [00:00<00:35, 112.45it/s, est. speed input: 115157.82 toks/s, output: 112.45 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:03<01:48, 36.39it/s, est. speed input: 43991.39 toks/s, output: 42.96 toks/s]  
Processed prompts:   4%|▍         | 162/4096 [00:05<02:37, 25.02it/s, est. speed input: 31994.71 toks/s, output: 31.24 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:07<03:08, 20.65it/s, est. speed input: 27061.95 toks/s, output: 26.43 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:09<03:30, 18.43it/s, est. speed input: 24356.08 toks/s, output: 23.79 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:11<03:43, 17.16it/s, est. speed input: 22661.60 toks/s, output: 22.13 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:13<03:52, 16.38it/s, est. speed input: 21492.03 toks/s, output: 20.99 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:15<03:56, 15.94it/s, est. speed input: 20671.17 toks/s, output: 20.19 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:18<03:59, 15.59it/s, est. speed input: 20020.94 toks/s, output: 19.55 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:20<04:01, 15.35it/s, est. speed input: 19504.91 toks/s, output: 19.05 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:22<04:02, 15.18it/s, est. speed input: 19085.03 toks/s, output: 18.64 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:25<04:18, 14.09it/s, est. speed input: 18372.50 toks/s, output: 17.94 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:27<04:12, 14.30it/s, est. speed input: 18118.50 toks/s, output: 17.69 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:29<04:07, 14.44it/s, est. speed input: 17900.58 toks/s, output: 17.48 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:31<04:03, 14.56it/s, est. speed input: 17715.86 toks/s, output: 17.30 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:33<04:00, 14.62it/s, est. speed input: 17549.33 toks/s, output: 17.14 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:35<03:57, 14.68it/s, est. speed input: 17406.87 toks/s, output: 17.00 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:38<03:54, 14.73it/s, est. speed input: 17281.91 toks/s, output: 16.88 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:40<03:51, 14.76it/s, est. speed input: 17170.00 toks/s, output: 16.77 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:42<03:49, 14.78it/s, est. speed input: 17068.53 toks/s, output: 16.67 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:44<03:47, 14.79it/s, est. speed input: 16975.84 toks/s, output: 16.58 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:46<03:44, 14.79it/s, est. speed input: 16891.26 toks/s, output: 16.50 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:48<03:42, 14.80it/s, est. speed input: 16814.75 toks/s, output: 16.42 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:50<03:40, 14.80it/s, est. speed input: 16745.66 toks/s, output: 16.35 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:53<03:38, 14.81it/s, est. speed input: 16681.56 toks/s, output: 16.29 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:55<03:36, 14.80it/s, est. speed input: 16621.32 toks/s, output: 16.23 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:57<03:39, 14.44it/s, est. speed input: 16514.59 toks/s, output: 16.13 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:59<03:34, 14.64it/s, est. speed input: 16477.66 toks/s, output: 16.09 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [01:01<03:31, 14.69it/s, est. speed input: 16432.13 toks/s, output: 16.05 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [01:04<03:28, 14.72it/s, est. speed input: 16388.25 toks/s, output: 16.00 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [01:06<03:26, 14.74it/s, est. speed input: 16347.77 toks/s, output: 15.96 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [01:08<03:23, 14.76it/s, est. speed input: 16310.95 toks/s, output: 15.93 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [01:10<03:21, 14.78it/s, est. speed input: 16276.33 toks/s, output: 15.89 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [01:12<03:17, 14.88it/s, est. speed input: 16252.76 toks/s, output: 15.87 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [01:14<03:16, 14.84it/s, est. speed input: 16220.04 toks/s, output: 15.84 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [01:17<03:14, 14.83it/s, est. speed input: 16190.12 toks/s, output: 15.81 toks/s]
Processed prompts:  31%|███       | 1250/4096 [01:19<03:10, 14.91it/s, est. speed input: 16170.42 toks/s, output: 15.79 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [01:21<03:07, 14.97it/s, est. speed input: 16152.29 toks/s, output: 15.77 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [01:23<03:06, 14.92it/s, est. speed input: 16126.62 toks/s, output: 15.75 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [01:25<03:04, 14.89it/s, est. speed input: 16102.39 toks/s, output: 15.72 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [01:27<03:02, 14.86it/s, est. speed input: 16079.23 toks/s, output: 15.70 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [01:30<03:06, 14.42it/s, est. speed input: 16019.13 toks/s, output: 15.64 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [01:32<03:02, 14.53it/s, est. speed input: 15999.22 toks/s, output: 15.62 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [01:34<02:59, 14.61it/s, est. speed input: 15979.97 toks/s, output: 15.61 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [01:36<02:56, 14.67it/s, est. speed input: 15961.31 toks/s, output: 15.59 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [01:38<02:53, 14.71it/s, est. speed input: 15944.28 toks/s, output: 15.57 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [01:40<02:50, 14.83it/s, est. speed input: 15934.49 toks/s, output: 15.56 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [01:43<02:48, 14.82it/s, est. speed input: 15917.72 toks/s, output: 15.54 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [01:45<02:45, 14.90it/s, est. speed input: 15908.44 toks/s, output: 15.54 toks/s]
Processed prompts:  41%|████      | 1666/4096 [01:47<02:43, 14.88it/s, est. speed input: 15893.89 toks/s, output: 15.52 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [01:49<02:41, 14.84it/s, est. speed input: 15878.44 toks/s, output: 15.51 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [01:51<02:39, 14.83it/s, est. speed input: 15864.40 toks/s, output: 15.49 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [01:53<02:37, 14.83it/s, est. speed input: 15851.27 toks/s, output: 15.48 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [01:55<02:35, 14.81it/s, est. speed input: 15837.58 toks/s, output: 15.47 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [01:58<02:33, 14.81it/s, est. speed input: 15825.50 toks/s, output: 15.45 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [02:00<02:30, 14.90it/s, est. speed input: 15819.11 toks/s, output: 15.45 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [02:02<02:28, 14.87it/s, est. speed input: 15807.58 toks/s, output: 15.44 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [02:04<02:29, 14.51it/s, est. speed input: 15775.14 toks/s, output: 15.41 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [02:06<02:25, 14.68it/s, est. speed input: 15769.85 toks/s, output: 15.40 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [02:08<02:22, 14.81it/s, est. speed input: 15765.02 toks/s, output: 15.40 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [02:11<02:20, 14.81it/s, est. speed input: 15755.19 toks/s, output: 15.39 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [02:13<02:18, 14.82it/s, est. speed input: 15745.93 toks/s, output: 15.38 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [02:15<02:16, 14.81it/s, est. speed input: 15736.21 toks/s, output: 15.37 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [02:17<02:13, 14.81it/s, est. speed input: 15727.51 toks/s, output: 15.36 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [02:19<02:11, 14.81it/s, est. speed input: 15718.67 toks/s, output: 15.35 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [02:21<02:09, 14.80it/s, est. speed input: 15709.65 toks/s, output: 15.34 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [02:24<02:05, 15.00it/s, est. speed input: 15712.09 toks/s, output: 15.34 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [02:26<02:04, 14.94it/s, est. speed input: 15703.75 toks/s, output: 15.34 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [02:28<02:01, 14.99it/s, est. speed input: 15700.37 toks/s, output: 15.33 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [02:30<01:59, 14.93it/s, est. speed input: 15692.70 toks/s, output: 15.32 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [02:32<01:57, 14.98it/s, est. speed input: 15689.46 toks/s, output: 15.32 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [02:34<01:53, 15.15it/s, est. speed input: 15692.70 toks/s, output: 15.32 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [02:36<01:54, 14.82it/s, est. speed input: 15674.37 toks/s, output: 15.31 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [02:39<01:52, 14.80it/s, est. speed input: 15666.99 toks/s, output: 15.30 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [02:41<01:50, 14.80it/s, est. speed input: 15660.07 toks/s, output: 15.29 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [02:43<01:47, 14.90it/s, est. speed input: 15657.84 toks/s, output: 15.29 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [02:45<01:45, 14.87it/s, est. speed input: 15651.15 toks/s, output: 15.28 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [02:47<01:42, 14.93it/s, est. speed input: 15648.56 toks/s, output: 15.28 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [02:49<01:40, 14.89it/s, est. speed input: 15642.20 toks/s, output: 15.28 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [02:51<01:38, 14.86it/s, est. speed input: 15635.91 toks/s, output: 15.27 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [02:54<01:36, 14.84it/s, est. speed input: 15629.91 toks/s, output: 15.26 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [02:56<01:34, 14.92it/s, est. speed input: 15627.88 toks/s, output: 15.26 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [02:58<01:32, 14.88it/s, est. speed input: 15622.19 toks/s, output: 15.26 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [03:00<01:30, 14.85it/s, est. speed input: 15616.32 toks/s, output: 15.25 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [03:02<01:28, 14.83it/s, est. speed input: 15610.66 toks/s, output: 15.24 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [03:04<01:26, 14.82it/s, est. speed input: 15605.29 toks/s, output: 15.24 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [03:07<01:23, 14.90it/s, est. speed input: 15603.64 toks/s, output: 15.24 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [03:09<01:23, 14.55it/s, est. speed input: 15585.44 toks/s, output: 15.22 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [03:11<01:20, 14.62it/s, est. speed input: 15580.34 toks/s, output: 15.22 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [03:13<01:18, 14.67it/s, est. speed input: 15575.71 toks/s, output: 15.21 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [03:15<01:16, 14.70it/s, est. speed input: 15570.68 toks/s, output: 15.21 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [03:18<01:13, 14.73it/s, est. speed input: 15566.09 toks/s, output: 15.20 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [03:20<01:11, 14.76it/s, est. speed input: 15561.87 toks/s, output: 15.20 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [03:22<01:09, 14.77it/s, est. speed input: 15557.50 toks/s, output: 15.19 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [03:24<01:05, 15.00it/s, est. speed input: 15561.37 toks/s, output: 15.20 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [03:26<01:03, 15.04it/s, est. speed input: 15560.58 toks/s, output: 15.20 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [03:28<01:01, 14.96it/s, est. speed input: 15556.14 toks/s, output: 15.19 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [03:30<00:59, 14.91it/s, est. speed input: 15552.07 toks/s, output: 15.19 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [03:32<00:57, 14.96it/s, est. speed input: 15550.94 toks/s, output: 15.19 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [03:35<00:55, 14.91it/s, est. speed input: 15546.91 toks/s, output: 15.18 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [03:37<00:53, 14.88it/s, est. speed input: 15542.96 toks/s, output: 15.18 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [03:39<00:51, 14.85it/s, est. speed input: 15539.07 toks/s, output: 15.17 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [03:41<00:50, 14.48it/s, est. speed input: 15522.97 toks/s, output: 15.16 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [03:43<00:48, 14.58it/s, est. speed input: 15519.50 toks/s, output: 15.16 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [03:46<00:45, 14.73it/s, est. speed input: 15519.04 toks/s, output: 15.16 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [03:48<00:43, 14.75it/s, est. speed input: 15515.49 toks/s, output: 15.15 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [03:50<00:40, 14.99it/s, est. speed input: 15519.53 toks/s, output: 15.16 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [03:52<00:38, 14.93it/s, est. speed input: 15516.09 toks/s, output: 15.15 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [03:54<00:36, 14.90it/s, est. speed input: 15512.96 toks/s, output: 15.15 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [03:56<00:34, 14.87it/s, est. speed input: 15509.77 toks/s, output: 15.15 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [03:58<00:32, 14.84it/s, est. speed input: 15506.28 toks/s, output: 15.14 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [04:01<00:30, 14.83it/s, est. speed input: 15503.18 toks/s, output: 15.14 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [04:03<00:27, 14.83it/s, est. speed input: 15500.32 toks/s, output: 15.14 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [04:05<00:25, 14.90it/s, est. speed input: 15499.79 toks/s, output: 15.14 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [04:07<00:23, 14.87it/s, est. speed input: 15496.61 toks/s, output: 15.13 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [04:09<00:21, 14.84it/s, est. speed input: 15493.41 toks/s, output: 15.13 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [04:11<00:19, 14.83it/s, est. speed input: 15490.67 toks/s, output: 15.13 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [04:14<00:17, 14.51it/s, est. speed input: 15478.41 toks/s, output: 15.12 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [04:16<00:15, 14.60it/s, est. speed input: 15475.68 toks/s, output: 15.11 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [04:18<00:12, 14.66it/s, est. speed input: 15473.14 toks/s, output: 15.11 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [04:20<00:10, 14.70it/s, est. speed input: 15470.43 toks/s, output: 15.11 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [04:22<00:08, 14.73it/s, est. speed input: 15467.85 toks/s, output: 15.11 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [04:24<00:06, 14.75it/s, est. speed input: 15465.15 toks/s, output: 15.10 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [04:26<00:04, 15.09it/s, est. speed input: 15471.65 toks/s, output: 15.11 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [04:29<00:01, 15.09it/s, est. speed input: 15471.65 toks/s, output: 15.11 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:29<00:00, 15.09it/s, est. speed input: 15585.79 toks/s, output: 15.22 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:29<00:00, 15.22it/s, est. speed input: 15585.79 toks/s, output: 15.22 toks/s]
[rank0]:[W128 09:16:50.288392234 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 325.3s

测试结果:
  Requests/s:   14.78
  Tokens/s:     15152.57
  Total Reqs:   4096
  Elapsed:      277.08s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     15137.78


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,27.6912,14205.5686,4.6224
1024,1024,1,128,128,15.1036,15481.1651,8.4748
2048,1024,2,256,128,15.4420,15828.0058,16.5782
4096,1024,4,512,128,14.5242,14887.2583,35.2516
8192,1024,8,1024,128,14.6266,14992.2344,70.0096
16384,1024,16,2048,128,14.9287,15301.9399,137.1852
32768,1024,32,4096,128,14.7830,15152.5673,277.0752

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:16:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:16:56 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3736616) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3736616) WARNING 01-28 09:17:25 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 27.56 requests/s, 14137.71 total tokens/s, 27.56 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:16:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:16:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:16:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:16:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:16:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:16:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:16:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:16:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:16:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:16:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:16:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:16:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:16:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:16:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:16:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:16:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:16:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:16:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:00] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:00] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:00] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:00] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:00] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3736616) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3736616) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.43s/it]
(EngineCore_DP0 pid=3736616) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.43s/it]
(EngineCore_DP0 pid=3736616) 
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3736616) [2026-01-28 09:17:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3736616) 2026-01-28 09:17:24,578 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3736616) 2026-01-28 09:17:24,602 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  70%|███████   | 90/128 [00:00<00:00, 895.67it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 979.62it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:04, 26.54it/s, est. speed input: 13589.43 toks/s, output: 26.54 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:04, 27.44it/s, est. speed input: 13979.22 toks/s, output: 27.30 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 27.87it/s, est. speed input: 14161.01 toks/s, output: 27.66 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:04, 28.16it/s, est. speed input: 14279.16 toks/s, output: 27.89 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:03, 28.35it/s, est. speed input: 14360.31 toks/s, output: 28.05 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:03, 28.23it/s, est. speed input: 14357.14 toks/s, output: 28.04 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 28.52it/s, est. speed input: 14434.85 toks/s, output: 28.19 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:03, 28.13it/s, est. speed input: 14377.24 toks/s, output: 28.08 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:03, 28.25it/s, est. speed input: 14402.61 toks/s, output: 28.13 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:03, 28.39it/s, est. speed input: 14431.67 toks/s, output: 28.19 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:03, 28.48it/s, est. speed input: 14453.80 toks/s, output: 28.23 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:03, 28.53it/s, est. speed input: 14471.91 toks/s, output: 28.26 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:03, 28.39it/s, est. speed input: 14464.02 toks/s, output: 28.25 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:03, 28.39it/s, est. speed input: 14469.70 toks/s, output: 28.26 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 28.46it/s, est. speed input: 14481.04 toks/s, output: 28.28 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 28.59it/s, est. speed input: 14500.95 toks/s, output: 28.32 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 28.44it/s, est. speed input: 14494.30 toks/s, output: 28.31 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:02, 28.15it/s, est. speed input: 14470.06 toks/s, output: 28.26 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:02<00:02, 28.18it/s, est. speed input: 14470.19 toks/s, output: 28.26 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:02<00:02, 28.39it/s, est. speed input: 14485.43 toks/s, output: 28.29 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:02<00:02, 28.44it/s, est. speed input: 14492.19 toks/s, output: 28.30 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:02<00:02, 28.45it/s, est. speed input: 14495.82 toks/s, output: 28.31 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:02, 28.56it/s, est. speed input: 14506.99 toks/s, output: 28.33 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:01, 28.59it/s, est. speed input: 14513.91 toks/s, output: 28.35 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:01, 28.60it/s, est. speed input: 14519.82 toks/s, output: 28.36 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 28.63it/s, est. speed input: 14526.19 toks/s, output: 28.37 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 28.69it/s, est. speed input: 14534.91 toks/s, output: 28.39 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 28.23it/s, est. speed input: 14512.48 toks/s, output: 28.34 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:03<00:01, 28.23it/s, est. speed input: 14510.53 toks/s, output: 28.34 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:03<00:01, 28.26it/s, est. speed input: 14510.17 toks/s, output: 28.34 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 28.43it/s, est. speed input: 14518.30 toks/s, output: 28.36 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:03<00:01, 28.32it/s, est. speed input: 14513.56 toks/s, output: 28.35 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:03<00:01, 28.50it/s, est. speed input: 14522.23 toks/s, output: 28.36 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:03<00:00, 28.47it/s, est. speed input: 14522.88 toks/s, output: 28.36 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 28.54it/s, est. speed input: 14527.86 toks/s, output: 28.37 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:03<00:00, 28.50it/s, est. speed input: 14528.40 toks/s, output: 28.38 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:03<00:00, 28.60it/s, est. speed input: 14534.44 toks/s, output: 28.39 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:04<00:00, 28.02it/s, est. speed input: 14511.40 toks/s, output: 28.34 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:04<00:00, 28.05it/s, est. speed input: 14508.21 toks/s, output: 28.34 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:04<00:00, 28.28it/s, est. speed input: 14514.57 toks/s, output: 28.35 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:04<00:00, 28.34it/s, est. speed input: 14516.27 toks/s, output: 28.35 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:04<00:00, 28.28it/s, est. speed input: 14513.44 toks/s, output: 28.35 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 28.28it/s, est. speed input: 14522.95 toks/s, output: 28.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 28.36it/s, est. speed input: 14522.95 toks/s, output: 28.37 toks/s]
[rank0]:[W128 09:17:30.117908666 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.7s

测试结果:
  Requests/s:   27.56
  Tokens/s:     14137.71
  Total Reqs:   128
  Elapsed:      4.64s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     14110.15

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:17:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:17:36 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3737370) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3737370) WARNING 01-28 09:18:04 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.65 requests/s, 15011.83 total tokens/s, 14.65 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:17:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:17:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:17:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:17:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:17:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:17:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:17:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:17:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:17:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:17:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:17:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:17:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:17:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:17:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:17:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:17:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:17:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:17:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:40] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:40] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3737370) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3737370) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.43s/it]
(EngineCore_DP0 pid=3737370) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.43s/it]
(EngineCore_DP0 pid=3737370) 
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3737370) [2026-01-28 09:17:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3737370) 2026-01-28 09:18:03,505 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3737370) 2026-01-28 09:18:03,517 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  51%|█████     | 65/128 [00:00<00:00, 645.08it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 677.49it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:13,  9.12it/s, est. speed input: 9337.76 toks/s, output: 9.12 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:09, 12.87it/s, est. speed input: 12656.68 toks/s, output: 12.36 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:08, 13.84it/s, est. speed input: 13586.63 toks/s, output: 13.27 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:08, 14.42it/s, est. speed input: 14121.96 toks/s, output: 13.79 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:08, 14.75it/s, est. speed input: 14441.79 toks/s, output: 14.10 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 14.82it/s, est. speed input: 14591.18 toks/s, output: 14.25 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:07, 14.78it/s, est. speed input: 14661.85 toks/s, output: 14.32 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 14.85it/s, est. speed input: 14750.60 toks/s, output: 14.40 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:07, 15.03it/s, est. speed input: 14866.01 toks/s, output: 14.52 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:07, 15.05it/s, est. speed input: 14926.70 toks/s, output: 14.58 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:07, 14.99it/s, est. speed input: 14952.75 toks/s, output: 14.60 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 15.10it/s, est. speed input: 15018.71 toks/s, output: 14.67 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 15.10it/s, est. speed input: 15051.87 toks/s, output: 14.70 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 15.03it/s, est. speed input: 15065.90 toks/s, output: 14.71 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:06, 14.84it/s, est. speed input: 15043.62 toks/s, output: 14.69 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:06, 14.95it/s, est. speed input: 15077.51 toks/s, output: 14.72 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:06, 15.09it/s, est. speed input: 15119.57 toks/s, output: 14.77 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:06, 15.03it/s, est. speed input: 15127.14 toks/s, output: 14.77 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:06, 15.08it/s, est. speed input: 15149.40 toks/s, output: 14.79 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 15.08it/s, est. speed input: 15164.57 toks/s, output: 14.81 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 15.02it/s, est. speed input: 15168.24 toks/s, output: 14.81 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 14.91it/s, est. speed input: 15160.94 toks/s, output: 14.81 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:03<00:05, 14.92it/s, est. speed input: 15167.60 toks/s, output: 14.81 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:05, 14.99it/s, est. speed input: 15182.40 toks/s, output: 14.83 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:05, 14.99it/s, est. speed input: 15188.24 toks/s, output: 14.83 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:05, 15.15it/s, est. speed input: 15215.57 toks/s, output: 14.86 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 15.22it/s, est. speed input: 15235.71 toks/s, output: 14.88 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 15.15it/s, est. speed input: 15238.90 toks/s, output: 14.88 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 15.15it/s, est. speed input: 15249.30 toks/s, output: 14.89 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 15.04it/s, est. speed input: 15244.93 toks/s, output: 14.89 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:04<00:04, 15.04it/s, est. speed input: 15249.83 toks/s, output: 14.89 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:04<00:04, 15.08it/s, est. speed input: 15259.45 toks/s, output: 14.90 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:04, 15.07it/s, est. speed input: 15263.69 toks/s, output: 14.91 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:04, 15.12it/s, est. speed input: 15273.57 toks/s, output: 14.92 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 15.17it/s, est. speed input: 15284.76 toks/s, output: 14.93 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 15.10it/s, est. speed input: 15284.57 toks/s, output: 14.93 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 15.14it/s, est. speed input: 15293.05 toks/s, output: 14.93 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:05<00:03, 14.96it/s, est. speed input: 15282.42 toks/s, output: 14.92 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:05<00:03, 14.95it/s, est. speed input: 15282.31 toks/s, output: 14.92 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:05<00:03, 15.08it/s, est. speed input: 15294.08 toks/s, output: 14.94 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:05<00:03, 15.08it/s, est. speed input: 15297.43 toks/s, output: 14.94 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 15.11it/s, est. speed input: 15303.77 toks/s, output: 14.95 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 15.10it/s, est. speed input: 15307.02 toks/s, output: 14.95 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 15.09it/s, est. speed input: 15309.60 toks/s, output: 14.95 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 15.09it/s, est. speed input: 15313.10 toks/s, output: 14.95 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:06<00:02, 14.97it/s, est. speed input: 15306.85 toks/s, output: 14.95 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:06<00:02, 15.00it/s, est. speed input: 15309.53 toks/s, output: 14.95 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:06<00:02, 15.05it/s, est. speed input: 15314.25 toks/s, output: 14.96 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:06<00:02, 15.10it/s, est. speed input: 15319.44 toks/s, output: 14.96 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 15.20it/s, est. speed input: 15328.91 toks/s, output: 14.97 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 15.18it/s, est. speed input: 15332.64 toks/s, output: 14.97 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 15.09it/s, est. speed input: 15330.53 toks/s, output: 14.97 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:07<00:01, 15.16it/s, est. speed input: 15337.55 toks/s, output: 14.98 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:07<00:01, 14.88it/s, est. speed input: 15323.19 toks/s, output: 14.96 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:07<00:01, 14.94it/s, est. speed input: 15325.06 toks/s, output: 14.97 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:07<00:01, 14.98it/s, est. speed input: 15327.12 toks/s, output: 14.97 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:07<00:01, 14.98it/s, est. speed input: 15327.30 toks/s, output: 14.97 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 14.96it/s, est. speed input: 15326.27 toks/s, output: 14.97 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 14.97it/s, est. speed input: 15326.59 toks/s, output: 14.97 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 15.07it/s, est. speed input: 15332.70 toks/s, output: 14.97 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:08<00:00, 15.06it/s, est. speed input: 15333.55 toks/s, output: 14.97 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:08<00:00, 14.88it/s, est. speed input: 15324.92 toks/s, output: 14.97 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:08<00:00, 14.84it/s, est. speed input: 15321.62 toks/s, output: 14.96 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:08<00:00, 15.03it/s, est. speed input: 15329.50 toks/s, output: 14.97 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 15.03it/s, est. speed input: 15331.48 toks/s, output: 14.97 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.97it/s, est. speed input: 15331.48 toks/s, output: 14.97 toks/s]
[rank0]:[W128 09:18:13.051095293 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.9s

测试结果:
  Requests/s:   14.65
  Tokens/s:     15011.83
  Total Reqs:   128
  Elapsed:      8.74s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     14997.18

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:18:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:18:19 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3738166) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3738166) WARNING 01-28 09:18:46 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.84 requests/s, 15211.39 total tokens/s, 14.84 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 09:18:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:18:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:18:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:18:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:18:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:18:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:18:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:18:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:18:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:18:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:18:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:18:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:18:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:18:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:18:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:18:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:18:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:18:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3738166) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3738166) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.08s/it]
(EngineCore_DP0 pid=3738166) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.08s/it]
(EngineCore_DP0 pid=3738166) 
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3738166) [2026-01-28 09:18:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3738166) 2026-01-28 09:18:46,102 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3738166) 2026-01-28 09:18:46,113 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 612.18it/s]
Adding requests:  48%|████▊     | 124/256 [00:00<00:00, 596.94it/s]
Adding requests:  72%|███████▏  | 184/256 [00:00<00:00, 572.39it/s]
Adding requests:  95%|█████████▍| 242/256 [00:00<00:00, 557.14it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 567.05it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 6/256 [00:00<00:06, 41.35it/s, est. speed input: 42349.51 toks/s, output: 41.35 toks/s]
Processed prompts:   4%|▍         | 11/256 [00:00<00:09, 24.76it/s, est. speed input: 27141.75 toks/s, output: 26.50 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:00<00:13, 17.98it/s, est. speed input: 21078.57 toks/s, output: 20.58 toks/s]
Processed prompts:   7%|▋         | 17/256 [00:00<00:12, 18.94it/s, est. speed input: 21234.34 toks/s, output: 20.74 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:01<00:15, 15.65it/s, est. speed input: 18852.42 toks/s, output: 18.41 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:01<00:15, 15.58it/s, est. speed input: 18519.06 toks/s, output: 18.08 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:01<00:15, 15.44it/s, est. speed input: 18211.47 toks/s, output: 17.78 toks/s]
Processed prompts:  10%|█         | 26/256 [00:01<00:14, 15.40it/s, est. speed input: 17983.56 toks/s, output: 17.56 toks/s]
Processed prompts:  11%|█         | 28/256 [00:01<00:14, 15.26it/s, est. speed input: 17758.28 toks/s, output: 17.34 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:01<00:14, 15.19it/s, est. speed input: 17573.80 toks/s, output: 17.16 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:01<00:14, 15.01it/s, est. speed input: 17382.72 toks/s, output: 16.98 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:02<00:14, 14.99it/s, est. speed input: 17243.80 toks/s, output: 16.84 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:02<00:14, 15.07it/s, est. speed input: 17144.75 toks/s, output: 16.74 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:02<00:14, 15.14it/s, est. speed input: 17062.50 toks/s, output: 16.66 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:02<00:14, 15.16it/s, est. speed input: 16980.93 toks/s, output: 16.58 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:02<00:14, 15.19it/s, est. speed input: 16911.17 toks/s, output: 16.51 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:02<00:13, 15.25it/s, est. speed input: 16854.53 toks/s, output: 16.46 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:02<00:13, 15.17it/s, est. speed input: 16783.72 toks/s, output: 16.39 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:02<00:13, 15.02it/s, est. speed input: 16702.23 toks/s, output: 16.31 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:03<00:13, 15.02it/s, est. speed input: 16645.76 toks/s, output: 16.26 toks/s]
Processed prompts:  20%|██        | 52/256 [00:03<00:13, 15.12it/s, est. speed input: 16607.34 toks/s, output: 16.22 toks/s]
Processed prompts:  21%|██        | 54/256 [00:03<00:13, 15.00it/s, est. speed input: 16546.47 toks/s, output: 16.16 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:03<00:13, 15.01it/s, est. speed input: 16501.30 toks/s, output: 16.11 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:03<00:13, 15.12it/s, est. speed input: 16474.07 toks/s, output: 16.09 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:03<00:13, 15.07it/s, est. speed input: 16433.04 toks/s, output: 16.05 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:03<00:12, 15.05it/s, est. speed input: 16395.86 toks/s, output: 16.01 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:04<00:12, 14.86it/s, est. speed input: 16340.06 toks/s, output: 15.96 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:04<00:12, 14.86it/s, est. speed input: 16304.00 toks/s, output: 15.92 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:04<00:12, 14.90it/s, est. speed input: 16274.48 toks/s, output: 15.89 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:04<00:12, 14.89it/s, est. speed input: 16242.48 toks/s, output: 15.86 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:04<00:12, 15.00it/s, est. speed input: 16224.87 toks/s, output: 15.84 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:04<00:12, 15.00it/s, est. speed input: 16200.18 toks/s, output: 15.82 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:04<00:11, 15.02it/s, est. speed input: 16179.26 toks/s, output: 15.80 toks/s]
Processed prompts:  30%|███       | 78/256 [00:04<00:11, 15.06it/s, est. speed input: 16161.49 toks/s, output: 15.78 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:05<00:11, 14.89it/s, est. speed input: 16126.27 toks/s, output: 15.75 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:05<00:11, 14.92it/s, est. speed input: 16106.29 toks/s, output: 15.73 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:05<00:11, 14.99it/s, est. speed input: 16091.98 toks/s, output: 15.71 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:05<00:11, 15.03it/s, est. speed input: 16077.43 toks/s, output: 15.70 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:05<00:11, 15.02it/s, est. speed input: 16060.02 toks/s, output: 15.68 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:05<00:11, 15.08it/s, est. speed input: 16049.32 toks/s, output: 15.67 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:05<00:10, 15.15it/s, est. speed input: 16040.93 toks/s, output: 15.66 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:06<00:10, 15.13it/s, est. speed input: 16028.33 toks/s, output: 15.65 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:06<00:10, 14.88it/s, est. speed input: 15997.50 toks/s, output: 15.62 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:06<00:10, 14.88it/s, est. speed input: 15980.99 toks/s, output: 15.61 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:06<00:10, 14.95it/s, est. speed input: 15970.49 toks/s, output: 15.60 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:06<00:10, 14.91it/s, est. speed input: 15954.45 toks/s, output: 15.58 toks/s]
Processed prompts:  41%|████      | 104/256 [00:06<00:10, 15.01it/s, est. speed input: 15947.86 toks/s, output: 15.57 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:06<00:09, 15.07it/s, est. speed input: 15940.31 toks/s, output: 15.57 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:06<00:09, 15.07it/s, est. speed input: 15930.66 toks/s, output: 15.56 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:07<00:09, 14.98it/s, est. speed input: 15915.12 toks/s, output: 15.54 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:07<00:09, 14.76it/s, est. speed input: 15889.85 toks/s, output: 15.52 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:07<00:09, 14.91it/s, est. speed input: 15885.68 toks/s, output: 15.51 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:07<00:09, 14.87it/s, est. speed input: 15871.75 toks/s, output: 15.50 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:07<00:09, 14.93it/s, est. speed input: 15864.44 toks/s, output: 15.49 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:07<00:09, 14.95it/s, est. speed input: 15855.55 toks/s, output: 15.48 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:07<00:08, 14.97it/s, est. speed input: 15847.31 toks/s, output: 15.48 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:08<00:08, 14.94it/s, est. speed input: 15837.26 toks/s, output: 15.47 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:08<00:08, 14.92it/s, est. speed input: 15826.89 toks/s, output: 15.46 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:08<00:08, 14.78it/s, est. speed input: 15810.35 toks/s, output: 15.44 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:08<00:08, 14.87it/s, est. speed input: 15804.20 toks/s, output: 15.43 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:08<00:08, 14.88it/s, est. speed input: 15795.65 toks/s, output: 15.43 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:08<00:08, 14.93it/s, est. speed input: 15789.95 toks/s, output: 15.42 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:08<00:08, 14.92it/s, est. speed input: 15781.77 toks/s, output: 15.41 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:08<00:07, 14.95it/s, est. speed input: 15775.64 toks/s, output: 15.41 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:09<00:07, 15.06it/s, est. speed input: 15774.32 toks/s, output: 15.40 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:09<00:07, 14.83it/s, est. speed input: 15757.66 toks/s, output: 15.39 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:09<00:07, 14.89it/s, est. speed input: 15752.25 toks/s, output: 15.38 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:09<00:07, 14.87it/s, est. speed input: 15744.19 toks/s, output: 15.38 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:09<00:07, 14.89it/s, est. speed input: 15738.07 toks/s, output: 15.37 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:09<00:07, 14.83it/s, est. speed input: 15728.31 toks/s, output: 15.36 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:09<00:06, 14.89it/s, est. speed input: 15723.80 toks/s, output: 15.36 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:10<00:06, 14.91it/s, est. speed input: 15718.17 toks/s, output: 15.35 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:10<00:06, 14.90it/s, est. speed input: 15711.90 toks/s, output: 15.34 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:10<00:06, 14.83it/s, est. speed input: 15702.59 toks/s, output: 15.33 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:10<00:06, 14.87it/s, est. speed input: 15697.69 toks/s, output: 15.33 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:10<00:06, 14.89it/s, est. speed input: 15692.87 toks/s, output: 15.33 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:10<00:06, 14.93it/s, est. speed input: 15688.84 toks/s, output: 15.32 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:10<00:06, 14.98it/s, est. speed input: 15686.22 toks/s, output: 15.32 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:10<00:05, 15.02it/s, est. speed input: 15683.50 toks/s, output: 15.32 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:11<00:05, 14.99it/s, est. speed input: 15678.52 toks/s, output: 15.31 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:11<00:05, 14.98it/s, est. speed input: 15674.38 toks/s, output: 15.31 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:11<00:05, 14.85it/s, est. speed input: 15665.14 toks/s, output: 15.30 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:11<00:05, 14.87it/s, est. speed input: 15660.41 toks/s, output: 15.29 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:11<00:05, 14.96it/s, est. speed input: 15658.92 toks/s, output: 15.29 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:11<00:05, 14.94it/s, est. speed input: 15654.44 toks/s, output: 15.29 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:11<00:04, 15.05it/s, est. speed input: 15654.83 toks/s, output: 15.29 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:12<00:04, 15.07it/s, est. speed input: 15652.72 toks/s, output: 15.29 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:12<00:04, 15.06it/s, est. speed input: 15650.13 toks/s, output: 15.28 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:12<00:04, 15.00it/s, est. speed input: 15645.33 toks/s, output: 15.28 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:12<00:04, 14.86it/s, est. speed input: 15637.19 toks/s, output: 15.27 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:12<00:04, 14.81it/s, est. speed input: 15630.69 toks/s, output: 15.26 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:12<00:04, 14.85it/s, est. speed input: 15627.40 toks/s, output: 15.26 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:12<00:04, 14.89it/s, est. speed input: 15624.22 toks/s, output: 15.26 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:12<00:03, 14.89it/s, est. speed input: 15620.45 toks/s, output: 15.25 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:13<00:03, 14.95it/s, est. speed input: 15618.77 toks/s, output: 15.25 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:13<00:03, 14.95it/s, est. speed input: 15615.78 toks/s, output: 15.25 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:13<00:03, 14.99it/s, est. speed input: 15613.93 toks/s, output: 15.25 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:13<00:03, 14.84it/s, est. speed input: 15606.04 toks/s, output: 15.24 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:13<00:03, 14.92it/s, est. speed input: 15604.69 toks/s, output: 15.24 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:13<00:03, 14.94it/s, est. speed input: 15602.37 toks/s, output: 15.24 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:13<00:02, 14.95it/s, est. speed input: 15599.84 toks/s, output: 15.23 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:14<00:02, 14.93it/s, est. speed input: 15596.48 toks/s, output: 15.23 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:14<00:02, 14.90it/s, est. speed input: 15592.59 toks/s, output: 15.23 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:14<00:02, 14.85it/s, est. speed input: 15587.87 toks/s, output: 15.22 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:14<00:02, 14.89it/s, est. speed input: 15585.66 toks/s, output: 15.22 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:14<00:02, 14.77it/s, est. speed input: 15578.48 toks/s, output: 15.21 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:14<00:02, 14.75it/s, est. speed input: 15573.71 toks/s, output: 15.21 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:14<00:02, 14.81it/s, est. speed input: 15571.31 toks/s, output: 15.21 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:14<00:01, 14.99it/s, est. speed input: 15573.26 toks/s, output: 15.21 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:15<00:01, 14.96it/s, est. speed input: 15570.24 toks/s, output: 15.21 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:15<00:01, 14.94it/s, est. speed input: 15567.56 toks/s, output: 15.20 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:15<00:01, 14.93it/s, est. speed input: 15564.87 toks/s, output: 15.20 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:15<00:01, 14.97it/s, est. speed input: 15563.85 toks/s, output: 15.20 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:15<00:01, 14.82it/s, est. speed input: 15557.25 toks/s, output: 15.19 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:15<00:01, 14.89it/s, est. speed input: 15556.17 toks/s, output: 15.19 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:15<00:00, 14.97it/s, est. speed input: 15555.72 toks/s, output: 15.19 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:16<00:00, 15.03it/s, est. speed input: 15555.75 toks/s, output: 15.19 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:16<00:00, 15.05it/s, est. speed input: 15554.97 toks/s, output: 15.19 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:16<00:00, 14.98it/s, est. speed input: 15551.69 toks/s, output: 15.19 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:16<00:00, 15.05it/s, est. speed input: 15551.95 toks/s, output: 15.19 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:16<00:00, 15.13it/s, est. speed input: 15552.98 toks/s, output: 15.19 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:16<00:00, 14.95it/s, est. speed input: 15547.50 toks/s, output: 15.18 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 14.95it/s, est. speed input: 15606.08 toks/s, output: 15.24 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 15.24it/s, est. speed input: 15606.08 toks/s, output: 15.24 toks/s]
[rank0]:[W128 09:19:04.298536020 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.3s

测试结果:
  Requests/s:   14.84
  Tokens/s:     15211.39
  Total Reqs:   256
  Elapsed:      17.25s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     15196.55

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:19:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:19:11 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3739073) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3739073) WARNING 01-28 09:19:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.07 requests/s, 14421.86 total tokens/s, 14.07 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 09:19:11] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:19:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:19:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:19:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:19:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:19:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:19:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:19:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:19:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:19:14] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:19:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:19:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:19:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:19:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:19:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:19:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:19:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:19:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:15] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:15] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:15] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:15] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:15] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3739073) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3739073) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.45s/it]
(EngineCore_DP0 pid=3739073) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.45s/it]
(EngineCore_DP0 pid=3739073) 
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3739073) [2026-01-28 09:19:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3739073) 2026-01-28 09:19:38,518 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3739073) 2026-01-28 09:19:38,529 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  12%|█▏        | 61/512 [00:00<00:00, 605.00it/s]
Adding requests:  24%|██▍       | 122/512 [00:00<00:00, 592.42it/s]
Adding requests:  36%|███▌      | 182/512 [00:00<00:00, 562.88it/s]
Adding requests:  47%|████▋     | 239/512 [00:00<00:00, 559.30it/s]
Adding requests:  58%|█████▊    | 296/512 [00:00<00:00, 551.09it/s]
Adding requests:  69%|██████▉   | 353/512 [00:00<00:00, 555.42it/s]
Adding requests:  80%|████████  | 410/512 [00:00<00:00, 558.86it/s]
Adding requests:  91%|█████████ | 466/512 [00:00<00:00, 552.79it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 555.79it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 10/512 [00:00<00:05, 95.85it/s, est. speed input: 98166.19 toks/s, output: 95.85 toks/s]
Processed prompts:   4%|▍         | 20/512 [00:00<00:18, 26.51it/s, est. speed input: 30446.89 toks/s, output: 29.73 toks/s]
Processed prompts:   5%|▍         | 25/512 [00:00<00:21, 23.03it/s, est. speed input: 26738.90 toks/s, output: 26.11 toks/s]
Processed prompts:   6%|▌         | 29/512 [00:01<00:24, 19.82it/s, est. speed input: 23902.54 toks/s, output: 23.34 toks/s]
Processed prompts:   6%|▋         | 32/512 [00:01<00:28, 16.63it/s, est. speed input: 21412.96 toks/s, output: 20.91 toks/s]
Processed prompts:   7%|▋         | 35/512 [00:01<00:32, 14.69it/s, est. speed input: 19773.60 toks/s, output: 19.31 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:02<00:35, 13.43it/s, est. speed input: 18587.72 toks/s, output: 18.15 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:02<00:34, 13.66it/s, est. speed input: 18103.38 toks/s, output: 17.68 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:02<00:33, 13.75it/s, est. speed input: 17691.42 toks/s, output: 17.28 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:02<00:33, 13.88it/s, est. speed input: 17386.36 toks/s, output: 16.98 toks/s]
Processed prompts:  11%|█         | 54/512 [00:03<00:32, 13.98it/s, est. speed input: 17140.08 toks/s, output: 16.74 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:03<00:32, 14.02it/s, est. speed input: 16922.87 toks/s, output: 16.53 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:03<00:32, 14.03it/s, est. speed input: 16731.44 toks/s, output: 16.34 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:04<00:31, 14.10it/s, est. speed input: 16585.77 toks/s, output: 16.20 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:04<00:31, 14.11it/s, est. speed input: 16448.47 toks/s, output: 16.06 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:04<00:31, 14.07it/s, est. speed input: 16316.47 toks/s, output: 15.93 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:04<00:30, 14.07it/s, est. speed input: 16206.21 toks/s, output: 15.83 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:05<00:30, 14.12it/s, est. speed input: 16119.47 toks/s, output: 15.74 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:05<00:30, 14.19it/s, est. speed input: 16047.62 toks/s, output: 15.67 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:05<00:29, 14.14it/s, est. speed input: 15964.84 toks/s, output: 15.59 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:06<00:29, 14.21it/s, est. speed input: 15906.17 toks/s, output: 15.53 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:06<00:29, 14.26it/s, est. speed input: 15854.20 toks/s, output: 15.48 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:06<00:28, 14.21it/s, est. speed input: 15793.93 toks/s, output: 15.42 toks/s]
Processed prompts:  21%|██        | 106/512 [00:06<00:28, 14.12it/s, est. speed input: 15728.72 toks/s, output: 15.36 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:07<00:28, 14.12it/s, est. speed input: 15678.50 toks/s, output: 15.31 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:07<00:28, 14.11it/s, est. speed input: 15630.56 toks/s, output: 15.26 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:07<00:27, 14.08it/s, est. speed input: 15583.28 toks/s, output: 15.22 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:08<00:27, 14.12it/s, est. speed input: 15547.18 toks/s, output: 15.18 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:08<00:27, 14.13it/s, est. speed input: 15511.76 toks/s, output: 15.15 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:08<00:26, 14.16it/s, est. speed input: 15480.79 toks/s, output: 15.12 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:08<00:26, 14.08it/s, est. speed input: 15440.46 toks/s, output: 15.08 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:09<00:26, 14.07it/s, est. speed input: 15408.15 toks/s, output: 15.05 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:09<00:26, 14.10it/s, est. speed input: 15380.94 toks/s, output: 15.02 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:09<00:25, 14.10it/s, est. speed input: 15353.32 toks/s, output: 14.99 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:10<00:25, 14.04it/s, est. speed input: 15321.56 toks/s, output: 14.96 toks/s]
Processed prompts:  30%|███       | 154/512 [00:10<00:25, 14.05it/s, est. speed input: 15296.12 toks/s, output: 14.94 toks/s]
Processed prompts:  31%|███       | 158/512 [00:10<00:25, 14.09it/s, est. speed input: 15275.66 toks/s, output: 14.92 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:10<00:24, 14.15it/s, est. speed input: 15258.94 toks/s, output: 14.90 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:11<00:24, 14.12it/s, est. speed input: 15236.92 toks/s, output: 14.88 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:11<00:24, 14.12it/s, est. speed input: 15217.46 toks/s, output: 14.86 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:11<00:23, 14.09it/s, est. speed input: 15196.37 toks/s, output: 14.84 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:12<00:23, 14.01it/s, est. speed input: 15171.83 toks/s, output: 14.82 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:12<00:23, 14.05it/s, est. speed input: 15155.66 toks/s, output: 14.80 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:12<00:23, 14.09it/s, est. speed input: 15142.03 toks/s, output: 14.79 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:12<00:22, 14.10it/s, est. speed input: 15126.76 toks/s, output: 14.77 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:13<00:22, 14.05it/s, est. speed input: 15108.42 toks/s, output: 14.75 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:13<00:22, 14.06it/s, est. speed input: 15093.36 toks/s, output: 14.74 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:13<00:22, 14.05it/s, est. speed input: 15078.34 toks/s, output: 14.72 toks/s]
Processed prompts:  40%|████      | 206/512 [00:13<00:21, 14.13it/s, est. speed input: 15069.72 toks/s, output: 14.72 toks/s]
Processed prompts:  41%|████      | 210/512 [00:14<00:21, 14.05it/s, est. speed input: 15052.38 toks/s, output: 14.70 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:14<00:21, 14.08it/s, est. speed input: 15041.80 toks/s, output: 14.69 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:14<00:20, 14.09it/s, est. speed input: 15030.39 toks/s, output: 14.68 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:15<00:20, 14.12it/s, est. speed input: 15020.73 toks/s, output: 14.67 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:15<00:20, 14.05it/s, est. speed input: 15005.84 toks/s, output: 14.65 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:15<00:20, 14.08it/s, est. speed input: 14996.95 toks/s, output: 14.65 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:15<00:19, 14.11it/s, est. speed input: 14988.51 toks/s, output: 14.64 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:16<00:19, 14.05it/s, est. speed input: 14975.03 toks/s, output: 14.62 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:16<00:19, 14.13it/s, est. speed input: 14969.90 toks/s, output: 14.62 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:16<00:18, 14.13it/s, est. speed input: 14961.27 toks/s, output: 14.61 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:17<00:18, 14.12it/s, est. speed input: 14952.67 toks/s, output: 14.60 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:17<00:18, 14.05it/s, est. speed input: 14940.69 toks/s, output: 14.59 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:17<00:18, 14.10it/s, est. speed input: 14934.51 toks/s, output: 14.58 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:17<00:17, 14.11it/s, est. speed input: 14927.27 toks/s, output: 14.58 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:18<00:17, 14.07it/s, est. speed input: 14917.56 toks/s, output: 14.57 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:18<00:17, 14.00it/s, est. speed input: 14905.95 toks/s, output: 14.56 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:18<00:17, 13.99it/s, est. speed input: 14896.53 toks/s, output: 14.55 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:19<00:16, 14.02it/s, est. speed input: 14889.97 toks/s, output: 14.54 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:19<00:16, 14.02it/s, est. speed input: 14882.08 toks/s, output: 14.53 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:19<00:16, 14.09it/s, est. speed input: 14877.99 toks/s, output: 14.53 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:19<00:15, 14.17it/s, est. speed input: 14875.52 toks/s, output: 14.53 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:20<00:15, 14.18it/s, est. speed input: 14870.81 toks/s, output: 14.52 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:20<00:15, 14.08it/s, est. speed input: 14861.27 toks/s, output: 14.51 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:20<00:14, 14.08it/s, est. speed input: 14855.20 toks/s, output: 14.51 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:21<00:14, 14.10it/s, est. speed input: 14850.18 toks/s, output: 14.50 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:21<00:14, 14.08it/s, est. speed input: 14844.05 toks/s, output: 14.50 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:21<00:14, 14.02it/s, est. speed input: 14835.36 toks/s, output: 14.49 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:21<00:13, 14.06it/s, est. speed input: 14830.90 toks/s, output: 14.48 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:22<00:13, 14.06it/s, est. speed input: 14825.39 toks/s, output: 14.48 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:22<00:13, 14.09it/s, est. speed input: 14821.38 toks/s, output: 14.47 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:22<00:12, 14.04it/s, est. speed input: 14814.29 toks/s, output: 14.47 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:23<00:12, 14.13it/s, est. speed input: 14812.64 toks/s, output: 14.47 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:23<00:12, 14.11it/s, est. speed input: 14807.87 toks/s, output: 14.46 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:23<00:11, 14.68it/s, est. speed input: 14826.30 toks/s, output: 14.48 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:23<00:11, 14.39it/s, est. speed input: 14817.37 toks/s, output: 14.47 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:24<00:11, 14.29it/s, est. speed input: 14812.49 toks/s, output: 14.47 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:24<00:11, 14.28it/s, est. speed input: 14809.97 toks/s, output: 14.46 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:24<00:10, 14.15it/s, est. speed input: 14802.85 toks/s, output: 14.46 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:25<00:10, 14.17it/s, est. speed input: 14799.92 toks/s, output: 14.45 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:25<00:10, 14.17it/s, est. speed input: 14796.87 toks/s, output: 14.45 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:25<00:10, 14.18it/s, est. speed input: 14793.88 toks/s, output: 14.45 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:25<00:09, 14.10it/s, est. speed input: 14787.92 toks/s, output: 14.44 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:26<00:09, 14.11it/s, est. speed input: 14784.63 toks/s, output: 14.44 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:26<00:09, 14.13it/s, est. speed input: 14781.53 toks/s, output: 14.44 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:26<00:08, 14.10it/s, est. speed input: 14777.39 toks/s, output: 14.43 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:27<00:08, 13.99it/s, est. speed input: 14769.74 toks/s, output: 14.42 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:27<00:08, 14.04it/s, est. speed input: 14766.78 toks/s, output: 14.42 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:27<00:08, 14.06it/s, est. speed input: 14763.60 toks/s, output: 14.42 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:27<00:07, 13.98it/s, est. speed input: 14756.92 toks/s, output: 14.41 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:28<00:07, 14.03it/s, est. speed input: 14754.43 toks/s, output: 14.41 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:28<00:07, 14.05it/s, est. speed input: 14751.16 toks/s, output: 14.41 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:28<00:06, 14.03it/s, est. speed input: 14747.00 toks/s, output: 14.40 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:29<00:06, 13.96it/s, est. speed input: 14740.62 toks/s, output: 14.40 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:29<00:06, 13.97it/s, est. speed input: 14736.82 toks/s, output: 14.39 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:29<00:06, 14.03it/s, est. speed input: 14734.55 toks/s, output: 14.39 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:29<00:05, 14.06it/s, est. speed input: 14732.20 toks/s, output: 14.39 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:30<00:05, 14.02it/s, est. speed input: 14727.59 toks/s, output: 14.38 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:30<00:05, 14.05it/s, est. speed input: 14725.10 toks/s, output: 14.38 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:30<00:04, 14.08it/s, est. speed input: 14723.05 toks/s, output: 14.38 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:31<00:04, 14.13it/s, est. speed input: 14721.78 toks/s, output: 14.38 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:31<00:04, 14.87it/s, est. speed input: 14741.50 toks/s, output: 14.40 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:31<00:03, 14.63it/s, est. speed input: 14738.76 toks/s, output: 14.39 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:31<00:03, 14.51it/s, est. speed input: 14737.48 toks/s, output: 14.39 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:32<00:03, 14.40it/s, est. speed input: 14735.31 toks/s, output: 14.39 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:32<00:03, 14.26it/s, est. speed input: 14731.16 toks/s, output: 14.39 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:32<00:02, 14.21it/s, est. speed input: 14728.57 toks/s, output: 14.38 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:32<00:02, 14.16it/s, est. speed input: 14725.74 toks/s, output: 14.38 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:33<00:02, 14.08it/s, est. speed input: 14721.25 toks/s, output: 14.38 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:33<00:02, 14.09it/s, est. speed input: 14718.99 toks/s, output: 14.37 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:33<00:01, 14.09it/s, est. speed input: 14716.46 toks/s, output: 14.37 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:34<00:01, 14.09it/s, est. speed input: 14714.05 toks/s, output: 14.37 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:34<00:01, 14.04it/s, est. speed input: 14710.25 toks/s, output: 14.37 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:34<00:00, 14.13it/s, est. speed input: 14710.01 toks/s, output: 14.37 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:34<00:00, 14.15it/s, est. speed input: 14708.79 toks/s, output: 14.36 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:35<00:00, 14.16it/s, est. speed input: 14707.20 toks/s, output: 14.36 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:35<00:00, 14.90it/s, est. speed input: 14724.93 toks/s, output: 14.38 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:35<00:00, 14.90it/s, est. speed input: 14782.59 toks/s, output: 14.44 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:35<00:00, 14.44it/s, est. speed input: 14782.59 toks/s, output: 14.44 toks/s]
[rank0]:[W128 09:20:16.020525345 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 71.8s

测试结果:
  Requests/s:   14.07
  Tokens/s:     14421.86
  Total Reqs:   512
  Elapsed:      36.39s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     14407.79

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:20:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:20:24 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3740281) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3740281) WARNING 01-28 09:20:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.14 requests/s, 14489.86 total tokens/s, 14.14 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 09:20:24] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:20:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:20:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:20:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:20:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:20:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:20:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:20:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:20:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:20:27] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:20:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:20:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:20:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:20:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:20:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:20:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:20:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:20:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:28] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:28] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:28] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:28] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:28] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3740281) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3740281) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.43s/it]
(EngineCore_DP0 pid=3740281) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.43s/it]
(EngineCore_DP0 pid=3740281) 
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3740281) [2026-01-28 09:20:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3740281) 2026-01-28 09:20:52,040 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3740281) 2026-01-28 09:20:52,093 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 60/1024 [00:00<00:01, 592.64it/s]
Adding requests:  12%|█▏        | 120/1024 [00:00<00:01, 581.66it/s]
Adding requests:  17%|█▋        | 179/1024 [00:00<00:01, 543.33it/s]
Adding requests:  23%|██▎       | 235/1024 [00:00<00:01, 549.54it/s]
Adding requests:  28%|██▊       | 291/1024 [00:00<00:01, 540.97it/s]
Adding requests:  34%|███▍      | 346/1024 [00:00<00:01, 538.50it/s]
Adding requests:  39%|███▉      | 402/1024 [00:00<00:01, 544.86it/s]
Adding requests:  45%|████▍     | 457/1024 [00:00<00:01, 535.42it/s]
Adding requests:  50%|████▉     | 511/1024 [00:00<00:00, 529.91it/s]
Adding requests:  55%|█████▌    | 565/1024 [00:01<00:00, 512.34it/s]
Adding requests:  60%|██████    | 618/1024 [00:01<00:00, 516.56it/s]
Adding requests:  66%|██████▌   | 672/1024 [00:01<00:00, 522.23it/s]
Adding requests:  71%|███████   | 726/1024 [00:01<00:00, 526.01it/s]
Adding requests:  76%|███████▌  | 779/1024 [00:01<00:00, 520.75it/s]
Adding requests:  81%|████████▏ | 832/1024 [00:01<00:00, 517.57it/s]
Adding requests:  87%|████████▋ | 888/1024 [00:01<00:00, 527.87it/s]
Adding requests:  92%|█████████▏| 942/1024 [00:01<00:00, 530.25it/s]
Adding requests:  97%|█████████▋| 996/1024 [00:01<00:00, 531.42it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 532.12it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 26/1024 [00:00<00:19, 50.57it/s, est. speed input: 51789.26 toks/s, output: 50.57 toks/s]
Processed prompts:   3%|▎         | 34/1024 [00:01<00:34, 28.34it/s, est. speed input: 32278.10 toks/s, output: 31.52 toks/s]
Processed prompts:   4%|▍         | 42/1024 [00:01<00:45, 21.75it/s, est. speed input: 26184.14 toks/s, output: 25.57 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:02<00:52, 18.72it/s, est. speed input: 23198.49 toks/s, output: 22.65 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:02<00:56, 17.03it/s, est. speed input: 21405.68 toks/s, output: 20.90 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:03<00:59, 16.08it/s, est. speed input: 20256.40 toks/s, output: 19.78 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:03<01:01, 15.45it/s, est. speed input: 19423.94 toks/s, output: 18.97 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:04<01:02, 15.06it/s, est. speed input: 18811.02 toks/s, output: 18.37 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:05<01:03, 14.76it/s, est. speed input: 18318.86 toks/s, output: 17.89 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:05<01:03, 14.59it/s, est. speed input: 17938.38 toks/s, output: 17.52 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:06<01:03, 14.45it/s, est. speed input: 17621.85 toks/s, output: 17.21 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:06<01:03, 14.41it/s, est. speed input: 17374.58 toks/s, output: 16.97 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:07<01:02, 14.34it/s, est. speed input: 17152.73 toks/s, output: 16.75 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:07<01:02, 14.32it/s, est. speed input: 16971.17 toks/s, output: 16.57 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:08<01:02, 14.26it/s, est. speed input: 16801.93 toks/s, output: 16.41 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:08<01:01, 14.23it/s, est. speed input: 16658.14 toks/s, output: 16.27 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:09<01:01, 14.21it/s, est. speed input: 16529.86 toks/s, output: 16.14 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:10<01:00, 14.20it/s, est. speed input: 16418.22 toks/s, output: 16.03 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:10<01:00, 14.19it/s, est. speed input: 16316.14 toks/s, output: 15.93 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:11<00:59, 14.18it/s, est. speed input: 16224.73 toks/s, output: 15.84 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:11<00:58, 14.21it/s, est. speed input: 16149.05 toks/s, output: 15.77 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:12<00:58, 14.18it/s, est. speed input: 16070.84 toks/s, output: 15.69 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:12<00:57, 14.19it/s, est. speed input: 16004.24 toks/s, output: 15.63 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:13<00:57, 14.18it/s, est. speed input: 15941.81 toks/s, output: 15.57 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:14<00:56, 14.19it/s, est. speed input: 15886.03 toks/s, output: 15.51 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:14<00:56, 14.17it/s, est. speed input: 15831.39 toks/s, output: 15.46 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:15<00:55, 14.20it/s, est. speed input: 15785.80 toks/s, output: 15.42 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:15<00:55, 14.19it/s, est. speed input: 15740.57 toks/s, output: 15.37 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:16<00:54, 14.21it/s, est. speed input: 15701.21 toks/s, output: 15.33 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:16<00:54, 14.18it/s, est. speed input: 15659.37 toks/s, output: 15.29 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:17<00:53, 14.19it/s, est. speed input: 15623.63 toks/s, output: 15.26 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:18<00:52, 14.16it/s, est. speed input: 15586.06 toks/s, output: 15.22 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:18<00:52, 14.17it/s, est. speed input: 15553.54 toks/s, output: 15.19 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:19<00:51, 14.15it/s, est. speed input: 15520.44 toks/s, output: 15.16 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:19<00:51, 14.15it/s, est. speed input: 15490.94 toks/s, output: 15.13 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:20<00:50, 14.20it/s, est. speed input: 15468.04 toks/s, output: 15.11 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:20<00:50, 14.17it/s, est. speed input: 15440.55 toks/s, output: 15.08 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:21<00:49, 14.19it/s, est. speed input: 15417.82 toks/s, output: 15.06 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:21<00:48, 14.17it/s, est. speed input: 15393.40 toks/s, output: 15.03 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:22<00:47, 14.36it/s, est. speed input: 15388.11 toks/s, output: 15.03 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:23<00:47, 14.29it/s, est. speed input: 15365.24 toks/s, output: 15.01 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:23<00:46, 14.27it/s, est. speed input: 15346.26 toks/s, output: 14.99 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:24<00:46, 14.21it/s, est. speed input: 15324.57 toks/s, output: 14.97 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:24<00:46, 14.22it/s, est. speed input: 15307.23 toks/s, output: 14.95 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:25<00:45, 14.19it/s, est. speed input: 15288.65 toks/s, output: 14.93 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:25<00:44, 14.20it/s, est. speed input: 15272.69 toks/s, output: 14.91 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:26<00:44, 14.17it/s, est. speed input: 15254.83 toks/s, output: 14.90 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:27<00:43, 14.20it/s, est. speed input: 15241.44 toks/s, output: 14.88 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:27<00:43, 14.17it/s, est. speed input: 15224.77 toks/s, output: 14.87 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:28<00:42, 14.15it/s, est. speed input: 15208.86 toks/s, output: 14.85 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:28<00:42, 14.15it/s, est. speed input: 15194.56 toks/s, output: 14.84 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:29<00:41, 14.14it/s, est. speed input: 15180.56 toks/s, output: 14.82 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:29<00:41, 14.16it/s, est. speed input: 15168.30 toks/s, output: 14.81 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:30<00:41, 13.68it/s, est. speed input: 15123.01 toks/s, output: 14.77 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:31<00:40, 13.82it/s, est. speed input: 15112.01 toks/s, output: 14.76 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:31<00:40, 13.89it/s, est. speed input: 15098.80 toks/s, output: 14.74 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:32<00:39, 13.99it/s, est. speed input: 15089.62 toks/s, output: 14.74 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:32<00:38, 14.02it/s, est. speed input: 15078.36 toks/s, output: 14.72 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:33<00:37, 14.08it/s, est. speed input: 15069.24 toks/s, output: 14.72 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:33<00:37, 14.06it/s, est. speed input: 15057.48 toks/s, output: 14.70 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:34<00:36, 14.09it/s, est. speed input: 15048.33 toks/s, output: 14.70 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:34<00:36, 14.14it/s, est. speed input: 15040.99 toks/s, output: 14.69 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:35<00:35, 14.10it/s, est. speed input: 15029.98 toks/s, output: 14.68 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:36<00:34, 14.15it/s, est. speed input: 15023.25 toks/s, output: 14.67 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:36<00:34, 14.13it/s, est. speed input: 15013.94 toks/s, output: 14.66 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:37<00:33, 14.17it/s, est. speed input: 15008.08 toks/s, output: 14.66 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:37<00:33, 14.16it/s, est. speed input: 15000.22 toks/s, output: 14.65 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:38<00:32, 14.16it/s, est. speed input: 14992.87 toks/s, output: 14.64 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:38<00:32, 14.13it/s, est. speed input: 14984.28 toks/s, output: 14.63 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:39<00:31, 14.15it/s, est. speed input: 14977.89 toks/s, output: 14.63 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:40<00:30, 14.14it/s, est. speed input: 14970.25 toks/s, output: 14.62 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:40<00:30, 14.17it/s, est. speed input: 14964.78 toks/s, output: 14.61 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:41<00:29, 14.13it/s, est. speed input: 14956.67 toks/s, output: 14.61 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:41<00:29, 14.16it/s, est. speed input: 14951.52 toks/s, output: 14.60 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:42<00:28, 14.13it/s, est. speed input: 14944.17 toks/s, output: 14.59 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:42<00:28, 14.16it/s, est. speed input: 14939.27 toks/s, output: 14.59 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:43<00:27, 14.13it/s, est. speed input: 14932.27 toks/s, output: 14.58 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:44<00:26, 14.15it/s, est. speed input: 14927.09 toks/s, output: 14.58 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:44<00:26, 14.12it/s, est. speed input: 14920.16 toks/s, output: 14.57 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:45<00:25, 14.12it/s, est. speed input: 14914.40 toks/s, output: 14.56 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:45<00:25, 14.15it/s, est. speed input: 14909.89 toks/s, output: 14.56 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:46<00:24, 14.12it/s, est. speed input: 14903.55 toks/s, output: 14.55 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:46<00:24, 14.14it/s, est. speed input: 14899.21 toks/s, output: 14.55 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:47<00:23, 14.10it/s, est. speed input: 14892.57 toks/s, output: 14.54 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:48<00:23, 14.13it/s, est. speed input: 14888.34 toks/s, output: 14.54 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:48<00:22, 14.12it/s, est. speed input: 14883.11 toks/s, output: 14.53 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:49<00:21, 14.16it/s, est. speed input: 14879.83 toks/s, output: 14.53 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:49<00:21, 14.15it/s, est. speed input: 14875.19 toks/s, output: 14.53 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:50<00:20, 14.17it/s, est. speed input: 14871.42 toks/s, output: 14.52 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:50<00:20, 14.15it/s, est. speed input: 14866.67 toks/s, output: 14.52 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:51<00:19, 14.14it/s, est. speed input: 14862.33 toks/s, output: 14.51 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:51<00:19, 14.13it/s, est. speed input: 14857.51 toks/s, output: 14.51 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:52<00:18, 14.13it/s, est. speed input: 14853.45 toks/s, output: 14.51 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:53<00:17, 14.12it/s, est. speed input: 14848.83 toks/s, output: 14.50 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:53<00:17, 14.12it/s, est. speed input: 14844.86 toks/s, output: 14.50 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:54<00:16, 14.14it/s, est. speed input: 14841.36 toks/s, output: 14.49 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:54<00:16, 14.11it/s, est. speed input: 14836.82 toks/s, output: 14.49 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:55<00:15, 14.13it/s, est. speed input: 14833.29 toks/s, output: 14.49 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:55<00:15, 14.13it/s, est. speed input: 14829.61 toks/s, output: 14.48 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:56<00:14, 14.14it/s, est. speed input: 14826.35 toks/s, output: 14.48 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:57<00:14, 14.13it/s, est. speed input: 14822.51 toks/s, output: 14.48 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:57<00:13, 14.14it/s, est. speed input: 14819.56 toks/s, output: 14.47 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:58<00:12, 14.13it/s, est. speed input: 14815.92 toks/s, output: 14.47 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:58<00:12, 14.14it/s, est. speed input: 14812.72 toks/s, output: 14.47 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:59<00:11, 14.14it/s, est. speed input: 14809.55 toks/s, output: 14.46 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:59<00:11, 14.16it/s, est. speed input: 14807.00 toks/s, output: 14.46 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [01:00<00:10, 14.15it/s, est. speed input: 14803.88 toks/s, output: 14.46 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [01:01<00:10, 14.15it/s, est. speed input: 14801.01 toks/s, output: 14.45 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [01:01<00:09, 14.12it/s, est. speed input: 14797.25 toks/s, output: 14.45 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [01:02<00:08, 14.11it/s, est. speed input: 14793.64 toks/s, output: 14.45 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [01:02<00:08, 14.11it/s, est. speed input: 14790.48 toks/s, output: 14.44 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [01:03<00:07, 13.96it/s, est. speed input: 14782.84 toks/s, output: 14.44 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [01:03<00:07, 14.04it/s, est. speed input: 14780.98 toks/s, output: 14.43 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [01:04<00:06, 14.06it/s, est. speed input: 14778.00 toks/s, output: 14.43 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [01:04<00:05, 14.52it/s, est. speed input: 14788.46 toks/s, output: 14.44 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [01:05<00:05, 14.39it/s, est. speed input: 14785.24 toks/s, output: 14.44 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [01:06<00:04, 14.32it/s, est. speed input: 14782.78 toks/s, output: 14.44 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [01:06<00:04, 14.25it/s, est. speed input: 14779.90 toks/s, output: 14.43 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [01:07<00:03, 14.22it/s, est. speed input: 14777.52 toks/s, output: 14.43 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [01:07<00:03, 14.18it/s, est. speed input: 14774.61 toks/s, output: 14.43 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [01:08<00:02, 14.64it/s, est. speed input: 14785.19 toks/s, output: 14.44 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [01:08<00:02, 14.49it/s, est. speed input: 14782.77 toks/s, output: 14.44 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [01:09<00:01, 14.38it/s, est. speed input: 14780.10 toks/s, output: 14.43 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [01:09<00:00, 14.29it/s, est. speed input: 14777.17 toks/s, output: 14.43 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [01:10<00:00, 14.58it/s, est. speed input: 14783.95 toks/s, output: 14.44 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:10<00:00, 14.58it/s, est. speed input: 14871.04 toks/s, output: 14.52 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:10<00:00, 14.52it/s, est. speed input: 14871.04 toks/s, output: 14.52 toks/s]
[rank0]:[W128 09:22:05.864806271 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 109.8s

测试结果:
  Requests/s:   14.14
  Tokens/s:     14489.86
  Total Reqs:   1024
  Elapsed:      72.44s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     14475.72

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:22:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:22:17 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3742012) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3742012) WARNING 01-28 09:22:47 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.39 requests/s, 14748.03 total tokens/s, 14.39 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 09:22:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:22:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:22:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:22:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:22:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:22:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:22:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:22:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:22:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:22:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:22:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:22:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:22:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:22:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:22:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:22:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:22:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:22:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:22:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:22:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:22:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:22:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:22:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:22:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:22:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:22:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:22:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:22:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:21] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:21] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:21] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:21] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:21] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3742012) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3742012) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.88s/it]
(EngineCore_DP0 pid=3742012) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.88s/it]
(EngineCore_DP0 pid=3742012) 
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3742012) [2026-01-28 09:22:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3742012) 2026-01-28 09:22:45,684 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3742012) 2026-01-28 09:22:45,754 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 64/2048 [00:00<00:03, 629.48it/s]
Adding requests:   6%|▌         | 127/2048 [00:00<00:03, 585.05it/s]
Adding requests:   9%|▉         | 186/2048 [00:00<00:03, 556.82it/s]
Adding requests:  12%|█▏        | 243/2048 [00:00<00:03, 558.69it/s]
Adding requests:  15%|█▍        | 299/2048 [00:00<00:03, 539.62it/s]
Adding requests:  17%|█▋        | 354/2048 [00:00<00:03, 534.22it/s]
Adding requests:  20%|█▉        | 409/2048 [00:00<00:03, 538.45it/s]
Adding requests:  23%|██▎       | 463/2048 [00:00<00:02, 537.99it/s]
Adding requests:  25%|██▌       | 517/2048 [00:00<00:02, 531.98it/s]
Adding requests:  28%|██▊       | 571/2048 [00:01<00:02, 533.51it/s]
Adding requests:  31%|███       | 627/2048 [00:01<00:02, 540.22it/s]
Adding requests:  33%|███▎      | 684/2048 [00:01<00:02, 545.82it/s]
Adding requests:  36%|███▌      | 739/2048 [00:01<00:02, 547.05it/s]
Adding requests:  39%|███▉      | 794/2048 [00:01<00:02, 518.98it/s]
Adding requests:  41%|████▏     | 847/2048 [00:01<00:02, 518.88it/s]
Adding requests:  44%|████▍     | 900/2048 [00:02<00:05, 207.67it/s]
Adding requests:  46%|████▋     | 952/2048 [00:02<00:04, 251.75it/s]
Adding requests:  49%|████▉     | 1006/2048 [00:02<00:03, 299.68it/s]
Adding requests:  52%|█████▏    | 1058/2048 [00:02<00:02, 341.98it/s]
Adding requests:  54%|█████▍    | 1112/2048 [00:02<00:02, 384.71it/s]
Adding requests:  57%|█████▋    | 1165/2048 [00:02<00:02, 418.23it/s]
Adding requests:  60%|█████▉    | 1222/2048 [00:02<00:01, 454.77it/s]
Adding requests:  62%|██████▏   | 1275/2048 [00:02<00:01, 470.27it/s]
Adding requests:  65%|██████▍   | 1327/2048 [00:03<00:01, 482.86it/s]
Adding requests:  68%|██████▊   | 1383/2048 [00:03<00:01, 501.71it/s]
Adding requests:  70%|███████   | 1440/2048 [00:03<00:01, 519.25it/s]
Adding requests:  73%|███████▎  | 1495/2048 [00:03<00:01, 526.99it/s]
Adding requests:  76%|███████▌  | 1550/2048 [00:03<00:00, 531.44it/s]
Adding requests:  78%|███████▊  | 1607/2048 [00:03<00:00, 542.30it/s]
Adding requests:  81%|████████  | 1662/2048 [00:03<00:00, 540.37it/s]
Adding requests:  84%|████████▍ | 1718/2048 [00:03<00:00, 544.73it/s]
Adding requests:  87%|████████▋ | 1773/2048 [00:03<00:00, 541.33it/s]
Adding requests:  89%|████████▉ | 1829/2048 [00:03<00:00, 546.75it/s]
Adding requests:  92%|█████████▏| 1884/2048 [00:04<00:00, 547.58it/s]
Adding requests:  95%|█████████▍| 1939/2048 [00:04<00:00, 546.65it/s]
Adding requests:  97%|█████████▋| 1994/2048 [00:04<00:00, 543.91it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 472.46it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 50/2048 [00:00<00:12, 163.74it/s, est. speed input: 167686.88 toks/s, output: 163.75 toks/s]
Processed prompts:   3%|▎         | 67/2048 [00:01<00:50, 39.11it/s, est. speed input: 48273.85 toks/s, output: 47.14 toks/s]   
Processed prompts:   4%|▍         | 82/2048 [00:02<01:18, 25.18it/s, est. speed input: 33162.73 toks/s, output: 32.39 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:03<01:35, 20.44it/s, est. speed input: 27525.17 toks/s, output: 26.88 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:04<01:46, 18.10it/s, est. speed input: 24521.91 toks/s, output: 23.95 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:05<01:54, 16.79it/s, est. speed input: 22672.72 toks/s, output: 22.14 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:06<01:58, 16.00it/s, est. speed input: 21421.75 toks/s, output: 20.92 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:08<02:01, 15.48it/s, est. speed input: 20504.15 toks/s, output: 20.02 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:09<02:03, 15.11it/s, est. speed input: 19796.01 toks/s, output: 19.33 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:10<02:04, 14.89it/s, est. speed input: 19250.39 toks/s, output: 18.80 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:11<02:04, 14.73it/s, est. speed input: 18811.21 toks/s, output: 18.37 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:12<02:04, 14.63it/s, est. speed input: 18449.10 toks/s, output: 18.02 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:13<02:04, 14.55it/s, est. speed input: 18145.87 toks/s, output: 17.72 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:14<02:03, 14.50it/s, est. speed input: 17887.96 toks/s, output: 17.47 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:15<02:02, 14.48it/s, est. speed input: 17670.22 toks/s, output: 17.26 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:16<02:01, 14.45it/s, est. speed input: 17477.24 toks/s, output: 17.07 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:18<02:00, 14.42it/s, est. speed input: 17305.63 toks/s, output: 16.90 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:19<01:59, 14.41it/s, est. speed input: 17156.97 toks/s, output: 16.75 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:20<01:57, 14.54it/s, est. speed input: 17053.36 toks/s, output: 16.65 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:21<01:56, 14.50it/s, est. speed input: 16933.36 toks/s, output: 16.54 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:22<01:56, 14.46it/s, est. speed input: 16824.15 toks/s, output: 16.43 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:23<01:55, 14.43it/s, est. speed input: 16724.13 toks/s, output: 16.33 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:24<01:54, 14.40it/s, est. speed input: 16631.52 toks/s, output: 16.24 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:25<01:53, 14.38it/s, est. speed input: 16547.64 toks/s, output: 16.16 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:26<01:52, 14.38it/s, est. speed input: 16472.93 toks/s, output: 16.09 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:28<01:52, 14.24it/s, est. speed input: 16382.63 toks/s, output: 16.00 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:29<01:50, 14.28it/s, est. speed input: 16319.46 toks/s, output: 15.94 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:30<01:49, 14.30it/s, est. speed input: 16259.93 toks/s, output: 15.88 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:31<01:48, 14.32it/s, est. speed input: 16204.71 toks/s, output: 15.82 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:32<01:46, 14.34it/s, est. speed input: 16154.89 toks/s, output: 15.78 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:33<01:45, 14.36it/s, est. speed input: 16107.87 toks/s, output: 15.73 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:34<01:44, 14.35it/s, est. speed input: 16062.12 toks/s, output: 15.69 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:35<01:43, 14.35it/s, est. speed input: 16019.75 toks/s, output: 15.64 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:37<01:42, 14.36it/s, est. speed input: 15981.42 toks/s, output: 15.61 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:38<01:41, 14.35it/s, est. speed input: 15942.81 toks/s, output: 15.57 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:39<01:40, 14.34it/s, est. speed input: 15906.61 toks/s, output: 15.53 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:40<01:39, 14.35it/s, est. speed input: 15873.26 toks/s, output: 15.50 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:41<01:38, 14.33it/s, est. speed input: 15840.29 toks/s, output: 15.47 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:42<01:36, 14.35it/s, est. speed input: 15811.37 toks/s, output: 15.44 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:43<01:35, 14.34it/s, est. speed input: 15782.26 toks/s, output: 15.41 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:44<01:34, 14.35it/s, est. speed input: 15755.58 toks/s, output: 15.39 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:45<01:33, 14.36it/s, est. speed input: 15730.45 toks/s, output: 15.36 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:47<01:32, 14.34it/s, est. speed input: 15704.51 toks/s, output: 15.34 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:48<01:31, 14.34it/s, est. speed input: 15681.06 toks/s, output: 15.31 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:49<01:30, 14.34it/s, est. speed input: 15658.75 toks/s, output: 15.29 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:50<01:29, 14.34it/s, est. speed input: 15637.21 toks/s, output: 15.27 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:51<01:27, 14.34it/s, est. speed input: 15616.75 toks/s, output: 15.25 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:52<01:26, 14.35it/s, est. speed input: 15597.24 toks/s, output: 15.23 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:53<01:25, 14.35it/s, est. speed input: 15578.41 toks/s, output: 15.21 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:54<01:24, 14.35it/s, est. speed input: 15560.68 toks/s, output: 15.20 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:56<01:23, 14.33it/s, est. speed input: 15542.12 toks/s, output: 15.18 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:57<01:22, 14.34it/s, est. speed input: 15526.01 toks/s, output: 15.16 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:58<01:21, 14.34it/s, est. speed input: 15509.55 toks/s, output: 15.15 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:59<01:20, 14.33it/s, est. speed input: 15493.65 toks/s, output: 15.13 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [01:00<01:19, 14.28it/s, est. speed input: 15475.21 toks/s, output: 15.11 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [01:01<01:16, 14.53it/s, est. speed input: 15475.93 toks/s, output: 15.11 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [01:02<01:16, 14.47it/s, est. speed input: 15461.40 toks/s, output: 15.10 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [01:03<01:15, 14.43it/s, est. speed input: 15447.64 toks/s, output: 15.09 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [01:04<01:12, 14.66it/s, est. speed input: 15450.22 toks/s, output: 15.09 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [01:05<01:12, 14.55it/s, est. speed input: 15436.54 toks/s, output: 15.07 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [01:07<01:11, 14.50it/s, est. speed input: 15424.89 toks/s, output: 15.06 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [01:08<01:10, 14.44it/s, est. speed input: 15411.89 toks/s, output: 15.05 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [01:09<01:09, 14.40it/s, est. speed input: 15399.71 toks/s, output: 15.04 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [01:10<01:08, 14.36it/s, est. speed input: 15387.03 toks/s, output: 15.03 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [01:11<01:07, 14.35it/s, est. speed input: 15376.01 toks/s, output: 15.02 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [01:12<01:06, 14.33it/s, est. speed input: 15364.61 toks/s, output: 15.00 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [01:13<01:05, 14.33it/s, est. speed input: 15354.01 toks/s, output: 14.99 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [01:14<01:04, 14.33it/s, est. speed input: 15343.77 toks/s, output: 14.98 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [01:15<01:03, 14.32it/s, est. speed input: 15333.23 toks/s, output: 14.97 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [01:17<01:01, 14.57it/s, est. speed input: 15336.44 toks/s, output: 14.98 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [01:18<01:00, 14.49it/s, est. speed input: 15326.69 toks/s, output: 14.97 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [01:19<00:59, 14.44it/s, est. speed input: 15317.14 toks/s, output: 14.96 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [01:20<00:58, 14.40it/s, est. speed input: 15308.18 toks/s, output: 14.95 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [01:21<00:57, 14.38it/s, est. speed input: 15299.48 toks/s, output: 14.94 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [01:22<00:56, 14.36it/s, est. speed input: 15290.61 toks/s, output: 14.93 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [01:23<00:55, 14.34it/s, est. speed input: 15282.07 toks/s, output: 14.92 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [01:24<00:53, 14.60it/s, est. speed input: 15286.09 toks/s, output: 14.93 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [01:25<00:52, 14.52it/s, est. speed input: 15278.22 toks/s, output: 14.92 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [01:26<00:50, 14.73it/s, est. speed input: 15282.15 toks/s, output: 14.92 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [01:28<00:50, 14.59it/s, est. speed input: 15273.90 toks/s, output: 14.92 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [01:29<00:49, 14.51it/s, est. speed input: 15266.33 toks/s, output: 14.91 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [01:30<00:48, 14.45it/s, est. speed input: 15258.91 toks/s, output: 14.90 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [01:31<00:47, 14.41it/s, est. speed input: 15251.32 toks/s, output: 14.89 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [01:32<00:46, 14.31it/s, est. speed input: 15241.27 toks/s, output: 14.88 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [01:33<00:45, 14.33it/s, est. speed input: 15235.05 toks/s, output: 14.88 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [01:34<00:44, 14.31it/s, est. speed input: 15227.70 toks/s, output: 14.87 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [01:35<00:43, 14.32it/s, est. speed input: 15221.24 toks/s, output: 14.86 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [01:37<00:42, 14.32it/s, est. speed input: 15214.96 toks/s, output: 14.86 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [01:38<00:41, 14.32it/s, est. speed input: 15208.76 toks/s, output: 14.85 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [01:39<00:40, 14.33it/s, est. speed input: 15202.83 toks/s, output: 14.85 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [01:40<00:38, 14.32it/s, est. speed input: 15196.54 toks/s, output: 14.84 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [01:41<00:37, 14.32it/s, est. speed input: 15190.61 toks/s, output: 14.83 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [01:42<00:36, 14.31it/s, est. speed input: 15184.74 toks/s, output: 14.83 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [01:43<00:35, 14.32it/s, est. speed input: 15179.43 toks/s, output: 14.82 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [01:44<00:34, 14.31it/s, est. speed input: 15173.57 toks/s, output: 14.82 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [01:45<00:33, 14.32it/s, est. speed input: 15168.34 toks/s, output: 14.81 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [01:47<00:31, 14.57it/s, est. speed input: 15172.04 toks/s, output: 14.82 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [01:48<00:30, 14.50it/s, est. speed input: 15167.12 toks/s, output: 14.81 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [01:49<00:29, 14.45it/s, est. speed input: 15162.22 toks/s, output: 14.81 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [01:50<00:28, 14.42it/s, est. speed input: 15157.27 toks/s, output: 14.80 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [01:51<00:27, 14.64it/s, est. speed input: 15161.06 toks/s, output: 14.81 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [01:52<00:26, 14.55it/s, est. speed input: 15156.20 toks/s, output: 14.80 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [01:53<00:25, 14.48it/s, est. speed input: 15151.35 toks/s, output: 14.80 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [01:54<00:24, 14.43it/s, est. speed input: 15146.74 toks/s, output: 14.79 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [01:55<00:23, 14.40it/s, est. speed input: 15142.17 toks/s, output: 14.79 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [01:57<00:22, 14.38it/s, est. speed input: 15137.57 toks/s, output: 14.78 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [01:58<00:21, 14.37it/s, est. speed input: 15133.47 toks/s, output: 14.78 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [01:59<00:19, 14.36it/s, est. speed input: 15129.11 toks/s, output: 14.77 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [02:00<00:18, 14.34it/s, est. speed input: 15124.49 toks/s, output: 14.77 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [02:01<00:17, 14.33it/s, est. speed input: 15120.08 toks/s, output: 14.77 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [02:02<00:16, 14.32it/s, est. speed input: 15115.86 toks/s, output: 14.76 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [02:03<00:15, 14.33it/s, est. speed input: 15111.92 toks/s, output: 14.76 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [02:04<00:14, 14.30it/s, est. speed input: 15107.26 toks/s, output: 14.75 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [02:05<00:13, 14.30it/s, est. speed input: 15103.10 toks/s, output: 14.75 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [02:07<00:11, 14.55it/s, est. speed input: 15106.69 toks/s, output: 14.75 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [02:08<00:10, 14.48it/s, est. speed input: 15102.81 toks/s, output: 14.75 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [02:09<00:09, 14.45it/s, est. speed input: 15099.55 toks/s, output: 14.75 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [02:10<00:08, 14.42it/s, est. speed input: 15095.98 toks/s, output: 14.74 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [02:11<00:07, 14.40it/s, est. speed input: 15092.53 toks/s, output: 14.74 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [02:12<00:06, 14.64it/s, est. speed input: 15096.52 toks/s, output: 14.74 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [02:13<00:05, 14.55it/s, est. speed input: 15093.16 toks/s, output: 14.74 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [02:14<00:04, 14.75it/s, est. speed input: 15097.13 toks/s, output: 14.74 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [02:15<00:03, 14.63it/s, est. speed input: 15093.84 toks/s, output: 14.74 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [02:16<00:02, 14.53it/s, est. speed input: 15090.18 toks/s, output: 14.74 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [02:18<00:00, 14.69it/s, est. speed input: 15092.78 toks/s, output: 14.74 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:18<00:00, 14.69it/s, est. speed input: 15196.63 toks/s, output: 14.84 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:18<00:00, 14.84it/s, est. speed input: 15196.63 toks/s, output: 14.84 toks/s]
[rank0]:[W128 09:25:10.161980092 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 184.4s

测试结果:
  Requests/s:   14.39
  Tokens/s:     14748.03
  Total Reqs:   2048
  Elapsed:      142.34s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     14733.64

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:25:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:25:27 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3744814) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3744814) WARNING 01-28 09:26:00 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.02 requests/s, 14374.44 total tokens/s, 14.02 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 09:25:27] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:25:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:25:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:25:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:25:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:25:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:25:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:25:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:25:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:25:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:25:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:25:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:25:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:25:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:25:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:25:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:25:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:25:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:31] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:31] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:31] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:31] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:31] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3744814) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3744814) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.51s/it]
(EngineCore_DP0 pid=3744814) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.51s/it]
(EngineCore_DP0 pid=3744814) 
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3744814) [2026-01-28 09:25:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3744814) 2026-01-28 09:25:56,619 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3744814) 2026-01-28 09:25:56,911 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 67/4096 [00:00<00:06, 667.18it/s]
Adding requests:   3%|▎         | 134/4096 [00:00<00:06, 632.88it/s]
Adding requests:   5%|▍         | 198/4096 [00:00<00:06, 566.80it/s]
Adding requests:   6%|▋         | 256/4096 [00:00<00:06, 555.05it/s]
Adding requests:   8%|▊         | 312/4096 [00:00<00:06, 546.77it/s]
Adding requests:   9%|▉         | 367/4096 [00:00<00:06, 546.48it/s]
Adding requests:  10%|█         | 422/4096 [00:00<00:06, 545.21it/s]
Adding requests:  12%|█▏        | 477/4096 [00:00<00:06, 542.70it/s]
Adding requests:  13%|█▎        | 532/4096 [00:00<00:06, 524.90it/s]
Adding requests:  14%|█▍        | 590/4096 [00:01<00:06, 539.07it/s]
Adding requests:  16%|█▌        | 645/4096 [00:01<00:06, 532.96it/s]
Adding requests:  17%|█▋        | 700/4096 [00:01<00:06, 536.63it/s]
Adding requests:  18%|█▊        | 754/4096 [00:01<00:06, 534.27it/s]
Adding requests:  20%|█▉        | 808/4096 [00:01<00:06, 523.67it/s]
Adding requests:  21%|██        | 863/4096 [00:01<00:06, 529.31it/s]
Adding requests:  22%|██▏       | 919/4096 [00:01<00:05, 537.58it/s]
Adding requests:  24%|██▍       | 973/4096 [00:01<00:05, 537.71it/s]
Adding requests:  25%|██▌       | 1028/4096 [00:01<00:05, 541.11it/s]
Adding requests:  26%|██▋       | 1083/4096 [00:01<00:05, 532.48it/s]
Adding requests:  28%|██▊       | 1137/4096 [00:02<00:05, 527.83it/s]
Adding requests:  29%|██▉       | 1190/4096 [00:02<00:05, 519.76it/s]
Adding requests:  30%|███       | 1243/4096 [00:02<00:05, 517.34it/s]
Adding requests:  32%|███▏      | 1297/4096 [00:02<00:05, 522.77it/s]
Adding requests:  33%|███▎      | 1351/4096 [00:02<00:05, 523.92it/s]
Adding requests:  34%|███▍      | 1405/4096 [00:02<00:05, 525.33it/s]
Adding requests:  36%|███▌      | 1460/4096 [00:02<00:04, 529.66it/s]
Adding requests:  37%|███▋      | 1516/4096 [00:02<00:04, 535.97it/s]
Adding requests:  38%|███▊      | 1571/4096 [00:02<00:04, 538.13it/s]
Adding requests:  40%|███▉      | 1627/4096 [00:03<00:04, 541.53it/s]
Adding requests:  41%|████      | 1682/4096 [00:03<00:04, 542.55it/s]
Adding requests:  42%|████▏     | 1738/4096 [00:03<00:04, 546.56it/s]
Adding requests:  44%|████▍     | 1793/4096 [00:03<00:04, 540.14it/s]
Adding requests:  45%|████▌     | 1848/4096 [00:03<00:04, 542.22it/s]
Adding requests:  46%|████▋     | 1903/4096 [00:03<00:04, 537.49it/s]
Adding requests:  48%|████▊     | 1957/4096 [00:03<00:03, 537.87it/s]
Adding requests:  49%|████▉     | 2013/4096 [00:03<00:03, 542.01it/s]
Adding requests:  50%|█████     | 2068/4096 [00:03<00:03, 543.94it/s]
Adding requests:  52%|█████▏    | 2123/4096 [00:03<00:03, 540.04it/s]
Adding requests:  53%|█████▎    | 2178/4096 [00:04<00:03, 534.65it/s]
Adding requests:  54%|█████▍    | 2232/4096 [00:04<00:03, 525.54it/s]
Adding requests:  56%|█████▌    | 2286/4096 [00:04<00:03, 527.49it/s]
Adding requests:  57%|█████▋    | 2341/4096 [00:04<00:03, 530.72it/s]
Adding requests:  58%|█████▊    | 2396/4096 [00:04<00:03, 533.49it/s]
Adding requests:  60%|█████▉    | 2450/4096 [00:04<00:03, 502.72it/s]
Adding requests:  61%|██████    | 2503/4096 [00:04<00:03, 510.34it/s]
Adding requests:  62%|██████▎   | 2560/4096 [00:04<00:02, 519.97it/s]
Adding requests:  64%|██████▍   | 2624/4096 [00:04<00:02, 551.50it/s]
Adding requests:  65%|██████▌   | 2680/4096 [00:04<00:02, 540.11it/s]
Adding requests:  67%|██████▋   | 2735/4096 [00:05<00:02, 532.19it/s]
Adding requests:  68%|██████▊   | 2789/4096 [00:05<00:02, 531.83it/s]
Adding requests:  69%|██████▉   | 2845/4096 [00:05<00:02, 539.78it/s]
Adding requests:  71%|███████   | 2900/4096 [00:05<00:02, 534.35it/s]
Adding requests:  72%|███████▏  | 2954/4096 [00:05<00:02, 527.90it/s]
Adding requests:  73%|███████▎  | 3008/4096 [00:05<00:02, 531.21it/s]
Adding requests:  75%|███████▍  | 3062/4096 [00:05<00:01, 529.55it/s]
Adding requests:  76%|███████▌  | 3117/4096 [00:05<00:01, 534.53it/s]
Adding requests:  77%|███████▋  | 3172/4096 [00:05<00:01, 537.94it/s]
Adding requests:  79%|███████▉  | 3228/4096 [00:06<00:01, 541.59it/s]
Adding requests:  80%|████████  | 3285/4096 [00:06<00:01, 547.24it/s]
Adding requests:  82%|████████▏ | 3340/4096 [00:06<00:01, 538.66it/s]
Adding requests:  83%|████████▎ | 3394/4096 [00:06<00:01, 537.80it/s]
Adding requests:  84%|████████▍ | 3450/4096 [00:06<00:01, 538.33it/s]
Adding requests:  86%|████████▌ | 3504/4096 [00:06<00:01, 526.72it/s]
Adding requests:  87%|████████▋ | 3560/4096 [00:06<00:01, 533.61it/s]
Adding requests:  88%|████████▊ | 3614/4096 [00:06<00:00, 534.83it/s]
Adding requests:  90%|████████▉ | 3669/4096 [00:06<00:00, 538.06it/s]
Adding requests:  91%|█████████ | 3724/4096 [00:06<00:00, 540.42it/s]
Adding requests:  92%|█████████▏| 3783/4096 [00:07<00:00, 554.06it/s]
Adding requests:  94%|█████████▎| 3839/4096 [00:07<00:00, 517.95it/s]
Adding requests:  95%|█████████▌| 3892/4096 [00:07<00:00, 520.00it/s]
Adding requests:  96%|█████████▋| 3947/4096 [00:07<00:00, 528.36it/s]
Adding requests:  98%|█████████▊| 4003/4096 [00:07<00:00, 535.71it/s]
Adding requests:  99%|█████████▉| 4057/4096 [00:07<00:00, 534.46it/s]
Adding requests: 100%|██████████| 4096/4096 [00:07<00:00, 536.03it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 98/4096 [00:01<01:14, 54.01it/s, est. speed input: 55306.62 toks/s, output: 54.01 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:04<02:20, 28.24it/s, est. speed input: 32420.84 toks/s, output: 31.66 toks/s]
Processed prompts:   4%|▍         | 162/4096 [00:06<03:03, 21.45it/s, est. speed input: 25968.41 toks/s, output: 25.36 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:08<03:31, 18.42it/s, est. speed input: 22897.35 toks/s, output: 22.36 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:10<03:50, 16.80it/s, est. speed input: 21113.91 toks/s, output: 20.62 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:13<04:02, 15.83it/s, est. speed input: 19938.57 toks/s, output: 19.47 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:15<04:10, 15.21it/s, est. speed input: 19106.67 toks/s, output: 18.66 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:17<04:13, 14.89it/s, est. speed input: 18529.35 toks/s, output: 18.09 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:20<04:16, 14.60it/s, est. speed input: 18047.51 toks/s, output: 17.62 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:22<04:17, 14.41it/s, est. speed input: 17665.16 toks/s, output: 17.25 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:24<04:17, 14.28it/s, est. speed input: 17355.67 toks/s, output: 16.95 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:26<04:15, 14.26it/s, est. speed input: 17120.04 toks/s, output: 16.72 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:29<04:15, 14.17it/s, est. speed input: 16899.82 toks/s, output: 16.50 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:31<04:16, 13.94it/s, est. speed input: 16661.97 toks/s, output: 16.27 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:33<04:14, 13.94it/s, est. speed input: 16500.12 toks/s, output: 16.11 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:36<04:12, 13.95it/s, est. speed input: 16360.46 toks/s, output: 15.98 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:38<04:09, 13.95it/s, est. speed input: 16237.69 toks/s, output: 15.86 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:40<04:07, 13.95it/s, est. speed input: 16128.11 toks/s, output: 15.75 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:43<04:05, 13.95it/s, est. speed input: 16030.59 toks/s, output: 15.65 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:45<04:02, 13.96it/s, est. speed input: 15943.30 toks/s, output: 15.57 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:47<04:00, 13.95it/s, est. speed input: 15862.91 toks/s, output: 15.49 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:49<03:58, 13.96it/s, est. speed input: 15791.31 toks/s, output: 15.42 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:52<03:55, 13.96it/s, est. speed input: 15726.09 toks/s, output: 15.36 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:54<03:53, 13.96it/s, est. speed input: 15666.22 toks/s, output: 15.30 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:56<03:51, 13.96it/s, est. speed input: 15611.25 toks/s, output: 15.25 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:59<03:49, 13.96it/s, est. speed input: 15559.79 toks/s, output: 15.20 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [01:01<03:45, 14.05it/s, est. speed input: 15524.68 toks/s, output: 15.16 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [01:03<03:42, 14.11it/s, est. speed input: 15491.75 toks/s, output: 15.13 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [01:05<03:41, 13.99it/s, est. speed input: 15440.25 toks/s, output: 15.08 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [01:08<03:39, 13.98it/s, est. speed input: 15402.14 toks/s, output: 15.04 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [01:10<03:37, 13.98it/s, est. speed input: 15366.28 toks/s, output: 15.01 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [01:12<03:35, 13.97it/s, est. speed input: 15331.93 toks/s, output: 14.97 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [01:15<03:32, 13.97it/s, est. speed input: 15300.77 toks/s, output: 14.94 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [01:17<03:29, 14.05it/s, est. speed input: 15280.03 toks/s, output: 14.92 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [01:19<03:27, 14.02it/s, est. speed input: 15251.63 toks/s, output: 14.89 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [01:21<03:25, 14.01it/s, est. speed input: 15225.29 toks/s, output: 14.87 toks/s]
Processed prompts:  31%|███       | 1250/4096 [01:24<03:21, 14.09it/s, est. speed input: 15209.31 toks/s, output: 14.85 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [01:26<03:19, 14.14it/s, est. speed input: 15193.41 toks/s, output: 14.84 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [01:28<03:17, 14.08it/s, est. speed input: 15170.01 toks/s, output: 14.81 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [01:30<03:15, 14.05it/s, est. speed input: 15147.98 toks/s, output: 14.79 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [01:33<03:13, 14.02it/s, est. speed input: 15127.07 toks/s, output: 14.77 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [01:35<03:11, 13.99it/s, est. speed input: 15106.11 toks/s, output: 14.75 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [01:37<03:10, 13.94it/s, est. speed input: 15083.58 toks/s, output: 14.73 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [01:40<03:07, 13.95it/s, est. speed input: 15066.02 toks/s, output: 14.71 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [01:42<03:05, 13.95it/s, est. speed input: 15048.13 toks/s, output: 14.70 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [01:44<03:03, 13.96it/s, est. speed input: 15032.08 toks/s, output: 14.68 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [01:47<02:59, 14.06it/s, est. speed input: 15023.95 toks/s, output: 14.67 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [01:49<02:57, 14.02it/s, est. speed input: 15007.96 toks/s, output: 14.66 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [01:51<02:54, 14.09it/s, est. speed input: 15000.00 toks/s, output: 14.65 toks/s]
Processed prompts:  41%|████      | 1666/4096 [01:53<02:52, 14.06it/s, est. speed input: 14986.28 toks/s, output: 14.64 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [01:56<02:51, 14.02it/s, est. speed input: 14972.20 toks/s, output: 14.62 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [01:58<02:48, 14.00it/s, est. speed input: 14958.85 toks/s, output: 14.61 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [02:00<02:46, 13.99it/s, est. speed input: 14946.61 toks/s, output: 14.60 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [02:03<02:44, 13.99it/s, est. speed input: 14934.65 toks/s, output: 14.58 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [02:05<02:42, 13.97it/s, est. speed input: 14922.68 toks/s, output: 14.57 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [02:07<02:39, 14.05it/s, est. speed input: 14916.69 toks/s, output: 14.57 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [02:09<02:37, 13.96it/s, est. speed input: 14901.87 toks/s, output: 14.55 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [02:12<02:35, 13.97it/s, est. speed input: 14891.53 toks/s, output: 14.54 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [02:14<02:32, 14.06it/s, est. speed input: 14887.03 toks/s, output: 14.54 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [02:16<02:29, 14.12it/s, est. speed input: 14882.22 toks/s, output: 14.53 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [02:18<02:27, 14.07it/s, est. speed input: 14872.34 toks/s, output: 14.52 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [02:21<02:25, 14.02it/s, est. speed input: 14862.38 toks/s, output: 14.51 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [02:23<02:23, 14.00it/s, est. speed input: 14853.21 toks/s, output: 14.51 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [02:25<02:21, 13.99it/s, est. speed input: 14844.72 toks/s, output: 14.50 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [02:28<02:19, 13.99it/s, est. speed input: 14836.72 toks/s, output: 14.49 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [02:30<02:17, 13.98it/s, est. speed input: 14828.09 toks/s, output: 14.48 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [02:32<02:12, 14.20it/s, est. speed input: 14832.00 toks/s, output: 14.48 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [02:34<02:11, 14.13it/s, est. speed input: 14824.01 toks/s, output: 14.48 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [02:37<02:08, 14.15it/s, est. speed input: 14820.27 toks/s, output: 14.47 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [02:39<02:07, 14.09it/s, est. speed input: 14812.52 toks/s, output: 14.47 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [02:41<02:04, 14.11it/s, est. speed input: 14807.93 toks/s, output: 14.46 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [02:43<02:00, 14.31it/s, est. speed input: 14812.36 toks/s, output: 14.47 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [02:46<01:58, 14.30it/s, est. speed input: 14809.69 toks/s, output: 14.46 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [02:48<01:57, 14.18it/s, est. speed input: 14802.22 toks/s, output: 14.46 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [02:50<01:55, 14.12it/s, est. speed input: 14795.39 toks/s, output: 14.45 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [02:52<01:52, 14.16it/s, est. speed input: 14792.86 toks/s, output: 14.45 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [02:55<01:51, 14.09it/s, est. speed input: 14786.00 toks/s, output: 14.44 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [02:57<01:48, 14.15it/s, est. speed input: 14783.87 toks/s, output: 14.44 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [02:59<01:46, 14.09it/s, est. speed input: 14777.61 toks/s, output: 14.43 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [03:02<01:44, 14.05it/s, est. speed input: 14771.57 toks/s, output: 14.43 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [03:04<01:42, 14.03it/s, est. speed input: 14765.77 toks/s, output: 14.42 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [03:06<01:39, 14.10it/s, est. speed input: 14763.81 toks/s, output: 14.42 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [03:08<01:37, 14.06it/s, est. speed input: 14758.22 toks/s, output: 14.41 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [03:11<01:35, 14.02it/s, est. speed input: 14752.27 toks/s, output: 14.41 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [03:13<01:33, 13.94it/s, est. speed input: 14744.29 toks/s, output: 14.40 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [03:15<01:31, 13.95it/s, est. speed input: 14739.33 toks/s, output: 14.39 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [03:18<01:28, 14.04it/s, est. speed input: 14737.82 toks/s, output: 14.39 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [03:20<01:26, 14.02it/s, est. speed input: 14732.91 toks/s, output: 14.39 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [03:22<01:24, 14.00it/s, est. speed input: 14727.65 toks/s, output: 14.38 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [03:24<01:22, 13.98it/s, est. speed input: 14722.54 toks/s, output: 14.38 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [03:27<01:20, 13.97it/s, est. speed input: 14717.80 toks/s, output: 14.37 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [03:29<01:17, 13.97it/s, est. speed input: 14713.08 toks/s, output: 14.37 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [03:31<01:15, 13.97it/s, est. speed input: 14708.63 toks/s, output: 14.36 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [03:34<01:13, 13.97it/s, est. speed input: 14704.40 toks/s, output: 14.36 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [03:36<01:09, 14.21it/s, est. speed input: 14708.86 toks/s, output: 14.36 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [03:38<01:07, 14.22it/s, est. speed input: 14707.51 toks/s, output: 14.36 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [03:40<01:05, 14.14it/s, est. speed input: 14703.20 toks/s, output: 14.36 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [03:43<01:03, 14.07it/s, est. speed input: 14698.45 toks/s, output: 14.35 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [03:45<01:01, 14.12it/s, est. speed input: 14697.37 toks/s, output: 14.35 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [03:47<00:58, 14.07it/s, est. speed input: 14693.35 toks/s, output: 14.35 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [03:49<00:56, 14.04it/s, est. speed input: 14689.43 toks/s, output: 14.35 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [03:52<00:54, 14.02it/s, est. speed input: 14685.50 toks/s, output: 14.34 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [03:54<00:52, 14.00it/s, est. speed input: 14681.56 toks/s, output: 14.34 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [03:56<00:50, 13.99it/s, est. speed input: 14677.85 toks/s, output: 14.33 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [03:59<00:47, 14.05it/s, est. speed input: 14676.69 toks/s, output: 14.33 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [04:01<00:45, 14.03it/s, est. speed input: 14673.10 toks/s, output: 14.33 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [04:03<00:42, 14.25it/s, est. speed input: 14677.36 toks/s, output: 14.33 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [04:05<00:40, 14.17it/s, est. speed input: 14674.02 toks/s, output: 14.33 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [04:08<00:38, 14.11it/s, est. speed input: 14670.57 toks/s, output: 14.33 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [04:10<00:36, 14.05it/s, est. speed input: 14666.85 toks/s, output: 14.32 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [04:12<00:34, 14.03it/s, est. speed input: 14663.50 toks/s, output: 14.32 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [04:14<00:31, 14.00it/s, est. speed input: 14659.95 toks/s, output: 14.32 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [04:17<00:29, 13.98it/s, est. speed input: 14656.47 toks/s, output: 14.31 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [04:19<00:27, 14.06it/s, est. speed input: 14655.79 toks/s, output: 14.31 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [04:21<00:24, 14.03it/s, est. speed input: 14652.59 toks/s, output: 14.31 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [04:24<00:22, 14.01it/s, est. speed input: 14649.55 toks/s, output: 14.31 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [04:26<00:20, 14.00it/s, est. speed input: 14646.64 toks/s, output: 14.30 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [04:28<00:18, 14.07it/s, est. speed input: 14646.19 toks/s, output: 14.30 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [04:30<00:15, 14.04it/s, est. speed input: 14643.19 toks/s, output: 14.30 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [04:33<00:13, 14.00it/s, est. speed input: 14639.95 toks/s, output: 14.30 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [04:35<00:11, 13.99it/s, est. speed input: 14637.10 toks/s, output: 14.29 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [04:37<00:09, 13.98it/s, est. speed input: 14634.36 toks/s, output: 14.29 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [04:40<00:06, 13.98it/s, est. speed input: 14631.63 toks/s, output: 14.29 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [04:42<00:04, 14.32it/s, est. speed input: 14638.58 toks/s, output: 14.30 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [04:44<00:02, 14.31it/s, est. speed input: 14638.32 toks/s, output: 14.30 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:44<00:00, 14.31it/s, est. speed input: 14746.31 toks/s, output: 14.40 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:44<00:00, 14.40it/s, est. speed input: 14746.31 toks/s, output: 14.40 toks/s]
[rank0]:[W128 09:30:52.847483187 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 342.7s

测试结果:
  Requests/s:   14.02
  Tokens/s:     14374.44
  Total Reqs:   4096
  Elapsed:      292.07s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     14360.42


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,27.5589,14137.7112,4.6446
1024,1024,1,128,128,14.6457,15011.8280,8.7398
2048,1024,2,256,128,14.8404,15211.3875,17.2502
4096,1024,4,512,128,14.0701,14421.8613,36.3892
8192,1024,8,1024,128,14.1364,14489.8556,72.4369
16384,1024,16,2048,128,14.3883,14748.0255,142.3377
32768,1024,32,4096,128,14.0238,14374.4418,292.0739

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:30:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:30:58 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3749552) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3749552) WARNING 01-28 09:31:29 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 25.94 requests/s, 13307.47 total tokens/s, 25.94 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:30:58] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:30:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:30:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:30:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:30:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:30:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:30:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:30:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:30:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:30:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:30:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:30:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:30:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:30:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:31:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:31:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:31:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:31:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:31:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:31:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:31:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:31:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:31:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:03] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:03] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:03] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:03] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:03] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3749552) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3749552) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:18<00:00, 18.10s/it]
(EngineCore_DP0 pid=3749552) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:18<00:00, 18.10s/it]
(EngineCore_DP0 pid=3749552) 
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3749552) [2026-01-28 09:31:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3749552) 2026-01-28 09:31:28,527 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3749552) 2026-01-28 09:31:28,544 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  97%|█████████▋| 124/128 [00:00<00:00, 1232.79it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1231.39it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:20,  6.32it/s, est. speed input: 3236.75 toks/s, output: 6.32 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:07, 16.98it/s, est. speed input: 7717.50 toks/s, output: 15.07 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:05, 21.34it/s, est. speed input: 9564.30 toks/s, output: 18.68 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:05, 23.58it/s, est. speed input: 10565.57 toks/s, output: 20.64 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 23.47it/s, est. speed input: 10851.14 toks/s, output: 21.19 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:04, 24.69it/s, est. speed input: 11325.34 toks/s, output: 22.12 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:04, 25.41it/s, est. speed input: 11656.33 toks/s, output: 22.77 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:04, 26.12it/s, est. speed input: 11948.50 toks/s, output: 23.34 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:03, 26.60it/s, est. speed input: 12178.86 toks/s, output: 23.79 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:03, 26.45it/s, est. speed input: 12297.05 toks/s, output: 24.02 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:03, 26.69it/s, est. speed input: 12438.63 toks/s, output: 24.29 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:03, 26.91it/s, est. speed input: 12565.24 toks/s, output: 24.54 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:03, 27.05it/s, est. speed input: 12672.14 toks/s, output: 24.75 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:03, 27.03it/s, est. speed input: 12751.60 toks/s, output: 24.91 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:03, 27.25it/s, est. speed input: 12843.81 toks/s, output: 25.09 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:03, 27.29it/s, est. speed input: 12914.07 toks/s, output: 25.22 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 27.26it/s, est. speed input: 12971.77 toks/s, output: 25.34 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:02, 27.28it/s, est. speed input: 13026.22 toks/s, output: 25.44 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:02<00:02, 27.44it/s, est. speed input: 13087.68 toks/s, output: 25.56 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:02<00:02, 26.90it/s, est. speed input: 13091.70 toks/s, output: 25.57 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:02, 27.13it/s, est. speed input: 13141.40 toks/s, output: 25.67 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:02<00:02, 27.29it/s, est. speed input: 13185.62 toks/s, output: 25.75 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:02<00:02, 27.05it/s, est. speed input: 13202.37 toks/s, output: 25.79 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:02<00:02, 27.29it/s, est. speed input: 13244.59 toks/s, output: 25.87 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:02, 27.29it/s, est. speed input: 13273.37 toks/s, output: 25.92 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 27.38it/s, est. speed input: 13305.18 toks/s, output: 25.99 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:03<00:01, 27.44it/s, est. speed input: 13334.34 toks/s, output: 26.04 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:03<00:01, 27.37it/s, est. speed input: 13355.44 toks/s, output: 26.08 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:03<00:01, 26.94it/s, est. speed input: 13353.24 toks/s, output: 26.08 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:03<00:01, 27.02it/s, est. speed input: 13372.38 toks/s, output: 26.12 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:03<00:01, 27.14it/s, est. speed input: 13393.11 toks/s, output: 26.16 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:03<00:01, 27.30it/s, est. speed input: 13417.07 toks/s, output: 26.21 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:01, 27.30it/s, est. speed input: 13433.81 toks/s, output: 26.24 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:03<00:01, 27.27it/s, est. speed input: 13447.77 toks/s, output: 26.27 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:03<00:00, 27.40it/s, est. speed input: 13468.40 toks/s, output: 26.31 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:04<00:00, 27.43it/s, est. speed input: 13485.02 toks/s, output: 26.34 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:04<00:00, 27.22it/s, est. speed input: 13490.55 toks/s, output: 26.35 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:04<00:00, 27.36it/s, est. speed input: 13507.91 toks/s, output: 26.38 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:04<00:00, 27.02it/s, est. speed input: 13506.27 toks/s, output: 26.38 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:04<00:00, 27.22it/s, est. speed input: 13522.80 toks/s, output: 26.41 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:04<00:00, 27.30it/s, est. speed input: 13535.97 toks/s, output: 26.44 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:04<00:00, 27.46it/s, est. speed input: 13552.42 toks/s, output: 26.47 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:04<00:00, 27.53it/s, est. speed input: 13566.59 toks/s, output: 26.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 27.53it/s, est. speed input: 13570.36 toks/s, output: 26.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 26.50it/s, est. speed input: 13570.36 toks/s, output: 26.50 toks/s]
[rank0]:[W128 09:31:34.315532224 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.4s

测试结果:
  Requests/s:   25.94
  Tokens/s:     13307.47
  Total Reqs:   128
  Elapsed:      4.93s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     13281.53

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:31:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:31:40 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3750339) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3750339) WARNING 01-28 09:32:09 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.23 requests/s, 14587.68 total tokens/s, 14.23 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:31:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:31:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:31:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:31:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:31:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:31:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:31:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:31:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:31:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:31:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:31:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:31:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:31:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:31:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:31:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:31:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:31:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:31:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:31:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3750339) [2026-01-28 09:31:44] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3750339) [2026-01-28 09:31:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3750339) [2026-01-28 09:31:44] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3750339) [2026-01-28 09:31:44] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3750339) [2026-01-28 09:31:44] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3750339) [2026-01-28 09:31:44] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3750339) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3750339) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.46s/it]
(EngineCore_DP0 pid=3750339) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.46s/it]
(EngineCore_DP0 pid=3750339) 
(EngineCore_DP0 pid=3750339) [2026-01-28 09:32:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3750339) [2026-01-28 09:32:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3750339) [2026-01-28 09:32:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3750339) [2026-01-28 09:32:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3750339) [2026-01-28 09:32:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3750339) [2026-01-28 09:32:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3750339) [2026-01-28 09:32:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3750339) [2026-01-28 09:32:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3750339) 2026-01-28 09:32:08,768 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3750339) 2026-01-28 09:32:08,780 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  53%|█████▎    | 68/128 [00:00<00:00, 679.63it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 681.52it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:14,  8.72it/s, est. speed input: 8927.30 toks/s, output: 8.72 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:10, 12.40it/s, est. speed input: 12184.57 toks/s, output: 11.90 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:09, 13.33it/s, est. speed input: 13080.01 toks/s, output: 12.77 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:08, 13.89it/s, est. speed input: 13596.49 toks/s, output: 13.28 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:08, 14.14it/s, est. speed input: 13872.79 toks/s, output: 13.55 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:08, 14.15it/s, est. speed input: 13983.41 toks/s, output: 13.66 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:08, 14.31it/s, est. speed input: 14130.08 toks/s, output: 13.80 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:07, 14.52it/s, est. speed input: 14280.07 toks/s, output: 13.95 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:07, 14.61it/s, est. speed input: 14377.53 toks/s, output: 14.04 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:07, 14.57it/s, est. speed input: 14425.20 toks/s, output: 14.09 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:07, 14.57it/s, est. speed input: 14471.30 toks/s, output: 14.13 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:07, 14.58it/s, est. speed input: 14511.51 toks/s, output: 14.17 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:07, 14.65it/s, est. speed input: 14561.10 toks/s, output: 14.22 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 14.46it/s, est. speed input: 14547.79 toks/s, output: 14.21 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:02<00:06, 14.51it/s, est. speed input: 14576.54 toks/s, output: 14.23 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:02<00:06, 14.57it/s, est. speed input: 14606.68 toks/s, output: 14.26 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:06, 14.63it/s, est. speed input: 14637.91 toks/s, output: 14.29 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:06, 14.61it/s, est. speed input: 14653.77 toks/s, output: 14.31 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:06, 14.69it/s, est. speed input: 14684.35 toks/s, output: 14.34 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:06, 14.80it/s, est. speed input: 14720.88 toks/s, output: 14.38 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 14.64it/s, est. speed input: 14715.08 toks/s, output: 14.37 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 14.62it/s, est. speed input: 14724.45 toks/s, output: 14.38 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:03<00:05, 14.68it/s, est. speed input: 14744.97 toks/s, output: 14.40 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:03<00:05, 14.66it/s, est. speed input: 14754.25 toks/s, output: 14.41 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:05, 14.66it/s, est. speed input: 14764.57 toks/s, output: 14.42 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:05, 14.65it/s, est. speed input: 14772.80 toks/s, output: 14.43 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:05, 14.66it/s, est. speed input: 14782.40 toks/s, output: 14.44 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 14.75it/s, est. speed input: 14801.36 toks/s, output: 14.45 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 14.56it/s, est. speed input: 14789.89 toks/s, output: 14.44 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:04<00:04, 14.51it/s, est. speed input: 14788.06 toks/s, output: 14.44 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:04<00:04, 14.61it/s, est. speed input: 14801.24 toks/s, output: 14.45 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:04<00:04, 14.72it/s, est. speed input: 14817.80 toks/s, output: 14.47 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:04, 14.71it/s, est. speed input: 14824.72 toks/s, output: 14.48 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:04, 14.70it/s, est. speed input: 14830.42 toks/s, output: 14.48 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:04, 14.73it/s, est. speed input: 14839.42 toks/s, output: 14.49 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 14.72it/s, est. speed input: 14845.59 toks/s, output: 14.50 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:05<00:03, 14.51it/s, est. speed input: 14832.21 toks/s, output: 14.48 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:05<00:03, 14.54it/s, est. speed input: 14835.59 toks/s, output: 14.49 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:05<00:03, 14.61it/s, est. speed input: 14842.97 toks/s, output: 14.50 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:05<00:03, 14.58it/s, est. speed input: 14843.99 toks/s, output: 14.50 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:05<00:03, 14.61it/s, est. speed input: 14848.45 toks/s, output: 14.50 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:03, 14.63it/s, est. speed input: 14852.46 toks/s, output: 14.50 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 14.58it/s, est. speed input: 14851.46 toks/s, output: 14.50 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 14.67it/s, est. speed input: 14860.43 toks/s, output: 14.51 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:06<00:02, 14.51it/s, est. speed input: 14852.02 toks/s, output: 14.50 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:06<00:02, 14.54it/s, est. speed input: 14853.98 toks/s, output: 14.51 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:06<00:02, 14.58it/s, est. speed input: 14857.86 toks/s, output: 14.51 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:06<00:02, 14.56it/s, est. speed input: 14858.24 toks/s, output: 14.51 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:06<00:02, 14.60it/s, est. speed input: 14862.08 toks/s, output: 14.51 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 14.60it/s, est. speed input: 14863.42 toks/s, output: 14.52 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 14.64it/s, est. speed input: 14867.75 toks/s, output: 14.52 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:07<00:01, 14.54it/s, est. speed input: 14863.42 toks/s, output: 14.52 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:07<00:01, 14.55it/s, est. speed input: 14864.49 toks/s, output: 14.52 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:07<00:01, 14.51it/s, est. speed input: 14862.71 toks/s, output: 14.51 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:07<00:01, 14.60it/s, est. speed input: 14868.43 toks/s, output: 14.52 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:07<00:01, 14.61it/s, est. speed input: 14870.71 toks/s, output: 14.52 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:07<00:01, 14.72it/s, est. speed input: 14878.52 toks/s, output: 14.53 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 14.65it/s, est. speed input: 14878.00 toks/s, output: 14.53 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:08<00:00, 14.63it/s, est. speed input: 14878.95 toks/s, output: 14.53 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:08<00:00, 14.53it/s, est. speed input: 14875.06 toks/s, output: 14.53 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:08<00:00, 14.55it/s, est. speed input: 14876.00 toks/s, output: 14.53 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:08<00:00, 14.63it/s, est. speed input: 14881.05 toks/s, output: 14.53 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:08<00:00, 14.64it/s, est. speed input: 14882.98 toks/s, output: 14.53 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:08<00:00, 14.68it/s, est. speed input: 14886.87 toks/s, output: 14.54 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.68it/s, est. speed input: 14885.79 toks/s, output: 14.54 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.54it/s, est. speed input: 14885.79 toks/s, output: 14.54 toks/s]
[rank0]:[W128 09:32:18.550810400 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.1s

测试结果:
  Requests/s:   14.23
  Tokens/s:     14587.68
  Total Reqs:   128
  Elapsed:      8.99s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     14573.45

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:32:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:32:25 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3751146) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3751146) WARNING 01-28 09:32:54 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.35 requests/s, 14707.68 total tokens/s, 14.35 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 09:32:24] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:32:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:32:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:32:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:32:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:32:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:32:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:32:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:32:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:32:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:32:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:32:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:32:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:32:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:32:28] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:32:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:32:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:32:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:32:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:32:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:32:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:32:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:32:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:32:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:32:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:32:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:32:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:32:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:29] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:29] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:29] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:29] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:29] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3751146) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3751146) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.74s/it]
(EngineCore_DP0 pid=3751146) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.74s/it]
(EngineCore_DP0 pid=3751146) 
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3751146) [2026-01-28 09:32:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3751146) 2026-01-28 09:32:53,571 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3751146) 2026-01-28 09:32:53,583 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  25%|██▌       | 64/256 [00:00<00:00, 630.17it/s]
Adding requests:  50%|█████     | 128/256 [00:00<00:00, 585.26it/s]
Adding requests:  73%|███████▎  | 187/256 [00:00<00:00, 571.95it/s]
Adding requests:  96%|█████████▌| 245/256 [00:00<00:00, 564.10it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 574.29it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 6/256 [00:00<00:08, 28.54it/s, est. speed input: 29228.17 toks/s, output: 28.54 toks/s]
Processed prompts:   4%|▎         | 9/256 [00:00<00:09, 25.20it/s, est. speed input: 26426.66 toks/s, output: 25.81 toks/s]
Processed prompts:   5%|▍         | 12/256 [00:00<00:14, 16.97it/s, est. speed input: 19742.45 toks/s, output: 19.28 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:00<00:14, 16.16it/s, est. speed input: 18792.53 toks/s, output: 18.35 toks/s]
Processed prompts:   6%|▋         | 16/256 [00:00<00:15, 15.67it/s, est. speed input: 18189.87 toks/s, output: 17.76 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:01<00:15, 15.39it/s, est. speed input: 17781.17 toks/s, output: 17.36 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:01<00:15, 15.24it/s, est. speed input: 17488.98 toks/s, output: 17.08 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:01<00:15, 15.00it/s, est. speed input: 17203.17 toks/s, output: 16.80 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:01<00:15, 14.90it/s, est. speed input: 16995.21 toks/s, output: 16.60 toks/s]
Processed prompts:  10%|█         | 26/256 [00:01<00:15, 14.75it/s, est. speed input: 16799.86 toks/s, output: 16.41 toks/s]
Processed prompts:  11%|█         | 28/256 [00:01<00:15, 14.64it/s, est. speed input: 16631.55 toks/s, output: 16.24 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:01<00:15, 14.45it/s, est. speed input: 16458.84 toks/s, output: 16.07 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:02<00:15, 14.49it/s, est. speed input: 16354.62 toks/s, output: 15.97 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:02<00:15, 14.49it/s, est. speed input: 16257.27 toks/s, output: 15.88 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:02<00:15, 14.62it/s, est. speed input: 16200.43 toks/s, output: 15.82 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:02<00:14, 14.62it/s, est. speed input: 16131.41 toks/s, output: 15.75 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:02<00:14, 14.66it/s, est. speed input: 16075.91 toks/s, output: 15.70 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:02<00:14, 14.59it/s, est. speed input: 16009.26 toks/s, output: 15.63 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:02<00:14, 14.64it/s, est. speed input: 15965.82 toks/s, output: 15.59 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:02<00:14, 14.51it/s, est. speed input: 15899.65 toks/s, output: 15.53 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:03<00:14, 14.56it/s, est. speed input: 15861.28 toks/s, output: 15.49 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:03<00:14, 14.54it/s, est. speed input: 15817.54 toks/s, output: 15.45 toks/s]
Processed prompts:  20%|██        | 52/256 [00:03<00:14, 14.49it/s, est. speed input: 15772.42 toks/s, output: 15.40 toks/s]
Processed prompts:  21%|██        | 54/256 [00:03<00:13, 14.55it/s, est. speed input: 15744.03 toks/s, output: 15.37 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:03<00:13, 14.59it/s, est. speed input: 15717.69 toks/s, output: 15.35 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:03<00:13, 14.65it/s, est. speed input: 15697.40 toks/s, output: 15.33 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:03<00:13, 14.59it/s, est. speed input: 15665.40 toks/s, output: 15.30 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:04<00:13, 14.53it/s, est. speed input: 15633.46 toks/s, output: 15.27 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:04<00:13, 14.55it/s, est. speed input: 15611.44 toks/s, output: 15.25 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:04<00:13, 14.57it/s, est. speed input: 15590.94 toks/s, output: 15.23 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:04<00:12, 14.62it/s, est. speed input: 15575.34 toks/s, output: 15.21 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:04<00:12, 14.56it/s, est. speed input: 15551.30 toks/s, output: 15.19 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:04<00:12, 14.59it/s, est. speed input: 15535.77 toks/s, output: 15.17 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:04<00:12, 14.55it/s, est. speed input: 15515.24 toks/s, output: 15.15 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:05<00:12, 14.40it/s, est. speed input: 15483.88 toks/s, output: 15.12 toks/s]
Processed prompts:  30%|███       | 78/256 [00:05<00:12, 14.35it/s, est. speed input: 15459.07 toks/s, output: 15.10 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:05<00:12, 14.45it/s, est. speed input: 15448.59 toks/s, output: 15.09 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:05<00:12, 14.47it/s, est. speed input: 15433.60 toks/s, output: 15.07 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:05<00:11, 14.48it/s, est. speed input: 15419.46 toks/s, output: 15.06 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:05<00:11, 14.47it/s, est. speed input: 15404.24 toks/s, output: 15.04 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:05<00:11, 14.50it/s, est. speed input: 15392.94 toks/s, output: 15.03 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:05<00:11, 14.53it/s, est. speed input: 15382.40 toks/s, output: 15.02 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:06<00:11, 14.31it/s, est. speed input: 15353.57 toks/s, output: 14.99 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:06<00:11, 14.36it/s, est. speed input: 15341.64 toks/s, output: 14.98 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:06<00:11, 14.41it/s, est. speed input: 15331.54 toks/s, output: 14.97 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:06<00:10, 14.42it/s, est. speed input: 15320.21 toks/s, output: 14.96 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:06<00:10, 14.42it/s, est. speed input: 15308.89 toks/s, output: 14.95 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:06<00:10, 14.53it/s, est. speed input: 15305.60 toks/s, output: 14.95 toks/s]
Processed prompts:  41%|████      | 104/256 [00:06<00:10, 14.56it/s, est. speed input: 15299.10 toks/s, output: 14.94 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:07<00:10, 14.41it/s, est. speed input: 15281.55 toks/s, output: 14.92 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:07<00:10, 14.40it/s, est. speed input: 15270.77 toks/s, output: 14.91 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:07<00:10, 14.48it/s, est. speed input: 15266.39 toks/s, output: 14.91 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:07<00:09, 14.45it/s, est. speed input: 15256.05 toks/s, output: 14.90 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:07<00:09, 14.48it/s, est. speed input: 15249.76 toks/s, output: 14.89 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:07<00:09, 14.53it/s, est. speed input: 15245.59 toks/s, output: 14.89 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:07<00:09, 14.55it/s, est. speed input: 15240.12 toks/s, output: 14.88 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:08<00:09, 14.54it/s, est. speed input: 15233.66 toks/s, output: 14.88 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:08<00:09, 14.38it/s, est. speed input: 15218.60 toks/s, output: 14.86 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:08<00:09, 14.42it/s, est. speed input: 15212.87 toks/s, output: 14.86 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:08<00:08, 14.49it/s, est. speed input: 15209.24 toks/s, output: 14.85 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:08<00:08, 14.50it/s, est. speed input: 15203.90 toks/s, output: 14.85 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:08<00:08, 14.51it/s, est. speed input: 15198.60 toks/s, output: 14.84 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:08<00:08, 14.53it/s, est. speed input: 15194.43 toks/s, output: 14.84 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:09<00:08, 14.58it/s, est. speed input: 15192.26 toks/s, output: 14.84 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:09<00:08, 14.56it/s, est. speed input: 15187.60 toks/s, output: 14.83 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:09<00:08, 14.39it/s, est. speed input: 15174.63 toks/s, output: 14.82 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:09<00:08, 14.39it/s, est. speed input: 15167.84 toks/s, output: 14.81 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:09<00:07, 14.41it/s, est. speed input: 15162.66 toks/s, output: 14.81 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:09<00:07, 14.45it/s, est. speed input: 15159.14 toks/s, output: 14.80 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:09<00:07, 14.43it/s, est. speed input: 15152.92 toks/s, output: 14.80 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:10<00:07, 14.40it/s, est. speed input: 15146.19 toks/s, output: 14.79 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:10<00:07, 14.44it/s, est. speed input: 15142.64 toks/s, output: 14.79 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:10<00:07, 14.30it/s, est. speed input: 15131.39 toks/s, output: 14.78 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:10<00:07, 14.38it/s, est. speed input: 15128.36 toks/s, output: 14.77 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:10<00:06, 14.51it/s, est. speed input: 15129.05 toks/s, output: 14.77 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:10<00:06, 14.43it/s, est. speed input: 15122.04 toks/s, output: 14.77 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:10<00:06, 14.51it/s, est. speed input: 15121.22 toks/s, output: 14.77 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:10<00:06, 14.61it/s, est. speed input: 15122.08 toks/s, output: 14.77 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:11<00:06, 14.62it/s, est. speed input: 15120.66 toks/s, output: 14.77 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:11<00:06, 14.56it/s, est. speed input: 15116.08 toks/s, output: 14.76 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:11<00:06, 14.35it/s, est. speed input: 15104.81 toks/s, output: 14.75 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:11<00:05, 14.41it/s, est. speed input: 15102.37 toks/s, output: 14.75 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:11<00:05, 14.46it/s, est. speed input: 15100.23 toks/s, output: 14.75 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:11<00:05, 14.44it/s, est. speed input: 15096.21 toks/s, output: 14.74 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:11<00:05, 14.44it/s, est. speed input: 15092.36 toks/s, output: 14.74 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:12<00:05, 14.46it/s, est. speed input: 15089.90 toks/s, output: 14.74 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:12<00:05, 14.49it/s, est. speed input: 15087.89 toks/s, output: 14.73 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:12<00:05, 14.50it/s, est. speed input: 15085.24 toks/s, output: 14.73 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:12<00:04, 14.42it/s, est. speed input: 15079.54 toks/s, output: 14.73 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:12<00:04, 14.53it/s, est. speed input: 15080.42 toks/s, output: 14.73 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:12<00:04, 14.51it/s, est. speed input: 15077.34 toks/s, output: 14.72 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:12<00:04, 14.51it/s, est. speed input: 15075.15 toks/s, output: 14.72 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:13<00:04, 14.52it/s, est. speed input: 15073.03 toks/s, output: 14.72 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:13<00:04, 14.53it/s, est. speed input: 15071.35 toks/s, output: 14.72 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:13<00:04, 14.50it/s, est. speed input: 15068.44 toks/s, output: 14.72 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:13<00:04, 14.38it/s, est. speed input: 15061.75 toks/s, output: 14.71 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:13<00:03, 14.40it/s, est. speed input: 15058.92 toks/s, output: 14.71 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:13<00:03, 14.50it/s, est. speed input: 15059.43 toks/s, output: 14.71 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:13<00:03, 14.51it/s, est. speed input: 15057.54 toks/s, output: 14.70 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:14<00:03, 14.50it/s, est. speed input: 15055.30 toks/s, output: 14.70 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:14<00:03, 14.57it/s, est. speed input: 15055.60 toks/s, output: 14.70 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:14<00:03, 14.57it/s, est. speed input: 15054.21 toks/s, output: 14.70 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:14<00:03, 14.64it/s, est. speed input: 15055.40 toks/s, output: 14.70 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:14<00:02, 14.45it/s, est. speed input: 15048.56 toks/s, output: 14.70 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:14<00:02, 14.48it/s, est. speed input: 15046.99 toks/s, output: 14.69 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:14<00:02, 14.52it/s, est. speed input: 15046.44 toks/s, output: 14.69 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:14<00:02, 14.52it/s, est. speed input: 15044.66 toks/s, output: 14.69 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:15<00:02, 14.54it/s, est. speed input: 15043.74 toks/s, output: 14.69 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:15<00:02, 14.53it/s, est. speed input: 15042.14 toks/s, output: 14.69 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:15<00:02, 14.54it/s, est. speed input: 15040.86 toks/s, output: 14.69 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:15<00:01, 14.51it/s, est. speed input: 15038.53 toks/s, output: 14.69 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:15<00:01, 14.36it/s, est. speed input: 15032.55 toks/s, output: 14.68 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:15<00:01, 14.38it/s, est. speed input: 15030.31 toks/s, output: 14.68 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:15<00:01, 14.49it/s, est. speed input: 15030.97 toks/s, output: 14.68 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:16<00:01, 14.50it/s, est. speed input: 15029.48 toks/s, output: 14.68 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:16<00:01, 14.51it/s, est. speed input: 15028.23 toks/s, output: 14.68 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:16<00:01, 14.53it/s, est. speed input: 15027.32 toks/s, output: 14.68 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:16<00:00, 14.50it/s, est. speed input: 15025.41 toks/s, output: 14.67 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:16<00:00, 14.38it/s, est. speed input: 15020.42 toks/s, output: 14.67 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:16<00:00, 14.42it/s, est. speed input: 15019.20 toks/s, output: 14.67 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:16<00:00, 14.44it/s, est. speed input: 15017.74 toks/s, output: 14.67 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:17<00:00, 14.46it/s, est. speed input: 15016.31 toks/s, output: 14.66 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:17<00:00, 14.40it/s, est. speed input: 15013.11 toks/s, output: 14.66 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:17<00:00, 14.42it/s, est. speed input: 15011.49 toks/s, output: 14.66 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:17<00:00, 14.42it/s, est. speed input: 15070.63 toks/s, output: 14.72 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:17<00:00, 14.72it/s, est. speed input: 15070.63 toks/s, output: 14.72 toks/s]
[rank0]:[W128 09:33:12.367141260 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.8s

测试结果:
  Requests/s:   14.35
  Tokens/s:     14707.68
  Total Reqs:   256
  Elapsed:      17.84s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     14693.33

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:33:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:33:19 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3752097) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3752097) WARNING 01-28 09:33:48 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 13.79 requests/s, 14136.32 total tokens/s, 13.79 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 09:33:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:33:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:33:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:33:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:33:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:33:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:33:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:33:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:33:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:33:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:33:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:33:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:33:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:33:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:33:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:33:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:33:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:33:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:33:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:33:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:33:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:33:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:33:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:33:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:33:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:33:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:33:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:33:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3752097) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3752097) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.35s/it]
(EngineCore_DP0 pid=3752097) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.35s/it]
(EngineCore_DP0 pid=3752097) 
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3752097) [2026-01-28 09:33:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3752097) 2026-01-28 09:33:47,579 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3752097) 2026-01-28 09:33:47,591 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  12%|█▏        | 62/512 [00:00<00:00, 609.72it/s]
Adding requests:  24%|██▍       | 123/512 [00:00<00:00, 583.25it/s]
Adding requests:  36%|███▌      | 182/512 [00:00<00:00, 534.63it/s]
Adding requests:  47%|████▋     | 240/512 [00:00<00:00, 549.08it/s]
Adding requests:  58%|█████▊    | 296/512 [00:00<00:00, 545.61it/s]
Adding requests:  69%|██████▊   | 351/512 [00:00<00:00, 546.84it/s]
Adding requests:  80%|███████▉  | 408/512 [00:00<00:00, 553.23it/s]
Adding requests:  91%|█████████ | 464/512 [00:00<00:00, 552.97it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 551.35it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 10/512 [00:00<00:06, 80.15it/s, est. speed input: 82086.12 toks/s, output: 80.16 toks/s]
Processed prompts:   4%|▎         | 19/512 [00:00<00:20, 24.14it/s, est. speed input: 27782.02 toks/s, output: 27.13 toks/s]
Processed prompts:   5%|▍         | 24/512 [00:00<00:22, 21.55it/s, est. speed input: 24873.31 toks/s, output: 24.29 toks/s]
Processed prompts:   5%|▌         | 28/512 [00:01<00:25, 18.78it/s, est. speed input: 22420.31 toks/s, output: 21.89 toks/s]
Processed prompts:   6%|▌         | 31/512 [00:01<00:30, 16.03it/s, est. speed input: 20296.21 toks/s, output: 19.82 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:01<00:33, 14.18it/s, est. speed input: 18781.79 toks/s, output: 18.34 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:02<00:33, 14.03it/s, est. speed input: 18135.80 toks/s, output: 17.71 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:02<00:33, 13.99it/s, est. speed input: 17673.84 toks/s, output: 17.26 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:02<00:33, 13.96it/s, est. speed input: 17310.55 toks/s, output: 16.90 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:03<00:33, 13.97it/s, est. speed input: 17027.40 toks/s, output: 16.63 toks/s]
Processed prompts:  11%|█         | 54/512 [00:03<00:33, 13.87it/s, est. speed input: 16756.24 toks/s, output: 16.36 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:03<00:32, 13.85it/s, est. speed input: 16542.88 toks/s, output: 16.16 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:03<00:32, 13.87it/s, est. speed input: 16373.74 toks/s, output: 15.99 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:04<00:32, 13.88it/s, est. speed input: 16226.45 toks/s, output: 15.85 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:04<00:31, 13.84it/s, est. speed input: 16087.01 toks/s, output: 15.71 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:04<00:31, 13.86it/s, est. speed input: 15974.22 toks/s, output: 15.60 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:05<00:31, 13.87it/s, est. speed input: 15873.73 toks/s, output: 15.50 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:05<00:31, 13.78it/s, est. speed input: 15764.96 toks/s, output: 15.40 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:05<00:30, 13.83it/s, est. speed input: 15689.42 toks/s, output: 15.32 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:05<00:30, 13.85it/s, est. speed input: 15617.92 toks/s, output: 15.25 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:06<00:30, 13.86it/s, est. speed input: 15553.40 toks/s, output: 15.19 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:06<00:29, 13.81it/s, est. speed input: 15483.87 toks/s, output: 15.12 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:06<00:29, 13.82it/s, est. speed input: 15427.99 toks/s, output: 15.07 toks/s]
Processed prompts:  21%|██        | 106/512 [00:07<00:29, 13.82it/s, est. speed input: 15375.34 toks/s, output: 15.01 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:07<00:29, 13.83it/s, est. speed input: 15328.29 toks/s, output: 14.97 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:07<00:28, 13.76it/s, est. speed input: 15275.26 toks/s, output: 14.92 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:07<00:28, 13.79it/s, est. speed input: 15235.99 toks/s, output: 14.88 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:08<00:28, 13.82it/s, est. speed input: 15200.12 toks/s, output: 14.84 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:08<00:28, 13.76it/s, est. speed input: 15156.77 toks/s, output: 14.80 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:08<00:27, 13.79it/s, est. speed input: 15125.54 toks/s, output: 14.77 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:09<00:27, 13.77it/s, est. speed input: 15090.98 toks/s, output: 14.74 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:09<00:27, 13.81it/s, est. speed input: 15064.93 toks/s, output: 14.71 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:09<00:26, 13.74it/s, est. speed input: 15029.61 toks/s, output: 14.68 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:09<00:26, 13.76it/s, est. speed input: 15003.73 toks/s, output: 14.65 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:10<00:26, 13.79it/s, est. speed input: 14980.78 toks/s, output: 14.63 toks/s]
Processed prompts:  30%|███       | 154/512 [00:10<00:26, 13.77it/s, est. speed input: 14954.74 toks/s, output: 14.60 toks/s]
Processed prompts:  31%|███       | 158/512 [00:10<00:25, 13.78it/s, est. speed input: 14932.68 toks/s, output: 14.58 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:11<00:25, 13.84it/s, est. speed input: 14917.31 toks/s, output: 14.57 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:11<00:24, 13.94it/s, est. speed input: 14907.25 toks/s, output: 14.56 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:11<00:24, 13.82it/s, est. speed input: 14881.52 toks/s, output: 14.53 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:11<00:24, 13.85it/s, est. speed input: 14866.64 toks/s, output: 14.52 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:12<00:24, 13.83it/s, est. speed input: 14848.61 toks/s, output: 14.50 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:12<00:23, 13.87it/s, est. speed input: 14836.41 toks/s, output: 14.49 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:12<00:23, 13.84it/s, est. speed input: 14819.37 toks/s, output: 14.47 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:13<00:23, 13.88it/s, est. speed input: 14808.28 toks/s, output: 14.46 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:13<00:22, 13.89it/s, est. speed input: 14796.43 toks/s, output: 14.45 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:13<00:22, 13.88it/s, est. speed input: 14783.60 toks/s, output: 14.44 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:14<00:22, 13.81it/s, est. speed input: 14766.46 toks/s, output: 14.42 toks/s]
Processed prompts:  40%|████      | 206/512 [00:14<00:22, 13.82it/s, est. speed input: 14754.65 toks/s, output: 14.41 toks/s]
Processed prompts:  41%|████      | 210/512 [00:14<00:21, 13.79it/s, est. speed input: 14740.80 toks/s, output: 14.40 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:14<00:21, 13.75it/s, est. speed input: 14725.54 toks/s, output: 14.38 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:15<00:21, 13.80it/s, est. speed input: 14716.40 toks/s, output: 14.37 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:15<00:20, 13.82it/s, est. speed input: 14706.73 toks/s, output: 14.36 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:15<00:20, 13.82it/s, est. speed input: 14696.93 toks/s, output: 14.35 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:16<00:20, 13.77it/s, est. speed input: 14683.58 toks/s, output: 14.34 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:16<00:20, 13.82it/s, est. speed input: 14676.41 toks/s, output: 14.33 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:16<00:19, 13.82it/s, est. speed input: 14667.49 toks/s, output: 14.32 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:16<00:19, 13.86it/s, est. speed input: 14660.69 toks/s, output: 14.32 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:17<00:19, 13.79it/s, est. speed input: 14648.80 toks/s, output: 14.31 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:17<00:18, 13.83it/s, est. speed input: 14642.18 toks/s, output: 14.30 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:17<00:18, 13.84it/s, est. speed input: 14635.11 toks/s, output: 14.29 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:18<00:18, 13.79it/s, est. speed input: 14625.01 toks/s, output: 14.28 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:18<00:18, 13.83it/s, est. speed input: 14619.12 toks/s, output: 14.28 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:18<00:17, 13.83it/s, est. speed input: 14612.08 toks/s, output: 14.27 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:18<00:17, 13.84it/s, est. speed input: 14605.89 toks/s, output: 14.26 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:19<00:17, 13.75it/s, est. speed input: 14594.70 toks/s, output: 14.25 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:19<00:16, 13.80it/s, est. speed input: 14589.39 toks/s, output: 14.25 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:19<00:16, 13.80it/s, est. speed input: 14582.93 toks/s, output: 14.24 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:20<00:16, 13.82it/s, est. speed input: 14577.36 toks/s, output: 14.24 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:20<00:16, 13.75it/s, est. speed input: 14567.97 toks/s, output: 14.23 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:20<00:15, 13.78it/s, est. speed input: 14562.25 toks/s, output: 14.22 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:20<00:15, 13.81it/s, est. speed input: 14557.70 toks/s, output: 14.22 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:21<00:15, 13.76it/s, est. speed input: 14549.54 toks/s, output: 14.21 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:21<00:14, 13.79it/s, est. speed input: 14544.78 toks/s, output: 14.20 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:21<00:14, 13.80it/s, est. speed input: 14539.77 toks/s, output: 14.20 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:22<00:14, 13.80it/s, est. speed input: 14534.24 toks/s, output: 14.19 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:22<00:14, 13.73it/s, est. speed input: 14525.77 toks/s, output: 14.19 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:22<00:13, 13.75it/s, est. speed input: 14520.92 toks/s, output: 14.18 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:22<00:13, 13.77it/s, est. speed input: 14515.97 toks/s, output: 14.18 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:23<00:13, 13.79it/s, est. speed input: 14511.79 toks/s, output: 14.17 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:23<00:12, 13.74it/s, est. speed input: 14504.85 toks/s, output: 14.16 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:23<00:12, 13.80it/s, est. speed input: 14502.08 toks/s, output: 14.16 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:24<00:11, 14.31it/s, est. speed input: 14518.50 toks/s, output: 14.18 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:24<00:11, 14.12it/s, est. speed input: 14512.32 toks/s, output: 14.17 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:24<00:11, 14.02it/s, est. speed input: 14507.94 toks/s, output: 14.17 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:24<00:11, 13.98it/s, est. speed input: 14504.70 toks/s, output: 14.16 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:25<00:11, 13.94it/s, est. speed input: 14500.97 toks/s, output: 14.16 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:25<00:10, 13.83it/s, est. speed input: 14493.98 toks/s, output: 14.15 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:25<00:10, 13.87it/s, est. speed input: 14491.97 toks/s, output: 14.15 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:26<00:10, 13.83it/s, est. speed input: 14487.15 toks/s, output: 14.15 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:26<00:09, 13.87it/s, est. speed input: 14485.05 toks/s, output: 14.15 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:26<00:09, 13.76it/s, est. speed input: 14477.80 toks/s, output: 14.14 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:27<00:09, 13.82it/s, est. speed input: 14475.94 toks/s, output: 14.14 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:27<00:09, 13.83it/s, est. speed input: 14472.79 toks/s, output: 14.13 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:27<00:08, 13.74it/s, est. speed input: 14466.40 toks/s, output: 14.13 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:27<00:08, 13.74it/s, est. speed input: 14462.11 toks/s, output: 14.12 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:28<00:08, 13.77it/s, est. speed input: 14459.02 toks/s, output: 14.12 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:28<00:07, 13.81it/s, est. speed input: 14456.74 toks/s, output: 14.12 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:28<00:07, 13.73it/s, est. speed input: 14450.90 toks/s, output: 14.11 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:29<00:07, 13.78it/s, est. speed input: 14448.66 toks/s, output: 14.11 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:29<00:07, 13.79it/s, est. speed input: 14445.83 toks/s, output: 14.11 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:29<00:06, 13.82it/s, est. speed input: 14443.47 toks/s, output: 14.10 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:29<00:06, 13.75it/s, est. speed input: 14438.46 toks/s, output: 14.10 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:30<00:06, 13.78it/s, est. speed input: 14435.79 toks/s, output: 14.10 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:30<00:05, 13.79it/s, est. speed input: 14433.19 toks/s, output: 14.09 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:30<00:05, 13.77it/s, est. speed input: 14429.61 toks/s, output: 14.09 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:31<00:05, 13.79it/s, est. speed input: 14426.99 toks/s, output: 14.09 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:31<00:05, 13.84it/s, est. speed input: 14425.92 toks/s, output: 14.09 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:31<00:04, 13.84it/s, est. speed input: 14423.62 toks/s, output: 14.09 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:31<00:04, 14.54it/s, est. speed input: 14442.25 toks/s, output: 14.10 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:32<00:04, 14.34it/s, est. speed input: 14440.47 toks/s, output: 14.10 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:32<00:03, 14.21it/s, est. speed input: 14438.65 toks/s, output: 14.10 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:32<00:03, 14.06it/s, est. speed input: 14435.33 toks/s, output: 14.10 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:33<00:03, 13.95it/s, est. speed input: 14431.54 toks/s, output: 14.09 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:33<00:03, 13.93it/s, est. speed input: 14429.63 toks/s, output: 14.09 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:33<00:02, 13.89it/s, est. speed input: 14427.13 toks/s, output: 14.09 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:33<00:02, 13.83it/s, est. speed input: 14423.73 toks/s, output: 14.09 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:34<00:02, 13.82it/s, est. speed input: 14421.17 toks/s, output: 14.08 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:34<00:01, 13.89it/s, est. speed input: 14420.83 toks/s, output: 14.08 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:34<00:01, 13.87it/s, est. speed input: 14418.71 toks/s, output: 14.08 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:35<00:01, 13.81it/s, est. speed input: 14415.07 toks/s, output: 14.08 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:35<00:01, 13.83it/s, est. speed input: 14413.36 toks/s, output: 14.08 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:35<00:00, 13.83it/s, est. speed input: 14411.39 toks/s, output: 14.07 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:35<00:00, 13.85it/s, est. speed input: 14410.09 toks/s, output: 14.07 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:36<00:00, 14.64it/s, est. speed input: 14428.81 toks/s, output: 14.09 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:36<00:00, 14.64it/s, est. speed input: 14485.35 toks/s, output: 14.15 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:36<00:00, 14.15it/s, est. speed input: 14485.35 toks/s, output: 14.15 toks/s]
[rank0]:[W128 09:34:25.744290214 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 73.3s

测试结果:
  Requests/s:   13.79
  Tokens/s:     14136.32
  Total Reqs:   512
  Elapsed:      37.12s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     14122.52

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:34:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:34:34 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3753313) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3753313) WARNING 01-28 09:35:04 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 13.70 requests/s, 14043.34 total tokens/s, 13.70 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 09:34:34] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:34:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:34:34] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:34:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:34:34] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:34:34] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:34:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:34:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:34:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:34:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:34:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:34:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:34:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:34:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:34:37] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:34:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:34:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:34:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:34:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:34:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:34:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:34:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:34:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:34:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:34:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:34:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:34:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:34:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3753313) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3753313) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.54s/it]
(EngineCore_DP0 pid=3753313) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.54s/it]
(EngineCore_DP0 pid=3753313) 
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3753313) [2026-01-28 09:34:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3753313) 2026-01-28 09:35:03,129 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3753313) 2026-01-28 09:35:03,180 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:01, 628.11it/s]
Adding requests:  12%|█▏        | 126/1024 [00:00<00:01, 598.45it/s]
Adding requests:  18%|█▊        | 186/1024 [00:00<00:01, 559.83it/s]
Adding requests:  24%|██▎       | 243/1024 [00:00<00:01, 562.83it/s]
Adding requests:  29%|██▉       | 300/1024 [00:00<00:01, 547.56it/s]
Adding requests:  35%|███▍      | 356/1024 [00:00<00:01, 548.78it/s]
Adding requests:  40%|████      | 411/1024 [00:00<00:01, 548.74it/s]
Adding requests:  46%|████▌     | 466/1024 [00:00<00:01, 539.80it/s]
Adding requests:  51%|█████     | 521/1024 [00:00<00:00, 529.82it/s]
Adding requests:  56%|█████▌    | 575/1024 [00:01<00:00, 518.65it/s]
Adding requests:  61%|██████▏   | 628/1024 [00:01<00:00, 521.14it/s]
Adding requests:  67%|██████▋   | 682/1024 [00:01<00:00, 524.65it/s]
Adding requests:  72%|███████▏  | 737/1024 [00:01<00:00, 531.91it/s]
Adding requests:  77%|███████▋  | 791/1024 [00:01<00:00, 522.31it/s]
Adding requests:  82%|████████▏ | 844/1024 [00:01<00:00, 517.33it/s]
Adding requests:  88%|████████▊ | 902/1024 [00:01<00:00, 533.70it/s]
Adding requests:  93%|█████████▎| 956/1024 [00:01<00:00, 531.02it/s]
Adding requests:  99%|█████████▊| 1010/1024 [00:01<00:00, 532.76it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 537.75it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 26/1024 [00:00<00:23, 42.32it/s, est. speed input: 43339.28 toks/s, output: 42.32 toks/s]
Processed prompts:   3%|▎         | 34/1024 [00:01<00:38, 25.92it/s, est. speed input: 29128.36 toks/s, output: 28.44 toks/s]
Processed prompts:   4%|▍         | 42/1024 [00:01<00:48, 20.33it/s, est. speed input: 24115.66 toks/s, output: 23.55 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:02<00:54, 17.72it/s, est. speed input: 21620.01 toks/s, output: 21.11 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:02<00:59, 16.28it/s, est. speed input: 20117.17 toks/s, output: 19.65 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:03<01:02, 15.40it/s, est. speed input: 19111.36 toks/s, output: 18.66 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:04<01:03, 14.86it/s, est. speed input: 18398.66 toks/s, output: 17.97 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:04<01:05, 14.49it/s, est. speed input: 17852.57 toks/s, output: 17.43 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:05<01:05, 14.27it/s, est. speed input: 17441.14 toks/s, output: 17.03 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:05<01:05, 14.06it/s, est. speed input: 17090.68 toks/s, output: 16.69 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:06<01:05, 13.99it/s, est. speed input: 16827.43 toks/s, output: 16.43 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:07<01:05, 13.90it/s, est. speed input: 16594.18 toks/s, output: 16.21 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:07<01:05, 13.84it/s, est. speed input: 16398.30 toks/s, output: 16.01 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:08<01:04, 13.79it/s, est. speed input: 16228.16 toks/s, output: 15.85 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:08<01:04, 13.75it/s, est. speed input: 16079.29 toks/s, output: 15.70 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:09<01:03, 13.76it/s, est. speed input: 15957.95 toks/s, output: 15.58 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:09<01:03, 13.72it/s, est. speed input: 15838.24 toks/s, output: 15.47 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:10<01:02, 13.71it/s, est. speed input: 15737.06 toks/s, output: 15.37 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:11<01:02, 13.69it/s, est. speed input: 15643.60 toks/s, output: 15.28 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:11<01:01, 13.71it/s, est. speed input: 15566.91 toks/s, output: 15.20 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:12<01:01, 13.68it/s, est. speed input: 15488.76 toks/s, output: 15.13 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:12<01:00, 13.73it/s, est. speed input: 15429.80 toks/s, output: 15.07 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:13<00:59, 13.72it/s, est. speed input: 15369.19 toks/s, output: 15.01 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:14<00:59, 13.70it/s, est. speed input: 15310.66 toks/s, output: 14.95 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:14<00:58, 13.71it/s, est. speed input: 15262.09 toks/s, output: 14.90 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:15<00:58, 13.70it/s, est. speed input: 15212.78 toks/s, output: 14.86 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:15<00:57, 13.71it/s, est. speed input: 15170.58 toks/s, output: 14.82 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:16<00:57, 13.70it/s, est. speed input: 15129.40 toks/s, output: 14.77 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:16<00:56, 13.73it/s, est. speed input: 15095.58 toks/s, output: 14.74 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:17<00:55, 13.71it/s, est. speed input: 15058.83 toks/s, output: 14.71 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:18<00:55, 13.73it/s, est. speed input: 15028.14 toks/s, output: 14.68 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:18<00:54, 13.70it/s, est. speed input: 14994.59 toks/s, output: 14.64 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:19<00:54, 13.70it/s, est. speed input: 14965.27 toks/s, output: 14.61 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:19<00:53, 13.68it/s, est. speed input: 14936.05 toks/s, output: 14.59 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:20<00:52, 13.70it/s, est. speed input: 14911.39 toks/s, output: 14.56 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:21<00:52, 13.72it/s, est. speed input: 14889.05 toks/s, output: 14.54 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:21<00:51, 13.69it/s, est. speed input: 14862.98 toks/s, output: 14.51 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:22<00:51, 13.72it/s, est. speed input: 14843.52 toks/s, output: 14.50 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:22<00:50, 13.71it/s, est. speed input: 14822.24 toks/s, output: 14.47 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:23<00:48, 14.04it/s, est. speed input: 14831.72 toks/s, output: 14.48 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:23<00:48, 13.91it/s, est. speed input: 14810.23 toks/s, output: 14.46 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:24<00:48, 13.85it/s, est. speed input: 14791.68 toks/s, output: 14.44 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:25<00:48, 13.78it/s, est. speed input: 14772.32 toks/s, output: 14.43 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:25<00:47, 13.76it/s, est. speed input: 14755.86 toks/s, output: 14.41 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:26<00:46, 13.78it/s, est. speed input: 14742.17 toks/s, output: 14.40 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:26<00:46, 13.74it/s, est. speed input: 14725.44 toks/s, output: 14.38 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:27<00:45, 13.74it/s, est. speed input: 14711.96 toks/s, output: 14.37 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:28<00:45, 13.70it/s, est. speed input: 14695.48 toks/s, output: 14.35 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:28<00:44, 13.71it/s, est. speed input: 14682.53 toks/s, output: 14.34 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:29<00:44, 13.67it/s, est. speed input: 14666.91 toks/s, output: 14.32 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:29<00:43, 13.69it/s, est. speed input: 14654.84 toks/s, output: 14.31 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:30<00:43, 13.65it/s, est. speed input: 14639.86 toks/s, output: 14.30 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:30<00:42, 13.67it/s, est. speed input: 14629.12 toks/s, output: 14.29 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:31<00:41, 13.98it/s, est. speed input: 14636.99 toks/s, output: 14.29 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:32<00:40, 13.88it/s, est. speed input: 14625.30 toks/s, output: 14.28 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:32<00:40, 13.83it/s, est. speed input: 14614.85 toks/s, output: 14.27 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:33<00:39, 13.80it/s, est. speed input: 14604.82 toks/s, output: 14.26 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:33<00:39, 13.77it/s, est. speed input: 14594.76 toks/s, output: 14.25 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:34<00:38, 13.73it/s, est. speed input: 14584.41 toks/s, output: 14.24 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:34<00:38, 13.73it/s, est. speed input: 14575.68 toks/s, output: 14.23 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:35<00:37, 13.70it/s, est. speed input: 14565.36 toks/s, output: 14.22 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:36<00:37, 13.72it/s, est. speed input: 14557.90 toks/s, output: 14.22 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:36<00:36, 13.69it/s, est. speed input: 14547.93 toks/s, output: 14.21 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:37<00:36, 13.70it/s, est. speed input: 14540.54 toks/s, output: 14.20 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:37<00:35, 13.67it/s, est. speed input: 14531.04 toks/s, output: 14.19 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:38<00:34, 13.67it/s, est. speed input: 14522.56 toks/s, output: 14.18 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:39<00:34, 13.70it/s, est. speed input: 14516.28 toks/s, output: 14.18 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:39<00:33, 13.68it/s, est. speed input: 14508.32 toks/s, output: 14.17 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:40<00:33, 13.70it/s, est. speed input: 14502.23 toks/s, output: 14.16 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:40<00:32, 13.66it/s, est. speed input: 14493.02 toks/s, output: 14.15 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:41<00:31, 13.69it/s, est. speed input: 14487.66 toks/s, output: 14.15 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:42<00:31, 13.66it/s, est. speed input: 14479.70 toks/s, output: 14.14 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:42<00:30, 13.68it/s, est. speed input: 14473.60 toks/s, output: 14.13 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:43<00:30, 13.68it/s, est. speed input: 14467.37 toks/s, output: 14.13 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:43<00:29, 13.67it/s, est. speed input: 14461.05 toks/s, output: 14.12 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:44<00:29, 13.71it/s, est. speed input: 14456.40 toks/s, output: 14.12 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:44<00:28, 13.69it/s, est. speed input: 14450.04 toks/s, output: 14.11 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:45<00:27, 13.72it/s, est. speed input: 14445.97 toks/s, output: 14.11 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:46<00:27, 13.70it/s, est. speed input: 14440.18 toks/s, output: 14.10 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:46<00:26, 13.72it/s, est. speed input: 14435.90 toks/s, output: 14.10 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:47<00:26, 13.68it/s, est. speed input: 14429.42 toks/s, output: 14.09 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:47<00:25, 13.69it/s, est. speed input: 14424.68 toks/s, output: 14.09 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:48<00:25, 13.65it/s, est. speed input: 14418.13 toks/s, output: 14.08 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:49<00:24, 13.68it/s, est. speed input: 14413.90 toks/s, output: 14.08 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:49<00:23, 13.68it/s, est. speed input: 14409.19 toks/s, output: 14.07 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:50<00:23, 13.65it/s, est. speed input: 14403.25 toks/s, output: 14.07 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:50<00:22, 13.67it/s, est. speed input: 14399.34 toks/s, output: 14.06 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:51<00:22, 13.65it/s, est. speed input: 14393.82 toks/s, output: 14.06 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:51<00:21, 13.66it/s, est. speed input: 14389.65 toks/s, output: 14.05 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:52<00:20, 13.64it/s, est. speed input: 14384.21 toks/s, output: 14.05 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:53<00:20, 13.65it/s, est. speed input: 14379.94 toks/s, output: 14.04 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:53<00:19, 13.65it/s, est. speed input: 14375.50 toks/s, output: 14.04 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:54<00:19, 13.66it/s, est. speed input: 14371.69 toks/s, output: 14.03 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:54<00:18, 13.64it/s, est. speed input: 14366.91 toks/s, output: 14.03 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:55<00:18, 13.64it/s, est. speed input: 14362.65 toks/s, output: 14.03 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:56<00:17, 13.68it/s, est. speed input: 14359.94 toks/s, output: 14.02 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:56<00:16, 13.68it/s, est. speed input: 14356.17 toks/s, output: 14.02 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:57<00:16, 13.67it/s, est. speed input: 14352.24 toks/s, output: 14.02 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:57<00:15, 13.64it/s, est. speed input: 14347.68 toks/s, output: 14.01 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:58<00:15, 13.67it/s, est. speed input: 14344.92 toks/s, output: 14.01 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:58<00:14, 13.66it/s, est. speed input: 14341.11 toks/s, output: 14.00 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:59<00:13, 13.70it/s, est. speed input: 14338.85 toks/s, output: 14.00 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [01:00<00:13, 13.68it/s, est. speed input: 14335.22 toks/s, output: 14.00 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [01:00<00:12, 13.67it/s, est. speed input: 14331.89 toks/s, output: 14.00 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [01:01<00:12, 13.68it/s, est. speed input: 14328.85 toks/s, output: 13.99 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [01:01<00:11, 13.67it/s, est. speed input: 14325.65 toks/s, output: 13.99 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [01:02<00:10, 13.70it/s, est. speed input: 14323.47 toks/s, output: 13.99 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [01:03<00:10, 13.67it/s, est. speed input: 14319.91 toks/s, output: 13.98 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [01:03<00:09, 13.69it/s, est. speed input: 14317.37 toks/s, output: 13.98 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [01:04<00:09, 13.66it/s, est. speed input: 14313.66 toks/s, output: 13.98 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [01:04<00:08, 13.64it/s, est. speed input: 14310.33 toks/s, output: 13.97 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [01:05<00:08, 13.64it/s, est. speed input: 14307.29 toks/s, output: 13.97 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [01:05<00:07, 13.67it/s, est. speed input: 14305.22 toks/s, output: 13.97 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [01:06<00:06, 13.68it/s, est. speed input: 14302.62 toks/s, output: 13.97 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [01:07<00:06, 14.14it/s, est. speed input: 14313.68 toks/s, output: 13.98 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [01:07<00:05, 14.00it/s, est. speed input: 14310.94 toks/s, output: 13.98 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [01:08<00:05, 13.89it/s, est. speed input: 14307.94 toks/s, output: 13.97 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [01:08<00:04, 13.84it/s, est. speed input: 14305.85 toks/s, output: 13.97 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [01:09<00:03, 13.77it/s, est. speed input: 14302.74 toks/s, output: 13.97 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [01:10<00:03, 13.74it/s, est. speed input: 14300.21 toks/s, output: 13.97 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [01:10<00:02, 14.22it/s, est. speed input: 14311.49 toks/s, output: 13.98 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [01:11<00:02, 14.08it/s, est. speed input: 14309.82 toks/s, output: 13.97 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [01:11<00:01, 13.94it/s, est. speed input: 14306.77 toks/s, output: 13.97 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [01:12<00:01, 13.87it/s, est. speed input: 14304.79 toks/s, output: 13.97 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [01:12<00:00, 14.19it/s, est. speed input: 14312.46 toks/s, output: 13.98 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:12<00:00, 14.19it/s, est. speed input: 14396.77 toks/s, output: 14.06 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [01:12<00:00, 14.06it/s, est. speed input: 14396.77 toks/s, output: 14.06 toks/s]
[rank0]:[W128 09:36:19.248406836 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 113.5s

测试结果:
  Requests/s:   13.70
  Tokens/s:     14043.34
  Total Reqs:   1024
  Elapsed:      74.74s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     14029.64

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:36:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:36:30 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3755105) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3755105) WARNING 01-28 09:37:01 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 13.83 requests/s, 14171.71 total tokens/s, 13.83 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 09:36:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:36:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:36:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:36:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:36:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:36:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:36:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:36:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:36:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:36:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:36:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:36:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:36:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:36:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:36:34] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:36:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:36:34] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:36:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:36:34] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:36:34] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:36:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:36:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:36:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:36:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:36:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:36:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:36:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:36:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3755105) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3755105) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.38s/it]
(EngineCore_DP0 pid=3755105) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.38s/it]
(EngineCore_DP0 pid=3755105) 
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3755105) [2026-01-28 09:36:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3755105) 2026-01-28 09:36:59,664 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3755105) 2026-01-28 09:36:59,770 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 62/2048 [00:00<00:03, 615.97it/s]
Adding requests:   6%|▌         | 124/2048 [00:00<00:03, 579.20it/s]
Adding requests:   9%|▉         | 183/2048 [00:00<00:03, 537.91it/s]
Adding requests:  12%|█▏        | 238/2048 [00:00<00:03, 535.39it/s]
Adding requests:  14%|█▍        | 292/2048 [00:00<00:03, 518.53it/s]
Adding requests:  17%|█▋        | 344/2048 [00:00<00:03, 517.55it/s]
Adding requests:  19%|█▉        | 396/2048 [00:00<00:03, 514.69it/s]
Adding requests:  22%|██▏       | 448/2048 [00:00<00:03, 515.75it/s]
Adding requests:  24%|██▍       | 500/2048 [00:00<00:03, 509.11it/s]
Adding requests:  27%|██▋       | 551/2048 [00:01<00:02, 509.27it/s]
Adding requests:  29%|██▉       | 602/2048 [00:01<00:02, 508.01it/s]
Adding requests:  32%|███▏      | 657/2048 [00:01<00:02, 517.31it/s]
Adding requests:  35%|███▍      | 712/2048 [00:01<00:02, 524.39it/s]
Adding requests:  37%|███▋      | 765/2048 [00:01<00:02, 511.75it/s]
Adding requests:  40%|███▉      | 817/2048 [00:01<00:02, 496.72it/s]
Adding requests:  42%|████▏     | 867/2048 [00:01<00:02, 480.02it/s]
Adding requests:  45%|████▍     | 919/2048 [00:01<00:02, 490.84it/s]
Adding requests:  47%|████▋     | 969/2048 [00:02<00:05, 181.85it/s]
Adding requests:  50%|████▉     | 1018/2048 [00:02<00:04, 222.07it/s]
Adding requests:  52%|█████▏    | 1069/2048 [00:02<00:03, 267.01it/s]
Adding requests:  55%|█████▍    | 1120/2048 [00:02<00:02, 310.94it/s]
Adding requests:  57%|█████▋    | 1172/2048 [00:02<00:02, 353.45it/s]
Adding requests:  60%|█████▉    | 1224/2048 [00:02<00:02, 391.46it/s]
Adding requests:  62%|██████▏   | 1274/2048 [00:03<00:01, 416.87it/s]
Adding requests:  65%|██████▍   | 1327/2048 [00:03<00:01, 443.48it/s]
Adding requests:  67%|██████▋   | 1380/2048 [00:03<00:01, 464.23it/s]
Adding requests:  70%|██████▉   | 1433/2048 [00:03<00:01, 480.66it/s]
Adding requests:  73%|███████▎  | 1487/2048 [00:03<00:01, 497.08it/s]
Adding requests:  75%|███████▌  | 1541/2048 [00:03<00:00, 507.68it/s]
Adding requests:  78%|███████▊  | 1595/2048 [00:03<00:00, 514.96it/s]
Adding requests:  80%|████████  | 1648/2048 [00:03<00:00, 512.80it/s]
Adding requests:  83%|████████▎ | 1701/2048 [00:03<00:00, 504.05it/s]
Adding requests:  86%|████████▌ | 1752/2048 [00:04<00:00, 505.18it/s]
Adding requests:  88%|████████▊ | 1804/2048 [00:04<00:00, 509.12it/s]
Adding requests:  91%|█████████ | 1857/2048 [00:04<00:00, 514.43it/s]
Adding requests:  93%|█████████▎| 1910/2048 [00:04<00:00, 518.33it/s]
Adding requests:  96%|█████████▌| 1962/2048 [00:04<00:00, 515.34it/s]
Adding requests:  98%|█████████▊| 2017/2048 [00:04<00:00, 525.45it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 447.12it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 50/2048 [00:00<00:09, 204.01it/s, est. speed input: 208946.75 toks/s, output: 204.02 toks/s]
Processed prompts:   3%|▎         | 71/2048 [00:01<00:47, 42.02it/s, est. speed input: 51704.14 toks/s, output: 50.49 toks/s]   
Processed prompts:   4%|▍         | 82/2048 [00:02<01:22, 23.97it/s, est. speed input: 32698.53 toks/s, output: 31.93 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:03<01:40, 19.49it/s, est. speed input: 26933.90 toks/s, output: 26.30 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:04<01:51, 17.28it/s, est. speed input: 23888.25 toks/s, output: 23.33 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:06<01:59, 16.05it/s, est. speed input: 22022.48 toks/s, output: 21.51 toks/s]
Processed prompts:   7%|▋         | 146/2048 [00:07<02:04, 15.29it/s, est. speed input: 20753.90 toks/s, output: 20.27 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:08<02:07, 14.80it/s, est. speed input: 19833.17 toks/s, output: 19.37 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:09<02:09, 14.46it/s, est. speed input: 19127.39 toks/s, output: 18.68 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:10<02:10, 14.25it/s, est. speed input: 18584.58 toks/s, output: 18.15 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:11<02:10, 14.10it/s, est. speed input: 18143.38 toks/s, output: 17.72 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:13<02:09, 14.03it/s, est. speed input: 17792.86 toks/s, output: 17.38 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:14<02:09, 13.95it/s, est. speed input: 17490.72 toks/s, output: 17.08 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:15<02:08, 13.91it/s, est. speed input: 17236.82 toks/s, output: 16.83 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:16<02:07, 13.87it/s, est. speed input: 17016.70 toks/s, output: 16.62 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:17<02:06, 13.84it/s, est. speed input: 16826.32 toks/s, output: 16.43 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:18<02:05, 13.83it/s, est. speed input: 16658.98 toks/s, output: 16.27 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:19<02:04, 13.82it/s, est. speed input: 16512.71 toks/s, output: 16.13 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:21<02:02, 13.99it/s, est. speed input: 16419.12 toks/s, output: 16.03 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:22<02:01, 13.91it/s, est. speed input: 16296.49 toks/s, output: 15.91 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:23<02:00, 13.88it/s, est. speed input: 16189.36 toks/s, output: 15.81 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:24<02:00, 13.85it/s, est. speed input: 16090.83 toks/s, output: 15.71 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:25<01:59, 13.83it/s, est. speed input: 16001.14 toks/s, output: 15.63 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:26<01:57, 13.82it/s, est. speed input: 15920.55 toks/s, output: 15.55 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:28<01:56, 13.81it/s, est. speed input: 15845.49 toks/s, output: 15.47 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:29<01:54, 14.01it/s, est. speed input: 15807.92 toks/s, output: 15.44 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:30<01:53, 13.95it/s, est. speed input: 15744.73 toks/s, output: 15.38 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:31<01:52, 13.90it/s, est. speed input: 15683.75 toks/s, output: 15.32 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:32<01:51, 13.85it/s, est. speed input: 15626.19 toks/s, output: 15.26 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:33<01:51, 13.82it/s, est. speed input: 15572.58 toks/s, output: 15.21 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:34<01:50, 13.78it/s, est. speed input: 15521.32 toks/s, output: 15.16 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:36<01:48, 13.78it/s, est. speed input: 15476.25 toks/s, output: 15.11 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:37<01:47, 13.78it/s, est. speed input: 15432.78 toks/s, output: 15.07 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:38<01:46, 13.77it/s, est. speed input: 15392.14 toks/s, output: 15.03 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:39<01:45, 13.76it/s, est. speed input: 15353.07 toks/s, output: 14.99 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:40<01:44, 13.77it/s, est. speed input: 15317.83 toks/s, output: 14.96 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:41<01:43, 13.76it/s, est. speed input: 15283.28 toks/s, output: 14.93 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:43<01:42, 13.77it/s, est. speed input: 15251.61 toks/s, output: 14.89 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:44<01:40, 13.76it/s, est. speed input: 15221.09 toks/s, output: 14.86 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:45<01:39, 13.76it/s, est. speed input: 15191.92 toks/s, output: 14.84 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:46<01:38, 13.75it/s, est. speed input: 15163.71 toks/s, output: 14.81 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:47<01:37, 13.73it/s, est. speed input: 15135.44 toks/s, output: 14.78 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:48<01:36, 13.73it/s, est. speed input: 15109.98 toks/s, output: 14.76 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:50<01:35, 13.73it/s, est. speed input: 15085.63 toks/s, output: 14.73 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:51<01:34, 13.74it/s, est. speed input: 15063.39 toks/s, output: 14.71 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:52<01:32, 13.76it/s, est. speed input: 15042.36 toks/s, output: 14.69 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:53<01:31, 13.75it/s, est. speed input: 15020.85 toks/s, output: 14.67 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:54<01:30, 13.73it/s, est. speed input: 14999.33 toks/s, output: 14.65 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:55<01:29, 13.74it/s, est. speed input: 14980.72 toks/s, output: 14.63 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:57<01:28, 13.76it/s, est. speed input: 14963.38 toks/s, output: 14.61 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:58<01:27, 13.75it/s, est. speed input: 14945.28 toks/s, output: 14.59 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:59<01:25, 13.76it/s, est. speed input: 14928.80 toks/s, output: 14.58 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [01:00<01:24, 13.75it/s, est. speed input: 14912.10 toks/s, output: 14.56 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [01:01<01:23, 13.74it/s, est. speed input: 14896.03 toks/s, output: 14.55 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [01:02<01:22, 13.76it/s, est. speed input: 14881.59 toks/s, output: 14.53 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [01:03<01:19, 14.01it/s, est. speed input: 14883.50 toks/s, output: 14.53 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [01:05<01:19, 13.93it/s, est. speed input: 14868.99 toks/s, output: 14.52 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [01:06<01:18, 13.88it/s, est. speed input: 14855.42 toks/s, output: 14.51 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [01:07<01:15, 14.12it/s, est. speed input: 14858.82 toks/s, output: 14.51 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [01:08<01:15, 14.01it/s, est. speed input: 14845.82 toks/s, output: 14.50 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [01:09<01:14, 13.93it/s, est. speed input: 14833.16 toks/s, output: 14.49 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [01:10<01:13, 13.89it/s, est. speed input: 14821.30 toks/s, output: 14.47 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [01:12<01:12, 13.85it/s, est. speed input: 14809.66 toks/s, output: 14.46 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [01:13<01:11, 13.82it/s, est. speed input: 14798.00 toks/s, output: 14.45 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [01:14<01:10, 13.79it/s, est. speed input: 14786.45 toks/s, output: 14.44 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [01:15<01:09, 13.79it/s, est. speed input: 14775.97 toks/s, output: 14.43 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [01:16<01:08, 13.77it/s, est. speed input: 14765.20 toks/s, output: 14.42 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [01:17<01:07, 13.76it/s, est. speed input: 14754.85 toks/s, output: 14.41 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [01:19<01:06, 13.76it/s, est. speed input: 14745.07 toks/s, output: 14.40 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [01:20<01:03, 14.03it/s, est. speed input: 14749.03 toks/s, output: 14.40 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [01:21<01:03, 13.94it/s, est. speed input: 14739.14 toks/s, output: 14.39 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [01:22<01:02, 13.89it/s, est. speed input: 14730.50 toks/s, output: 14.39 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [01:23<01:01, 13.84it/s, est. speed input: 14721.05 toks/s, output: 14.38 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [01:24<01:00, 13.81it/s, est. speed input: 14712.24 toks/s, output: 14.37 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [01:25<00:59, 13.80it/s, est. speed input: 14703.69 toks/s, output: 14.36 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [01:27<00:57, 13.79it/s, est. speed input: 14695.85 toks/s, output: 14.35 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [01:28<00:55, 14.04it/s, est. speed input: 14699.64 toks/s, output: 14.36 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [01:29<00:54, 13.95it/s, est. speed input: 14691.33 toks/s, output: 14.35 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [01:30<00:53, 14.15it/s, est. speed input: 14695.08 toks/s, output: 14.35 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [01:31<00:52, 14.04it/s, est. speed input: 14687.73 toks/s, output: 14.34 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [01:32<00:51, 13.96it/s, est. speed input: 14680.31 toks/s, output: 14.34 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [01:33<00:50, 13.89it/s, est. speed input: 14672.83 toks/s, output: 14.33 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [01:35<00:49, 13.86it/s, est. speed input: 14665.91 toks/s, output: 14.32 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [01:36<00:48, 13.81it/s, est. speed input: 14658.39 toks/s, output: 14.31 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [01:37<00:47, 13.79it/s, est. speed input: 14651.07 toks/s, output: 14.31 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [01:38<00:46, 13.78it/s, est. speed input: 14644.69 toks/s, output: 14.30 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [01:39<00:45, 13.78it/s, est. speed input: 14638.34 toks/s, output: 14.30 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [01:40<00:43, 13.78it/s, est. speed input: 14632.50 toks/s, output: 14.29 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [01:42<00:42, 13.75it/s, est. speed input: 14625.11 toks/s, output: 14.28 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [01:43<00:41, 13.77it/s, est. speed input: 14619.64 toks/s, output: 14.28 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [01:44<00:40, 13.76it/s, est. speed input: 14613.72 toks/s, output: 14.27 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [01:45<00:39, 13.78it/s, est. speed input: 14608.46 toks/s, output: 14.27 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [01:46<00:38, 13.77it/s, est. speed input: 14602.83 toks/s, output: 14.26 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [01:47<00:37, 13.76it/s, est. speed input: 14597.04 toks/s, output: 14.25 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [01:49<00:35, 13.76it/s, est. speed input: 14591.57 toks/s, output: 14.25 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [01:50<00:34, 13.75it/s, est. speed input: 14586.00 toks/s, output: 14.24 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [01:51<00:32, 14.02it/s, est. speed input: 14590.40 toks/s, output: 14.25 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [01:52<00:31, 13.95it/s, est. speed input: 14585.42 toks/s, output: 14.24 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [01:53<00:30, 13.88it/s, est. speed input: 14580.13 toks/s, output: 14.24 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [01:54<00:29, 13.82it/s, est. speed input: 14574.28 toks/s, output: 14.23 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [01:55<00:28, 14.06it/s, est. speed input: 14578.43 toks/s, output: 14.24 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [01:57<00:27, 13.97it/s, est. speed input: 14573.43 toks/s, output: 14.23 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [01:58<00:26, 13.90it/s, est. speed input: 14568.41 toks/s, output: 14.23 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [01:59<00:25, 13.86it/s, est. speed input: 14563.79 toks/s, output: 14.22 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [02:00<00:24, 13.82it/s, est. speed input: 14558.97 toks/s, output: 14.22 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [02:01<00:23, 13.81it/s, est. speed input: 14554.83 toks/s, output: 14.21 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [02:02<00:21, 13.80it/s, est. speed input: 14550.40 toks/s, output: 14.21 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [02:04<00:20, 13.79it/s, est. speed input: 14546.12 toks/s, output: 14.21 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [02:05<00:19, 13.77it/s, est. speed input: 14541.72 toks/s, output: 14.20 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [02:06<00:18, 13.76it/s, est. speed input: 14537.36 toks/s, output: 14.20 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [02:07<00:17, 13.76it/s, est. speed input: 14533.21 toks/s, output: 14.19 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [02:08<00:16, 13.76it/s, est. speed input: 14529.26 toks/s, output: 14.19 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [02:09<00:14, 13.76it/s, est. speed input: 14525.21 toks/s, output: 14.18 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [02:11<00:13, 13.76it/s, est. speed input: 14521.25 toks/s, output: 14.18 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [02:12<00:12, 14.02it/s, est. speed input: 14525.48 toks/s, output: 14.19 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [02:13<00:11, 13.94it/s, est. speed input: 14521.58 toks/s, output: 14.18 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [02:14<00:10, 13.89it/s, est. speed input: 14518.08 toks/s, output: 14.18 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [02:15<00:09, 13.84it/s, est. speed input: 14514.06 toks/s, output: 14.17 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [02:16<00:07, 13.82it/s, est. speed input: 14510.44 toks/s, output: 14.17 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [02:17<00:06, 14.07it/s, est. speed input: 14514.66 toks/s, output: 14.17 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [02:19<00:05, 13.98it/s, est. speed input: 14511.31 toks/s, output: 14.17 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [02:20<00:04, 14.18it/s, est. speed input: 14515.22 toks/s, output: 14.18 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [02:21<00:03, 14.04it/s, est. speed input: 14511.35 toks/s, output: 14.17 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [02:22<00:02, 13.95it/s, est. speed input: 14507.90 toks/s, output: 14.17 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [02:23<00:00, 14.09it/s, est. speed input: 14509.91 toks/s, output: 14.17 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:23<00:00, 14.09it/s, est. speed input: 14609.77 toks/s, output: 14.27 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [02:23<00:00, 14.27it/s, est. speed input: 14609.77 toks/s, output: 14.27 toks/s]
[rank0]:[W128 09:39:29.746472371 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 190.5s

测试结果:
  Requests/s:   13.83
  Tokens/s:     14171.71
  Total Reqs:   2048
  Elapsed:      148.13s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     14157.88

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:39:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:39:46 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3757986) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3757986) WARNING 01-28 09:40:20 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.00 requests/s, 14346.47 total tokens/s, 14.00 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 09:39:46] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:39:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:39:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:39:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:39:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:39:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:39:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:39:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:39:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:39:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:39:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:39:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:39:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:39:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:39:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:39:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:39:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:39:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:39:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:39:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:39:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:39:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:39:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:39:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:39:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:39:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:39:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:39:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3757986) [2026-01-28 09:39:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3757986) [2026-01-28 09:39:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3757986) [2026-01-28 09:39:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3757986) [2026-01-28 09:39:51] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3757986) [2026-01-28 09:39:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3757986) [2026-01-28 09:39:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3757986) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3757986) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.57s/it]
(EngineCore_DP0 pid=3757986) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.58s/it]
(EngineCore_DP0 pid=3757986) 
(EngineCore_DP0 pid=3757986) [2026-01-28 09:40:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3757986) [2026-01-28 09:40:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3757986) [2026-01-28 09:40:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3757986) [2026-01-28 09:40:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3757986) [2026-01-28 09:40:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3757986) [2026-01-28 09:40:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3757986) [2026-01-28 09:40:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3757986) [2026-01-28 09:40:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3757986) 2026-01-28 09:40:17,379 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3757986) 2026-01-28 09:40:17,569 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 64/4096 [00:00<00:06, 638.08it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:06, 608.62it/s]
Adding requests:   5%|▍         | 189/4096 [00:00<00:07, 551.07it/s]
Adding requests:   6%|▌         | 245/4096 [00:00<00:07, 539.60it/s]
Adding requests:   7%|▋         | 300/4096 [00:00<00:07, 523.70it/s]
Adding requests:   9%|▊         | 356/4096 [00:00<00:06, 534.39it/s]
Adding requests:  10%|█         | 411/4096 [00:00<00:06, 535.79it/s]
Adding requests:  11%|█▏        | 465/4096 [00:00<00:06, 532.96it/s]
Adding requests:  13%|█▎        | 519/4096 [00:00<00:06, 522.04it/s]
Adding requests:  14%|█▍        | 572/4096 [00:01<00:06, 521.76it/s]
Adding requests:  15%|█▌        | 625/4096 [00:01<00:06, 511.52it/s]
Adding requests:  17%|█▋        | 677/4096 [00:01<00:06, 512.94it/s]
Adding requests:  18%|█▊        | 731/4096 [00:01<00:06, 518.15it/s]
Adding requests:  19%|█▉        | 783/4096 [00:01<00:06, 508.83it/s]
Adding requests:  20%|██        | 834/4096 [00:01<00:06, 501.46it/s]
Adding requests:  22%|██▏       | 892/4096 [00:01<00:06, 523.61it/s]
Adding requests:  23%|██▎       | 945/4096 [00:01<00:06, 517.83it/s]
Adding requests:  24%|██▍       | 997/4096 [00:01<00:05, 517.25it/s]
Adding requests:  26%|██▌       | 1051/4096 [00:01<00:05, 523.30it/s]
Adding requests:  27%|██▋       | 1104/4096 [00:02<00:05, 514.29it/s]
Adding requests:  28%|██▊       | 1157/4096 [00:02<00:05, 515.96it/s]
Adding requests:  30%|██▉       | 1209/4096 [00:02<00:05, 496.30it/s]
Adding requests:  31%|███       | 1259/4096 [00:02<00:05, 496.30it/s]
Adding requests:  32%|███▏      | 1309/4096 [00:02<00:05, 492.80it/s]
Adding requests:  33%|███▎      | 1359/4096 [00:02<00:05, 487.87it/s]
Adding requests:  34%|███▍      | 1412/4096 [00:02<00:05, 496.78it/s]
Adding requests:  36%|███▌      | 1464/4096 [00:02<00:05, 502.18it/s]
Adding requests:  37%|███▋      | 1515/4096 [00:02<00:05, 504.08it/s]
Adding requests:  38%|███▊      | 1568/4096 [00:03<00:04, 511.47it/s]
Adding requests:  40%|███▉      | 1620/4096 [00:03<00:04, 507.17it/s]
Adding requests:  41%|████      | 1671/4096 [00:03<00:04, 501.88it/s]
Adding requests:  42%|████▏     | 1725/4096 [00:03<00:04, 509.44it/s]
Adding requests:  43%|████▎     | 1776/4096 [00:03<00:04, 506.79it/s]
Adding requests:  45%|████▍     | 1827/4096 [00:03<00:04, 506.02it/s]
Adding requests:  46%|████▌     | 1880/4096 [00:03<00:04, 511.57it/s]
Adding requests:  47%|████▋     | 1933/4096 [00:03<00:04, 516.09it/s]
Adding requests:  48%|████▊     | 1985/4096 [00:03<00:04, 510.92it/s]
Adding requests:  50%|████▉     | 2037/4096 [00:03<00:04, 510.47it/s]
Adding requests:  51%|█████     | 2089/4096 [00:04<00:03, 512.23it/s]
Adding requests:  52%|█████▏    | 2141/4096 [00:04<00:03, 506.78it/s]
Adding requests:  54%|█████▎    | 2192/4096 [00:04<00:03, 500.66it/s]
Adding requests:  55%|█████▍    | 2245/4096 [00:04<00:03, 508.69it/s]
Adding requests:  56%|█████▌    | 2298/4096 [00:04<00:03, 513.70it/s]
Adding requests:  57%|█████▋    | 2350/4096 [00:04<00:03, 513.80it/s]
Adding requests:  59%|█████▊    | 2404/4096 [00:04<00:03, 520.14it/s]
Adding requests:  60%|█████▉    | 2457/4096 [00:04<00:03, 499.64it/s]
Adding requests:  61%|██████▏   | 2510/4096 [00:04<00:03, 506.23it/s]
Adding requests:  63%|██████▎   | 2561/4096 [00:04<00:03, 506.17it/s]
Adding requests:  64%|██████▍   | 2612/4096 [00:05<00:02, 507.24it/s]
Adding requests:  65%|██████▌   | 2667/4096 [00:05<00:02, 516.93it/s]
Adding requests:  66%|██████▋   | 2719/4096 [00:05<00:02, 514.51it/s]
Adding requests:  68%|██████▊   | 2771/4096 [00:05<00:02, 508.77it/s]
Adding requests:  69%|██████▉   | 2823/4096 [00:05<00:02, 511.42it/s]
Adding requests:  70%|███████   | 2878/4096 [00:05<00:02, 522.51it/s]
Adding requests:  72%|███████▏  | 2931/4096 [00:05<00:02, 511.92it/s]
Adding requests:  73%|███████▎  | 2984/4096 [00:05<00:02, 516.04it/s]
Adding requests:  74%|███████▍  | 3036/4096 [00:05<00:02, 511.95it/s]
Adding requests:  75%|███████▌  | 3088/4096 [00:06<00:01, 514.11it/s]
Adding requests:  77%|███████▋  | 3140/4096 [00:06<00:01, 509.45it/s]
Adding requests:  78%|███████▊  | 3192/4096 [00:06<00:01, 511.65it/s]
Adding requests:  79%|███████▉  | 3244/4096 [00:06<00:01, 509.03it/s]
Adding requests:  81%|████████  | 3298/4096 [00:06<00:01, 517.21it/s]
Adding requests:  82%|████████▏ | 3351/4096 [00:06<00:01, 518.72it/s]
Adding requests:  83%|████████▎ | 3403/4096 [00:06<00:01, 511.81it/s]
Adding requests:  84%|████████▍ | 3457/4096 [00:06<00:01, 516.00it/s]
Adding requests:  86%|████████▌ | 3509/4096 [00:06<00:01, 517.10it/s]
Adding requests:  87%|████████▋ | 3561/4096 [00:06<00:01, 514.53it/s]
Adding requests:  88%|████████▊ | 3613/4096 [00:07<00:00, 514.24it/s]
Adding requests:  90%|████████▉ | 3667/4096 [00:07<00:00, 520.31it/s]
Adding requests:  91%|█████████ | 3720/4096 [00:07<00:00, 512.38it/s]
Adding requests:  92%|█████████▏| 3774/4096 [00:07<00:00, 518.45it/s]
Adding requests:  93%|█████████▎| 3826/4096 [00:07<00:00, 475.33it/s]
Adding requests:  95%|█████████▍| 3881/4096 [00:07<00:00, 495.42it/s]
Adding requests:  96%|█████████▌| 3935/4096 [00:07<00:00, 506.49it/s]
Adding requests:  97%|█████████▋| 3987/4096 [00:07<00:00, 509.25it/s]
Adding requests:  99%|█████████▊| 4041/4096 [00:07<00:00, 518.07it/s]
Adding requests: 100%|█████████▉| 4095/4096 [00:07<00:00, 522.21it/s]
Adding requests: 100%|██████████| 4096/4096 [00:07<00:00, 513.46it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 98/4096 [00:01<00:57, 69.58it/s, est. speed input: 71248.66 toks/s, output: 69.58 toks/s]
Processed prompts:   3%|▎         | 130/4096 [00:03<02:09, 30.71it/s, est. speed input: 35995.34 toks/s, output: 35.15 toks/s]
Processed prompts:   4%|▍         | 162/4096 [00:05<02:56, 22.32it/s, est. speed input: 27678.34 toks/s, output: 27.03 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:08<03:26, 18.85it/s, est. speed input: 23985.74 toks/s, output: 23.42 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:10<03:47, 17.03it/s, est. speed input: 21890.32 toks/s, output: 21.38 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:12<04:00, 15.96it/s, est. speed input: 20536.32 toks/s, output: 20.05 toks/s]
Processed prompts:   7%|▋         | 290/4096 [00:15<04:08, 15.29it/s, est. speed input: 19589.56 toks/s, output: 19.13 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:17<04:12, 14.96it/s, est. speed input: 18940.95 toks/s, output: 18.50 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:19<04:15, 14.63it/s, est. speed input: 18396.81 toks/s, output: 17.97 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:21<04:17, 14.43it/s, est. speed input: 17970.17 toks/s, output: 17.55 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:24<04:17, 14.28it/s, est. speed input: 17623.46 toks/s, output: 17.21 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:26<04:16, 14.24it/s, est. speed input: 17354.92 toks/s, output: 16.95 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:28<04:15, 14.14it/s, est. speed input: 17108.85 toks/s, output: 16.71 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:31<04:14, 14.08it/s, est. speed input: 16898.06 toks/s, output: 16.50 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:33<04:12, 14.04it/s, est. speed input: 16719.87 toks/s, output: 16.33 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:35<04:11, 14.01it/s, est. speed input: 16562.70 toks/s, output: 16.17 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:38<04:09, 13.99it/s, est. speed input: 16424.88 toks/s, output: 16.04 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:40<04:07, 13.98it/s, est. speed input: 16303.01 toks/s, output: 15.92 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:42<04:05, 13.97it/s, est. speed input: 16193.73 toks/s, output: 15.81 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:44<04:03, 13.95it/s, est. speed input: 16093.56 toks/s, output: 15.72 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:47<04:00, 13.94it/s, est. speed input: 16003.39 toks/s, output: 15.63 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:49<03:58, 13.94it/s, est. speed input: 15923.19 toks/s, output: 15.55 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:51<03:56, 13.93it/s, est. speed input: 15849.17 toks/s, output: 15.48 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:54<03:54, 13.93it/s, est. speed input: 15781.87 toks/s, output: 15.41 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:56<03:51, 13.94it/s, est. speed input: 15721.71 toks/s, output: 15.35 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:58<03:49, 13.93it/s, est. speed input: 15664.22 toks/s, output: 15.30 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [01:00<03:46, 14.01it/s, est. speed input: 15622.01 toks/s, output: 15.26 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [01:03<03:42, 14.06it/s, est. speed input: 15582.59 toks/s, output: 15.22 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [01:05<03:41, 14.02it/s, est. speed input: 15536.13 toks/s, output: 15.17 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [01:07<03:39, 13.99it/s, est. speed input: 15492.36 toks/s, output: 15.13 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [01:10<03:37, 13.97it/s, est. speed input: 15452.55 toks/s, output: 15.09 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [01:12<03:35, 13.96it/s, est. speed input: 15414.49 toks/s, output: 15.05 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [01:14<03:33, 13.94it/s, est. speed input: 15378.38 toks/s, output: 15.02 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [01:16<03:29, 14.01it/s, est. speed input: 15353.32 toks/s, output: 14.99 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [01:19<03:28, 13.98it/s, est. speed input: 15320.77 toks/s, output: 14.96 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [01:21<03:26, 13.97it/s, est. speed input: 15291.35 toks/s, output: 14.93 toks/s]
Processed prompts:  31%|███       | 1250/4096 [01:23<03:22, 14.03it/s, est. speed input: 15270.87 toks/s, output: 14.91 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [01:26<03:19, 14.08it/s, est. speed input: 15251.79 toks/s, output: 14.89 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [01:28<03:18, 14.03it/s, est. speed input: 15225.68 toks/s, output: 14.87 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [01:30<03:16, 14.00it/s, est. speed input: 15201.34 toks/s, output: 14.85 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [01:32<03:14, 13.96it/s, est. speed input: 15176.50 toks/s, output: 14.82 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [01:35<03:12, 13.95it/s, est. speed input: 15154.18 toks/s, output: 14.80 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [01:37<03:10, 13.94it/s, est. speed input: 15133.36 toks/s, output: 14.78 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [01:39<03:08, 13.94it/s, est. speed input: 15113.49 toks/s, output: 14.76 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [01:42<03:05, 13.93it/s, est. speed input: 15094.00 toks/s, output: 14.74 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [01:44<03:03, 13.93it/s, est. speed input: 15075.70 toks/s, output: 14.72 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [01:46<03:00, 14.01it/s, est. speed input: 15064.34 toks/s, output: 14.71 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [01:49<02:58, 13.97it/s, est. speed input: 15046.60 toks/s, output: 14.69 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [01:51<02:55, 14.04it/s, est. speed input: 15036.49 toks/s, output: 14.68 toks/s]
Processed prompts:  41%|████      | 1666/4096 [01:53<02:53, 14.01it/s, est. speed input: 15020.92 toks/s, output: 14.67 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [01:55<02:51, 13.99it/s, est. speed input: 15006.04 toks/s, output: 14.65 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [01:58<02:49, 13.97it/s, est. speed input: 14991.70 toks/s, output: 14.64 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [02:00<02:47, 13.96it/s, est. speed input: 14978.00 toks/s, output: 14.63 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [02:02<02:45, 13.94it/s, est. speed input: 14963.82 toks/s, output: 14.61 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [02:05<02:42, 13.93it/s, est. speed input: 14950.74 toks/s, output: 14.60 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [02:07<02:39, 14.01it/s, est. speed input: 14943.22 toks/s, output: 14.59 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [02:09<02:37, 13.99it/s, est. speed input: 14931.29 toks/s, output: 14.58 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [02:11<02:35, 13.97it/s, est. speed input: 14919.62 toks/s, output: 14.57 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [02:14<02:32, 14.04it/s, est. speed input: 14913.36 toks/s, output: 14.56 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [02:16<02:29, 14.08it/s, est. speed input: 14906.87 toks/s, output: 14.56 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [02:18<02:28, 14.04it/s, est. speed input: 14896.43 toks/s, output: 14.55 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [02:21<02:26, 14.00it/s, est. speed input: 14885.83 toks/s, output: 14.54 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [02:23<02:24, 13.97it/s, est. speed input: 14875.53 toks/s, output: 14.53 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [02:25<02:21, 13.96it/s, est. speed input: 14865.75 toks/s, output: 14.52 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [02:27<02:19, 13.95it/s, est. speed input: 14856.48 toks/s, output: 14.51 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [02:30<02:17, 13.95it/s, est. speed input: 14847.62 toks/s, output: 14.50 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [02:32<02:13, 14.17it/s, est. speed input: 14850.71 toks/s, output: 14.50 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [02:34<02:11, 14.10it/s, est. speed input: 14842.27 toks/s, output: 14.49 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [02:36<02:09, 14.12it/s, est. speed input: 14837.20 toks/s, output: 14.49 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [02:39<02:07, 14.06it/s, est. speed input: 14828.80 toks/s, output: 14.48 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [02:41<02:04, 14.09it/s, est. speed input: 14824.45 toks/s, output: 14.48 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [02:43<02:00, 14.29it/s, est. speed input: 14828.50 toks/s, output: 14.48 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [02:45<01:58, 14.26it/s, est. speed input: 14824.41 toks/s, output: 14.48 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [02:48<01:57, 14.16it/s, est. speed input: 14816.66 toks/s, output: 14.47 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [02:50<01:55, 14.09it/s, est. speed input: 14809.14 toks/s, output: 14.46 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [02:52<01:53, 14.11it/s, est. speed input: 14804.98 toks/s, output: 14.46 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [02:55<01:51, 14.04it/s, est. speed input: 14797.10 toks/s, output: 14.45 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [02:57<01:48, 14.08it/s, est. speed input: 14793.61 toks/s, output: 14.45 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [02:59<01:47, 14.03it/s, est. speed input: 14786.79 toks/s, output: 14.44 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [03:01<01:45, 13.99it/s, est. speed input: 14779.84 toks/s, output: 14.43 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [03:04<01:42, 13.97it/s, est. speed input: 14773.09 toks/s, output: 14.43 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [03:06<01:40, 14.03it/s, est. speed input: 14769.97 toks/s, output: 14.42 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [03:08<01:38, 14.00it/s, est. speed input: 14763.69 toks/s, output: 14.42 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [03:11<01:36, 13.97it/s, est. speed input: 14757.29 toks/s, output: 14.41 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [03:13<01:33, 13.94it/s, est. speed input: 14750.97 toks/s, output: 14.41 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [03:15<01:31, 13.93it/s, est. speed input: 14744.94 toks/s, output: 14.40 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [03:17<01:28, 14.01it/s, est. speed input: 14742.54 toks/s, output: 14.40 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [03:20<01:26, 13.98it/s, est. speed input: 14736.86 toks/s, output: 14.39 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [03:22<01:24, 13.96it/s, est. speed input: 14731.20 toks/s, output: 14.39 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [03:24<01:22, 13.93it/s, est. speed input: 14725.14 toks/s, output: 14.38 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [03:27<01:20, 13.93it/s, est. speed input: 14719.99 toks/s, output: 14.37 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [03:29<01:18, 13.92it/s, est. speed input: 14714.76 toks/s, output: 14.37 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [03:31<01:15, 13.92it/s, est. speed input: 14709.72 toks/s, output: 14.36 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [03:34<01:13, 13.92it/s, est. speed input: 14704.83 toks/s, output: 14.36 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [03:36<01:09, 14.17it/s, est. speed input: 14709.26 toks/s, output: 14.36 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [03:38<01:07, 14.17it/s, est. speed input: 14707.25 toks/s, output: 14.36 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [03:40<01:05, 14.08it/s, est. speed input: 14702.01 toks/s, output: 14.36 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [03:43<01:03, 14.03it/s, est. speed input: 14697.46 toks/s, output: 14.35 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [03:45<01:01, 14.08it/s, est. speed input: 14695.67 toks/s, output: 14.35 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [03:47<00:59, 14.03it/s, est. speed input: 14691.10 toks/s, output: 14.35 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [03:49<00:57, 14.00it/s, est. speed input: 14686.82 toks/s, output: 14.34 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [03:52<00:54, 13.98it/s, est. speed input: 14682.61 toks/s, output: 14.34 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [03:54<00:52, 13.95it/s, est. speed input: 14678.05 toks/s, output: 14.33 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [03:56<00:50, 13.93it/s, est. speed input: 14673.60 toks/s, output: 14.33 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [03:59<00:47, 14.00it/s, est. speed input: 14672.18 toks/s, output: 14.33 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [04:01<00:45, 13.98it/s, est. speed input: 14668.09 toks/s, output: 14.32 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [04:03<00:42, 14.21it/s, est. speed input: 14672.40 toks/s, output: 14.33 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [04:05<00:40, 14.13it/s, est. speed input: 14668.57 toks/s, output: 14.32 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [04:08<00:38, 14.06it/s, est. speed input: 14664.63 toks/s, output: 14.32 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [04:10<00:36, 14.01it/s, est. speed input: 14660.67 toks/s, output: 14.32 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [04:12<00:34, 13.96it/s, est. speed input: 14656.35 toks/s, output: 14.31 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [04:15<00:31, 13.95it/s, est. speed input: 14652.82 toks/s, output: 14.31 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [04:17<00:29, 13.95it/s, est. speed input: 14649.29 toks/s, output: 14.31 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [04:19<00:27, 14.01it/s, est. speed input: 14648.02 toks/s, output: 14.30 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [04:21<00:25, 13.98it/s, est. speed input: 14644.59 toks/s, output: 14.30 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [04:24<00:22, 13.97it/s, est. speed input: 14641.41 toks/s, output: 14.30 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [04:26<00:20, 13.94it/s, est. speed input: 14637.72 toks/s, output: 14.29 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [04:28<00:18, 14.01it/s, est. speed input: 14636.63 toks/s, output: 14.29 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [04:31<00:15, 13.98it/s, est. speed input: 14633.42 toks/s, output: 14.29 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [04:33<00:13, 13.96it/s, est. speed input: 14630.03 toks/s, output: 14.29 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [04:35<00:11, 13.95it/s, est. speed input: 14627.08 toks/s, output: 14.28 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [04:37<00:09, 13.94it/s, est. speed input: 14623.83 toks/s, output: 14.28 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [04:40<00:06, 13.94it/s, est. speed input: 14620.90 toks/s, output: 14.28 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [04:42<00:04, 14.27it/s, est. speed input: 14627.22 toks/s, output: 14.28 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [04:44<00:02, 14.24it/s, est. speed input: 14626.31 toks/s, output: 14.28 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:44<00:00, 14.24it/s, est. speed input: 14734.22 toks/s, output: 14.39 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [04:44<00:00, 14.39it/s, est. speed input: 14734.22 toks/s, output: 14.39 toks/s]
[rank0]:[W128 09:45:13.200122521 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 343.5s

测试结果:
  Requests/s:   14.00
  Tokens/s:     14346.47
  Total Reqs:   4096
  Elapsed:      292.64s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     14332.47


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,25.9405,13307.4681,4.9344
1024,1024,1,128,128,14.2319,14587.6825,8.9939
2048,1024,2,256,128,14.3490,14707.6755,17.8410
4096,1024,4,512,128,13.7915,14136.3157,37.1242
8192,1024,8,1024,128,13.7008,14043.3442,74.7400
16384,1024,16,2048,128,13.8261,14171.7056,148.1261
32768,1024,32,4096,128,13.9966,14346.4665,292.6435

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_083917.log
[SUCCESS] bitnet1.58-2b-fp8 Prefill 完成 (3969.2s)

[INFO] Prefill 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 4: 完整 Prefill Benchmark - SUCCESS
Duration: 8043.7 seconds (134.1 minutes)
----------------------------------------------------------------------


======================================================================
TASK 5: 完整 Decode Benchmark
Started: 2026-01-28 09:45:24
======================================================================


------------------------------------------------------------
  Decode Benchmark: bitnet1.58-2b-int8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-int8 --backend cublaslt,cusparselt --stage decode --sparsity 2_4,2_6,2_8,2_10 --M 64,128,256,512

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[WARNING] GPU 架构不被 Triton 支持: GB10 (sm_121a) is not yet supported by Triton/ptxas
[WARNING] 将使用 eager mode (禁用 torch.compile)
[WARNING] 检测到不支持 torch.compile 的 GPU 架构
[WARNING] 自动启用 eager mode

============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA GB10                               ││
│ GPU (short):      GB10                                      │
│ Memory:           119.7 GB                                    │
│ CC:               cc121 (Blackwell)                            │
│ SM Code:          sm_121                                    │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.95.05                                 │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✗ GB10 (sm_121a) is not yet supp          ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-int8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['decode']
  M_prefill:        [64, 128, 256, 512]
  M_decode:         [64, 128, 256, 512]
  GPU 内存利用率:   0.8
  编译模式:         Eager

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_094526.log


============================================================
  BitNet-2B-INT8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:45:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:45:30 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3763201) WARNING 01-28 09:45:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.05 requests/s, 2462.92 total tokens/s, 2318.05 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 09:45:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:45:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:45:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:45:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:45:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:45:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:45:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:45:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:45:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:45:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:45:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:45:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:45:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:45:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:45:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:45:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:45:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:45:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3763201) [2026-01-28 09:45:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3763201) [2026-01-28 09:45:34] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3763201) [2026-01-28 09:45:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3763201) [2026-01-28 09:45:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3763201) [2026-01-28 09:45:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3763201) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3763201) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.20s/it]
(EngineCore_DP0 pid=3763201) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.20s/it]
(EngineCore_DP0 pid=3763201) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3763201) 2026-01-28 09:45:52,741 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3763201) 2026-01-28 09:45:52,760 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 11644.78it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:06<07:17,  6.94s/it, est. speed input: 2.31 toks/s, output: 36.88 toks/s]
Processed prompts:  75%|███████▌  | 48/64 [00:07<00:01,  9.62it/s, est. speed input: 109.06 toks/s, output: 1744.98 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:07<00:00,  9.62it/s, est. speed input: 145.01 toks/s, output: 2320.18 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:07<00:00,  9.06it/s, est. speed input: 145.01 toks/s, output: 2320.18 toks/s]
[rank0]:[W128 09:46:00.632105800 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.8s

测试结果:
  Requests/s:   9.05
  Tokens/s:     2462.92
  Total Reqs:   64
  Elapsed:      7.07s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      2318.05

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:46:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:46:06 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3763892) WARNING 01-28 09:46:28 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 13.64 requests/s, 3709.03 total tokens/s, 3490.85 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 09:46:06] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:46:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:46:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:46:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:46:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:46:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:46:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:46:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:46:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:46:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:46:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:46:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:46:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:46:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:46:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:46:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:46:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:46:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3763892) [2026-01-28 09:46:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3763892) [2026-01-28 09:46:10] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3763892) [2026-01-28 09:46:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3763892) [2026-01-28 09:46:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3763892) [2026-01-28 09:46:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3763892) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3763892) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.57s/it]
(EngineCore_DP0 pid=3763892) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.57s/it]
(EngineCore_DP0 pid=3763892) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3763892) 2026-01-28 09:46:28,121 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3763892) 2026-01-28 09:46:28,139 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 13110.08it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:09<19:11,  9.06s/it, est. speed input: 1.77 toks/s, output: 28.25 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:09<00:18,  5.06it/s, est. speed input: 57.54 toks/s, output: 920.70 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:09<00:02, 16.59it/s, est. speed input: 149.83 toks/s, output: 2397.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 16.59it/s, est. speed input: 218.42 toks/s, output: 3494.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 13.65it/s, est. speed input: 218.42 toks/s, output: 3494.78 toks/s]
[rank0]:[W128 09:46:38.318305961 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 37.7s

测试结果:
  Requests/s:   13.64
  Tokens/s:     3709.03
  Total Reqs:   128
  Elapsed:      9.39s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      3490.85

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:46:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:46:43 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3764729) WARNING 01-28 09:47:05 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 17.70 requests/s, 4814.98 total tokens/s, 4531.75 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 09:46:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:46:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:46:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:46:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:46:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:46:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:46:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:46:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:46:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:46:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:46:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:46:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:46:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:46:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:46:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:46:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:46:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:46:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3764729) [2026-01-28 09:46:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3764729) [2026-01-28 09:46:48] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3764729) [2026-01-28 09:46:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3764729) [2026-01-28 09:46:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3764729) [2026-01-28 09:46:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3764729) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3764729) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.46s/it]
(EngineCore_DP0 pid=3764729) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.46s/it]
(EngineCore_DP0 pid=3764729) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3764729) 2026-01-28 09:47:05,263 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3764729) 2026-01-28 09:47:05,280 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 12691.83it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:12<54:21, 12.79s/it, est. speed input: 1.25 toks/s, output: 20.02 toks/s]
Processed prompts:   7%|▋         | 19/256 [00:12<01:54,  2.06it/s, est. speed input: 23.55 toks/s, output: 376.86 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:13<00:31,  6.52it/s, est. speed input: 59.03 toks/s, output: 944.40 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:13<00:11, 14.68it/s, est. speed input: 105.80 toks/s, output: 1692.83 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:13<00:05, 23.28it/s, est. speed input: 142.13 toks/s, output: 2274.12 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:13<00:03, 32.64it/s, est. speed input: 171.95 toks/s, output: 2751.19 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:13<00:02, 43.83it/s, est. speed input: 199.08 toks/s, output: 3185.30 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:13<00:01, 56.88it/s, est. speed input: 225.43 toks/s, output: 3606.81 toks/s]
Processed prompts:  84%|████████▍ | 215/256 [00:13<00:00, 68.53it/s, est. speed input: 249.37 toks/s, output: 3989.97 toks/s]
Processed prompts:  92%|█████████▏| 235/256 [00:13<00:00, 75.77it/s, est. speed input: 268.94 toks/s, output: 4302.98 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:14<00:00, 70.76it/s, est. speed input: 282.54 toks/s, output: 4520.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:14<00:00, 70.76it/s, est. speed input: 283.65 toks/s, output: 4538.35 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:14<00:00, 17.73it/s, est. speed input: 283.65 toks/s, output: 4538.35 toks/s]
[rank0]:[W128 09:47:20.509213472 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.2s

测试结果:
  Requests/s:   17.70
  Tokens/s:     4814.98
  Total Reqs:   256
  Elapsed:      14.46s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      4531.75

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:47:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:47:26 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3765508) WARNING 01-28 09:47:48 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 17.54 requests/s, 4772.12 total tokens/s, 4491.41 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 09:47:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:47:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:47:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:47:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:47:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:47:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:47:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:47:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:47:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:47:29] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:47:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:47:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:47:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:47:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:47:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:47:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:47:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:47:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3765508) [2026-01-28 09:47:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3765508) [2026-01-28 09:47:30] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3765508) [2026-01-28 09:47:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3765508) [2026-01-28 09:47:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3765508) [2026-01-28 09:47:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3765508) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3765508) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.80s/it]
(EngineCore_DP0 pid=3765508) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.80s/it]
(EngineCore_DP0 pid=3765508) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3765508) 2026-01-28 09:47:48,131 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3765508) 2026-01-28 09:47:48,161 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 13762.30it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:22<3:08:12, 22.10s/it, est. speed input: 0.72 toks/s, output: 11.58 toks/s]
Processed prompts:   1%|          | 5/512 [00:22<28:13,  3.34s/it, est. speed input: 3.58 toks/s, output: 57.33 toks/s]  
Processed prompts:   7%|▋         | 36/512 [00:22<02:35,  3.06it/s, est. speed input: 25.54 toks/s, output: 408.68 toks/s]
Processed prompts:  13%|█▎        | 65/512 [00:22<01:07,  6.58it/s, est. speed input: 45.70 toks/s, output: 731.19 toks/s]
Processed prompts:  18%|█▊        | 93/512 [00:22<00:37, 11.16it/s, est. speed input: 64.83 toks/s, output: 1037.22 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:23<00:23, 16.78it/s, est. speed input: 82.30 toks/s, output: 1316.77 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:23<00:11, 30.90it/s, est. speed input: 114.03 toks/s, output: 1824.49 toks/s]
Processed prompts:  40%|████      | 207/512 [00:23<00:06, 46.53it/s, est. speed input: 141.33 toks/s, output: 2261.30 toks/s]
Processed prompts:  47%|████▋     | 243/512 [00:23<00:04, 63.38it/s, est. speed input: 164.98 toks/s, output: 2639.70 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:23<00:02, 80.64it/s, est. speed input: 185.09 toks/s, output: 2961.49 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:23<00:02, 99.03it/s, est. speed input: 203.10 toks/s, output: 3249.57 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:23<00:01, 123.46it/s, est. speed input: 225.93 toks/s, output: 3614.89 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:24<00:01, 141.29it/s, est. speed input: 244.02 toks/s, output: 3904.33 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:24<00:00, 154.68it/s, est. speed input: 259.39 toks/s, output: 4150.22 toks/s]
Processed prompts:  81%|████████▏ | 416/512 [00:24<00:00, 168.37it/s, est. speed input: 274.07 toks/s, output: 4385.06 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [00:24<00:00, 164.17it/s, est. speed input: 288.03 toks/s, output: 4608.42 toks/s]
Processed prompts:  90%|█████████ | 461/512 [00:24<00:00, 151.21it/s, est. speed input: 299.67 toks/s, output: 4794.72 toks/s]
Processed prompts:  94%|█████████▍| 480/512 [00:24<00:00, 127.10it/s, est. speed input: 309.22 toks/s, output: 4947.55 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [00:25<00:00, 93.72it/s, est. speed input: 315.43 toks/s, output: 5046.93 toks/s] 
Processed prompts:  99%|█████████▉| 509/512 [00:29<00:00, 14.07it/s, est. speed input: 280.34 toks/s, output: 4485.46 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:29<00:00, 14.07it/s, est. speed input: 281.08 toks/s, output: 4497.32 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:29<00:00, 17.57it/s, est. speed input: 281.08 toks/s, output: 4497.32 toks/s]
[rank0]:[W128 09:48:18.179915597 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 57.8s

测试结果:
  Requests/s:   17.54
  Tokens/s:     4772.12
  Total Reqs:   512
  Elapsed:      29.18s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      4491.41


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,9.0549,2462.9232,7.0680
128,16,128,128,256,256,13.6361,3709.0328,9.3868
256,16,256,256,256,256,17.7021,4814.9810,14.4615
512,16,512,512,256,256,17.5446,4772.1229,29.1828

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:48:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:48:23 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3766491) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3766491) WARNING 01-28 09:48:47 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 11.09 requests/s, 3015.79 total tokens/s, 2838.39 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 09:48:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:48:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:48:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:48:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:48:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:48:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:48:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:48:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:48:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:48:27] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:48:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:48:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:48:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:48:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:48:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:48:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:48:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:48:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:28] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:28] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:28] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:28] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3766491) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3766491) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.09s/it]
(EngineCore_DP0 pid=3766491) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.09s/it]
(EngineCore_DP0 pid=3766491) 
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3766491) 2026-01-28 09:48:46,518 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3766491) 2026-01-28 09:48:46,543 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 12167.32it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:05<05:53,  5.62s/it, est. speed input: 2.85 toks/s, output: 45.58 toks/s]
Processed prompts:  56%|█████▋    | 36/64 [00:05<00:03,  8.84it/s, est. speed input: 100.67 toks/s, output: 1610.68 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:05<00:00,  8.84it/s, est. speed input: 177.59 toks/s, output: 2841.46 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:05<00:00, 11.10it/s, est. speed input: 177.59 toks/s, output: 2841.46 toks/s]
[rank0]:[W128 09:48:53.023144263 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.8s

测试结果:
  Requests/s:   11.09
  Tokens/s:     3015.79
  Total Reqs:   64
  Elapsed:      5.77s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      2838.39

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:48:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:48:58 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3767175) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3767175) WARNING 01-28 09:49:21 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.14 requests/s, 4389.73 total tokens/s, 4131.51 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 09:48:58] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:48:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:48:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:48:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:48:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:48:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:48:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:48:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:48:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:49:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:49:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:49:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:49:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:49:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:49:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:49:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:49:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:49:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3767175) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3767175) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.58s/it]
(EngineCore_DP0 pid=3767175) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.58s/it]
(EngineCore_DP0 pid=3767175) 
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3767175) 2026-01-28 09:49:20,973 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3767175) 2026-01-28 09:49:20,985 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 12873.37it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:07<15:50,  7.48s/it, est. speed input: 2.14 toks/s, output: 34.21 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:07<00:13,  6.65it/s, est. speed input: 75.64 toks/s, output: 1210.17 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:07<00:02, 19.83it/s, est. speed input: 181.94 toks/s, output: 2911.01 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 32.38it/s, est. speed input: 258.57 toks/s, output: 4137.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 32.38it/s, est. speed input: 258.57 toks/s, output: 4137.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.16it/s, est. speed input: 258.57 toks/s, output: 4137.16 toks/s]
[rank0]:[W128 09:49:29.779493924 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.8s

测试结果:
  Requests/s:   16.14
  Tokens/s:     4389.73
  Total Reqs:   128
  Elapsed:      7.93s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      4131.51

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:49:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:49:35 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3767892) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3767892) WARNING 01-28 09:49:57 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 20.06 requests/s, 5455.62 total tokens/s, 5134.70 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 09:49:35] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:49:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:49:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:49:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:49:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:49:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:49:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:49:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:49:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:49:38] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:49:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:49:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:49:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:49:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:49:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:49:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:49:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:49:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:39] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:39] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:39] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:39] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:39] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3767892) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3767892) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.33s/it]
(EngineCore_DP0 pid=3767892) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.33s/it]
(EngineCore_DP0 pid=3767892) 
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3767892) 2026-01-28 09:49:57,067 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3767892) 2026-01-28 09:49:57,079 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13279.19it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:11<47:32, 11.18s/it, est. speed input: 1.43 toks/s, output: 22.89 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:11<01:25,  2.73it/s, est. speed input: 31.15 toks/s, output: 498.34 toks/s]
Processed prompts:  20%|█▉        | 51/256 [00:11<00:26,  7.78it/s, est. speed input: 71.44 toks/s, output: 1143.08 toks/s]
Processed prompts:  35%|███▍      | 89/256 [00:11<00:10, 16.69it/s, est. speed input: 123.05 toks/s, output: 1968.72 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:11<00:05, 26.10it/s, est. speed input: 163.83 toks/s, output: 2621.22 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:11<00:03, 36.08it/s, est. speed input: 197.10 toks/s, output: 3153.64 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:11<00:01, 46.96it/s, est. speed input: 224.86 toks/s, output: 3597.74 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:12<00:01, 60.39it/s, est. speed input: 252.14 toks/s, output: 4034.31 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:12<00:00, 73.67it/s, est. speed input: 278.18 toks/s, output: 4450.86 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:12<00:00, 82.60it/s, est. speed input: 300.38 toks/s, output: 4806.15 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:12<00:00, 82.55it/s, est. speed input: 318.07 toks/s, output: 5089.08 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:12<00:00, 82.55it/s, est. speed input: 321.43 toks/s, output: 5142.92 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:12<00:00, 20.09it/s, est. speed input: 321.43 toks/s, output: 5142.92 toks/s]
[rank0]:[W128 09:50:10.711031711 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.0s

测试结果:
  Requests/s:   20.06
  Tokens/s:     5455.62
  Total Reqs:   256
  Elapsed:      12.76s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      5134.70

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:50:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:50:16 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3768701) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3768701) WARNING 01-28 09:50:38 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 20.59 requests/s, 5599.42 total tokens/s, 5270.05 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 09:50:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:50:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:50:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:50:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:50:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:50:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:50:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:50:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:50:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:50:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:50:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:50:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:50:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:50:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:50:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:50:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:50:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:50:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3768701) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3768701) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.50s/it]
(EngineCore_DP0 pid=3768701) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.50s/it]
(EngineCore_DP0 pid=3768701) 
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3768701) 2026-01-28 09:50:38,334 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3768701) 2026-01-28 09:50:38,345 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 14515.32it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:18<2:41:00, 18.91s/it, est. speed input: 0.85 toks/s, output: 13.54 toks/s]
Processed prompts:   1%|          | 6/512 [00:19<19:54,  2.36s/it, est. speed input: 5.02 toks/s, output: 80.32 toks/s]  
Processed prompts:   7%|▋         | 37/512 [00:19<02:10,  3.64it/s, est. speed input: 30.63 toks/s, output: 490.12 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:19<00:57,  7.72it/s, est. speed input: 54.10 toks/s, output: 865.61 toks/s]
Processed prompts:  18%|█▊        | 93/512 [00:19<00:32, 12.82it/s, est. speed input: 75.54 toks/s, output: 1208.65 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:19<00:20, 19.22it/s, est. speed input: 95.78 toks/s, output: 1532.54 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:20<00:09, 35.06it/s, est. speed input: 132.48 toks/s, output: 2119.63 toks/s]
Processed prompts:  40%|████      | 207/512 [00:20<00:05, 52.24it/s, est. speed input: 163.97 toks/s, output: 2623.56 toks/s]
Processed prompts:  47%|████▋     | 243/512 [00:20<00:03, 70.25it/s, est. speed input: 191.18 toks/s, output: 3058.82 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:20<00:02, 87.92it/s, est. speed input: 214.22 toks/s, output: 3427.47 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:20<00:01, 106.84it/s, est. speed input: 234.87 toks/s, output: 3757.93 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:20<00:01, 123.21it/s, est. speed input: 252.94 toks/s, output: 4047.02 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:20<00:01, 142.12it/s, est. speed input: 275.01 toks/s, output: 4400.09 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [00:20<00:00, 157.92it/s, est. speed input: 293.35 toks/s, output: 4693.62 toks/s]
Processed prompts:  79%|███████▉  | 407/512 [00:21<00:00, 162.87it/s, est. speed input: 309.03 toks/s, output: 4944.46 toks/s]
Processed prompts:  84%|████████▍ | 429/512 [00:21<00:00, 167.20it/s, est. speed input: 323.87 toks/s, output: 5181.84 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:21<00:00, 165.38it/s, est. speed input: 337.63 toks/s, output: 5402.13 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:21<00:00, 152.18it/s, est. speed input: 350.00 toks/s, output: 5599.95 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [00:21<00:00, 124.03it/s, est. speed input: 359.64 toks/s, output: 5754.19 toks/s]
Processed prompts:  98%|█████████▊| 503/512 [00:24<00:00, 19.00it/s, est. speed input: 325.45 toks/s, output: 5207.15 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:24<00:00, 19.00it/s, est. speed input: 329.86 toks/s, output: 5277.80 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:24<00:00, 20.62it/s, est. speed input: 329.86 toks/s, output: 5277.80 toks/s]
[rank0]:[W128 09:51:04.039842412 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.3s

测试结果:
  Requests/s:   20.59
  Tokens/s:     5599.42
  Total Reqs:   512
  Elapsed:      24.87s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      5270.05


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,11.0875,3015.7893,5.7723
128,16,128,128,256,256,16.1387,4389.7287,7.9312
256,16,256,256,256,256,20.0574,5455.6230,12.7633
512,16,512,512,256,256,20.5861,5599.4230,24.8711

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:51:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:51:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3769656) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3769656) WARNING 01-28 09:51:36 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 8.97 requests/s, 2438.65 total tokens/s, 2295.20 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 09:51:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:51:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:51:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:51:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:51:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:51:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:51:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:51:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:51:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:51:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:51:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:51:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:51:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:51:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:51:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:51:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:51:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:51:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3769656) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3769656) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.14s/it]
(EngineCore_DP0 pid=3769656) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.14s/it]
(EngineCore_DP0 pid=3769656) 
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3769656) [2026-01-28 09:51:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3769656) 2026-01-28 09:51:35,713 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3769656) 2026-01-28 09:51:35,726 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 12120.08it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:06<07:19,  6.98s/it, est. speed input: 2.29 toks/s, output: 36.70 toks/s]
Processed prompts:  52%|█████▏    | 33/64 [00:07<00:04,  6.54it/s, est. speed input: 74.44 toks/s, output: 1191.00 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:07<00:00,  6.54it/s, est. speed input: 143.58 toks/s, output: 2297.28 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:07<00:00,  8.97it/s, est. speed input: 143.58 toks/s, output: 2297.28 toks/s]
[rank0]:[W128 09:51:43.595182552 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.6s

测试结果:
  Requests/s:   8.97
  Tokens/s:     2438.65
  Total Reqs:   64
  Elapsed:      7.14s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      2295.20

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:51:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:51:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3770458) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3770458) WARNING 01-28 09:52:15 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.31 requests/s, 3891.77 total tokens/s, 3662.84 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 09:51:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:51:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:51:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:51:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:51:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:51:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:51:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:51:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:51:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:51:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:51:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:51:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:51:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:51:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:51:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:51:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:51:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:51:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:51:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3770458) [2026-01-28 09:51:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3770458) [2026-01-28 09:51:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3770458) [2026-01-28 09:51:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3770458) [2026-01-28 09:51:53] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3770458) [2026-01-28 09:51:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3770458) [2026-01-28 09:51:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3770458) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3770458) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.55s/it]
(EngineCore_DP0 pid=3770458) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.55s/it]
(EngineCore_DP0 pid=3770458) 
(EngineCore_DP0 pid=3770458) [2026-01-28 09:52:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3770458) [2026-01-28 09:52:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3770458) [2026-01-28 09:52:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3770458) [2026-01-28 09:52:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3770458) [2026-01-28 09:52:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3770458) [2026-01-28 09:52:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3770458) [2026-01-28 09:52:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3770458) [2026-01-28 09:52:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3770458) 2026-01-28 09:52:14,442 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3770458) 2026-01-28 09:52:14,454 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 11444.70it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:08<17:54,  8.46s/it, est. speed input: 1.89 toks/s, output: 30.25 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:08<00:16,  5.73it/s, est. speed input: 65.14 toks/s, output: 1042.25 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:08<00:03, 14.98it/s, est. speed input: 139.55 toks/s, output: 2232.83 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:08<00:00, 28.04it/s, est. speed input: 215.71 toks/s, output: 3451.31 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 28.04it/s, est. speed input: 229.24 toks/s, output: 3667.82 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.33it/s, est. speed input: 229.24 toks/s, output: 3667.82 toks/s]
[rank0]:[W128 09:52:24.304456423 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.7s

测试结果:
  Requests/s:   14.31
  Tokens/s:     3891.77
  Total Reqs:   128
  Elapsed:      8.95s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      3662.84

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:52:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:52:30 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3771211) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3771211) WARNING 01-28 09:52:56 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.17 requests/s, 4943.38 total tokens/s, 4652.60 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 09:52:29] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:52:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:52:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:52:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:52:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:52:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:52:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:52:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:52:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:52:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:52:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:52:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:52:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:52:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:52:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:52:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:52:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:52:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:52:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:52:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:52:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:52:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:52:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:52:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:52:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:52:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:52:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:52:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:34] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3771211) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3771211) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.56s/it]
(EngineCore_DP0 pid=3771211) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.56s/it]
(EngineCore_DP0 pid=3771211) 
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3771211) [2026-01-28 09:52:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3771211) 2026-01-28 09:52:55,465 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3771211) 2026-01-28 09:52:55,477 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13189.64it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:12<52:12, 12.28s/it, est. speed input: 1.30 toks/s, output: 20.84 toks/s]
Processed prompts:   8%|▊         | 21/256 [00:12<01:39,  2.37it/s, est. speed input: 27.07 toks/s, output: 433.15 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:12<00:29,  6.98it/s, est. speed input: 63.81 toks/s, output: 1020.89 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:12<00:14, 12.62it/s, est. speed input: 96.15 toks/s, output: 1538.39 toks/s]
Processed prompts:  39%|███▊      | 99/256 [00:12<00:08, 19.23it/s, est. speed input: 124.07 toks/s, output: 1985.10 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:12<00:04, 30.27it/s, est. speed input: 158.82 toks/s, output: 2541.13 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:13<00:02, 40.77it/s, est. speed input: 184.60 toks/s, output: 2953.62 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:13<00:01, 53.57it/s, est. speed input: 209.88 toks/s, output: 3358.00 toks/s]
Processed prompts:  75%|███████▌  | 193/256 [00:13<00:00, 66.14it/s, est. speed input: 233.08 toks/s, output: 3729.26 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:13<00:00, 75.08it/s, est. speed input: 253.87 toks/s, output: 4061.94 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:13<00:00, 81.80it/s, est. speed input: 271.03 toks/s, output: 4336.45 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:13<00:00, 79.97it/s, est. speed input: 284.50 toks/s, output: 4552.02 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:14<00:00, 79.97it/s, est. speed input: 291.22 toks/s, output: 4659.49 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:14<00:00, 18.20it/s, est. speed input: 291.22 toks/s, output: 4659.49 toks/s]
[rank0]:[W128 09:53:10.485900876 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 46.2s

测试结果:
  Requests/s:   18.17
  Tokens/s:     4943.38
  Total Reqs:   256
  Elapsed:      14.09s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      4652.60

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:53:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:53:16 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3772043) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3772043) WARNING 01-28 09:53:42 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.86 requests/s, 5129.72 total tokens/s, 4827.97 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 09:53:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:53:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:53:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:53:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:53:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:53:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:53:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:53:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:53:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:53:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:53:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:53:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:53:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:53:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:53:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:53:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:53:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:53:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:53:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:53:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:53:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:53:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:53:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:53:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:53:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:53:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:53:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:53:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3772043) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3772043) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.65s/it]
(EngineCore_DP0 pid=3772043) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.65s/it]
(EngineCore_DP0 pid=3772043) 
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3772043) [2026-01-28 09:53:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3772043) 2026-01-28 09:53:41,754 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3772043) 2026-01-28 09:53:41,765 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 6599.40it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:20<2:51:20, 20.12s/it, est. speed input: 0.80 toks/s, output: 12.72 toks/s]
Processed prompts:   0%|          | 2/512 [00:20<1:11:30,  8.41s/it, est. speed input: 1.57 toks/s, output: 25.17 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:20<02:32,  3.15it/s, est. speed input: 25.70 toks/s, output: 411.14 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:20<01:02,  7.14it/s, est. speed input: 48.58 toks/s, output: 777.29 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:20<00:34, 12.13it/s, est. speed input: 69.53 toks/s, output: 1112.41 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:21<00:21, 18.16it/s, est. speed input: 88.59 toks/s, output: 1417.38 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:21<00:10, 33.16it/s, est. speed input: 123.17 toks/s, output: 1970.64 toks/s]
Processed prompts:  40%|████      | 205/512 [00:21<00:06, 49.32it/s, est. speed input: 152.80 toks/s, output: 2444.88 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:21<00:04, 66.87it/s, est. speed input: 179.15 toks/s, output: 2866.39 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:21<00:02, 82.91it/s, est. speed input: 200.75 toks/s, output: 3212.07 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:21<00:02, 100.91it/s, est. speed input: 220.85 toks/s, output: 3533.57 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:22<00:01, 115.36it/s, est. speed input: 237.78 toks/s, output: 3804.44 toks/s]
Processed prompts:  68%|██████▊   | 349/512 [00:22<00:01, 129.72it/s, est. speed input: 252.59 toks/s, output: 4041.51 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [00:22<00:00, 141.93it/s, est. speed input: 270.33 toks/s, output: 4325.26 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [00:22<00:00, 150.48it/s, est. speed input: 285.23 toks/s, output: 4563.73 toks/s]
Processed prompts:  82%|████████▏ | 419/512 [00:22<00:00, 153.56it/s, est. speed input: 297.90 toks/s, output: 4766.48 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:22<00:00, 150.73it/s, est. speed input: 309.58 toks/s, output: 4953.35 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [00:22<00:00, 141.43it/s, est. speed input: 320.18 toks/s, output: 5122.89 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [00:22<00:00, 126.11it/s, est. speed input: 328.97 toks/s, output: 5263.50 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:23<00:00, 104.48it/s, est. speed input: 335.66 toks/s, output: 5370.63 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:26<00:01, 12.75it/s, est. speed input: 295.17 toks/s, output: 4722.69 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:27<00:00, 12.75it/s, est. speed input: 302.63 toks/s, output: 4842.05 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:27<00:00, 18.91it/s, est. speed input: 302.63 toks/s, output: 4842.05 toks/s]
[rank0]:[W128 09:54:09.738674292 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 59.3s

测试结果:
  Requests/s:   18.86
  Tokens/s:     5129.72
  Total Reqs:   512
  Elapsed:      27.15s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      4827.97


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,8.9656,2438.6494,7.1384
128,16,128,128,256,256,14.3080,3891.7673,8.9461
256,16,256,256,256,256,18.1742,4943.3837,14.0859
512,16,512,512,256,256,18.8593,5129.7160,27.1485

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_8) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:54:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:54:15 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3773058) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3773058) WARNING 01-28 09:54:43 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.65 requests/s, 2623.57 total tokens/s, 2469.25 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 09:54:15] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:54:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:54:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:54:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:54:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:54:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:54:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:54:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:54:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:54:18] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:54:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:54:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:54:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:54:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:54:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:54:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:54:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:54:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3773058) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3773058) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.48s/it]
(EngineCore_DP0 pid=3773058) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.48s/it]
(EngineCore_DP0 pid=3773058) 
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3773058) [2026-01-28 09:54:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3773058) 2026-01-28 09:54:42,619 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3773058) 2026-01-28 09:54:42,641 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 12204.39it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:06<06:47,  6.47s/it, est. speed input: 2.47 toks/s, output: 39.58 toks/s]
Processed prompts:  56%|█████▋    | 36/64 [00:06<00:03,  7.70it/s, est. speed input: 87.58 toks/s, output: 1401.25 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:06<00:00,  7.70it/s, est. speed input: 154.47 toks/s, output: 2471.50 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:06<00:00,  9.65it/s, est. speed input: 154.47 toks/s, output: 2471.50 toks/s]
[rank0]:[W128 09:54:49.012940291 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.2s

测试结果:
  Requests/s:   9.65
  Tokens/s:     2623.57
  Total Reqs:   64
  Elapsed:      6.64s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      2469.25

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:54:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:54:55 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3773803) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3773803) WARNING 01-28 09:55:23 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.51 requests/s, 3946.49 total tokens/s, 3714.34 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 09:54:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:54:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:54:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:54:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:54:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:54:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:54:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:54:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:54:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:54:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:54:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:54:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:54:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:54:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:54:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:54:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:54:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:54:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:54:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3773803) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3773803) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.31s/it]
(EngineCore_DP0 pid=3773803) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.31s/it]
(EngineCore_DP0 pid=3773803) 
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3773803) [2026-01-28 09:55:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3773803) 2026-01-28 09:55:22,871 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3773803) 2026-01-28 09:55:22,883 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 13249.86it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:08<17:41,  8.36s/it, est. speed input: 1.91 toks/s, output: 30.62 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:08<00:15,  5.97it/s, est. speed input: 67.84 toks/s, output: 1085.43 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:08<00:03, 15.16it/s, est. speed input: 141.55 toks/s, output: 2264.77 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:08<00:00, 28.30it/s, est. speed input: 218.43 toks/s, output: 3494.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 28.30it/s, est. speed input: 232.43 toks/s, output: 3718.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.53it/s, est. speed input: 232.43 toks/s, output: 3718.93 toks/s]
[rank0]:[W128 09:55:32.557850165 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.6s

测试结果:
  Requests/s:   14.51
  Tokens/s:     3946.49
  Total Reqs:   128
  Elapsed:      8.82s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      3714.34

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:55:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:55:38 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3774580) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3774580) WARNING 01-28 09:56:06 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.31 requests/s, 4980.61 total tokens/s, 4687.63 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 09:55:38] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:55:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:55:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:55:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:55:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:55:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:55:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:55:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:55:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:55:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:55:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:55:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:55:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:55:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:55:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:55:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:55:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:55:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:55:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:55:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:55:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:55:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:55:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:55:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:55:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:55:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:55:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:55:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3774580) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3774580) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.39s/it]
(EngineCore_DP0 pid=3774580) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.39s/it]
(EngineCore_DP0 pid=3774580) 
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3774580) [2026-01-28 09:55:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3774580) 2026-01-28 09:56:05,407 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3774580) 2026-01-28 09:56:05,419 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13491.93it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:12<51:56, 12.22s/it, est. speed input: 1.31 toks/s, output: 20.94 toks/s]
Processed prompts:   8%|▊         | 21/256 [00:12<01:38,  2.38it/s, est. speed input: 27.21 toks/s, output: 435.34 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:12<00:29,  7.00it/s, est. speed input: 64.10 toks/s, output: 1025.53 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:12<00:14, 12.67it/s, est. speed input: 96.58 toks/s, output: 1545.28 toks/s]
Processed prompts:  39%|███▊      | 99/256 [00:12<00:08, 19.34it/s, est. speed input: 124.69 toks/s, output: 1995.11 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:12<00:04, 30.40it/s, est. speed input: 159.58 toks/s, output: 2553.24 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:12<00:02, 41.02it/s, est. speed input: 185.52 toks/s, output: 2968.28 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:13<00:01, 53.92it/s, est. speed input: 210.93 toks/s, output: 3374.89 toks/s]
Processed prompts:  75%|███████▌  | 193/256 [00:13<00:00, 66.73it/s, est. speed input: 234.30 toks/s, output: 3748.85 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:13<00:00, 76.32it/s, est. speed input: 255.35 toks/s, output: 4085.57 toks/s]
Processed prompts:  90%|█████████ | 231/256 [00:13<00:00, 84.85it/s, est. speed input: 273.91 toks/s, output: 4382.51 toks/s]
Processed prompts:  96%|█████████▋| 247/256 [00:13<00:00, 79.96it/s, est. speed input: 287.88 toks/s, output: 4606.13 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 79.96it/s, est. speed input: 293.40 toks/s, output: 4694.35 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 18.34it/s, est. speed input: 293.40 toks/s, output: 4694.35 toks/s]
[rank0]:[W128 09:56:20.266319179 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.7s

测试结果:
  Requests/s:   18.31
  Tokens/s:     4980.61
  Total Reqs:   256
  Elapsed:      13.98s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      4687.63

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:56:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:56:26 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3775438) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3775438) WARNING 01-28 09:56:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.81 requests/s, 5116.60 total tokens/s, 4815.63 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 09:56:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:56:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:56:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:56:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:56:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:56:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:56:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:56:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:56:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:56:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:56:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:56:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:56:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:56:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:56:29] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:56:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:56:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:56:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:56:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:56:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:56:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:56:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:56:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:56:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:56:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:56:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:56:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:56:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:30] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3775438) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3775438) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.51s/it]
(EngineCore_DP0 pid=3775438) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.51s/it]
(EngineCore_DP0 pid=3775438) 
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3775438) [2026-01-28 09:56:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=3775438) 2026-01-28 09:56:53,210 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3775438) 2026-01-28 09:56:53,221 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 13063.98it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:20<2:53:59, 20.43s/it, est. speed input: 0.78 toks/s, output: 12.53 toks/s]
Processed prompts:   1%|          | 3/512 [00:20<45:38,  5.38s/it, est. speed input: 2.32 toks/s, output: 37.18 toks/s]  
Processed prompts:   7%|▋         | 34/512 [00:20<02:30,  3.17it/s, est. speed input: 26.07 toks/s, output: 417.18 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:21<01:04,  6.97it/s, est. speed input: 47.86 toks/s, output: 765.69 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:21<00:35, 11.90it/s, est. speed input: 68.51 toks/s, output: 1096.23 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:21<00:22, 17.84it/s, est. speed input: 87.28 toks/s, output: 1396.48 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:21<00:10, 32.65it/s, est. speed input: 121.36 toks/s, output: 1941.75 toks/s]
Processed prompts:  40%|████      | 205/512 [00:21<00:06, 48.68it/s, est. speed input: 150.59 toks/s, output: 2409.44 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:21<00:04, 66.07it/s, est. speed input: 176.57 toks/s, output: 2825.07 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:22<00:02, 82.04it/s, est. speed input: 197.88 toks/s, output: 3166.08 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:22<00:02, 100.86it/s, est. speed input: 217.79 toks/s, output: 3484.70 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:22<00:01, 116.00it/s, est. speed input: 234.56 toks/s, output: 3752.97 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:22<00:01, 131.71it/s, est. speed input: 249.90 toks/s, output: 3998.45 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [00:22<00:00, 143.12it/s, est. speed input: 266.77 toks/s, output: 4268.35 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [00:22<00:00, 152.41it/s, est. speed input: 281.54 toks/s, output: 4504.70 toks/s]
Processed prompts:  82%|████████▏ | 420/512 [00:22<00:00, 155.90it/s, est. speed input: 294.73 toks/s, output: 4715.62 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [00:22<00:00, 154.79it/s, est. speed input: 306.99 toks/s, output: 4911.78 toks/s]
Processed prompts:  90%|████████▉ | 459/512 [00:23<00:00, 140.88it/s, est. speed input: 317.90 toks/s, output: 5086.33 toks/s]
Processed prompts:  93%|█████████▎| 475/512 [00:23<00:00, 129.79it/s, est. speed input: 326.79 toks/s, output: 5228.71 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:23<00:00, 101.22it/s, est. speed input: 333.52 toks/s, output: 5336.39 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:27<00:00, 13.62it/s, est. speed input: 295.91 toks/s, output: 4734.60 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:27<00:00, 13.62it/s, est. speed input: 301.42 toks/s, output: 4822.77 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:27<00:00, 18.84it/s, est. speed input: 301.42 toks/s, output: 4822.77 toks/s]
[rank0]:[W128 09:57:21.278652999 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 61.1s

测试结果:
  Requests/s:   18.81
  Tokens/s:     5116.60
  Total Reqs:   512
  Elapsed:      27.22s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      4815.63


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,9.6455,2623.5748,6.6352
128,16,128,128,256,256,14.5091,3946.4863,8.8220
256,16,256,256,256,256,18.3111,4980.6104,13.9806
512,16,512,512,256,256,18.8110,5116.6041,27.2181

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:57:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:57:27 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3776465) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3776465) WARNING 01-28 09:57:56 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 8.91 requests/s, 2424.85 total tokens/s, 2282.21 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 09:57:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:57:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:57:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:57:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:57:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:57:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:57:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:57:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:57:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:57:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:57:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:57:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:57:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:57:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:57:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:57:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:57:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:57:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:57:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:57:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:57:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:57:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:57:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:57:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:57:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:57:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:57:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:57:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3776465) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3776465) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.74s/it]
(EngineCore_DP0 pid=3776465) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.74s/it]
(EngineCore_DP0 pid=3776465) 
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3776465) [2026-01-28 09:57:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3776465) 2026-01-28 09:57:55,899 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3776465) 2026-01-28 09:57:55,911 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 11779.68it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:07<07:23,  7.04s/it, est. speed input: 2.27 toks/s, output: 36.39 toks/s]
Processed prompts:  64%|██████▍   | 41/64 [00:07<00:02,  8.09it/s, est. speed input: 91.85 toks/s, output: 1469.66 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:07<00:00,  8.09it/s, est. speed input: 142.76 toks/s, output: 2284.23 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:07<00:00,  8.92it/s, est. speed input: 142.76 toks/s, output: 2284.23 toks/s]
[rank0]:[W128 09:58:03.810901601 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.5s

测试结果:
  Requests/s:   8.91
  Tokens/s:     2424.85
  Total Reqs:   64
  Elapsed:      7.18s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      2282.21

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:58:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:58:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3777261) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3777261) WARNING 01-28 09:58:38 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.03 requests/s, 3815.53 total tokens/s, 3591.08 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 09:58:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:58:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:58:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:58:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:58:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:58:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:58:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:58:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:58:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:58:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:58:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:58:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:58:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:58:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:58:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:58:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:58:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:58:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3777261) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3777261) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.34s/it]
(EngineCore_DP0 pid=3777261) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.34s/it]
(EngineCore_DP0 pid=3777261) 
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3777261) [2026-01-28 09:58:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3777261) 2026-01-28 09:58:37,733 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3777261) 2026-01-28 09:58:37,744 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4296.89it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:08<18:03,  8.53s/it, est. speed input: 1.88 toks/s, output: 30.02 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:08<07:38,  3.64s/it, est. speed input: 3.66 toks/s, output: 58.58 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:08<00:07, 10.59it/s, est. speed input: 86.60 toks/s, output: 1385.63 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:08<00:01, 22.18it/s, est. speed input: 154.72 toks/s, output: 2475.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 22.18it/s, est. speed input: 225.21 toks/s, output: 3603.33 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 14.08it/s, est. speed input: 225.21 toks/s, output: 3603.33 toks/s]
[rank0]:[W128 09:58:47.767896600 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.0s

测试结果:
  Requests/s:   14.03
  Tokens/s:     3815.53
  Total Reqs:   128
  Elapsed:      9.12s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      3591.08

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:58:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:58:53 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3778051) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3778051) WARNING 01-28 09:59:22 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.02 requests/s, 4902.39 total tokens/s, 4614.01 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 09:58:53] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:58:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:58:53] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:58:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:53] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:53] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:58:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:58:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:58:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:58:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:58:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:58:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:58:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:58:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:58:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:58:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:58:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:58:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:58:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:58:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:58:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3778051) [2026-01-28 09:58:57] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3778051) [2026-01-28 09:58:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3778051) [2026-01-28 09:58:57] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3778051) [2026-01-28 09:58:57] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3778051) [2026-01-28 09:58:57] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3778051) [2026-01-28 09:58:57] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3778051) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3778051) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.50s/it]
(EngineCore_DP0 pid=3778051) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.50s/it]
(EngineCore_DP0 pid=3778051) 
(EngineCore_DP0 pid=3778051) [2026-01-28 09:59:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3778051) [2026-01-28 09:59:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3778051) [2026-01-28 09:59:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3778051) [2026-01-28 09:59:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3778051) [2026-01-28 09:59:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3778051) [2026-01-28 09:59:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3778051) [2026-01-28 09:59:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3778051) [2026-01-28 09:59:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3778051) 2026-01-28 09:59:21,469 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3778051) 2026-01-28 09:59:21,481 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13329.96it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:12<52:39, 12.39s/it, est. speed input: 1.29 toks/s, output: 20.66 toks/s]
Processed prompts:   9%|▉         | 23/256 [00:12<01:30,  2.58it/s, est. speed input: 29.41 toks/s, output: 470.50 toks/s]
Processed prompts:  20%|██        | 52/256 [00:12<00:28,  7.14it/s, est. speed input: 65.81 toks/s, output: 1053.02 toks/s]
Processed prompts:  30%|███       | 78/256 [00:12<00:14, 12.71it/s, est. speed input: 97.81 toks/s, output: 1564.96 toks/s]
Processed prompts:  39%|███▉      | 101/256 [00:12<00:08, 19.30it/s, est. speed input: 125.55 toks/s, output: 2008.77 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:13<00:04, 30.02it/s, est. speed input: 159.77 toks/s, output: 2556.28 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:13<00:02, 40.67it/s, est. speed input: 186.34 toks/s, output: 2981.43 toks/s]
Processed prompts:  70%|██████▉   | 179/256 [00:13<00:01, 55.03it/s, est. speed input: 215.73 toks/s, output: 3451.60 toks/s]
Processed prompts:  78%|███████▊  | 199/256 [00:13<00:00, 66.36it/s, est. speed input: 237.47 toks/s, output: 3799.48 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:13<00:00, 76.81it/s, est. speed input: 257.44 toks/s, output: 4119.10 toks/s]
Processed prompts:  92%|█████████▏| 235/256 [00:13<00:00, 80.81it/s, est. speed input: 273.92 toks/s, output: 4382.68 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:13<00:00, 73.82it/s, est. speed input: 286.03 toks/s, output: 4576.40 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:14<00:00, 73.82it/s, est. speed input: 288.79 toks/s, output: 4620.58 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:14<00:00, 18.05it/s, est. speed input: 288.79 toks/s, output: 4620.58 toks/s]
[rank0]:[W128 09:59:36.451759993 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 48.6s

测试结果:
  Requests/s:   18.02
  Tokens/s:     4902.39
  Total Reqs:   256
  Elapsed:      14.20s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      4614.01

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:59:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:59:42 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3778912) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3778912) WARNING 01-28 10:00:10 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.66 requests/s, 5074.47 total tokens/s, 4775.98 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 09:59:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:59:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:59:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:59:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:59:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:59:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:59:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:59:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:59:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:59:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:59:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:59:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:59:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:59:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:59:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:59:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:59:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:59:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:59:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:59:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:59:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:59:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:59:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:59:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:59:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:59:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:59:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:59:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3778912) [2026-01-28 09:59:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3778912) [2026-01-28 09:59:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3778912) [2026-01-28 09:59:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3778912) [2026-01-28 09:59:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3778912) [2026-01-28 09:59:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3778912) [2026-01-28 09:59:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3778912) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3778912) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.30s/it]
(EngineCore_DP0 pid=3778912) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.30s/it]
(EngineCore_DP0 pid=3778912) 
(EngineCore_DP0 pid=3778912) [2026-01-28 10:00:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3778912) [2026-01-28 10:00:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=3778912) [2026-01-28 10:00:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3778912) [2026-01-28 10:00:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=3778912) [2026-01-28 10:00:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3778912) [2026-01-28 10:00:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=3778912) [2026-01-28 10:00:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3778912) [2026-01-28 10:00:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=3778912) 2026-01-28 10:00:10,023 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3778912) 2026-01-28 10:00:10,034 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 14380.21it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:20<2:53:31, 20.37s/it, est. speed input: 0.79 toks/s, output: 12.57 toks/s]
Processed prompts:   1%|          | 3/512 [00:20<45:30,  5.36s/it, est. speed input: 2.33 toks/s, output: 37.29 toks/s]  
Processed prompts:   7%|▋         | 34/512 [00:20<02:30,  3.18it/s, est. speed input: 26.15 toks/s, output: 418.35 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:21<01:04,  6.98it/s, est. speed input: 47.97 toks/s, output: 767.59 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:21<00:35, 11.92it/s, est. speed input: 68.68 toks/s, output: 1098.86 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:21<00:22, 17.90it/s, est. speed input: 87.52 toks/s, output: 1400.36 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:21<00:10, 32.76it/s, est. speed input: 121.69 toks/s, output: 1947.06 toks/s]
Processed prompts:  40%|████      | 205/512 [00:21<00:06, 48.75it/s, est. speed input: 150.98 toks/s, output: 2415.61 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:21<00:04, 66.06it/s, est. speed input: 176.99 toks/s, output: 2831.91 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:22<00:02, 81.44it/s, est. speed input: 198.27 toks/s, output: 3172.29 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:22<00:02, 99.85it/s, est. speed input: 218.18 toks/s, output: 3490.91 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:22<00:01, 114.81it/s, est. speed input: 234.96 toks/s, output: 3759.40 toks/s]
Processed prompts:  68%|██████▊   | 349/512 [00:22<00:01, 129.42it/s, est. speed input: 249.63 toks/s, output: 3994.09 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [00:22<00:00, 141.46it/s, est. speed input: 267.17 toks/s, output: 4274.66 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [00:22<00:00, 149.89it/s, est. speed input: 281.91 toks/s, output: 4510.48 toks/s]
Processed prompts:  82%|████████▏ | 419/512 [00:22<00:00, 152.30it/s, est. speed input: 294.41 toks/s, output: 4710.58 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:22<00:00, 148.72it/s, est. speed input: 305.93 toks/s, output: 4894.81 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [00:23<00:00, 137.82it/s, est. speed input: 316.31 toks/s, output: 5060.90 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [00:23<00:00, 125.23it/s, est. speed input: 325.09 toks/s, output: 5201.51 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:23<00:00, 106.07it/s, est. speed input: 331.91 toks/s, output: 5310.48 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:27<00:01, 12.55it/s, est. speed input: 291.52 toks/s, output: 4664.38 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:27<00:00, 12.55it/s, est. speed input: 298.90 toks/s, output: 4782.34 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:27<00:00, 18.68it/s, est. speed input: 298.90 toks/s, output: 4782.34 toks/s]
[rank0]:[W128 10:00:38.296420024 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 61.9s

测试结果:
  Requests/s:   18.66
  Tokens/s:     5074.47
  Total Reqs:   512
  Elapsed:      27.44s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      4775.98


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,8.9149,2424.8512,7.1790
128,16,128,128,256,256,14.0277,3815.5271,9.1248
256,16,256,256,256,256,18.0235,4902.3885,14.2037
512,16,512,512,256,256,18.6562,5074.4749,27.4440

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 20 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_094526.log
[SUCCESS] bitnet1.58-2b-int8 Decode 完成 (916.5s)

------------------------------------------------------------
  Decode Benchmark: bitnet1.58-2b-fp8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-fp8 --backend cublaslt,cusparselt --stage decode --sparsity 2_4,2_6,2_8,2_10 --M 64,128,256,512

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[WARNING] GPU 架构不被 Triton 支持: GB10 (sm_121a) is not yet supported by Triton/ptxas
[WARNING] 将使用 eager mode (禁用 torch.compile)
[WARNING] 检测到不支持 torch.compile 的 GPU 架构
[WARNING] 自动启用 eager mode

============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA GB10                               ││
│ GPU (short):      GB10                                      │
│ Memory:           119.7 GB                                    │
│ CC:               cc121 (Blackwell)                            │
│ SM Code:          sm_121                                    │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           580.95.05                                 │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✗ GB10 (sm_121a) is not yet supp          ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-fp8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['decode']
  M_prefill:        [64, 128, 256, 512]
  M_decode:         [64, 128, 256, 512]
  GPU 内存利用率:   0.8
  编译模式:         Eager

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_100043.log


============================================================
  BitNet-2B-FP8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:00:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:00:46 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3780038) WARNING 01-28 10:01:09 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.28 requests/s, 2524.90 total tokens/s, 2376.38 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 10:00:46] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:00:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:00:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:00:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:00:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:00:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:00:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:00:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:00:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:00:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:00:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:00:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:00:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:00:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:00:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:00:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:00:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:00:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:00:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:00:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:00:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:00:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:00:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:00:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:00:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:00:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:00:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:00:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3780038) [2026-01-28 10:00:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3780038) [2026-01-28 10:00:51] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3780038) [2026-01-28 10:00:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3780038) [2026-01-28 10:00:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3780038) [2026-01-28 10:00:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3780038) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3780038) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.87s/it]
(EngineCore_DP0 pid=3780038) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.87s/it]
(EngineCore_DP0 pid=3780038) 
(EngineCore_DP0 pid=3780038) 2026-01-28 10:01:08,952 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3780038) 2026-01-28 10:01:08,978 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 11708.78it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:06<07:06,  6.77s/it, est. speed input: 2.36 toks/s, output: 37.81 toks/s]
Processed prompts:  97%|█████████▋| 62/64 [00:06<00:00, 12.70it/s, est. speed input: 144.02 toks/s, output: 2304.25 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:06<00:00, 12.70it/s, est. speed input: 148.66 toks/s, output: 2378.55 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:06<00:00,  9.29it/s, est. speed input: 148.66 toks/s, output: 2378.55 toks/s]
[rank0]:[W128 10:01:16.589881279 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.4s

测试结果:
  Requests/s:   9.28
  Tokens/s:     2524.90
  Total Reqs:   64
  Elapsed:      6.89s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      2376.38

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:01:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:01:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3780724) WARNING 01-28 10:01:44 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.44 requests/s, 3927.94 total tokens/s, 3696.89 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 10:01:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:01:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:01:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:01:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:01:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:01:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:01:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:01:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:01:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:01:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:01:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:01:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:01:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:01:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:01:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:01:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:01:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:01:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3780724) [2026-01-28 10:01:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3780724) [2026-01-28 10:01:26] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3780724) [2026-01-28 10:01:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3780724) [2026-01-28 10:01:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3780724) [2026-01-28 10:01:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3780724) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3780724) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.41s/it]
(EngineCore_DP0 pid=3780724) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.41s/it]
(EngineCore_DP0 pid=3780724) 
(EngineCore_DP0 pid=3780724) 2026-01-28 10:01:43,871 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3780724) 2026-01-28 10:01:43,886 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 13369.63it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:08<18:05,  8.55s/it, est. speed input: 1.87 toks/s, output: 29.96 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:08<00:16,  5.53it/s, est. speed input: 62.86 toks/s, output: 1005.69 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:08<00:02, 17.50it/s, est. speed input: 158.76 toks/s, output: 2540.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 17.50it/s, est. speed input: 231.32 toks/s, output: 3701.19 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.46it/s, est. speed input: 231.32 toks/s, output: 3701.19 toks/s]
[rank0]:[W128 10:01:53.476993045 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.8s

测试结果:
  Requests/s:   14.44
  Tokens/s:     3927.94
  Total Reqs:   128
  Elapsed:      8.86s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      3696.89

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:01:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:01:59 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3781438) WARNING 01-28 10:02:21 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.39 requests/s, 5001.28 total tokens/s, 4707.09 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 10:01:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:01:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:01:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:01:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:01:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:01:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:01:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:01:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:01:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:01:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:02:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:02:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:02:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:02:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:02:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:02:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:02:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:02:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:02:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3781438) [2026-01-28 10:02:03] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3781438) [2026-01-28 10:02:03] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3781438) [2026-01-28 10:02:03] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3781438) [2026-01-28 10:02:03] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3781438) [2026-01-28 10:02:03] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3781438) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3781438) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.42s/it]
(EngineCore_DP0 pid=3781438) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.42s/it]
(EngineCore_DP0 pid=3781438) 
(EngineCore_DP0 pid=3781438) 2026-01-28 10:02:20,616 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3781438) 2026-01-28 10:02:20,631 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13400.67it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:12<52:08, 12.27s/it, est. speed input: 1.30 toks/s, output: 20.87 toks/s]
Processed prompts:   8%|▊         | 21/256 [00:12<01:38,  2.38it/s, est. speed input: 27.12 toks/s, output: 433.93 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:12<00:29,  7.02it/s, est. speed input: 64.04 toks/s, output: 1024.66 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:12<00:14, 12.71it/s, est. speed input: 96.56 toks/s, output: 1544.90 toks/s]
Processed prompts:  43%|████▎     | 109/256 [00:12<00:06, 22.41it/s, est. speed input: 137.10 toks/s, output: 2193.67 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:12<00:03, 33.08it/s, est. speed input: 170.76 toks/s, output: 2732.22 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:12<00:02, 45.26it/s, est. speed input: 200.30 toks/s, output: 3204.88 toks/s]
Processed prompts:  73%|███████▎  | 187/256 [00:13<00:01, 59.49it/s, est. speed input: 229.02 toks/s, output: 3664.36 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:13<00:00, 72.11it/s, est. speed input: 255.18 toks/s, output: 4082.84 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:13<00:00, 79.47it/s, est. speed input: 276.57 toks/s, output: 4425.09 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:13<00:00, 75.52it/s, est. speed input: 292.06 toks/s, output: 4672.99 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 75.52it/s, est. speed input: 294.62 toks/s, output: 4713.87 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 18.41it/s, est. speed input: 294.62 toks/s, output: 4713.87 toks/s]
[rank0]:[W128 10:02:35.305235251 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.9s

测试结果:
  Requests/s:   18.39
  Tokens/s:     5001.28
  Total Reqs:   256
  Elapsed:      13.92s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      4707.09

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:02:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:02:41 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3782226) WARNING 01-28 10:03:03 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 19.03 requests/s, 5175.72 total tokens/s, 4871.27 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 10:02:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:02:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:02:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:02:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:02:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:02:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:02:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:02:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:02:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:02:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:02:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:02:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:02:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:02:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:02:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:02:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:02:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:02:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:02:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3782226) [2026-01-28 10:02:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3782226) [2026-01-28 10:02:45] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3782226) [2026-01-28 10:02:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3782226) [2026-01-28 10:02:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3782226) [2026-01-28 10:02:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3782226) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3782226) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.37s/it]
(EngineCore_DP0 pid=3782226) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.37s/it]
(EngineCore_DP0 pid=3782226) 
(EngineCore_DP0 pid=3782226) 2026-01-28 10:03:02,476 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3782226) 2026-01-28 10:03:02,492 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 14525.92it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:19<2:49:53, 19.95s/it, est. speed input: 0.80 toks/s, output: 12.83 toks/s]
Processed prompts:   0%|          | 2/512 [00:20<1:10:57,  8.35s/it, est. speed input: 1.59 toks/s, output: 25.38 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:20<02:30,  3.17it/s, est. speed input: 25.90 toks/s, output: 414.43 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:20<01:02,  7.20it/s, est. speed input: 48.97 toks/s, output: 783.58 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:20<00:34, 12.23it/s, est. speed input: 70.09 toks/s, output: 1121.49 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:20<00:21, 18.35it/s, est. speed input: 89.35 toks/s, output: 1429.63 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:21<00:10, 33.83it/s, est. speed input: 124.39 toks/s, output: 1990.21 toks/s]
Processed prompts:  40%|████      | 205/512 [00:21<00:06, 50.83it/s, est. speed input: 154.49 toks/s, output: 2471.84 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:21<00:03, 69.78it/s, est. speed input: 181.32 toks/s, output: 2901.15 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:21<00:02, 87.86it/s, est. speed input: 203.42 toks/s, output: 3254.64 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:21<00:01, 108.42it/s, est. speed input: 223.97 toks/s, output: 3583.52 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:21<00:01, 133.67it/s, est. speed input: 249.05 toks/s, output: 3984.87 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:21<00:00, 150.91it/s, est. speed input: 268.86 toks/s, output: 4301.77 toks/s]
Processed prompts:  77%|███████▋  | 393/512 [00:21<00:00, 165.06it/s, est. speed input: 286.40 toks/s, output: 4582.34 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:22<00:00, 169.15it/s, est. speed input: 302.72 toks/s, output: 4843.50 toks/s]
Processed prompts:  86%|████████▌ | 441/512 [00:22<00:00, 172.48it/s, est. speed input: 317.57 toks/s, output: 5081.09 toks/s]
Processed prompts:  90%|█████████ | 463/512 [00:22<00:00, 152.56it/s, est. speed input: 330.56 toks/s, output: 5288.93 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:22<00:00, 130.12it/s, est. speed input: 340.88 toks/s, output: 5454.11 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:26<00:00, 15.43it/s, est. speed input: 297.40 toks/s, output: 4758.42 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:26<00:00, 15.43it/s, est. speed input: 304.86 toks/s, output: 4877.83 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:26<00:00, 19.05it/s, est. speed input: 304.86 toks/s, output: 4877.83 toks/s]
[rank0]:[W128 10:03:30.222979091 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.9s

测试结果:
  Requests/s:   19.03
  Tokens/s:     5175.72
  Total Reqs:   512
  Elapsed:      26.91s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      4871.27


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,9.2827,2524.9046,6.8945
128,16,128,128,256,256,14.4410,3927.9440,8.8637
256,16,256,256,256,256,18.3871,5001.2796,13.9228
512,16,512,512,256,256,19.0284,5175.7214,26.9072

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:03:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:03:35 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3783177) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3783177) WARNING 01-28 10:03:58 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.69 requests/s, 2908.46 total tokens/s, 2737.37 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 10:03:35] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:03:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:03:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:03:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:03:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:03:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:03:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:03:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:03:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:03:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:03:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:03:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:03:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:03:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:03:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:03:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:03:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:03:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:03:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:03:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:03:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:03:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:03:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:03:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:03:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:03:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:03:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:03:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:40] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:40] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3783177) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3783177) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.75s/it]
(EngineCore_DP0 pid=3783177) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.75s/it]
(EngineCore_DP0 pid=3783177) 
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3783177) [2026-01-28 10:03:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3783177) 2026-01-28 10:03:57,776 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3783177) 2026-01-28 10:03:57,795 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 12038.54it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:05<06:07,  5.83s/it, est. speed input: 2.75 toks/s, output: 43.94 toks/s]
Processed prompts:  52%|█████▏    | 33/64 [00:05<00:03,  7.79it/s, est. speed input: 88.79 toks/s, output: 1420.69 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:05<00:00,  7.79it/s, est. speed input: 171.26 toks/s, output: 2740.17 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:05<00:00, 10.70it/s, est. speed input: 171.26 toks/s, output: 2740.17 toks/s]
[rank0]:[W128 10:04:04.527063937 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.3s

测试结果:
  Requests/s:   10.69
  Tokens/s:     2908.46
  Total Reqs:   64
  Elapsed:      5.99s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      2737.37

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:04:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:04:10 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3783840) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3783840) WARNING 01-28 10:04:32 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.54 requests/s, 4497.72 total tokens/s, 4233.15 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 10:04:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:04:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:04:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:04:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:04:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:04:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:04:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:04:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:04:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:04:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:04:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:04:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:04:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:04:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:04:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:04:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:04:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:04:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3783840) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3783840) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.45s/it]
(EngineCore_DP0 pid=3783840) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.45s/it]
(EngineCore_DP0 pid=3783840) 
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3783840) [2026-01-28 10:04:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3783840) 2026-01-28 10:04:31,856 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3783840) 2026-01-28 10:04:31,872 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4200.01it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:07<15:28,  7.31s/it, est. speed input: 2.19 toks/s, output: 35.02 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:07<00:15,  6.21it/s, est. speed input: 70.83 toks/s, output: 1133.25 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:07<00:03, 17.07it/s, est. speed input: 158.47 toks/s, output: 2535.49 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:07<00:00, 28.35it/s, est. speed input: 225.07 toks/s, output: 3601.05 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 28.35it/s, est. speed input: 265.66 toks/s, output: 4250.56 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.60it/s, est. speed input: 265.66 toks/s, output: 4250.56 toks/s]
[rank0]:[W128 10:04:40.430105675 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.0s

测试结果:
  Requests/s:   16.54
  Tokens/s:     4497.72
  Total Reqs:   128
  Elapsed:      7.74s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      4233.15

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:04:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:04:46 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3784534) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3784534) WARNING 01-28 10:05:08 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 20.18 requests/s, 5488.14 total tokens/s, 5165.31 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 10:04:46] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:04:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:04:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:04:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:04:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:04:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:04:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:04:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:04:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:04:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:04:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:04:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:04:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:04:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:04:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:04:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:04:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:04:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:04:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3784534) [2026-01-28 10:04:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3784534) [2026-01-28 10:04:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3784534) [2026-01-28 10:04:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3784534) [2026-01-28 10:04:50] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3784534) [2026-01-28 10:04:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3784534) [2026-01-28 10:04:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3784534) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3784534) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.70s/it]
(EngineCore_DP0 pid=3784534) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.70s/it]
(EngineCore_DP0 pid=3784534) 
(EngineCore_DP0 pid=3784534) [2026-01-28 10:05:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3784534) [2026-01-28 10:05:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3784534) [2026-01-28 10:05:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3784534) [2026-01-28 10:05:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3784534) [2026-01-28 10:05:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3784534) [2026-01-28 10:05:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3784534) [2026-01-28 10:05:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3784534) [2026-01-28 10:05:02] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3784534) 2026-01-28 10:05:08,042 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3784534) 2026-01-28 10:05:08,057 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13422.44it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:11<46:51, 11.03s/it, est. speed input: 1.45 toks/s, output: 23.22 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:11<01:45,  2.26it/s, est. speed input: 25.83 toks/s, output: 413.26 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:11<00:27,  7.56it/s, est. speed input: 68.14 toks/s, output: 1090.26 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:11<00:12, 13.94it/s, est. speed input: 105.19 toks/s, output: 1683.08 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:11<00:07, 21.10it/s, est. speed input: 135.96 toks/s, output: 2175.37 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:11<00:03, 33.00it/s, est. speed input: 174.24 toks/s, output: 2787.89 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:11<00:02, 45.08it/s, est. speed input: 206.06 toks/s, output: 3297.02 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:11<00:01, 56.79it/s, est. speed input: 231.05 toks/s, output: 3696.76 toks/s]
Processed prompts:  75%|███████▌  | 193/256 [00:12<00:00, 69.06it/s, est. speed input: 256.16 toks/s, output: 4098.60 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:12<00:00, 81.37it/s, est. speed input: 277.51 toks/s, output: 4440.20 toks/s]
Processed prompts:  89%|████████▉ | 229/256 [00:12<00:00, 90.24it/s, est. speed input: 297.72 toks/s, output: 4763.57 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:12<00:00, 90.71it/s, est. speed input: 314.09 toks/s, output: 5025.45 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:12<00:00, 90.71it/s, est. speed input: 323.34 toks/s, output: 5173.43 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:12<00:00, 20.21it/s, est. speed input: 323.34 toks/s, output: 5173.43 toks/s]
[rank0]:[W128 10:05:21.559750660 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.1s

测试结果:
  Requests/s:   20.18
  Tokens/s:     5488.14
  Total Reqs:   256
  Elapsed:      12.69s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      5165.31

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:05:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:05:27 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3785295) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3785295) WARNING 01-28 10:05:49 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 20.17 requests/s, 5487.28 total tokens/s, 5164.50 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 10:05:27] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:05:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:05:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:05:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:05:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:05:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:05:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:05:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:05:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:05:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:05:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:05:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:05:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:05:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:05:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:05:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:05:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:05:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:05:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:05:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:05:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:05:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:05:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:05:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:05:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:05:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:05:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:05:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:31] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:31] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:31] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:31] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:31] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3785295) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3785295) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.38s/it]
(EngineCore_DP0 pid=3785295) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.38s/it]
(EngineCore_DP0 pid=3785295) 
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3785295) [2026-01-28 10:05:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3785295) 2026-01-28 10:05:48,844 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3785295) 2026-01-28 10:05:48,859 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 13870.48it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:19<2:42:30, 19.08s/it, est. speed input: 0.84 toks/s, output: 13.42 toks/s]
Processed prompts:   0%|          | 2/512 [00:19<1:07:52,  7.98s/it, est. speed input: 1.66 toks/s, output: 26.53 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:19<02:24,  3.32it/s, est. speed input: 27.07 toks/s, output: 433.15 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:19<00:59,  7.51it/s, est. speed input: 51.16 toks/s, output: 818.55 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:19<00:33, 12.75it/s, est. speed input: 73.21 toks/s, output: 1171.42 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:20<00:20, 19.02it/s, est. speed input: 93.22 toks/s, output: 1491.56 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:20<00:10, 34.51it/s, est. speed input: 129.48 toks/s, output: 2071.74 toks/s]
Processed prompts:  40%|████      | 205/512 [00:20<00:06, 50.91it/s, est. speed input: 160.49 toks/s, output: 2567.89 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:20<00:03, 68.47it/s, est. speed input: 188.02 toks/s, output: 3008.33 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:20<00:02, 84.13it/s, est. speed input: 210.54 toks/s, output: 3368.67 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:20<00:02, 102.74it/s, est. speed input: 231.61 toks/s, output: 3705.76 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:20<00:01, 115.99it/s, est. speed input: 249.20 toks/s, output: 3987.13 toks/s]
Processed prompts:  68%|██████▊   | 349/512 [00:21<00:01, 130.40it/s, est. speed input: 264.67 toks/s, output: 4234.71 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [00:21<00:00, 141.80it/s, est. speed input: 283.12 toks/s, output: 4529.91 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [00:21<00:00, 152.02it/s, est. speed input: 298.74 toks/s, output: 4779.78 toks/s]
Processed prompts:  82%|████████▏ | 419/512 [00:21<00:00, 151.90it/s, est. speed input: 311.78 toks/s, output: 4988.56 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:21<00:00, 151.56it/s, est. speed input: 324.02 toks/s, output: 5184.34 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [00:21<00:00, 143.34it/s, est. speed input: 335.08 toks/s, output: 5361.31 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [00:21<00:00, 133.49it/s, est. speed input: 344.54 toks/s, output: 5512.62 toks/s]
Processed prompts:  95%|█████████▌| 487/512 [00:22<00:00, 111.28it/s, est. speed input: 352.22 toks/s, output: 5635.50 toks/s]
Processed prompts:  98%|█████████▊| 500/512 [00:25<00:00, 15.65it/s, est. speed input: 316.27 toks/s, output: 5060.31 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:25<00:00, 15.65it/s, est. speed input: 323.27 toks/s, output: 5172.34 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:25<00:00, 20.20it/s, est. speed input: 323.27 toks/s, output: 5172.34 toks/s]
[rank0]:[W128 10:06:15.070561541 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.5s

测试结果:
  Requests/s:   20.17
  Tokens/s:     5487.28
  Total Reqs:   512
  Elapsed:      25.38s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      5164.50


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,10.6929,2908.4599,5.9853
128,16,128,128,256,256,16.5358,4497.7240,7.7408
256,16,256,256,256,256,20.1770,5488.1445,12.6877
512,16,512,512,256,256,20.1738,5487.2830,25.3794

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:06:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:06:20 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3786235) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3786235) WARNING 01-28 10:06:46 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.10 requests/s, 2474.85 total tokens/s, 2329.27 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 10:06:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:06:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:06:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:06:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:06:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:06:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:06:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:06:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:06:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:06:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:06:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:06:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:06:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:06:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:06:24] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:06:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:06:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:06:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:06:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:06:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:06:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:06:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:06:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:06:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:06:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:06:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:06:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:06:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3786235) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3786235) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.85s/it]
(EngineCore_DP0 pid=3786235) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.85s/it]
(EngineCore_DP0 pid=3786235) 
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3786235) [2026-01-28 10:06:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3786235) 2026-01-28 10:06:46,361 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3786235) 2026-01-28 10:06:46,381 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 12126.65it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:06<07:10,  6.83s/it, est. speed input: 2.34 toks/s, output: 37.48 toks/s]
Processed prompts:  56%|█████▋    | 36/64 [00:06<00:03,  7.26it/s, est. speed input: 82.70 toks/s, output: 1323.14 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:07<00:00,  7.26it/s, est. speed input: 145.72 toks/s, output: 2331.52 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:07<00:00,  9.11it/s, est. speed input: 145.72 toks/s, output: 2331.52 toks/s]
[rank0]:[W128 10:06:54.151564866 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.0s

测试结果:
  Requests/s:   9.10
  Tokens/s:     2474.85
  Total Reqs:   64
  Elapsed:      7.03s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      2329.27

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:07:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:07:00 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3786978) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3786978) WARNING 01-28 10:07:25 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.60 requests/s, 3970.06 total tokens/s, 3736.53 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 10:07:00] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:07:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:07:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:07:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:07:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:07:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:07:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:07:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:07:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:07:03] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:07:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:07:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:07:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:07:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:07:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:07:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:07:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:07:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:04] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:04] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:04] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:04] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:04] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3786978) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3786978) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.42s/it]
(EngineCore_DP0 pid=3786978) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.42s/it]
(EngineCore_DP0 pid=3786978) 
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3786978) [2026-01-28 10:07:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3786978) 2026-01-28 10:07:25,015 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3786978) 2026-01-28 10:07:25,031 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 13123.54it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:08<17:39,  8.34s/it, est. speed input: 1.92 toks/s, output: 30.69 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:08<00:14,  6.32it/s, est. speed input: 71.78 toks/s, output: 1148.48 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:08<00:05, 12.46it/s, est. speed input: 121.25 toks/s, output: 1940.05 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:08<00:01, 23.52it/s, est. speed input: 185.79 toks/s, output: 2972.67 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 23.52it/s, est. speed input: 233.83 toks/s, output: 3741.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.61it/s, est. speed input: 233.83 toks/s, output: 3741.20 toks/s]
[rank0]:[W128 10:07:34.680503010 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.6s

测试结果:
  Requests/s:   14.60
  Tokens/s:     3970.06
  Total Reqs:   128
  Elapsed:      8.77s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      3736.53

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:07:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:07:40 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3787730) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3787730) WARNING 01-28 10:08:05 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.40 requests/s, 5003.52 total tokens/s, 4709.20 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 10:07:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:07:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:07:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:07:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:07:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:07:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:07:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:07:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:07:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:07:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:07:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:07:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:07:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:07:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:07:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:07:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:07:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:07:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:07:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:44] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:44] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:44] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:44] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:44] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3787730) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3787730) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.45s/it]
(EngineCore_DP0 pid=3787730) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.45s/it]
(EngineCore_DP0 pid=3787730) 
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3787730) [2026-01-28 10:07:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3787730) 2026-01-28 10:08:05,154 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3787730) 2026-01-28 10:08:05,170 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13598.73it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:12<51:15, 12.06s/it, est. speed input: 1.33 toks/s, output: 21.23 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:12<01:55,  2.06it/s, est. speed input: 23.61 toks/s, output: 377.75 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:12<00:30,  6.91it/s, est. speed input: 62.31 toks/s, output: 996.91 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:12<00:14, 12.79it/s, est. speed input: 96.27 toks/s, output: 1540.30 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:12<00:08, 19.37it/s, est. speed input: 124.47 toks/s, output: 1991.45 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:12<00:05, 26.99it/s, est. speed input: 148.67 toks/s, output: 2378.78 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:12<00:03, 36.25it/s, est. speed input: 171.25 toks/s, output: 2739.93 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:12<00:01, 49.09it/s, est. speed input: 196.80 toks/s, output: 3148.87 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:13<00:01, 60.48it/s, est. speed input: 218.04 toks/s, output: 3488.62 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:13<00:00, 73.03it/s, est. speed input: 237.97 toks/s, output: 3807.59 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:13<00:00, 80.73it/s, est. speed input: 255.66 toks/s, output: 4090.63 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:13<00:00, 86.66it/s, est. speed input: 270.90 toks/s, output: 4334.32 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:13<00:00, 88.58it/s, est. speed input: 284.41 toks/s, output: 4550.50 toks/s]
Processed prompts: 100%|█████████▉| 255/256 [00:13<00:00, 76.10it/s, est. speed input: 294.46 toks/s, output: 4711.28 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 76.10it/s, est. speed input: 294.74 toks/s, output: 4715.90 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 18.42it/s, est. speed input: 294.74 toks/s, output: 4715.90 toks/s]
[rank0]:[W128 10:08:19.942510993 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.2s

测试结果:
  Requests/s:   18.40
  Tokens/s:     5003.52
  Total Reqs:   256
  Elapsed:      13.92s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      4709.20

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:08:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:08:25 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3788547) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3788547) WARNING 01-28 10:08:51 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.77 requests/s, 5106.12 total tokens/s, 4805.76 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 10:08:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:08:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:08:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:08:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:08:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:08:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:08:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:08:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:08:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:08:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:08:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:08:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:08:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:08:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:08:28] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:08:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:08:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:08:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:08:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:08:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:08:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:08:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:08:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:08:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:08:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:08:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:08:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:08:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:29] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:29] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:29] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:29] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:29] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3788547) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3788547) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.43s/it]
(EngineCore_DP0 pid=3788547) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.43s/it]
(EngineCore_DP0 pid=3788547) 
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3788547) [2026-01-28 10:08:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3788547) 2026-01-28 10:08:50,572 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3788547) 2026-01-28 10:08:50,590 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 14425.52it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:19<2:50:18, 20.00s/it, est. speed input: 0.80 toks/s, output: 12.80 toks/s]
Processed prompts:   1%|          | 5/512 [00:20<25:33,  3.03s/it, est. speed input: 3.96 toks/s, output: 63.30 toks/s]  
Processed prompts:   7%|▋         | 36/512 [00:20<02:21,  3.37it/s, est. speed input: 28.19 toks/s, output: 450.99 toks/s]
Processed prompts:  13%|█▎        | 65/512 [00:20<01:01,  7.25it/s, est. speed input: 50.41 toks/s, output: 806.62 toks/s]
Processed prompts:  18%|█▊        | 93/512 [00:20<00:34, 12.27it/s, est. speed input: 71.48 toks/s, output: 1143.74 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:21<00:21, 18.30it/s, est. speed input: 90.61 toks/s, output: 1449.72 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:21<00:10, 33.14it/s, est. speed input: 125.25 toks/s, output: 2004.02 toks/s]
Processed prompts:  40%|████      | 207/512 [00:21<00:06, 48.82it/s, est. speed input: 154.86 toks/s, output: 2477.83 toks/s]
Processed prompts:  47%|████▋     | 243/512 [00:21<00:04, 64.70it/s, est. speed input: 180.34 toks/s, output: 2885.42 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:21<00:02, 79.83it/s, est. speed input: 201.87 toks/s, output: 3229.84 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:21<00:02, 96.97it/s, est. speed input: 221.28 toks/s, output: 3540.51 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:21<00:01, 110.00it/s, est. speed input: 238.10 toks/s, output: 3809.57 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:22<00:01, 122.21it/s, est. speed input: 252.13 toks/s, output: 4034.14 toks/s]
Processed prompts:  72%|███████▏  | 368/512 [00:22<00:01, 134.09it/s, est. speed input: 265.38 toks/s, output: 4246.08 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:22<00:00, 138.00it/s, est. speed input: 280.64 toks/s, output: 4490.29 toks/s]
Processed prompts:  80%|████████  | 412/512 [00:22<00:00, 143.30it/s, est. speed input: 293.33 toks/s, output: 4693.30 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:22<00:00, 147.37it/s, est. speed input: 304.63 toks/s, output: 4874.12 toks/s]
Processed prompts:  88%|████████▊ | 448/512 [00:22<00:00, 141.63it/s, est. speed input: 315.42 toks/s, output: 5046.65 toks/s]
Processed prompts:  91%|█████████ | 464/512 [00:22<00:00, 135.68it/s, est. speed input: 324.78 toks/s, output: 5196.52 toks/s]
Processed prompts:  94%|█████████▎| 479/512 [00:23<00:00, 115.77it/s, est. speed input: 332.57 toks/s, output: 5321.14 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [00:23<00:00, 86.81it/s, est. speed input: 337.64 toks/s, output: 5402.18 toks/s] 
Processed prompts:  98%|█████████▊| 503/512 [00:27<00:00, 11.62it/s, est. speed input: 296.71 toks/s, output: 4747.37 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:27<00:00, 14.11it/s, est. speed input: 300.76 toks/s, output: 4812.22 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:27<00:00, 14.11it/s, est. speed input: 300.76 toks/s, output: 4812.22 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:27<00:00, 18.80it/s, est. speed input: 300.76 toks/s, output: 4812.22 toks/s]
[rank0]:[W128 10:09:18.711269769 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 58.7s

测试结果:
  Requests/s:   18.77
  Tokens/s:     5106.12
  Total Reqs:   512
  Elapsed:      27.27s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      4805.76


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,9.0987,2474.8514,7.0340
128,16,128,128,256,256,14.5958,3970.0650,8.7696
256,16,256,256,256,256,18.3953,5003.5247,13.9166
512,16,512,512,256,256,18.7725,5106.1160,27.2740

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_8) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:09:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:09:24 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3789549) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3789549) WARNING 01-28 10:09:52 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.25 requests/s, 2516.51 total tokens/s, 2368.48 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 10:09:24] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:09:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:09:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:09:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:09:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:09:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:09:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:09:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:09:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:09:27] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:09:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:09:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:09:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:09:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:09:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:09:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:09:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:09:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:28] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:28] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:28] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:28] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:28] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3789549) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3789549) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.70s/it]
(EngineCore_DP0 pid=3789549) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.70s/it]
(EngineCore_DP0 pid=3789549) 
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3789549) 2026-01-28 10:09:51,699 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3789549) 2026-01-28 10:09:51,717 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 12072.65it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:06<07:04,  6.74s/it, est. speed input: 2.37 toks/s, output: 37.99 toks/s]
Processed prompts:  52%|█████▏    | 33/64 [00:06<00:04,  6.74it/s, est. speed input: 76.82 toks/s, output: 1229.11 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:06<00:00,  6.74it/s, est. speed input: 148.16 toks/s, output: 2370.55 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:06<00:00,  9.26it/s, est. speed input: 148.16 toks/s, output: 2370.55 toks/s]
[rank0]:[W128 10:09:59.361733638 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.7s

测试结果:
  Requests/s:   9.25
  Tokens/s:     2516.51
  Total Reqs:   64
  Elapsed:      6.92s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      2368.48

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:10:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:10:05 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3790299) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3790299) WARNING 01-28 10:10:32 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.25 requests/s, 3876.97 total tokens/s, 3648.91 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 10:10:05] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:10:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:10:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:10:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:10:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:10:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:10:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:10:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:10:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:10:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:10:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:10:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:10:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:10:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:10:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:10:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:10:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:10:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:09] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:09] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:09] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:09] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3790299) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3790299) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.48s/it]
(EngineCore_DP0 pid=3790299) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.48s/it]
(EngineCore_DP0 pid=3790299) 
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3790299) 2026-01-28 10:10:32,113 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3790299) 2026-01-28 10:10:32,129 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3306.41it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:08<18:00,  8.51s/it, est. speed input: 1.88 toks/s, output: 30.10 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:08<00:17,  5.36it/s, est. speed input: 60.99 toks/s, output: 975.77 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:08<00:03, 14.73it/s, est. speed input: 136.52 toks/s, output: 2184.28 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:08<00:00, 24.54it/s, est. speed input: 194.12 toks/s, output: 3105.91 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 24.54it/s, est. speed input: 229.08 toks/s, output: 3665.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.32it/s, est. speed input: 229.08 toks/s, output: 3665.30 toks/s]
[rank0]:[W128 10:10:41.971377723 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.6s

测试结果:
  Requests/s:   14.25
  Tokens/s:     3876.97
  Total Reqs:   128
  Elapsed:      8.98s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      3648.91

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:10:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:10:47 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3791075) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3791075) WARNING 01-28 10:11:15 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.44 requests/s, 5014.59 total tokens/s, 4719.62 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 10:10:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:10:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:10:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:10:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:10:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:10:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:10:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:10:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:10:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:10:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:10:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:10:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:10:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:10:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:10:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:10:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:10:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:10:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3791075) [2026-01-28 10:10:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3791075) [2026-01-28 10:10:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3791075) [2026-01-28 10:10:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3791075) [2026-01-28 10:10:51] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3791075) [2026-01-28 10:10:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3791075) [2026-01-28 10:10:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3791075) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3791075) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.37s/it]
(EngineCore_DP0 pid=3791075) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.37s/it]
(EngineCore_DP0 pid=3791075) 
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3791075) 2026-01-28 10:11:14,430 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3791075) 2026-01-28 10:11:14,445 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13724.92it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:12<51:11, 12.05s/it, est. speed input: 1.33 toks/s, output: 21.25 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:12<01:55,  2.07it/s, est. speed input: 23.64 toks/s, output: 378.23 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:12<00:30,  6.91it/s, est. speed input: 62.35 toks/s, output: 997.61 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:12<00:14, 12.78it/s, est. speed input: 96.31 toks/s, output: 1541.02 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:12<00:08, 19.36it/s, est. speed input: 124.52 toks/s, output: 1992.30 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:12<00:05, 26.99it/s, est. speed input: 148.75 toks/s, output: 2379.98 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:12<00:03, 36.26it/s, est. speed input: 171.34 toks/s, output: 2741.48 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:12<00:01, 49.23it/s, est. speed input: 196.97 toks/s, output: 3151.46 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:13<00:01, 60.74it/s, est. speed input: 218.25 toks/s, output: 3491.99 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:13<00:00, 74.14it/s, est. speed input: 238.38 toks/s, output: 3814.08 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:13<00:00, 82.36it/s, est. speed input: 257.18 toks/s, output: 4114.86 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:13<00:00, 86.43it/s, est. speed input: 273.12 toks/s, output: 4369.97 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:13<00:00, 86.29it/s, est. speed input: 286.29 toks/s, output: 4580.56 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 86.29it/s, est. speed input: 295.39 toks/s, output: 4726.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 18.46it/s, est. speed input: 295.39 toks/s, output: 4726.27 toks/s]
[rank0]:[W128 10:11:29.149188521 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.1s

测试结果:
  Requests/s:   18.44
  Tokens/s:     5014.59
  Total Reqs:   256
  Elapsed:      13.89s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      4719.62

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:11:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:11:34 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3791939) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3791939) WARNING 01-28 10:12:02 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.67 requests/s, 5078.57 total tokens/s, 4779.83 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 10:11:34] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:11:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:11:34] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:11:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:34] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:34] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:11:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:11:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:11:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:11:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:11:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:11:38] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:11:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:11:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:11:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:11:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:11:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:11:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:11:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:11:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3791939) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3791939) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.55s/it]
(EngineCore_DP0 pid=3791939) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.55s/it]
(EngineCore_DP0 pid=3791939) 
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3791939) 2026-01-28 10:12:01,797 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3791939) 2026-01-28 10:12:01,813 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 14502.77it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:20<2:52:45, 20.29s/it, est. speed input: 0.79 toks/s, output: 12.62 toks/s]
Processed prompts:   1%|          | 3/512 [00:20<45:19,  5.34s/it, est. speed input: 2.34 toks/s, output: 37.44 toks/s]  
Processed prompts:   7%|▋         | 34/512 [00:20<02:29,  3.19it/s, est. speed input: 26.25 toks/s, output: 420.04 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:20<01:03,  7.02it/s, est. speed input: 48.18 toks/s, output: 770.96 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:21<00:35, 11.98it/s, est. speed input: 68.98 toks/s, output: 1103.68 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:21<00:22, 17.92it/s, est. speed input: 87.85 toks/s, output: 1405.55 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:21<00:10, 32.57it/s, est. speed input: 122.02 toks/s, output: 1952.37 toks/s]
Processed prompts:  40%|████      | 205/512 [00:21<00:06, 48.05it/s, est. speed input: 151.24 toks/s, output: 2419.85 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:21<00:04, 64.67it/s, est. speed input: 177.19 toks/s, output: 2835.01 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:22<00:03, 79.57it/s, est. speed input: 198.43 toks/s, output: 3174.88 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:22<00:02, 97.46it/s, est. speed input: 218.32 toks/s, output: 3493.16 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:22<00:01, 110.31it/s, est. speed input: 234.93 toks/s, output: 3758.91 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:22<00:01, 122.36it/s, est. speed input: 248.80 toks/s, output: 3980.73 toks/s]
Processed prompts:  72%|███████▏  | 368/512 [00:22<00:01, 134.67it/s, est. speed input: 261.91 toks/s, output: 4190.56 toks/s]
Processed prompts:  76%|███████▌  | 388/512 [00:22<00:00, 145.72it/s, est. speed input: 274.87 toks/s, output: 4397.89 toks/s]
Processed prompts:  80%|███████▉  | 408/512 [00:22<00:00, 141.69it/s, est. speed input: 287.11 toks/s, output: 4593.69 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:22<00:00, 145.19it/s, est. speed input: 298.26 toks/s, output: 4772.21 toks/s]
Processed prompts:  87%|████████▋ | 444/512 [00:22<00:00, 141.30it/s, est. speed input: 309.02 toks/s, output: 4944.26 toks/s]
Processed prompts:  90%|████████▉ | 460/512 [00:23<00:00, 129.88it/s, est. speed input: 318.04 toks/s, output: 5088.68 toks/s]
Processed prompts:  93%|█████████▎| 475/512 [00:23<00:00, 117.69it/s, est. speed input: 326.12 toks/s, output: 5217.93 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [00:23<00:00, 96.44it/s, est. speed input: 332.02 toks/s, output: 5312.34 toks/s] 
Processed prompts:  97%|█████████▋| 499/512 [00:27<00:01, 11.66it/s, est. speed input: 292.11 toks/s, output: 4673.73 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:27<00:00, 11.66it/s, est. speed input: 299.14 toks/s, output: 4786.18 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:27<00:00, 18.70it/s, est. speed input: 299.14 toks/s, output: 4786.18 toks/s]
[rank0]:[W128 10:12:30.069320132 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 61.0s

测试结果:
  Requests/s:   18.67
  Tokens/s:     5078.57
  Total Reqs:   512
  Elapsed:      27.42s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      4779.83


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,9.2519,2516.5087,6.9175
128,16,128,128,256,256,14.2536,3876.9674,8.9802
256,16,256,256,256,256,18.4360,5014.5945,13.8859
512,16,512,512,256,256,18.6712,5078.5726,27.4219

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:12:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:12:35 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3792967) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3792967) WARNING 01-28 10:13:04 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 8.99 requests/s, 2446.62 total tokens/s, 2302.70 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 10:12:35] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:12:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:12:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:12:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:12:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:12:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:12:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:12:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:12:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:12:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:12:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:12:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:12:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:12:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:12:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:12:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:12:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:12:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:12:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:12:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:12:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:12:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:12:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:12:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:12:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:12:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:12:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:12:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:40] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:40] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3792967) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3792967) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.43s/it]
(EngineCore_DP0 pid=3792967) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.43s/it]
(EngineCore_DP0 pid=3792967) 
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3792967) [2026-01-28 10:12:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3792967) 2026-01-28 10:13:03,971 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3792967) 2026-01-28 10:13:03,991 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 11753.38it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:06<07:15,  6.91s/it, est. speed input: 2.31 toks/s, output: 37.02 toks/s]
Processed prompts:  56%|█████▋    | 36/64 [00:07<00:03,  7.18it/s, est. speed input: 81.72 toks/s, output: 1307.48 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:07<00:00,  7.18it/s, est. speed input: 144.04 toks/s, output: 2304.71 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:07<00:00,  9.00it/s, est. speed input: 144.04 toks/s, output: 2304.71 toks/s]
[rank0]:[W128 10:13:11.859812600 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.7s

测试结果:
  Requests/s:   8.99
  Tokens/s:     2446.62
  Total Reqs:   64
  Elapsed:      7.12s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      2302.70

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:13:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:13:17 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3793730) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3793730) WARNING 01-28 10:13:46 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.07 requests/s, 3825.83 total tokens/s, 3600.78 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 10:13:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:13:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:13:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:13:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:13:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:13:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:13:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:13:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:13:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:13:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:13:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:13:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:13:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:13:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:13:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:13:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:13:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:13:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:13:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:13:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:13:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:13:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:13:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:13:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:13:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:13:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:13:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:13:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:21] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:21] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:21] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:21] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:21] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3793730) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3793730) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.30s/it]
(EngineCore_DP0 pid=3793730) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.30s/it]
(EngineCore_DP0 pid=3793730) 
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3793730) [2026-01-28 10:13:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3793730) 2026-01-28 10:13:45,494 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3793730) 2026-01-28 10:13:45,511 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 12871.52it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:08<18:17,  8.64s/it, est. speed input: 1.85 toks/s, output: 29.64 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:08<00:38,  2.88it/s, est. speed input: 32.95 toks/s, output: 527.24 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:08<00:05, 12.69it/s, est. speed input: 111.70 toks/s, output: 1787.13 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:08<00:02, 20.06it/s, est. speed input: 154.96 toks/s, output: 2479.35 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:09<00:00, 36.14it/s, est. speed input: 223.56 toks/s, output: 3576.88 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 36.14it/s, est. speed input: 225.31 toks/s, output: 3605.01 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 14.08it/s, est. speed input: 225.31 toks/s, output: 3605.01 toks/s]
[rank0]:[W128 10:13:55.497029013 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.6s

测试结果:
  Requests/s:   14.07
  Tokens/s:     3825.83
  Total Reqs:   128
  Elapsed:      9.10s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      3600.78

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:14:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:14:01 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3794528) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3794528) WARNING 01-28 10:14:29 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 17.74 requests/s, 4826.51 total tokens/s, 4542.60 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 10:14:01] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:14:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:14:01] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:14:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:01] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:01] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:14:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:14:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:14:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:14:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:14:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:14:04] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:14:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:14:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:14:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:14:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:14:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:14:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:14:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:14:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:05] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3794528) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3794528) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.59s/it]
(EngineCore_DP0 pid=3794528) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.59s/it]
(EngineCore_DP0 pid=3794528) 
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3794528) [2026-01-28 10:14:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3794528) 2026-01-28 10:14:29,141 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3794528) 2026-01-28 10:14:29,160 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 4331.39it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:12<53:04, 12.49s/it, est. speed input: 1.28 toks/s, output: 20.50 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:12<01:59,  1.99it/s, est. speed input: 22.80 toks/s, output: 364.88 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:12<00:31,  6.68it/s, est. speed input: 60.18 toks/s, output: 962.93 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:12<00:14, 12.33it/s, est. speed input: 92.94 toks/s, output: 1487.08 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:13<00:08, 18.70it/s, est. speed input: 120.19 toks/s, output: 1923.09 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:13<00:05, 26.03it/s, est. speed input: 143.53 toks/s, output: 2296.54 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:13<00:03, 34.97it/s, est. speed input: 165.34 toks/s, output: 2645.39 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:13<00:02, 47.54it/s, est. speed input: 190.09 toks/s, output: 3041.36 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:13<00:01, 58.73it/s, est. speed input: 210.65 toks/s, output: 3370.40 toks/s]
Processed prompts:  76%|███████▌  | 195/256 [00:13<00:00, 70.60it/s, est. speed input: 228.92 toks/s, output: 3662.65 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:13<00:00, 80.97it/s, est. speed input: 246.52 toks/s, output: 3944.34 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:13<00:00, 84.98it/s, est. speed input: 262.02 toks/s, output: 4192.24 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:14<00:00, 85.88it/s, est. speed input: 274.98 toks/s, output: 4399.74 toks/s]
Processed prompts: 100%|█████████▉| 255/256 [00:14<00:00, 73.00it/s, est. speed input: 284.55 toks/s, output: 4552.87 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:14<00:00, 73.00it/s, est. speed input: 285.10 toks/s, output: 4561.67 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:14<00:00, 17.82it/s, est. speed input: 285.10 toks/s, output: 4561.67 toks/s]
[rank0]:[W128 10:14:44.460257991 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.0s

测试结果:
  Requests/s:   17.74
  Tokens/s:     4826.51
  Total Reqs:   256
  Elapsed:      14.43s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      4542.60

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:14:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:14:50 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3795402) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3795402) WARNING 01-28 10:15:18 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.18 requests/s, 4945.52 total tokens/s, 4654.60 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 10:14:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:14:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:14:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:14:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:14:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:14:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:14:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:14:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:14:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:14:53] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:14:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:14:53] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:14:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:53] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:53] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:14:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:14:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:14:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:14:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:14:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:14:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3795402) [2026-01-28 10:14:54] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3795402) [2026-01-28 10:14:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3795402) [2026-01-28 10:14:54] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3795402) [2026-01-28 10:14:54] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3795402) [2026-01-28 10:14:54] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3795402) [2026-01-28 10:14:54] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3795402) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3795402) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.44s/it]
(EngineCore_DP0 pid=3795402) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:17<00:00, 17.44s/it]
(EngineCore_DP0 pid=3795402) 
(EngineCore_DP0 pid=3795402) [2026-01-28 10:15:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3795402) [2026-01-28 10:15:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3795402) [2026-01-28 10:15:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3795402) [2026-01-28 10:15:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3795402) [2026-01-28 10:15:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3795402) [2026-01-28 10:15:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3795402) [2026-01-28 10:15:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3795402) [2026-01-28 10:15:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3795402) 2026-01-28 10:15:18,231 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3795402) 2026-01-28 10:15:18,247 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 14183.52it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:20<2:55:01, 20.55s/it, est. speed input: 0.78 toks/s, output: 12.46 toks/s]
Processed prompts:   1%|          | 6/512 [00:20<21:37,  2.56s/it, est. speed input: 4.62 toks/s, output: 73.92 toks/s]  
Processed prompts:   7%|▋         | 37/512 [00:20<02:21,  3.35it/s, est. speed input: 28.20 toks/s, output: 451.19 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:21<01:02,  7.12it/s, est. speed input: 49.83 toks/s, output: 797.27 toks/s]
Processed prompts:  18%|█▊        | 93/512 [00:21<00:35, 11.84it/s, est. speed input: 69.60 toks/s, output: 1113.52 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:21<00:22, 17.73it/s, est. speed input: 88.23 toks/s, output: 1411.70 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:21<00:10, 32.11it/s, est. speed input: 121.91 toks/s, output: 1950.56 toks/s]
Processed prompts:  40%|████      | 207/512 [00:21<00:06, 47.52it/s, est. speed input: 150.78 toks/s, output: 2412.55 toks/s]
Processed prompts:  47%|████▋     | 243/512 [00:22<00:04, 63.19it/s, est. speed input: 175.63 toks/s, output: 2810.08 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:22<00:03, 78.09it/s, est. speed input: 196.61 toks/s, output: 3145.83 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:22<00:02, 94.35it/s, est. speed input: 215.47 toks/s, output: 3447.57 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:22<00:01, 107.11it/s, est. speed input: 231.85 toks/s, output: 3709.65 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:22<00:01, 118.56it/s, est. speed input: 245.49 toks/s, output: 3927.85 toks/s]
Processed prompts:  72%|███████▏  | 368/512 [00:22<00:01, 129.77it/s, est. speed input: 258.37 toks/s, output: 4133.84 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:22<00:00, 134.53it/s, est. speed input: 273.27 toks/s, output: 4372.25 toks/s]
Processed prompts:  80%|████████  | 412/512 [00:23<00:00, 139.84it/s, est. speed input: 285.63 toks/s, output: 4570.07 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:23<00:00, 142.82it/s, est. speed input: 296.59 toks/s, output: 4745.48 toks/s]
Processed prompts:  87%|████████▋ | 447/512 [00:23<00:00, 144.42it/s, est. speed input: 306.81 toks/s, output: 4909.01 toks/s]
Processed prompts:  91%|█████████ | 464/512 [00:23<00:00, 132.62it/s, est. speed input: 316.35 toks/s, output: 5061.55 toks/s]
Processed prompts:  94%|█████████▎| 479/512 [00:23<00:00, 112.30it/s, est. speed input: 323.88 toks/s, output: 5182.13 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [00:23<00:00, 83.49it/s, est. speed input: 328.72 toks/s, output: 5259.49 toks/s] 
Processed prompts:  98%|█████████▊| 503/512 [00:28<00:00, 10.95it/s, est. speed input: 287.37 toks/s, output: 4597.94 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:28<00:00, 13.31it/s, est. speed input: 291.30 toks/s, output: 4660.75 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:28<00:00, 13.31it/s, est. speed input: 291.30 toks/s, output: 4660.75 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:28<00:00, 18.21it/s, est. speed input: 291.30 toks/s, output: 4660.75 toks/s]
[rank0]:[W128 10:15:47.253616991 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 62.9s

测试结果:
  Requests/s:   18.18
  Tokens/s:     4945.52
  Total Reqs:   512
  Elapsed:      28.16s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      4654.60


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,8.9949,2446.6170,7.1151
128,16,128,128,256,256,14.0656,3825.8332,9.1002
256,16,256,256,256,256,17.7445,4826.5089,14.4270
512,16,512,512,256,256,18.1820,4945.5171,28.1596

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 20 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_100043.log
[SUCCESS] bitnet1.58-2b-fp8 Decode 完成 (909.2s)

[INFO] Decode 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 5: 完整 Decode Benchmark - SUCCESS
Duration: 1825.7 seconds (30.4 minutes)
----------------------------------------------------------------------


======================================================================
TASK 6: Kernel: cuBLASLt
Started: 2026-01-28 10:15:50
======================================================================


------------------------------------------------------------
  cuBLASLt Kernel: BitNet-2B [fp8e4m3]
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype fp8e4m3 --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cublaslt --model BitNet-2B

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
============================================================
cuBLASLt Dense GEMM 算法搜索
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuBLASLt 可用

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 8, 有效: 8
    NK 2/4: (2560, 2560)
      → 算法数: 8, 有效: 8
    NK 3/4: (13824, 2560)
      → 算法数: 8, 有效: 8
    NK 4/4: (2560, 6912)
      → 算法数: 8, 有效: 8

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8
============================================================
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['fp8e4m3']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[SUCCESS] cuBLASLt Kernel [fp8e4m3] 测试完成 (46.4s)

------------------------------------------------------------
  cuBLASLt Kernel: BitNet-2B [int8]
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype int8 --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cublaslt --model BitNet-2B

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
============================================================
cuBLASLt Dense GEMM 算法搜索
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuBLASLt 可用

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 3, 有效: 3
    NK 2/4: (2560, 2560)
      → 算法数: 3, 有效: 3
    NK 3/4: (13824, 2560)
      → 算法数: 3, 有效: 3
    NK 4/4: (2560, 6912)
      → 算法数: 3, 有效: 3

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8
============================================================
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['int8']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[SUCCESS] cuBLASLt Kernel [int8] 测试完成 (17.2s)

[INFO] cuBLASLt Kernel 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 6: Kernel: cuBLASLt - SUCCESS
Duration: 63.6 seconds (1.1 minutes)
----------------------------------------------------------------------


======================================================================
TASK 7: Kernel: cuSPARSELt 高稀疏 (2_4~2_10)
Started: 2026-01-28 10:16:53
======================================================================


------------------------------------------------------------
  cuSPARSELt 高稀疏 Kernel: BitNet-2B [fp8e4m3]
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype fp8e4m3 --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cusparselt --model BitNet-2B --sparsity 2_4,2_6,2_8,2_10

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3840, 2560), (2560, 2560), (13824, 2560), (2560, 6912)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_4
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 4, 有效: 14
    NK 2/4: (2560, 2560)
      → 算法数: 4, 有效: 17
    NK 3/4: (13824, 2560)
      → 算法数: 4, 有效: 15
    NK 4/4: (2560, 6912)
      → 算法数: 4, 有效: 14

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_4.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8
============================================================
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3840, 3424), (2560, 3424), (13824, 3424), (2560, 9216)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_6
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3424)
      → 算法数: 4, 有效: 16
    NK 2/4: (2560, 3424)
      → 算法数: 4, 有效: 20
    NK 3/4: (13824, 3424)
      → 算法数: 4, 有效: 13
    NK 4/4: (2560, 9216)
      → 算法数: 4, 有效: 17

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_6.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8
============================================================
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3840, 3840), (2560, 3840), (13824, 3840), (2560, 10368)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_8
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3840)
      → 算法数: 4, 有效: 13
    NK 2/4: (2560, 3840)
      → 算法数: 4, 有效: 13
    NK 3/4: (13824, 3840)
      → 算法数: 4, 有效: 14
    NK 4/4: (2560, 10368)
      → 算法数: 4, 有效: 14

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8
============================================================
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3840, 4096), (2560, 4096), (13824, 4096), (2560, 11072)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_10
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4096)
      → 算法数: 4, 有效: 16
    NK 2/4: (2560, 4096)
      → 算法数: 4, 有效: 14
    NK 3/4: (13824, 4096)
      → 算法数: 4, 有效: 14
    NK 4/4: (2560, 11072)
      → 算法数: 4, 有效: 14

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_10.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8
============================================================
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['fp8e4m3']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_4

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_6

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_8

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[SUCCESS] cuSPARSELt 高稀疏 [fp8e4m3] 测试完成 (227.4s)

------------------------------------------------------------
  cuSPARSELt 高稀疏 Kernel: BitNet-2B [int8]
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype int8 --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cusparselt --model BitNet-2B --sparsity 2_4,2_6,2_8,2_10

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3840, 2560), (2560, 2560), (13824, 2560), (2560, 6912)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_4
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 2560)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 2560)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 6912)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_4.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8
============================================================
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3840, 3424), (2560, 3424), (13824, 3424), (2560, 9216)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_6
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3424)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 3424)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 3424)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 9216)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_6.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8
============================================================
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3840, 3840), (2560, 3840), (13824, 3840), (2560, 10368)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_8
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3840)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 3840)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 3840)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 10368)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8
============================================================
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3840, 4096), (2560, 4096), (13824, 4096), (2560, 11072)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_10
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4096)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 4096)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 4096)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 11072)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_10.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8
============================================================
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['int8']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_4

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_6

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_8

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[SUCCESS] cuSPARSELt 高稀疏 [int8] 测试完成 (54.6s)

[INFO] cuSPARSELt 高稀疏 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 7: Kernel: cuSPARSELt 高稀疏 (2_4~2_10) - SUCCESS
Duration: 282.0 seconds (4.7 minutes)
----------------------------------------------------------------------


======================================================================
TASK 8: Kernel: cuSPARSELt 低稀疏 (2_12~2_inf)
Started: 2026-01-28 10:21:35
======================================================================


------------------------------------------------------------
  cuSPARSELt 低稀疏 Kernel: BitNet-2B [fp8e4m3]
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype fp8e4m3 --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cusparselt --model BitNet-2B --sparsity 2_12,2_14,2_16,2_inf

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3840, 4288), (2560, 4288), (13824, 4288), (2560, 11520)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_12
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4288)
      → 算法数: 4, 有效: 18
    NK 2/4: (2560, 4288)
      → 算法数: 4, 有效: 23
    NK 3/4: (13824, 4288)
      → 算法数: 4, 有效: 16
    NK 4/4: (2560, 11520)
      → 算法数: 4, 有效: 16

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_12.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8
============================================================
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3840, 4416), (2560, 4416), (13824, 4416), (2560, 11872)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_14
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4416)
      → 算法数: 4, 有效: 18
    NK 2/4: (2560, 4416)
      → 算法数: 4, 有效: 14
    NK 3/4: (13824, 4416)
      → 算法数: 4, 有效: 12
    NK 4/4: (2560, 11872)
      → 算法数: 4, 有效: 18

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_14.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8
============================================================
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3840, 4480), (2560, 4480), (13824, 4480), (2560, 12096)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_16
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4480)
      → 算法数: 4, 有效: 14
    NK 2/4: (2560, 4480)
      → 算法数: 4, 有效: 13
    NK 3/4: (13824, 4480)
      → 算法数: 4, 有效: 17
    NK 4/4: (2560, 12096)
      → 算法数: 4, 有效: 14

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_16.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8
============================================================
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3840, 5120), (2560, 5120), (13824, 5120), (2560, 13824)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_inf
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 5120)
      → 算法数: 4, 有效: 17
    NK 2/4: (2560, 5120)
      → 算法数: 4, 有效: 16
    NK 3/4: (13824, 5120)
      → 算法数: 4, 有效: 14
    NK 4/4: (2560, 13824)
      → 算法数: 4, 有效: 16

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8/alg_search_BitNet-2B-INT8_2_inf.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/FP8
============================================================
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['fp8e4m3']
Backend: cusparselt
Sparsity: ['2_12', '2_14', '2_16', '2_inf']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_12

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_14

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_16

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_inf

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[SUCCESS] cuSPARSELt 低稀疏 [fp8e4m3] 测试完成 (318.5s)

------------------------------------------------------------
  cuSPARSELt 低稀疏 Kernel: BitNet-2B [int8]
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype int8 --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cusparselt --model BitNet-2B --sparsity 2_12,2_14,2_16,2_inf

/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3840, 4288), (2560, 4288), (13824, 4288), (2560, 11520)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_12
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4288)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 4288)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 4288)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 11520)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_12.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8
============================================================
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3840, 4416), (2560, 4416), (13824, 4416), (2560, 11872)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_14
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4416)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 4416)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 4416)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 11872)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_14.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8
============================================================
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3840, 4480), (2560, 4480), (13824, 4480), (2560, 12096)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_16
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4480)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 4480)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 4480)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 12096)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_16.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8
============================================================
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3840, 5120), (2560, 5120), (13824, 5120), (2560, 13824)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_inf
Segment-K 测试: 开启
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 是

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 5120)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 5120)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 5120)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 13824)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8/alg_search_BitNet-2B-INT8_2_inf.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/GB10_cc121_py312_cu129_aarch64/INT8
============================================================
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GB10 (cc121)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['int8']
Backend: cusparselt
Sparsity: ['2_12', '2_14', '2_16', '2_inf']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_12

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_14

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_16

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_inf

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[SUCCESS] cuSPARSELt 低稀疏 [int8] 测试完成 (60.9s)

[INFO] cuSPARSELt 低稀疏 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 8: Kernel: cuSPARSELt 低稀疏 (2_12~2_inf) - SUCCESS
Duration: 379.5 seconds (6.3 minutes)
----------------------------------------------------------------------



============================================================
  最终总结
============================================================


  Task 1: 基础模型准备 (下载 + 量化) - SUCCESS (170.2s)
  Task 2: SlideSparse 转换 (prune + slide) - SUCCESS (217.1s)
  Task 3: 离线调优 (粗调优 + 细调优) - SUCCESS (1198.1s)
  Task 4: 完整 Prefill Benchmark - SUCCESS (8043.7s)
  Task 5: 完整 Decode Benchmark - SUCCESS (1825.7s)
  Task 6: Kernel: cuBLASLt - SUCCESS (63.6s)
  Task 7: Kernel: cuSPARSELt 高稀疏 (2_4~2_10) - SUCCESS (282.0s)
  Task 8: Kernel: cuSPARSELt 低稀疏 (2_12~2_inf) - SUCCESS (379.5s)

  总计: 8 成功, 0 失败, 0 跳过
  总耗时: 12179.9 秒 (3.38 小时)

[INFO] 日志文件: /root/vllmbench/slidesparse/tools/bitnet_bench_20260128_070455.log
[INFO] 状态文件: /root/vllmbench/slidesparse/tools/bitnet_bench_20260128_070455_status.json
